{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import *\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 2 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import Metric\n",
    "class RSquare(Metric):\n",
    "    \"\"\"Compute R^2 score.\n",
    "     This is also called as coefficient of determination.\n",
    "     It tells how close are data to the fitted regression line.\n",
    "     - Highest score can be 1.0 and it indicates that the predictors\n",
    "       perfectly accounts for variation in the target.\n",
    "     - Score 0.0 indicates that the predictors do not\n",
    "       account for variation in the target.\n",
    "     - It can also be negative if the model is worse.\n",
    "     Usage:\n",
    "     ```python\n",
    "     actuals = tf.constant([1, 4, 3], dtype=tf.float32)\n",
    "     preds = tf.constant([2, 4, 4], dtype=tf.float32)\n",
    "     result = tf.keras.metrics.RSquare()\n",
    "     result.update_state(actuals, preds)\n",
    "     print('R^2 score is: ', r1.result().numpy()) # 0.57142866\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name='r_square', dtype=tf.float32):\n",
    "        super(RSquare, self).__init__(name=name, dtype=dtype)\n",
    "        self.squared_sum = self.add_weight(\"squared_sum\", initializer=\"zeros\")\n",
    "        self.sum = self.add_weight(\"sum\", initializer=\"zeros\")\n",
    "        self.res = self.add_weight(\"residual\", initializer=\"zeros\")\n",
    "        self.count = self.add_weight(\"count\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred):\n",
    "        y_true = tf.convert_to_tensor(y_true, tf.float32)\n",
    "        y_pred = tf.convert_to_tensor(y_pred, tf.float32)\n",
    "        self.squared_sum.assign_add(tf.reduce_sum(y_true**2))\n",
    "        self.sum.assign_add(tf.reduce_sum(y_true))\n",
    "        self.res.assign_add(\n",
    "            tf.reduce_sum(tf.square(tf.subtract(y_true, y_pred))))\n",
    "        self.count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        mean = self.sum / self.count\n",
    "        total = self.squared_sum - 2 * self.sum * mean + self.count * mean**2\n",
    "        return 1 - (self.res / total)\n",
    "\n",
    "    def reset_states(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.squared_sum.assign(0.0)\n",
    "        self.sum.assign(0.0)\n",
    "        self.res.assign(0.0)\n",
    "        self.count.assign(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = ((8/2.54), (6/2.54))\n",
    "plt.rcParams[\"font.family\"] = \"Arial\"\n",
    "plt.rcParams[\"mathtext.default\"] = \"rm\"\n",
    "plt.rcParams.update({'font.size': 11})\n",
    "MARKER_SIZE = 15\n",
    "cmap_m = [\"#f4a6ad\", \"#f6957e\", \"#fccfa2\", \"#8de7be\", \"#86d6f2\", \"#24a9e4\", \"#b586e0\", \"#d7f293\"]\n",
    "cmap = [\"#e94d5b\", \"#ef4d28\", \"#f9a54f\", \"#25b575\", \"#1bb1e7\", \"#1477a2\", \"#a662e5\", \"#c2f442\"]\n",
    "\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "# plt.rcParams['axes.edgecolor'] = \n",
    "plt.rcParams['axes.linewidth'] = 1\n",
    "plt.rcParams['lines.linewidth'] = 1.5\n",
    "plt.rcParams['xtick.major.width'] = 1\n",
    "plt.rcParams['xtick.minor.width'] = 1\n",
    "plt.rcParams['ytick.major.width'] = 1\n",
    "plt.rcParams['ytick.minor.width'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMBlock(layers.Layer):\n",
    "    def __init__(self, num_hidden):\n",
    "        super(BiLSTMBlock, self).__init__()\n",
    "        self.num_hidden = num_hidden\n",
    "        \n",
    "        self.bilstm = layers.Bidirectional(layers.LSTM(self.num_hidden, return_sequences=True))\n",
    "        self.layernorm = layers.LayerNormalization()\n",
    "        \n",
    "    def call(self, inp):\n",
    "        \n",
    "        inp = self.bilstm(inp)\n",
    "        inp = self.layernorm(inp)\n",
    "        \n",
    "        return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(Model):\n",
    "    def __init__(self, out_len):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        \n",
    "        self.out_len = out_len\n",
    "        \n",
    "        self.bilstm_block1 = BiLSTMBlock(64)\n",
    "        self.bilstm_block2 = BiLSTMBlock(64)\n",
    "        self.ffnn = layers.Dense(self.out_len)\n",
    "        \n",
    "    def call(self, inp):\n",
    "        output = self.bilstm_block1(inp)\n",
    "        output = self.bilstm_block2(inp)\n",
    "        output = self.ffnn(inp)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss inputs should be masked.\n",
    "loss_object = tf.keras.losses.MeanSquaredError()\n",
    "def loss_function(model, inp, tar): #RNN specialized\n",
    "    \n",
    "    masked_real = tar * (1 - inp[..., 5:10])\n",
    "    masked_pred = model(inp) * (1 - inp[..., 5:10])\n",
    "    \n",
    "    return loss_object(masked_real, masked_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for loss rate 0.10 start.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['bi_lstm/bi_lstm_block/bidirectional/forward_lstm/lstm_cell_1/kernel:0', 'bi_lstm/bi_lstm_block/bidirectional/forward_lstm/lstm_cell_1/recurrent_kernel:0', 'bi_lstm/bi_lstm_block/bidirectional/forward_lstm/lstm_cell_1/bias:0', 'bi_lstm/bi_lstm_block/bidirectional/backward_lstm/lstm_cell_2/kernel:0', 'bi_lstm/bi_lstm_block/bidirectional/backward_lstm/lstm_cell_2/recurrent_kernel:0', 'bi_lstm/bi_lstm_block/bidirectional/backward_lstm/lstm_cell_2/bias:0', 'bi_lstm/bi_lstm_block/layer_normalization/gamma:0', 'bi_lstm/bi_lstm_block/layer_normalization/beta:0', 'bi_lstm/bi_lstm_block_1/bidirectional_1/forward_lstm_1/lstm_cell_4/kernel:0', 'bi_lstm/bi_lstm_block_1/bidirectional_1/forward_lstm_1/lstm_cell_4/recurrent_kernel:0', 'bi_lstm/bi_lstm_block_1/bidirectional_1/forward_lstm_1/lstm_cell_4/bias:0', 'bi_lstm/bi_lstm_block_1/bidirectional_1/backward_lstm_1/lstm_cell_5/kernel:0', 'bi_lstm/bi_lstm_block_1/bidirectional_1/backward_lstm_1/lstm_cell_5/recurrent_kernel:0', 'bi_lstm/bi_lstm_block_1/bidirectional_1/backward_lstm_1/lstm_cell_5/bias:0', 'bi_lstm/bi_lstm_block_1/layer_normalization_1/gamma:0', 'bi_lstm/bi_lstm_block_1/layer_normalization_1/beta:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['bi_lstm/bi_lstm_block/bidirectional/forward_lstm/lstm_cell_1/kernel:0', 'bi_lstm/bi_lstm_block/bidirectional/forward_lstm/lstm_cell_1/recurrent_kernel:0', 'bi_lstm/bi_lstm_block/bidirectional/forward_lstm/lstm_cell_1/bias:0', 'bi_lstm/bi_lstm_block/bidirectional/backward_lstm/lstm_cell_2/kernel:0', 'bi_lstm/bi_lstm_block/bidirectional/backward_lstm/lstm_cell_2/recurrent_kernel:0', 'bi_lstm/bi_lstm_block/bidirectional/backward_lstm/lstm_cell_2/bias:0', 'bi_lstm/bi_lstm_block/layer_normalization/gamma:0', 'bi_lstm/bi_lstm_block/layer_normalization/beta:0', 'bi_lstm/bi_lstm_block_1/bidirectional_1/forward_lstm_1/lstm_cell_4/kernel:0', 'bi_lstm/bi_lstm_block_1/bidirectional_1/forward_lstm_1/lstm_cell_4/recurrent_kernel:0', 'bi_lstm/bi_lstm_block_1/bidirectional_1/forward_lstm_1/lstm_cell_4/bias:0', 'bi_lstm/bi_lstm_block_1/bidirectional_1/backward_lstm_1/lstm_cell_5/kernel:0', 'bi_lstm/bi_lstm_block_1/bidirectional_1/backward_lstm_1/lstm_cell_5/recurrent_kernel:0', 'bi_lstm/bi_lstm_block_1/bidirectional_1/backward_lstm_1/lstm_cell_5/bias:0', 'bi_lstm/bi_lstm_block_1/layer_normalization_1/gamma:0', 'bi_lstm/bi_lstm_block_1/layer_normalization_1/beta:0'] when minimizing the loss.\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.10p/ckpt-1\n",
      "Epoch 0 batch 0 train loss: 0.3591 test loss: 0.3226\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.10p/ckpt-2\n",
      "Epoch 0 batch 100 train loss: 0.1431 test loss: 0.1631\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.10p/ckpt-3\n",
      "Epoch 0 batch 200 train loss: 0.0766 test loss: 0.0939\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.10p/ckpt-4\n",
      "Epoch 0 batch 300 train loss: 0.0527 test loss: 0.0556\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.10p/ckpt-5\n",
      "Epoch 0 batch 400 train loss: 0.0386 test loss: 0.0338\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.10p/ckpt-6\n",
      "Epoch 0 batch 500 train loss: 0.0192 test loss: 0.0217\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.10p/ckpt-7\n",
      "Epoch 0 batch 600 train loss: 0.0143 test loss: 0.0149\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.10p/ckpt-8\n",
      "Epoch 0 batch 700 train loss: 0.0131 test loss: 0.0114\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.10p/ckpt-9\n",
      "Epoch 0 batch 800 train loss: 0.0085 test loss: 0.0093\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.10p/ckpt-10\n",
      "Epoch 0 batch 900 train loss: 0.0097 test loss: 0.0081\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.10p/ckpt-11\n",
      "Epoch 0 batch 1000 train loss: 0.0089 test loss: 0.0078\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.10p/ckpt-12\n",
      "Epoch 0 batch 1100 train loss: 0.0090 test loss: 0.0076\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.10p/ckpt-13\n",
      "Epoch 0 batch 1200 train loss: 0.0110 test loss: 0.0071\n",
      "Epoch 0 batch 1300 train loss: 0.0087 test loss: 0.0074\n",
      "Epoch 0 batch 1400 train loss: 0.0098 test loss: 0.0074\n",
      "Epoch 0 batch 1500 train loss: 0.0059 test loss: 0.0075\n",
      "Epoch 0 batch 1600 train loss: 0.0082 test loss: 0.0072\n",
      "Epoch 0 batch 1700 train loss: 0.0068 test loss: 0.0074\n",
      "Epoch 0 batch 1800 train loss: 0.0092 test loss: 0.0072\n",
      "Epoch 0 batch 1900 train loss: 0.0091 test loss: 0.0074\n",
      "Epoch 0 batch 2000 train loss: 0.0098 test loss: 0.0073\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.10p/ckpt-14\n",
      "Epoch 0 batch 2100 train loss: 0.0072 test loss: 0.0069\n",
      "Epoch 0 batch 2200 train loss: 0.0069 test loss: 0.0072\n",
      "Epoch 0 batch 2300 train loss: 0.0085 test loss: 0.0073\n",
      "Epoch 0 batch 2400 train loss: 0.0082 test loss: 0.0072\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.10p/ckpt-15\n",
      "Epoch 0 batch 2500 train loss: 0.0076 test loss: 0.0068\n",
      "Epoch 0 batch 2600 train loss: 0.0101 test loss: 0.0070\n",
      "Epoch 0 batch 2700 train loss: 0.0083 test loss: 0.0068\n",
      "Epoch 0 batch 2800 train loss: 0.0108 test loss: 0.0069\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.10p/ckpt-16\n",
      "Epoch 0 batch 2900 train loss: 0.0077 test loss: 0.0065\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['bi_lstm/bi_lstm_block/bidirectional/forward_lstm/lstm_cell_1/kernel:0', 'bi_lstm/bi_lstm_block/bidirectional/forward_lstm/lstm_cell_1/recurrent_kernel:0', 'bi_lstm/bi_lstm_block/bidirectional/forward_lstm/lstm_cell_1/bias:0', 'bi_lstm/bi_lstm_block/bidirectional/backward_lstm/lstm_cell_2/kernel:0', 'bi_lstm/bi_lstm_block/bidirectional/backward_lstm/lstm_cell_2/recurrent_kernel:0', 'bi_lstm/bi_lstm_block/bidirectional/backward_lstm/lstm_cell_2/bias:0', 'bi_lstm/bi_lstm_block/layer_normalization/gamma:0', 'bi_lstm/bi_lstm_block/layer_normalization/beta:0', 'bi_lstm/bi_lstm_block_1/bidirectional_1/forward_lstm_1/lstm_cell_4/kernel:0', 'bi_lstm/bi_lstm_block_1/bidirectional_1/forward_lstm_1/lstm_cell_4/recurrent_kernel:0', 'bi_lstm/bi_lstm_block_1/bidirectional_1/forward_lstm_1/lstm_cell_4/bias:0', 'bi_lstm/bi_lstm_block_1/bidirectional_1/backward_lstm_1/lstm_cell_5/kernel:0', 'bi_lstm/bi_lstm_block_1/bidirectional_1/backward_lstm_1/lstm_cell_5/recurrent_kernel:0', 'bi_lstm/bi_lstm_block_1/bidirectional_1/backward_lstm_1/lstm_cell_5/bias:0', 'bi_lstm/bi_lstm_block_1/layer_normalization_1/gamma:0', 'bi_lstm/bi_lstm_block_1/layer_normalization_1/beta:0'] when minimizing the loss.\n",
      "Epoch 1 batch 0 train loss: 0.0069 test loss: 0.0070\n",
      "Epoch 1 batch 100 train loss: 0.0079 test loss: 0.0069\n",
      "Epoch 1 batch 200 train loss: 0.0080 test loss: 0.0069\n",
      "Epoch 1 batch 300 train loss: 0.0092 test loss: 0.0068\n",
      "Epoch 1 batch 400 train loss: 0.0064 test loss: 0.0066\n",
      "Epoch 1 batch 500 train loss: 0.0098 test loss: 0.0069\n",
      "Epoch 1 batch 600 train loss: 0.0079 test loss: 0.0068\n",
      "Epoch 1 batch 700 train loss: 0.0069 test loss: 0.0068\n",
      "Epoch 1 batch 800 train loss: 0.0067 test loss: 0.0072\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.10p/ckpt-17\n",
      "Epoch 1 batch 900 train loss: 0.0092 test loss: 0.0065\n",
      "Epoch 1 batch 1000 train loss: 0.0070 test loss: 0.0070\n",
      "Epoch 1 batch 1100 train loss: 0.0077 test loss: 0.0072\n",
      "Epoch 1 batch 1200 train loss: 0.0075 test loss: 0.0070\n",
      "Epoch 1 batch 1300 train loss: 0.0072 test loss: 0.0066\n",
      "Epoch 1 batch 1400 train loss: 0.0078 test loss: 0.0067\n",
      "Epoch 1 batch 1500 train loss: 0.0082 test loss: 0.0068\n",
      "Epoch 1 batch 1600 train loss: 0.0089 test loss: 0.0072\n",
      "Epoch 1 batch 1700 train loss: 0.0094 test loss: 0.0069\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.10p/ckpt-18\n",
      "Epoch 1 batch 1800 train loss: 0.0080 test loss: 0.0064\n",
      "Epoch 1 batch 1900 train loss: 0.0108 test loss: 0.0066\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.10p/ckpt-19\n",
      "Epoch 1 batch 2000 train loss: 0.0074 test loss: 0.0062\n",
      "Epoch 1 batch 2100 train loss: 0.0079 test loss: 0.0065\n",
      "Epoch 1 batch 2200 train loss: 0.0080 test loss: 0.0068\n",
      "Epoch 1 batch 2300 train loss: 0.0069 test loss: 0.0066\n",
      "Epoch 1 batch 2400 train loss: 0.0073 test loss: 0.0063\n",
      "Epoch 1 batch 2500 train loss: 0.0092 test loss: 0.0069\n",
      "Epoch 1 batch 2600 train loss: 0.0084 test loss: 0.0063\n",
      "Epoch 1 batch 2700 train loss: 0.0065 test loss: 0.0065\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.10p/ckpt-20\n",
      "Epoch 1 batch 2800 train loss: 0.0073 test loss: 0.0061\n",
      "Epoch 1 batch 2900 train loss: 0.0079 test loss: 0.0064\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.10p/ckpt-21\n",
      "Epoch 2 batch 0 train loss: 0.0077 test loss: 0.0059\n",
      "Epoch 2 batch 100 train loss: 0.0089 test loss: 0.0061\n",
      "Epoch 2 batch 200 train loss: 0.0079 test loss: 0.0066\n",
      "Epoch 2 batch 300 train loss: 0.0088 test loss: 0.0066\n",
      "Epoch 2 batch 400 train loss: 0.0075 test loss: 0.0066\n",
      "Epoch 2 batch 500 train loss: 0.0069 test loss: 0.0066\n",
      "Epoch 2 batch 600 train loss: 0.0079 test loss: 0.0066\n",
      "Epoch 2 batch 700 train loss: 0.0062 test loss: 0.0064\n",
      "Epoch 2 batch 800 train loss: 0.0071 test loss: 0.0065\n",
      "Epoch 2 batch 900 train loss: 0.0077 test loss: 0.0063\n",
      "Epoch 2 batch 1000 train loss: 0.0073 test loss: 0.0066\n",
      "Epoch 2 batch 1100 train loss: 0.0096 test loss: 0.0061\n",
      "Epoch 2 batch 1200 train loss: 0.0064 test loss: 0.0061\n",
      "Epoch 2 batch 1300 train loss: 0.0074 test loss: 0.0064\n",
      "Epoch 2 batch 1400 train loss: 0.0069 test loss: 0.0064\n",
      "Epoch 2 batch 1500 train loss: 0.0087 test loss: 0.0063\n",
      "Epoch 2 batch 1600 train loss: 0.0085 test loss: 0.0067\n",
      "Epoch 2 batch 1700 train loss: 0.0088 test loss: 0.0063\n",
      "Epoch 2 batch 1800 train loss: 0.0085 test loss: 0.0062\n",
      "Epoch 2 batch 1900 train loss: 0.0090 test loss: 0.0065\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.10p/ckpt-22\n",
      "Epoch 2 batch 2000 train loss: 0.0086 test loss: 0.0059\n",
      "Epoch 2 batch 2100 train loss: 0.0088 test loss: 0.0062\n",
      "Epoch 2 batch 2200 train loss: 0.0084 test loss: 0.0065\n",
      "Epoch 2 batch 2300 train loss: 0.0086 test loss: 0.0064\n",
      "Epoch 2 batch 2400 train loss: 0.0084 test loss: 0.0063\n",
      "Epoch 2 batch 2500 train loss: 0.0080 test loss: 0.0062\n",
      "Epoch 2 batch 2600 train loss: 0.0089 test loss: 0.0061\n",
      "Epoch 2 batch 2700 train loss: 0.0077 test loss: 0.0063\n",
      "Epoch 2 batch 2800 train loss: 0.0070 test loss: 0.0064\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.10p/ckpt-23\n",
      "Epoch 2 batch 2900 train loss: 0.0078 test loss: 0.0058\n",
      "Epoch 3 batch 0 train loss: 0.0072 test loss: 0.0059\n",
      "Epoch 3 batch 100 train loss: 0.0076 test loss: 0.0060\n",
      "Epoch 3 batch 200 train loss: 0.0081 test loss: 0.0063\n",
      "Epoch 3 batch 300 train loss: 0.0048 test loss: 0.0059\n",
      "Epoch 3 batch 400 train loss: 0.0084 test loss: 0.0064\n",
      "Epoch 3 batch 500 train loss: 0.0069 test loss: 0.0062\n",
      "Epoch 3 batch 600 train loss: 0.0057 test loss: 0.0058\n",
      "Epoch 3 batch 700 train loss: 0.0069 test loss: 0.0059\n",
      "Epoch 3 batch 800 train loss: 0.0063 test loss: 0.0060\n",
      "Epoch 3 batch 900 train loss: 0.0104 test loss: 0.0062\n",
      "Epoch 3 batch 1000 train loss: 0.0080 test loss: 0.0067\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.10p/ckpt-24\n",
      "Epoch 3 batch 1100 train loss: 0.0071 test loss: 0.0057\n",
      "Epoch 3 batch 1200 train loss: 0.0065 test loss: 0.0062\n",
      "Epoch 3 batch 1300 train loss: 0.0101 test loss: 0.0063\n",
      "Epoch 3 batch 1400 train loss: 0.0092 test loss: 0.0063\n",
      "Epoch 3 batch 1500 train loss: 0.0070 test loss: 0.0060\n",
      "Epoch 3 batch 1600 train loss: 0.0074 test loss: 0.0064\n",
      "Epoch 3 batch 1700 train loss: 0.0086 test loss: 0.0066\n",
      "Epoch 3 batch 1800 train loss: 0.0080 test loss: 0.0058\n",
      "Epoch 3 batch 1900 train loss: 0.0076 test loss: 0.0060\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.10p/ckpt-25\n",
      "Epoch 3 batch 2000 train loss: 0.0096 test loss: 0.0056\n",
      "Epoch 3 batch 2100 train loss: 0.0095 test loss: 0.0062\n",
      "Epoch 3 batch 2200 train loss: 0.0059 test loss: 0.0062\n",
      "Epoch 3 batch 2300 train loss: 0.0078 test loss: 0.0067\n",
      "Epoch 3 batch 2400 train loss: 0.0064 test loss: 0.0062\n",
      "Epoch 3 batch 2500 train loss: 0.0086 test loss: 0.0066\n",
      "Epoch 3 batch 2600 train loss: 0.0072 test loss: 0.0063\n",
      "Epoch 3 batch 2700 train loss: 0.0080 test loss: 0.0062\n",
      "Epoch 3 batch 2800 train loss: 0.0072 test loss: 0.0062\n",
      "Epoch 3 batch 2900 train loss: 0.0048 test loss: 0.0060\n",
      "Epoch 4 batch 0 train loss: 0.0065 test loss: 0.0064\n",
      "Epoch 4 batch 100 train loss: 0.0075 test loss: 0.0060\n",
      "Epoch 4 batch 200 train loss: 0.0073 test loss: 0.0061\n",
      "Epoch 4 batch 300 train loss: 0.0059 test loss: 0.0060\n",
      "Epoch 4 batch 400 train loss: 0.0076 test loss: 0.0062\n",
      "Epoch 4 batch 500 train loss: 0.0093 test loss: 0.0066\n",
      "Epoch 4 batch 600 train loss: 0.0069 test loss: 0.0064\n",
      "Epoch 4 batch 700 train loss: 0.0061 test loss: 0.0063\n",
      "Epoch 4 batch 800 train loss: 0.0079 test loss: 0.0059\n",
      "Epoch 4 batch 900 train loss: 0.0082 test loss: 0.0062\n",
      "Epoch 4 batch 1000 train loss: 0.0080 test loss: 0.0059\n",
      "Epoch 4 batch 1100 train loss: 0.0087 test loss: 0.0062\n",
      "Epoch 4 batch 1200 train loss: 0.0086 test loss: 0.0058\n",
      "Epoch 4 batch 1300 train loss: 0.0082 test loss: 0.0060\n",
      "Epoch 4 batch 1400 train loss: 0.0076 test loss: 0.0063\n",
      "Epoch 4 batch 1500 train loss: 0.0061 test loss: 0.0063\n",
      "Epoch 4 batch 1600 train loss: 0.0065 test loss: 0.0058\n",
      "Epoch 4 batch 1700 train loss: 0.0077 test loss: 0.0060\n",
      "Epoch 4 batch 1800 train loss: 0.0080 test loss: 0.0057\n",
      "Epoch 4 batch 1900 train loss: 0.0078 test loss: 0.0058\n",
      "Epoch 4 batch 2000 train loss: 0.0070 test loss: 0.0058\n",
      "Epoch 4 batch 2100 train loss: 0.0074 test loss: 0.0062\n",
      "Epoch 4 batch 2200 train loss: 0.0071 test loss: 0.0060\n",
      "Epoch 4 batch 2300 train loss: 0.0060 test loss: 0.0065\n",
      "Epoch 4 batch 2400 train loss: 0.0058 test loss: 0.0058\n",
      "Epoch 4 batch 2500 train loss: 0.0094 test loss: 0.0061\n",
      "Epoch 4 batch 2600 train loss: 0.0077 test loss: 0.0064\n",
      "Epoch 4 batch 2700 train loss: 0.0079 test loss: 0.0058\n",
      "Epoch 4 batch 2800 train loss: 0.0072 test loss: 0.0057\n",
      "Epoch 4 batch 2900 train loss: 0.0085 test loss: 0.0057\n",
      "Epoch 5 batch 0 train loss: 0.0080 test loss: 0.0059\n",
      "Epoch 5 batch 100 train loss: 0.0073 test loss: 0.0061\n",
      "Epoch 5 batch 200 train loss: 0.0072 test loss: 0.0062\n",
      "Epoch 5 batch 300 train loss: 0.0069 test loss: 0.0062\n",
      "Epoch 5 batch 400 train loss: 0.0057 test loss: 0.0057\n",
      "Epoch 5 batch 500 train loss: 0.0051 test loss: 0.0066\n",
      "Epoch 5 batch 600 train loss: 0.0059 test loss: 0.0059\n",
      "Epoch 5 batch 700 train loss: 0.0067 test loss: 0.0058\n",
      "Epoch 5 batch 800 train loss: 0.0072 test loss: 0.0064\n",
      "Epoch 5 batch 900 train loss: 0.0056 test loss: 0.0058\n",
      "Epoch 5 batch 1000 train loss: 0.0110 test loss: 0.0063\n",
      "Epoch 5 batch 1100 train loss: 0.0075 test loss: 0.0060\n",
      "Epoch 5 batch 1200 train loss: 0.0085 test loss: 0.0061\n",
      "Epoch 5 batch 1300 train loss: 0.0065 test loss: 0.0062\n",
      "Epoch 5 batch 1400 train loss: 0.0057 test loss: 0.0058\n",
      "Epoch 5 batch 1500 train loss: 0.0062 test loss: 0.0063\n",
      "Epoch 5 batch 1600 train loss: 0.0073 test loss: 0.0062\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.10p/ckpt-26\n",
      "Epoch 5 batch 1700 train loss: 0.0096 test loss: 0.0056\n",
      "Epoch 5 batch 1800 train loss: 0.0063 test loss: 0.0061\n",
      "Epoch 5 batch 1900 train loss: 0.0055 test loss: 0.0064\n",
      "Epoch 5 batch 2000 train loss: 0.0067 test loss: 0.0061\n",
      "Epoch 5 batch 2100 train loss: 0.0056 test loss: 0.0060\n",
      "Epoch 5 batch 2200 train loss: 0.0068 test loss: 0.0059\n",
      "Epoch 5 batch 2300 train loss: 0.0065 test loss: 0.0058\n",
      "Epoch 5 batch 2400 train loss: 0.0078 test loss: 0.0059\n",
      "Epoch 5 batch 2500 train loss: 0.0078 test loss: 0.0059\n",
      "Epoch 5 batch 2600 train loss: 0.0071 test loss: 0.0059\n",
      "Epoch 5 batch 2700 train loss: 0.0075 test loss: 0.0059\n",
      "Epoch 5 batch 2800 train loss: 0.0083 test loss: 0.0063\n",
      "Epoch 5 batch 2900 train loss: 0.0096 test loss: 0.0058\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.10p/ckpt-27\n",
      "Epoch 6 batch 0 train loss: 0.0056 test loss: 0.0054\n",
      "Epoch 6 batch 100 train loss: 0.0074 test loss: 0.0058\n",
      "Epoch 6 batch 200 train loss: 0.0068 test loss: 0.0058\n",
      "Epoch 6 batch 300 train loss: 0.0055 test loss: 0.0057\n",
      "Epoch 6 batch 400 train loss: 0.0071 test loss: 0.0058\n",
      "Epoch 6 batch 500 train loss: 0.0087 test loss: 0.0059\n",
      "Epoch 6 batch 600 train loss: 0.0090 test loss: 0.0062\n",
      "Epoch 6 batch 700 train loss: 0.0088 test loss: 0.0058\n",
      "Epoch 6 batch 800 train loss: 0.0067 test loss: 0.0059\n",
      "Epoch 6 batch 900 train loss: 0.0079 test loss: 0.0055\n",
      "Epoch 6 batch 1000 train loss: 0.0068 test loss: 0.0059\n",
      "Epoch 6 batch 1100 train loss: 0.0098 test loss: 0.0055\n",
      "Epoch 6 batch 1200 train loss: 0.0067 test loss: 0.0061\n",
      "Epoch 6 batch 1300 train loss: 0.0075 test loss: 0.0064\n",
      "Epoch 6 batch 1400 train loss: 0.0095 test loss: 0.0060\n",
      "Epoch 6 batch 1500 train loss: 0.0061 test loss: 0.0055\n",
      "Epoch 6 batch 1600 train loss: 0.0078 test loss: 0.0057\n",
      "Epoch 6 batch 1700 train loss: 0.0078 test loss: 0.0061\n",
      "Epoch 6 batch 1800 train loss: 0.0061 test loss: 0.0056\n",
      "Epoch 6 batch 1900 train loss: 0.0067 test loss: 0.0057\n",
      "Epoch 6 batch 2000 train loss: 0.0082 test loss: 0.0059\n",
      "Epoch 6 batch 2100 train loss: 0.0086 test loss: 0.0062\n",
      "Epoch 6 batch 2200 train loss: 0.0056 test loss: 0.0058\n",
      "Epoch 6 batch 2300 train loss: 0.0071 test loss: 0.0057\n",
      "Epoch 6 batch 2400 train loss: 0.0079 test loss: 0.0061\n",
      "Epoch 6 batch 2500 train loss: 0.0066 test loss: 0.0059\n",
      "Epoch 6 batch 2600 train loss: 0.0069 test loss: 0.0056\n",
      "Epoch 6 batch 2700 train loss: 0.0069 test loss: 0.0057\n",
      "Epoch 6 batch 2800 train loss: 0.0074 test loss: 0.0058\n",
      "Epoch 6 batch 2900 train loss: 0.0066 test loss: 0.0056\n",
      "Epoch 7 batch 0 train loss: 0.0077 test loss: 0.0059\n",
      "Epoch 7 batch 100 train loss: 0.0072 test loss: 0.0055\n",
      "Epoch 7 batch 200 train loss: 0.0095 test loss: 0.0061\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.10p/ckpt-28\n",
      "Epoch 7 batch 300 train loss: 0.0076 test loss: 0.0053\n",
      "Epoch 7 batch 400 train loss: 0.0079 test loss: 0.0059\n",
      "Epoch 7 batch 500 train loss: 0.0108 test loss: 0.0057\n",
      "Epoch 7 batch 600 train loss: 0.0067 test loss: 0.0055\n",
      "Epoch 7 batch 700 train loss: 0.0084 test loss: 0.0058\n",
      "Epoch 7 batch 800 train loss: 0.0075 test loss: 0.0061\n",
      "Epoch 7 batch 900 train loss: 0.0063 test loss: 0.0061\n",
      "Epoch 7 batch 1000 train loss: 0.0062 test loss: 0.0054\n",
      "Epoch 7 batch 1100 train loss: 0.0096 test loss: 0.0056\n",
      "Epoch 7 batch 1200 train loss: 0.0056 test loss: 0.0058\n",
      "Epoch 7 batch 1300 train loss: 0.0070 test loss: 0.0061\n",
      "Epoch 7 batch 1400 train loss: 0.0060 test loss: 0.0058\n",
      "Epoch 7 batch 1500 train loss: 0.0049 test loss: 0.0058\n",
      "Epoch 7 batch 1600 train loss: 0.0066 test loss: 0.0057\n",
      "Epoch 7 batch 1700 train loss: 0.0076 test loss: 0.0055\n",
      "Epoch 7 batch 1800 train loss: 0.0061 test loss: 0.0056\n",
      "Epoch 7 batch 1900 train loss: 0.0087 test loss: 0.0056\n",
      "Epoch 7 batch 2000 train loss: 0.0089 test loss: 0.0060\n",
      "Epoch 7 batch 2100 train loss: 0.0099 test loss: 0.0056\n",
      "Epoch 7 batch 2200 train loss: 0.0069 test loss: 0.0061\n",
      "Epoch 7 batch 2300 train loss: 0.0049 test loss: 0.0054\n",
      "Epoch 7 batch 2400 train loss: 0.0082 test loss: 0.0058\n",
      "Epoch 7 batch 2500 train loss: 0.0078 test loss: 0.0057\n",
      "Epoch 7 batch 2600 train loss: 0.0067 test loss: 0.0057\n",
      "Epoch 7 batch 2700 train loss: 0.0057 test loss: 0.0056\n",
      "Epoch 7 batch 2800 train loss: 0.0075 test loss: 0.0056\n",
      "Epoch 7 batch 2900 train loss: 0.0057 test loss: 0.0056\n",
      "Epoch 8 batch 0 train loss: 0.0070 test loss: 0.0053\n",
      "Epoch 8 batch 100 train loss: 0.0055 test loss: 0.0057\n",
      "Epoch 8 batch 200 train loss: 0.0077 test loss: 0.0056\n",
      "Epoch 8 batch 300 train loss: 0.0056 test loss: 0.0053\n",
      "Epoch 8 batch 400 train loss: 0.0073 test loss: 0.0057\n",
      "Epoch 8 batch 500 train loss: 0.0056 test loss: 0.0061\n",
      "Epoch 8 batch 600 train loss: 0.0054 test loss: 0.0058\n",
      "Epoch 8 batch 700 train loss: 0.0084 test loss: 0.0057\n",
      "Epoch 8 batch 800 train loss: 0.0079 test loss: 0.0057\n",
      "Epoch 8 batch 900 train loss: 0.0056 test loss: 0.0057\n",
      "Epoch 8 batch 1000 train loss: 0.0066 test loss: 0.0059\n",
      "Epoch 8 batch 1100 train loss: 0.0043 test loss: 0.0056\n",
      "Epoch 8 batch 1200 train loss: 0.0075 test loss: 0.0058\n",
      "Epoch 8 batch 1300 train loss: 0.0090 test loss: 0.0059\n",
      "Epoch 8 batch 1400 train loss: 0.0060 test loss: 0.0057\n",
      "Epoch 8 batch 1500 train loss: 0.0076 test loss: 0.0058\n",
      "Epoch 8 batch 1600 train loss: 0.0088 test loss: 0.0060\n",
      "Epoch 8 batch 1700 train loss: 0.0095 test loss: 0.0057\n",
      "Epoch 8 batch 1800 train loss: 0.0078 test loss: 0.0057\n",
      "Epoch 8 batch 1900 train loss: 0.0085 test loss: 0.0058\n",
      "Epoch 8 batch 2000 train loss: 0.0054 test loss: 0.0057\n",
      "Epoch 8 batch 2100 train loss: 0.0092 test loss: 0.0059\n",
      "Epoch 8 batch 2200 train loss: 0.0081 test loss: 0.0061\n",
      "Epoch 8 batch 2300 train loss: 0.0071 test loss: 0.0057\n",
      "Epoch 8 batch 2400 train loss: 0.0064 test loss: 0.0057\n",
      "Epoch 8 batch 2500 train loss: 0.0105 test loss: 0.0058\n",
      "Epoch 8 batch 2600 train loss: 0.0082 test loss: 0.0056\n",
      "Epoch 8 batch 2700 train loss: 0.0057 test loss: 0.0066\n",
      "Epoch 8 batch 2800 train loss: 0.0050 test loss: 0.0058\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.10p/ckpt-29\n",
      "Epoch 8 batch 2900 train loss: 0.0073 test loss: 0.0052\n",
      "Epoch 9 batch 0 train loss: 0.0060 test loss: 0.0055\n",
      "Epoch 9 batch 100 train loss: 0.0091 test loss: 0.0059\n",
      "Epoch 9 batch 200 train loss: 0.0084 test loss: 0.0057\n",
      "Epoch 9 batch 300 train loss: 0.0049 test loss: 0.0053\n",
      "Epoch 9 batch 400 train loss: 0.0061 test loss: 0.0057\n",
      "Epoch 9 batch 500 train loss: 0.0062 test loss: 0.0057\n",
      "Epoch 9 batch 600 train loss: 0.0075 test loss: 0.0055\n",
      "Epoch 9 batch 700 train loss: 0.0084 test loss: 0.0056\n",
      "Epoch 9 batch 800 train loss: 0.0073 test loss: 0.0057\n",
      "Epoch 9 batch 900 train loss: 0.0068 test loss: 0.0058\n",
      "Epoch 9 batch 1000 train loss: 0.0082 test loss: 0.0057\n",
      "Epoch 9 batch 1100 train loss: 0.0078 test loss: 0.0057\n",
      "Epoch 9 batch 1200 train loss: 0.0088 test loss: 0.0057\n",
      "Epoch 9 batch 1300 train loss: 0.0067 test loss: 0.0058\n",
      "Epoch 9 batch 1400 train loss: 0.0067 test loss: 0.0058\n",
      "Epoch 9 batch 1500 train loss: 0.0123 test loss: 0.0058\n",
      "Epoch 9 batch 1600 train loss: 0.0094 test loss: 0.0057\n",
      "Epoch 9 batch 1700 train loss: 0.0056 test loss: 0.0059\n",
      "Epoch 9 batch 1800 train loss: 0.0066 test loss: 0.0060\n",
      "Epoch 9 batch 1900 train loss: 0.0092 test loss: 0.0062\n",
      "Epoch 9 batch 2000 train loss: 0.0086 test loss: 0.0055\n",
      "Epoch 9 batch 2100 train loss: 0.0080 test loss: 0.0058\n",
      "Epoch 9 batch 2200 train loss: 0.0072 test loss: 0.0055\n",
      "Epoch 9 batch 2300 train loss: 0.0061 test loss: 0.0059\n",
      "Epoch 9 batch 2400 train loss: 0.0069 test loss: 0.0057\n",
      "Epoch 9 batch 2500 train loss: 0.0073 test loss: 0.0057\n",
      "Epoch 9 batch 2600 train loss: 0.0080 test loss: 0.0057\n",
      "Epoch 9 batch 2700 train loss: 0.0063 test loss: 0.0059\n",
      "Epoch 9 batch 2800 train loss: 0.0067 test loss: 0.0052\n",
      "Epoch 9 batch 2900 train loss: 0.0082 test loss: 0.0055\n",
      "Epoch 10 batch 0 train loss: 0.0083 test loss: 0.0060\n",
      "Epoch 10 batch 100 train loss: 0.0064 test loss: 0.0055\n",
      "Epoch 10 batch 200 train loss: 0.0083 test loss: 0.0057\n",
      "Epoch 10 batch 300 train loss: 0.0073 test loss: 0.0057\n",
      "Epoch 10 batch 400 train loss: 0.0082 test loss: 0.0058\n",
      "Epoch 10 batch 500 train loss: 0.0067 test loss: 0.0056\n",
      "Epoch 10 batch 600 train loss: 0.0069 test loss: 0.0057\n",
      "Epoch 10 batch 700 train loss: 0.0080 test loss: 0.0057\n",
      "Epoch 10 batch 800 train loss: 0.0079 test loss: 0.0058\n",
      "Epoch 10 batch 900 train loss: 0.0058 test loss: 0.0056\n",
      "Epoch 10 batch 1000 train loss: 0.0071 test loss: 0.0057\n",
      "Epoch 10 batch 1100 train loss: 0.0095 test loss: 0.0060\n",
      "Epoch 10 batch 1200 train loss: 0.0078 test loss: 0.0056\n",
      "Epoch 10 batch 1300 train loss: 0.0093 test loss: 0.0058\n",
      "Epoch 10 batch 1400 train loss: 0.0093 test loss: 0.0058\n",
      "Epoch 10 batch 1500 train loss: 0.0053 test loss: 0.0057\n",
      "Epoch 10 batch 1600 train loss: 0.0094 test loss: 0.0057\n",
      "Epoch 10 batch 1700 train loss: 0.0064 test loss: 0.0055\n",
      "Epoch 10 batch 1800 train loss: 0.0079 test loss: 0.0057\n",
      "Epoch 10 batch 1900 train loss: 0.0090 test loss: 0.0059\n",
      "Epoch 10 batch 2000 train loss: 0.0074 test loss: 0.0054\n",
      "Epoch 10 batch 2100 train loss: 0.0055 test loss: 0.0056\n",
      "Epoch 10 batch 2200 train loss: 0.0083 test loss: 0.0058\n",
      "Epoch 10 batch 2300 train loss: 0.0079 test loss: 0.0062\n",
      "Epoch 10 batch 2400 train loss: 0.0056 test loss: 0.0058\n",
      "Epoch 10 batch 2500 train loss: 0.0072 test loss: 0.0060\n",
      "Epoch 10 batch 2600 train loss: 0.0070 test loss: 0.0053\n",
      "Epoch 10 batch 2700 train loss: 0.0080 test loss: 0.0056\n",
      "Epoch 10 batch 2800 train loss: 0.0104 test loss: 0.0058\n",
      "Epoch 10 batch 2900 train loss: 0.0074 test loss: 0.0054\n",
      "Epoch 11 batch 0 train loss: 0.0059 test loss: 0.0053\n",
      "Epoch 11 batch 100 train loss: 0.0068 test loss: 0.0056\n",
      "Epoch 11 batch 200 train loss: 0.0068 test loss: 0.0058\n",
      "Epoch 11 batch 300 train loss: 0.0063 test loss: 0.0055\n",
      "Epoch 11 batch 400 train loss: 0.0066 test loss: 0.0058\n",
      "Epoch 11 batch 500 train loss: 0.0082 test loss: 0.0055\n",
      "Epoch 11 batch 600 train loss: 0.0061 test loss: 0.0059\n",
      "Epoch 11 batch 700 train loss: 0.0053 test loss: 0.0056\n",
      "Epoch 11 batch 800 train loss: 0.0089 test loss: 0.0057\n",
      "Epoch 11 batch 900 train loss: 0.0071 test loss: 0.0055\n",
      "Epoch 11 batch 1000 train loss: 0.0080 test loss: 0.0059\n",
      "Epoch 11 batch 1100 train loss: 0.0047 test loss: 0.0058\n",
      "Epoch 11 batch 1200 train loss: 0.0071 test loss: 0.0056\n",
      "Epoch 11 batch 1300 train loss: 0.0072 test loss: 0.0057\n",
      "Epoch 11 batch 1400 train loss: 0.0090 test loss: 0.0059\n",
      "Epoch 11 batch 1500 train loss: 0.0094 test loss: 0.0054\n",
      "Epoch 11 batch 1600 train loss: 0.0068 test loss: 0.0056\n",
      "Epoch 11 batch 1700 train loss: 0.0097 test loss: 0.0060\n",
      "Epoch 11 batch 1800 train loss: 0.0055 test loss: 0.0057\n",
      "Epoch 11 batch 1900 train loss: 0.0067 test loss: 0.0057\n",
      "Epoch 11 batch 2000 train loss: 0.0082 test loss: 0.0055\n",
      "Epoch 11 batch 2100 train loss: 0.0048 test loss: 0.0057\n",
      "Epoch 11 batch 2200 train loss: 0.0078 test loss: 0.0057\n",
      "Epoch 11 batch 2300 train loss: 0.0060 test loss: 0.0057\n",
      "Epoch 11 batch 2400 train loss: 0.0063 test loss: 0.0056\n",
      "Epoch 11 batch 2500 train loss: 0.0057 test loss: 0.0057\n",
      "Epoch 11 batch 2600 train loss: 0.0080 test loss: 0.0055\n",
      "Epoch 11 batch 2700 train loss: 0.0082 test loss: 0.0058\n",
      "Epoch 11 batch 2800 train loss: 0.0070 test loss: 0.0055\n",
      "Epoch 11 batch 2900 train loss: 0.0078 test loss: 0.0056\n",
      "Epoch 12 batch 0 train loss: 0.0066 test loss: 0.0054\n",
      "Epoch 12 batch 100 train loss: 0.0094 test loss: 0.0054\n",
      "Epoch 12 batch 200 train loss: 0.0054 test loss: 0.0063\n",
      "Epoch 12 batch 300 train loss: 0.0075 test loss: 0.0054\n",
      "Epoch 12 batch 400 train loss: 0.0072 test loss: 0.0056\n",
      "Epoch 12 batch 500 train loss: 0.0066 test loss: 0.0058\n",
      "Epoch 12 batch 600 train loss: 0.0070 test loss: 0.0058\n",
      "Epoch 12 batch 700 train loss: 0.0072 test loss: 0.0054\n",
      "Epoch 12 batch 800 train loss: 0.0072 test loss: 0.0057\n",
      "Epoch 12 batch 900 train loss: 0.0059 test loss: 0.0055\n",
      "Epoch 12 batch 1000 train loss: 0.0066 test loss: 0.0058\n",
      "Epoch 12 batch 1100 train loss: 0.0080 test loss: 0.0058\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.10p/ckpt-30\n",
      "Epoch 12 batch 1200 train loss: 0.0071 test loss: 0.0051\n",
      "Epoch 12 batch 1300 train loss: 0.0057 test loss: 0.0054\n",
      "Epoch 12 batch 1400 train loss: 0.0069 test loss: 0.0061\n",
      "Epoch 12 batch 1500 train loss: 0.0061 test loss: 0.0062\n",
      "Epoch 12 batch 1600 train loss: 0.0080 test loss: 0.0055\n",
      "Epoch 12 batch 1700 train loss: 0.0100 test loss: 0.0056\n",
      "Epoch 12 batch 1800 train loss: 0.0074 test loss: 0.0056\n",
      "Epoch 12 batch 1900 train loss: 0.0069 test loss: 0.0060\n",
      "Epoch 12 batch 2000 train loss: 0.0097 test loss: 0.0055\n",
      "Epoch 12 batch 2100 train loss: 0.0071 test loss: 0.0062\n",
      "Epoch 12 batch 2200 train loss: 0.0066 test loss: 0.0060\n",
      "Epoch 12 batch 2300 train loss: 0.0079 test loss: 0.0057\n",
      "Epoch 12 batch 2400 train loss: 0.0095 test loss: 0.0054\n",
      "Epoch 12 batch 2500 train loss: 0.0073 test loss: 0.0056\n",
      "Epoch 12 batch 2600 train loss: 0.0093 test loss: 0.0052\n",
      "Epoch 12 batch 2700 train loss: 0.0064 test loss: 0.0059\n",
      "Epoch 12 batch 2800 train loss: 0.0074 test loss: 0.0051\n",
      "Epoch 12 batch 2900 train loss: 0.0046 test loss: 0.0055\n",
      "Epoch 13 batch 0 train loss: 0.0074 test loss: 0.0053\n",
      "Epoch 13 batch 100 train loss: 0.0072 test loss: 0.0059\n",
      "Epoch 13 batch 200 train loss: 0.0065 test loss: 0.0053\n",
      "Epoch 13 batch 300 train loss: 0.0082 test loss: 0.0055\n",
      "Epoch 13 batch 400 train loss: 0.0072 test loss: 0.0057\n",
      "Epoch 13 batch 500 train loss: 0.0074 test loss: 0.0058\n",
      "Epoch 13 batch 600 train loss: 0.0074 test loss: 0.0058\n",
      "Epoch 13 batch 700 train loss: 0.0067 test loss: 0.0058\n",
      "Epoch 13 batch 800 train loss: 0.0066 test loss: 0.0056\n",
      "Epoch 13 batch 900 train loss: 0.0063 test loss: 0.0058\n",
      "Epoch 13 batch 1000 train loss: 0.0086 test loss: 0.0060\n",
      "Epoch 13 batch 1100 train loss: 0.0074 test loss: 0.0062\n",
      "Epoch 13 batch 1200 train loss: 0.0091 test loss: 0.0055\n",
      "Epoch 13 batch 1300 train loss: 0.0050 test loss: 0.0059\n",
      "Epoch 13 batch 1400 train loss: 0.0058 test loss: 0.0058\n",
      "Epoch 13 batch 1500 train loss: 0.0085 test loss: 0.0056\n",
      "Epoch 13 batch 1600 train loss: 0.0053 test loss: 0.0055\n",
      "Epoch 13 batch 1700 train loss: 0.0054 test loss: 0.0056\n",
      "Epoch 13 batch 1800 train loss: 0.0074 test loss: 0.0056\n",
      "Epoch 13 batch 1900 train loss: 0.0068 test loss: 0.0055\n",
      "Epoch 13 batch 2000 train loss: 0.0085 test loss: 0.0053\n",
      "Epoch 13 batch 2100 train loss: 0.0086 test loss: 0.0054\n",
      "Epoch 13 batch 2200 train loss: 0.0063 test loss: 0.0054\n",
      "Epoch 13 batch 2300 train loss: 0.0075 test loss: 0.0055\n",
      "Epoch 13 batch 2400 train loss: 0.0070 test loss: 0.0058\n",
      "Epoch 13 batch 2500 train loss: 0.0090 test loss: 0.0060\n",
      "Epoch 13 batch 2600 train loss: 0.0092 test loss: 0.0055\n",
      "Epoch 13 batch 2700 train loss: 0.0072 test loss: 0.0058\n",
      "Epoch 13 batch 2800 train loss: 0.0074 test loss: 0.0056\n",
      "Epoch 13 batch 2900 train loss: 0.0087 test loss: 0.0053\n",
      "Epoch 14 batch 0 train loss: 0.0057 test loss: 0.0056\n",
      "Epoch 14 batch 100 train loss: 0.0056 test loss: 0.0057\n",
      "Epoch 14 batch 200 train loss: 0.0087 test loss: 0.0056\n",
      "Epoch 14 batch 300 train loss: 0.0076 test loss: 0.0051\n",
      "Epoch 14 batch 400 train loss: 0.0072 test loss: 0.0054\n",
      "Epoch 14 batch 500 train loss: 0.0054 test loss: 0.0056\n",
      "Epoch 14 batch 600 train loss: 0.0083 test loss: 0.0055\n",
      "Epoch 14 batch 700 train loss: 0.0069 test loss: 0.0057\n",
      "Epoch 14 batch 800 train loss: 0.0086 test loss: 0.0062\n",
      "Epoch 14 batch 900 train loss: 0.0057 test loss: 0.0054\n",
      "Epoch 14 batch 1000 train loss: 0.0058 test loss: 0.0057\n",
      "Epoch 14 batch 1100 train loss: 0.0081 test loss: 0.0057\n",
      "Epoch 14 batch 1200 train loss: 0.0067 test loss: 0.0056\n",
      "Epoch 14 batch 1300 train loss: 0.0059 test loss: 0.0059\n",
      "Epoch 14 batch 1400 train loss: 0.0064 test loss: 0.0053\n",
      "Epoch 14 batch 1500 train loss: 0.0074 test loss: 0.0056\n",
      "Epoch 14 batch 1600 train loss: 0.0064 test loss: 0.0057\n",
      "Epoch 14 batch 1700 train loss: 0.0046 test loss: 0.0056\n",
      "Epoch 14 batch 1800 train loss: 0.0078 test loss: 0.0055\n",
      "Epoch 14 batch 1900 train loss: 0.0069 test loss: 0.0055\n",
      "Epoch 14 batch 2000 train loss: 0.0082 test loss: 0.0053\n",
      "Epoch 14 batch 2100 train loss: 0.0052 test loss: 0.0055\n",
      "Epoch 14 batch 2200 train loss: 0.0079 test loss: 0.0056\n",
      "Epoch 14 batch 2300 train loss: 0.0062 test loss: 0.0057\n",
      "Epoch 14 batch 2400 train loss: 0.0090 test loss: 0.0054\n",
      "Epoch 14 batch 2500 train loss: 0.0073 test loss: 0.0056\n",
      "Epoch 14 batch 2600 train loss: 0.0057 test loss: 0.0055\n",
      "Epoch 14 batch 2700 train loss: 0.0064 test loss: 0.0062\n",
      "Epoch 14 batch 2800 train loss: 0.0083 test loss: 0.0052\n",
      "Epoch 14 batch 2900 train loss: 0.0081 test loss: 0.0052\n",
      "Epoch 15 batch 0 train loss: 0.0067 test loss: 0.0055\n",
      "Epoch 15 batch 100 train loss: 0.0066 test loss: 0.0055\n",
      "Epoch 15 batch 200 train loss: 0.0090 test loss: 0.0056\n",
      "Epoch 15 batch 300 train loss: 0.0077 test loss: 0.0055\n",
      "Epoch 15 batch 400 train loss: 0.0065 test loss: 0.0055\n",
      "Epoch 15 batch 500 train loss: 0.0062 test loss: 0.0059\n",
      "Epoch 15 batch 600 train loss: 0.0063 test loss: 0.0053\n",
      "Epoch 15 batch 700 train loss: 0.0073 test loss: 0.0055\n",
      "Epoch 15 batch 800 train loss: 0.0040 test loss: 0.0056\n",
      "Epoch 15 batch 900 train loss: 0.0061 test loss: 0.0053\n",
      "Epoch 15 batch 1000 train loss: 0.0066 test loss: 0.0054\n",
      "Epoch 15 batch 1100 train loss: 0.0077 test loss: 0.0060\n",
      "Epoch 15 batch 1200 train loss: 0.0090 test loss: 0.0057\n",
      "Epoch 15 batch 1300 train loss: 0.0080 test loss: 0.0059\n",
      "Epoch 15 batch 1400 train loss: 0.0063 test loss: 0.0054\n",
      "Epoch 15 batch 1500 train loss: 0.0065 test loss: 0.0054\n",
      "Epoch 15 batch 1600 train loss: 0.0072 test loss: 0.0058\n",
      "Epoch 15 batch 1700 train loss: 0.0113 test loss: 0.0058\n",
      "Epoch 15 batch 1800 train loss: 0.0077 test loss: 0.0054\n",
      "Epoch 15 batch 1900 train loss: 0.0091 test loss: 0.0055\n",
      "Epoch 15 batch 2000 train loss: 0.0061 test loss: 0.0054\n",
      "Epoch 15 batch 2100 train loss: 0.0098 test loss: 0.0056\n",
      "Epoch 15 batch 2200 train loss: 0.0077 test loss: 0.0056\n",
      "Epoch 15 batch 2300 train loss: 0.0055 test loss: 0.0054\n",
      "Epoch 15 batch 2400 train loss: 0.0065 test loss: 0.0054\n",
      "Epoch 15 batch 2500 train loss: 0.0060 test loss: 0.0056\n",
      "Epoch 15 batch 2600 train loss: 0.0072 test loss: 0.0055\n",
      "Epoch 15 batch 2700 train loss: 0.0076 test loss: 0.0054\n",
      "Epoch 15 batch 2800 train loss: 0.0088 test loss: 0.0055\n",
      "Epoch 15 batch 2900 train loss: 0.0065 test loss: 0.0054\n",
      "Epoch 16 batch 0 train loss: 0.0079 test loss: 0.0057\n",
      "Epoch 16 batch 100 train loss: 0.0063 test loss: 0.0055\n",
      "Epoch 16 batch 200 train loss: 0.0074 test loss: 0.0055\n",
      "Epoch 16 batch 300 train loss: 0.0103 test loss: 0.0057\n",
      "Epoch 16 batch 400 train loss: 0.0087 test loss: 0.0054\n",
      "Epoch 16 batch 500 train loss: 0.0070 test loss: 0.0056\n",
      "Epoch 16 batch 600 train loss: 0.0066 test loss: 0.0060\n",
      "Epoch 16 batch 700 train loss: 0.0077 test loss: 0.0056\n",
      "Epoch 16 batch 800 train loss: 0.0078 test loss: 0.0062\n",
      "Epoch 16 batch 900 train loss: 0.0079 test loss: 0.0055\n",
      "Epoch 16 batch 1000 train loss: 0.0069 test loss: 0.0055\n",
      "Epoch 16 batch 1100 train loss: 0.0081 test loss: 0.0060\n",
      "Epoch 16 batch 1200 train loss: 0.0070 test loss: 0.0053\n",
      "Epoch 16 batch 1300 train loss: 0.0066 test loss: 0.0055\n",
      "Epoch 16 batch 1400 train loss: 0.0066 test loss: 0.0056\n",
      "Epoch 16 batch 1500 train loss: 0.0078 test loss: 0.0052\n",
      "Epoch 16 batch 1600 train loss: 0.0085 test loss: 0.0053\n",
      "Epoch 16 batch 1700 train loss: 0.0102 test loss: 0.0059\n",
      "Epoch 16 batch 1800 train loss: 0.0095 test loss: 0.0055\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.10p/ckpt-31\n",
      "Epoch 16 batch 1900 train loss: 0.0088 test loss: 0.0051\n",
      "Epoch 16 batch 2000 train loss: 0.0099 test loss: 0.0054\n",
      "Epoch 16 batch 2100 train loss: 0.0079 test loss: 0.0055\n",
      "Epoch 16 batch 2200 train loss: 0.0093 test loss: 0.0054\n",
      "Epoch 16 batch 2300 train loss: 0.0071 test loss: 0.0058\n",
      "Epoch 16 batch 2400 train loss: 0.0080 test loss: 0.0059\n",
      "Epoch 16 batch 2500 train loss: 0.0075 test loss: 0.0053\n",
      "Epoch 16 batch 2600 train loss: 0.0079 test loss: 0.0056\n",
      "Epoch 16 batch 2700 train loss: 0.0083 test loss: 0.0055\n",
      "Epoch 16 batch 2800 train loss: 0.0083 test loss: 0.0053\n",
      "Epoch 16 batch 2900 train loss: 0.0068 test loss: 0.0055\n",
      "Epoch 17 batch 0 train loss: 0.0068 test loss: 0.0054\n",
      "Epoch 17 batch 100 train loss: 0.0096 test loss: 0.0059\n",
      "Epoch 17 batch 200 train loss: 0.0064 test loss: 0.0058\n",
      "Epoch 17 batch 300 train loss: 0.0115 test loss: 0.0052\n",
      "Epoch 17 batch 400 train loss: 0.0087 test loss: 0.0054\n",
      "Epoch 17 batch 500 train loss: 0.0083 test loss: 0.0056\n",
      "Epoch 17 batch 600 train loss: 0.0051 test loss: 0.0056\n",
      "Epoch 17 batch 700 train loss: 0.0070 test loss: 0.0058\n",
      "Epoch 17 batch 800 train loss: 0.0090 test loss: 0.0058\n",
      "Epoch 17 batch 900 train loss: 0.0075 test loss: 0.0054\n",
      "Epoch 17 batch 1000 train loss: 0.0089 test loss: 0.0058\n",
      "Epoch 17 batch 1100 train loss: 0.0075 test loss: 0.0056\n",
      "Epoch 17 batch 1200 train loss: 0.0071 test loss: 0.0054\n",
      "Epoch 17 batch 1300 train loss: 0.0073 test loss: 0.0058\n",
      "Epoch 17 batch 1400 train loss: 0.0065 test loss: 0.0059\n",
      "Epoch 17 batch 1500 train loss: 0.0087 test loss: 0.0058\n",
      "Epoch 17 batch 1600 train loss: 0.0077 test loss: 0.0059\n",
      "Epoch 17 batch 1700 train loss: 0.0063 test loss: 0.0054\n",
      "Epoch 17 batch 1800 train loss: 0.0097 test loss: 0.0053\n",
      "Epoch 17 batch 1900 train loss: 0.0071 test loss: 0.0056\n",
      "Epoch 17 batch 2000 train loss: 0.0082 test loss: 0.0052\n",
      "early stop.\n",
      "Checkpoint 31 restored!!\n",
      "Training for loss rate 0.20 start.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['bi_lstm_1/bi_lstm_block_2/bidirectional_2/forward_lstm_2/lstm_cell_7/kernel:0', 'bi_lstm_1/bi_lstm_block_2/bidirectional_2/forward_lstm_2/lstm_cell_7/recurrent_kernel:0', 'bi_lstm_1/bi_lstm_block_2/bidirectional_2/forward_lstm_2/lstm_cell_7/bias:0', 'bi_lstm_1/bi_lstm_block_2/bidirectional_2/backward_lstm_2/lstm_cell_8/kernel:0', 'bi_lstm_1/bi_lstm_block_2/bidirectional_2/backward_lstm_2/lstm_cell_8/recurrent_kernel:0', 'bi_lstm_1/bi_lstm_block_2/bidirectional_2/backward_lstm_2/lstm_cell_8/bias:0', 'bi_lstm_1/bi_lstm_block_2/layer_normalization_2/gamma:0', 'bi_lstm_1/bi_lstm_block_2/layer_normalization_2/beta:0', 'bi_lstm_1/bi_lstm_block_3/bidirectional_3/forward_lstm_3/lstm_cell_10/kernel:0', 'bi_lstm_1/bi_lstm_block_3/bidirectional_3/forward_lstm_3/lstm_cell_10/recurrent_kernel:0', 'bi_lstm_1/bi_lstm_block_3/bidirectional_3/forward_lstm_3/lstm_cell_10/bias:0', 'bi_lstm_1/bi_lstm_block_3/bidirectional_3/backward_lstm_3/lstm_cell_11/kernel:0', 'bi_lstm_1/bi_lstm_block_3/bidirectional_3/backward_lstm_3/lstm_cell_11/recurrent_kernel:0', 'bi_lstm_1/bi_lstm_block_3/bidirectional_3/backward_lstm_3/lstm_cell_11/bias:0', 'bi_lstm_1/bi_lstm_block_3/layer_normalization_3/gamma:0', 'bi_lstm_1/bi_lstm_block_3/layer_normalization_3/beta:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['bi_lstm_1/bi_lstm_block_2/bidirectional_2/forward_lstm_2/lstm_cell_7/kernel:0', 'bi_lstm_1/bi_lstm_block_2/bidirectional_2/forward_lstm_2/lstm_cell_7/recurrent_kernel:0', 'bi_lstm_1/bi_lstm_block_2/bidirectional_2/forward_lstm_2/lstm_cell_7/bias:0', 'bi_lstm_1/bi_lstm_block_2/bidirectional_2/backward_lstm_2/lstm_cell_8/kernel:0', 'bi_lstm_1/bi_lstm_block_2/bidirectional_2/backward_lstm_2/lstm_cell_8/recurrent_kernel:0', 'bi_lstm_1/bi_lstm_block_2/bidirectional_2/backward_lstm_2/lstm_cell_8/bias:0', 'bi_lstm_1/bi_lstm_block_2/layer_normalization_2/gamma:0', 'bi_lstm_1/bi_lstm_block_2/layer_normalization_2/beta:0', 'bi_lstm_1/bi_lstm_block_3/bidirectional_3/forward_lstm_3/lstm_cell_10/kernel:0', 'bi_lstm_1/bi_lstm_block_3/bidirectional_3/forward_lstm_3/lstm_cell_10/recurrent_kernel:0', 'bi_lstm_1/bi_lstm_block_3/bidirectional_3/forward_lstm_3/lstm_cell_10/bias:0', 'bi_lstm_1/bi_lstm_block_3/bidirectional_3/backward_lstm_3/lstm_cell_11/kernel:0', 'bi_lstm_1/bi_lstm_block_3/bidirectional_3/backward_lstm_3/lstm_cell_11/recurrent_kernel:0', 'bi_lstm_1/bi_lstm_block_3/bidirectional_3/backward_lstm_3/lstm_cell_11/bias:0', 'bi_lstm_1/bi_lstm_block_3/layer_normalization_3/gamma:0', 'bi_lstm_1/bi_lstm_block_3/layer_normalization_3/beta:0'] when minimizing the loss.\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.20p/ckpt-1\n",
      "Epoch 0 batch 0 train loss: 0.6356 test loss: 0.6238\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.20p/ckpt-2\n",
      "Epoch 0 batch 100 train loss: 0.2032 test loss: 0.2446\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.20p/ckpt-3\n",
      "Epoch 0 batch 200 train loss: 0.1054 test loss: 0.1250\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.20p/ckpt-4\n",
      "Epoch 0 batch 300 train loss: 0.0640 test loss: 0.0745\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.20p/ckpt-5\n",
      "Epoch 0 batch 400 train loss: 0.0399 test loss: 0.0484\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.20p/ckpt-6\n",
      "Epoch 0 batch 500 train loss: 0.0325 test loss: 0.0336\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.20p/ckpt-7\n",
      "Epoch 0 batch 600 train loss: 0.0209 test loss: 0.0246\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.20p/ckpt-8\n",
      "Epoch 0 batch 700 train loss: 0.0161 test loss: 0.0189\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.20p/ckpt-9\n",
      "Epoch 0 batch 800 train loss: 0.0142 test loss: 0.0153\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.20p/ckpt-10\n",
      "Epoch 0 batch 900 train loss: 0.0110 test loss: 0.0131\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.20p/ckpt-11\n",
      "Epoch 0 batch 1000 train loss: 0.0105 test loss: 0.0112\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.20p/ckpt-12\n",
      "Epoch 0 batch 1100 train loss: 0.0094 test loss: 0.0103\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.20p/ckpt-13\n",
      "Epoch 0 batch 1200 train loss: 0.0076 test loss: 0.0095\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.20p/ckpt-14\n",
      "Epoch 0 batch 1300 train loss: 0.0085 test loss: 0.0091\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.20p/ckpt-15\n",
      "Epoch 0 batch 1400 train loss: 0.0086 test loss: 0.0089\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.20p/ckpt-16\n",
      "Epoch 0 batch 1500 train loss: 0.0082 test loss: 0.0085\n",
      "Epoch 0 batch 1600 train loss: 0.0102 test loss: 0.0086\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.20p/ckpt-17\n",
      "Epoch 0 batch 1700 train loss: 0.0077 test loss: 0.0083\n",
      "Epoch 0 batch 1800 train loss: 0.0081 test loss: 0.0083\n",
      "Epoch 0 batch 1900 train loss: 0.0096 test loss: 0.0087\n",
      "Epoch 0 batch 2000 train loss: 0.0092 test loss: 0.0084\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.20p/ckpt-18\n",
      "Epoch 0 batch 2100 train loss: 0.0094 test loss: 0.0082\n",
      "Epoch 0 batch 2200 train loss: 0.0082 test loss: 0.0083\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.20p/ckpt-19\n",
      "Epoch 0 batch 2300 train loss: 0.0070 test loss: 0.0082\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.20p/ckpt-20\n",
      "Epoch 0 batch 2400 train loss: 0.0067 test loss: 0.0081\n",
      "Epoch 0 batch 2500 train loss: 0.0110 test loss: 0.0084\n",
      "Epoch 0 batch 2600 train loss: 0.0088 test loss: 0.0085\n",
      "Epoch 0 batch 2700 train loss: 0.0081 test loss: 0.0083\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.20p/ckpt-21\n",
      "Epoch 0 batch 2800 train loss: 0.0089 test loss: 0.0079\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.20p/ckpt-22\n",
      "Epoch 0 batch 2900 train loss: 0.0090 test loss: 0.0077\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['bi_lstm_1/bi_lstm_block_2/bidirectional_2/forward_lstm_2/lstm_cell_7/kernel:0', 'bi_lstm_1/bi_lstm_block_2/bidirectional_2/forward_lstm_2/lstm_cell_7/recurrent_kernel:0', 'bi_lstm_1/bi_lstm_block_2/bidirectional_2/forward_lstm_2/lstm_cell_7/bias:0', 'bi_lstm_1/bi_lstm_block_2/bidirectional_2/backward_lstm_2/lstm_cell_8/kernel:0', 'bi_lstm_1/bi_lstm_block_2/bidirectional_2/backward_lstm_2/lstm_cell_8/recurrent_kernel:0', 'bi_lstm_1/bi_lstm_block_2/bidirectional_2/backward_lstm_2/lstm_cell_8/bias:0', 'bi_lstm_1/bi_lstm_block_2/layer_normalization_2/gamma:0', 'bi_lstm_1/bi_lstm_block_2/layer_normalization_2/beta:0', 'bi_lstm_1/bi_lstm_block_3/bidirectional_3/forward_lstm_3/lstm_cell_10/kernel:0', 'bi_lstm_1/bi_lstm_block_3/bidirectional_3/forward_lstm_3/lstm_cell_10/recurrent_kernel:0', 'bi_lstm_1/bi_lstm_block_3/bidirectional_3/forward_lstm_3/lstm_cell_10/bias:0', 'bi_lstm_1/bi_lstm_block_3/bidirectional_3/backward_lstm_3/lstm_cell_11/kernel:0', 'bi_lstm_1/bi_lstm_block_3/bidirectional_3/backward_lstm_3/lstm_cell_11/recurrent_kernel:0', 'bi_lstm_1/bi_lstm_block_3/bidirectional_3/backward_lstm_3/lstm_cell_11/bias:0', 'bi_lstm_1/bi_lstm_block_3/layer_normalization_3/gamma:0', 'bi_lstm_1/bi_lstm_block_3/layer_normalization_3/beta:0'] when minimizing the loss.\n",
      "Epoch 1 batch 0 train loss: 0.0103 test loss: 0.0078\n",
      "Epoch 1 batch 100 train loss: 0.0079 test loss: 0.0080\n",
      "Epoch 1 batch 200 train loss: 0.0096 test loss: 0.0080\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.20p/ckpt-23\n",
      "Epoch 1 batch 300 train loss: 0.0097 test loss: 0.0076\n",
      "Epoch 1 batch 400 train loss: 0.0097 test loss: 0.0079\n",
      "Epoch 1 batch 500 train loss: 0.0087 test loss: 0.0078\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.20p/ckpt-24\n",
      "Epoch 1 batch 600 train loss: 0.0088 test loss: 0.0076\n",
      "Epoch 1 batch 700 train loss: 0.0074 test loss: 0.0080\n",
      "Epoch 1 batch 800 train loss: 0.0090 test loss: 0.0081\n",
      "Epoch 1 batch 900 train loss: 0.0096 test loss: 0.0081\n",
      "Epoch 1 batch 1000 train loss: 0.0076 test loss: 0.0079\n",
      "Epoch 1 batch 1100 train loss: 0.0075 test loss: 0.0081\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.20p/ckpt-25\n",
      "Epoch 1 batch 1200 train loss: 0.0066 test loss: 0.0074\n",
      "Epoch 1 batch 1300 train loss: 0.0099 test loss: 0.0077\n",
      "Epoch 1 batch 1400 train loss: 0.0088 test loss: 0.0079\n",
      "Epoch 1 batch 1500 train loss: 0.0075 test loss: 0.0077\n",
      "Epoch 1 batch 1600 train loss: 0.0107 test loss: 0.0076\n",
      "Epoch 1 batch 1700 train loss: 0.0099 test loss: 0.0080\n",
      "Epoch 1 batch 1800 train loss: 0.0071 test loss: 0.0076\n",
      "Epoch 1 batch 1900 train loss: 0.0088 test loss: 0.0084\n",
      "Epoch 1 batch 2000 train loss: 0.0095 test loss: 0.0077\n",
      "Epoch 1 batch 2100 train loss: 0.0073 test loss: 0.0079\n",
      "Epoch 1 batch 2200 train loss: 0.0107 test loss: 0.0076\n",
      "Epoch 1 batch 2300 train loss: 0.0089 test loss: 0.0075\n",
      "Epoch 1 batch 2400 train loss: 0.0082 test loss: 0.0075\n",
      "Epoch 1 batch 2500 train loss: 0.0093 test loss: 0.0078\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.20p/ckpt-26\n",
      "Epoch 1 batch 2600 train loss: 0.0062 test loss: 0.0074\n",
      "Epoch 1 batch 2700 train loss: 0.0072 test loss: 0.0075\n",
      "Epoch 1 batch 2800 train loss: 0.0095 test loss: 0.0074\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.20p/ckpt-27\n",
      "Epoch 1 batch 2900 train loss: 0.0081 test loss: 0.0071\n",
      "Epoch 2 batch 0 train loss: 0.0065 test loss: 0.0074\n",
      "Epoch 2 batch 100 train loss: 0.0092 test loss: 0.0073\n",
      "Epoch 2 batch 200 train loss: 0.0085 test loss: 0.0075\n",
      "Epoch 2 batch 300 train loss: 0.0083 test loss: 0.0073\n",
      "Epoch 2 batch 400 train loss: 0.0057 test loss: 0.0075\n",
      "Epoch 2 batch 500 train loss: 0.0069 test loss: 0.0079\n",
      "Epoch 2 batch 600 train loss: 0.0091 test loss: 0.0078\n",
      "Epoch 2 batch 700 train loss: 0.0097 test loss: 0.0072\n",
      "Epoch 2 batch 800 train loss: 0.0075 test loss: 0.0078\n",
      "Epoch 2 batch 900 train loss: 0.0073 test loss: 0.0074\n",
      "Epoch 2 batch 1000 train loss: 0.0091 test loss: 0.0078\n",
      "Epoch 2 batch 1100 train loss: 0.0083 test loss: 0.0073\n",
      "Epoch 2 batch 1200 train loss: 0.0090 test loss: 0.0072\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.20p/ckpt-28\n",
      "Epoch 2 batch 1300 train loss: 0.0064 test loss: 0.0069\n",
      "Epoch 2 batch 1400 train loss: 0.0066 test loss: 0.0074\n",
      "Epoch 2 batch 1500 train loss: 0.0107 test loss: 0.0078\n",
      "Epoch 2 batch 1600 train loss: 0.0083 test loss: 0.0082\n",
      "Epoch 2 batch 1700 train loss: 0.0071 test loss: 0.0079\n",
      "Epoch 2 batch 1800 train loss: 0.0088 test loss: 0.0072\n",
      "Epoch 2 batch 1900 train loss: 0.0110 test loss: 0.0078\n",
      "Epoch 2 batch 2000 train loss: 0.0081 test loss: 0.0077\n",
      "Epoch 2 batch 2100 train loss: 0.0060 test loss: 0.0075\n",
      "Epoch 2 batch 2200 train loss: 0.0090 test loss: 0.0080\n",
      "Epoch 2 batch 2300 train loss: 0.0081 test loss: 0.0080\n",
      "Epoch 2 batch 2400 train loss: 0.0067 test loss: 0.0074\n",
      "Epoch 2 batch 2500 train loss: 0.0047 test loss: 0.0077\n",
      "Epoch 2 batch 2600 train loss: 0.0071 test loss: 0.0073\n",
      "Epoch 2 batch 2700 train loss: 0.0067 test loss: 0.0082\n",
      "Epoch 2 batch 2800 train loss: 0.0072 test loss: 0.0069\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.20p/ckpt-29\n",
      "Epoch 2 batch 2900 train loss: 0.0080 test loss: 0.0068\n",
      "Epoch 3 batch 0 train loss: 0.0067 test loss: 0.0069\n",
      "Epoch 3 batch 100 train loss: 0.0081 test loss: 0.0071\n",
      "Epoch 3 batch 200 train loss: 0.0080 test loss: 0.0075\n",
      "Epoch 3 batch 300 train loss: 0.0074 test loss: 0.0069\n",
      "Epoch 3 batch 400 train loss: 0.0085 test loss: 0.0070\n",
      "Epoch 3 batch 500 train loss: 0.0069 test loss: 0.0070\n",
      "Epoch 3 batch 600 train loss: 0.0082 test loss: 0.0073\n",
      "Epoch 3 batch 700 train loss: 0.0097 test loss: 0.0073\n",
      "Epoch 3 batch 800 train loss: 0.0107 test loss: 0.0072\n",
      "Epoch 3 batch 900 train loss: 0.0075 test loss: 0.0071\n",
      "Epoch 3 batch 1000 train loss: 0.0068 test loss: 0.0077\n",
      "Epoch 3 batch 1100 train loss: 0.0077 test loss: 0.0069\n",
      "Epoch 3 batch 1200 train loss: 0.0079 test loss: 0.0071\n",
      "Epoch 3 batch 1300 train loss: 0.0086 test loss: 0.0075\n",
      "Epoch 3 batch 1400 train loss: 0.0089 test loss: 0.0077\n",
      "Epoch 3 batch 1500 train loss: 0.0059 test loss: 0.0076\n",
      "Epoch 3 batch 1600 train loss: 0.0094 test loss: 0.0073\n",
      "Epoch 3 batch 1700 train loss: 0.0072 test loss: 0.0073\n",
      "Epoch 3 batch 1800 train loss: 0.0073 test loss: 0.0070\n",
      "Epoch 3 batch 1900 train loss: 0.0075 test loss: 0.0074\n",
      "Epoch 3 batch 2000 train loss: 0.0095 test loss: 0.0070\n",
      "Epoch 3 batch 2100 train loss: 0.0076 test loss: 0.0075\n",
      "Epoch 3 batch 2200 train loss: 0.0074 test loss: 0.0072\n",
      "Epoch 3 batch 2300 train loss: 0.0084 test loss: 0.0071\n",
      "Epoch 3 batch 2400 train loss: 0.0066 test loss: 0.0077\n",
      "Epoch 3 batch 2500 train loss: 0.0104 test loss: 0.0074\n",
      "Epoch 3 batch 2600 train loss: 0.0091 test loss: 0.0070\n",
      "Epoch 3 batch 2700 train loss: 0.0066 test loss: 0.0081\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.20p/ckpt-30\n",
      "Epoch 3 batch 2800 train loss: 0.0083 test loss: 0.0068\n",
      "Epoch 3 batch 2900 train loss: 0.0075 test loss: 0.0071\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.20p/ckpt-31\n",
      "Epoch 4 batch 0 train loss: 0.0087 test loss: 0.0066\n",
      "Epoch 4 batch 100 train loss: 0.0068 test loss: 0.0074\n",
      "Epoch 4 batch 200 train loss: 0.0088 test loss: 0.0071\n",
      "Epoch 4 batch 300 train loss: 0.0081 test loss: 0.0069\n",
      "Epoch 4 batch 400 train loss: 0.0088 test loss: 0.0073\n",
      "Epoch 4 batch 500 train loss: 0.0074 test loss: 0.0077\n",
      "Epoch 4 batch 600 train loss: 0.0083 test loss: 0.0073\n",
      "Epoch 4 batch 700 train loss: 0.0085 test loss: 0.0073\n",
      "Epoch 4 batch 800 train loss: 0.0081 test loss: 0.0072\n",
      "Epoch 4 batch 900 train loss: 0.0078 test loss: 0.0073\n",
      "Epoch 4 batch 1000 train loss: 0.0071 test loss: 0.0071\n",
      "Epoch 4 batch 1100 train loss: 0.0067 test loss: 0.0070\n",
      "Epoch 4 batch 1200 train loss: 0.0089 test loss: 0.0069\n",
      "Epoch 4 batch 1300 train loss: 0.0114 test loss: 0.0076\n",
      "Epoch 4 batch 1400 train loss: 0.0058 test loss: 0.0073\n",
      "Epoch 4 batch 1500 train loss: 0.0072 test loss: 0.0071\n",
      "Epoch 4 batch 1600 train loss: 0.0077 test loss: 0.0072\n",
      "Epoch 4 batch 1700 train loss: 0.0070 test loss: 0.0074\n",
      "Epoch 4 batch 1800 train loss: 0.0091 test loss: 0.0075\n",
      "Epoch 4 batch 1900 train loss: 0.0085 test loss: 0.0075\n",
      "Epoch 4 batch 2000 train loss: 0.0063 test loss: 0.0074\n",
      "Epoch 4 batch 2100 train loss: 0.0063 test loss: 0.0072\n",
      "Epoch 4 batch 2200 train loss: 0.0074 test loss: 0.0071\n",
      "Epoch 4 batch 2300 train loss: 0.0067 test loss: 0.0075\n",
      "Epoch 4 batch 2400 train loss: 0.0064 test loss: 0.0072\n",
      "Epoch 4 batch 2500 train loss: 0.0085 test loss: 0.0069\n",
      "Epoch 4 batch 2600 train loss: 0.0052 test loss: 0.0069\n",
      "Epoch 4 batch 2700 train loss: 0.0086 test loss: 0.0072\n",
      "Epoch 4 batch 2800 train loss: 0.0073 test loss: 0.0069\n",
      "Epoch 4 batch 2900 train loss: 0.0071 test loss: 0.0068\n",
      "Epoch 5 batch 0 train loss: 0.0060 test loss: 0.0071\n",
      "Epoch 5 batch 100 train loss: 0.0063 test loss: 0.0070\n",
      "Epoch 5 batch 200 train loss: 0.0088 test loss: 0.0069\n",
      "Epoch 5 batch 300 train loss: 0.0069 test loss: 0.0071\n",
      "Epoch 5 batch 400 train loss: 0.0073 test loss: 0.0076\n",
      "Epoch 5 batch 500 train loss: 0.0069 test loss: 0.0073\n",
      "Epoch 5 batch 600 train loss: 0.0089 test loss: 0.0069\n",
      "Epoch 5 batch 700 train loss: 0.0065 test loss: 0.0074\n",
      "Epoch 5 batch 800 train loss: 0.0059 test loss: 0.0071\n",
      "Epoch 5 batch 900 train loss: 0.0066 test loss: 0.0068\n",
      "Epoch 5 batch 1000 train loss: 0.0073 test loss: 0.0070\n",
      "Epoch 5 batch 1100 train loss: 0.0076 test loss: 0.0073\n",
      "Epoch 5 batch 1200 train loss: 0.0075 test loss: 0.0073\n",
      "Epoch 5 batch 1300 train loss: 0.0065 test loss: 0.0072\n",
      "Epoch 5 batch 1400 train loss: 0.0076 test loss: 0.0073\n",
      "Epoch 5 batch 1500 train loss: 0.0084 test loss: 0.0072\n",
      "Epoch 5 batch 1600 train loss: 0.0051 test loss: 0.0070\n",
      "Epoch 5 batch 1700 train loss: 0.0093 test loss: 0.0069\n",
      "Epoch 5 batch 1800 train loss: 0.0067 test loss: 0.0069\n",
      "Epoch 5 batch 1900 train loss: 0.0059 test loss: 0.0076\n",
      "Epoch 5 batch 2000 train loss: 0.0092 test loss: 0.0068\n",
      "Epoch 5 batch 2100 train loss: 0.0085 test loss: 0.0074\n",
      "Epoch 5 batch 2200 train loss: 0.0059 test loss: 0.0072\n",
      "Epoch 5 batch 2300 train loss: 0.0069 test loss: 0.0080\n",
      "Epoch 5 batch 2400 train loss: 0.0074 test loss: 0.0070\n",
      "Epoch 5 batch 2500 train loss: 0.0067 test loss: 0.0070\n",
      "Epoch 5 batch 2600 train loss: 0.0039 test loss: 0.0069\n",
      "Epoch 5 batch 2700 train loss: 0.0087 test loss: 0.0071\n",
      "Epoch 5 batch 2800 train loss: 0.0074 test loss: 0.0068\n",
      "Epoch 5 batch 2900 train loss: 0.0066 test loss: 0.0069\n",
      "Epoch 6 batch 0 train loss: 0.0074 test loss: 0.0067\n",
      "Epoch 6 batch 100 train loss: 0.0054 test loss: 0.0072\n",
      "Epoch 6 batch 200 train loss: 0.0089 test loss: 0.0072\n",
      "Epoch 6 batch 300 train loss: 0.0109 test loss: 0.0072\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.20p/ckpt-32\n",
      "Epoch 6 batch 400 train loss: 0.0073 test loss: 0.0066\n",
      "Epoch 6 batch 500 train loss: 0.0063 test loss: 0.0072\n",
      "Epoch 6 batch 600 train loss: 0.0079 test loss: 0.0069\n",
      "Epoch 6 batch 700 train loss: 0.0085 test loss: 0.0068\n",
      "Epoch 6 batch 800 train loss: 0.0082 test loss: 0.0076\n",
      "Epoch 6 batch 900 train loss: 0.0079 test loss: 0.0071\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.20p/ckpt-33\n",
      "Epoch 6 batch 1000 train loss: 0.0065 test loss: 0.0065\n",
      "Epoch 6 batch 1100 train loss: 0.0071 test loss: 0.0070\n",
      "Epoch 6 batch 1200 train loss: 0.0079 test loss: 0.0069\n",
      "Epoch 6 batch 1300 train loss: 0.0062 test loss: 0.0073\n",
      "Epoch 6 batch 1400 train loss: 0.0077 test loss: 0.0069\n",
      "Epoch 6 batch 1500 train loss: 0.0078 test loss: 0.0070\n",
      "Epoch 6 batch 1600 train loss: 0.0068 test loss: 0.0071\n",
      "Epoch 6 batch 1700 train loss: 0.0062 test loss: 0.0078\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.20p/ckpt-34\n",
      "Epoch 6 batch 1800 train loss: 0.0059 test loss: 0.0064\n",
      "Epoch 6 batch 1900 train loss: 0.0076 test loss: 0.0070\n",
      "Epoch 6 batch 2000 train loss: 0.0064 test loss: 0.0072\n",
      "Epoch 6 batch 2100 train loss: 0.0074 test loss: 0.0072\n",
      "Epoch 6 batch 2200 train loss: 0.0113 test loss: 0.0076\n",
      "Epoch 6 batch 2300 train loss: 0.0101 test loss: 0.0071\n",
      "Epoch 6 batch 2400 train loss: 0.0080 test loss: 0.0070\n",
      "Epoch 6 batch 2500 train loss: 0.0084 test loss: 0.0072\n",
      "Epoch 6 batch 2600 train loss: 0.0083 test loss: 0.0073\n",
      "Epoch 6 batch 2700 train loss: 0.0075 test loss: 0.0070\n",
      "Epoch 6 batch 2800 train loss: 0.0068 test loss: 0.0069\n",
      "Epoch 6 batch 2900 train loss: 0.0070 test loss: 0.0068\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.20p/ckpt-35\n",
      "Epoch 7 batch 0 train loss: 0.0087 test loss: 0.0063\n",
      "Epoch 7 batch 100 train loss: 0.0085 test loss: 0.0074\n",
      "Epoch 7 batch 200 train loss: 0.0099 test loss: 0.0071\n",
      "Epoch 7 batch 300 train loss: 0.0105 test loss: 0.0073\n",
      "Epoch 7 batch 400 train loss: 0.0083 test loss: 0.0066\n",
      "Epoch 7 batch 500 train loss: 0.0079 test loss: 0.0074\n",
      "Epoch 7 batch 600 train loss: 0.0065 test loss: 0.0067\n",
      "Epoch 7 batch 700 train loss: 0.0087 test loss: 0.0071\n",
      "Epoch 7 batch 800 train loss: 0.0082 test loss: 0.0074\n",
      "Epoch 7 batch 900 train loss: 0.0075 test loss: 0.0074\n",
      "Epoch 7 batch 1000 train loss: 0.0084 test loss: 0.0071\n",
      "Epoch 7 batch 1100 train loss: 0.0065 test loss: 0.0072\n",
      "Epoch 7 batch 1200 train loss: 0.0070 test loss: 0.0066\n",
      "Epoch 7 batch 1300 train loss: 0.0078 test loss: 0.0072\n",
      "Epoch 7 batch 1400 train loss: 0.0052 test loss: 0.0067\n",
      "Epoch 7 batch 1500 train loss: 0.0053 test loss: 0.0074\n",
      "Epoch 7 batch 1600 train loss: 0.0115 test loss: 0.0068\n",
      "Epoch 7 batch 1700 train loss: 0.0060 test loss: 0.0074\n",
      "Epoch 7 batch 1800 train loss: 0.0086 test loss: 0.0072\n",
      "Epoch 7 batch 1900 train loss: 0.0065 test loss: 0.0072\n",
      "Epoch 7 batch 2000 train loss: 0.0085 test loss: 0.0075\n",
      "Epoch 7 batch 2100 train loss: 0.0069 test loss: 0.0072\n",
      "Epoch 7 batch 2200 train loss: 0.0080 test loss: 0.0073\n",
      "Epoch 7 batch 2300 train loss: 0.0059 test loss: 0.0073\n",
      "Epoch 7 batch 2400 train loss: 0.0087 test loss: 0.0073\n",
      "Epoch 7 batch 2500 train loss: 0.0087 test loss: 0.0073\n",
      "Epoch 7 batch 2600 train loss: 0.0075 test loss: 0.0070\n",
      "Epoch 7 batch 2700 train loss: 0.0042 test loss: 0.0068\n",
      "Epoch 7 batch 2800 train loss: 0.0106 test loss: 0.0064\n",
      "Epoch 7 batch 2900 train loss: 0.0064 test loss: 0.0065\n",
      "Epoch 8 batch 0 train loss: 0.0068 test loss: 0.0068\n",
      "Epoch 8 batch 100 train loss: 0.0068 test loss: 0.0069\n",
      "Epoch 8 batch 200 train loss: 0.0068 test loss: 0.0070\n",
      "Epoch 8 batch 300 train loss: 0.0065 test loss: 0.0066\n",
      "Epoch 8 batch 400 train loss: 0.0063 test loss: 0.0069\n",
      "Epoch 8 batch 500 train loss: 0.0051 test loss: 0.0072\n",
      "Epoch 8 batch 600 train loss: 0.0094 test loss: 0.0074\n",
      "Epoch 8 batch 700 train loss: 0.0069 test loss: 0.0072\n",
      "Epoch 8 batch 800 train loss: 0.0077 test loss: 0.0066\n",
      "Epoch 8 batch 900 train loss: 0.0051 test loss: 0.0069\n",
      "Epoch 8 batch 1000 train loss: 0.0072 test loss: 0.0072\n",
      "Epoch 8 batch 1100 train loss: 0.0062 test loss: 0.0070\n",
      "Epoch 8 batch 1200 train loss: 0.0099 test loss: 0.0070\n",
      "Epoch 8 batch 1300 train loss: 0.0073 test loss: 0.0070\n",
      "Epoch 8 batch 1400 train loss: 0.0061 test loss: 0.0068\n",
      "Epoch 8 batch 1500 train loss: 0.0056 test loss: 0.0076\n",
      "Epoch 8 batch 1600 train loss: 0.0081 test loss: 0.0072\n",
      "Epoch 8 batch 1700 train loss: 0.0069 test loss: 0.0075\n",
      "Epoch 8 batch 1800 train loss: 0.0071 test loss: 0.0074\n",
      "Epoch 8 batch 1900 train loss: 0.0057 test loss: 0.0068\n",
      "Epoch 8 batch 2000 train loss: 0.0074 test loss: 0.0066\n",
      "Epoch 8 batch 2100 train loss: 0.0068 test loss: 0.0073\n",
      "Epoch 8 batch 2200 train loss: 0.0065 test loss: 0.0073\n",
      "Epoch 8 batch 2300 train loss: 0.0102 test loss: 0.0073\n",
      "Epoch 8 batch 2400 train loss: 0.0074 test loss: 0.0070\n",
      "Epoch 8 batch 2500 train loss: 0.0084 test loss: 0.0066\n",
      "Epoch 8 batch 2600 train loss: 0.0076 test loss: 0.0070\n",
      "Epoch 8 batch 2700 train loss: 0.0103 test loss: 0.0072\n",
      "Epoch 8 batch 2800 train loss: 0.0079 test loss: 0.0068\n",
      "Epoch 8 batch 2900 train loss: 0.0069 test loss: 0.0068\n",
      "Epoch 9 batch 0 train loss: 0.0098 test loss: 0.0066\n",
      "Epoch 9 batch 100 train loss: 0.0079 test loss: 0.0070\n",
      "Epoch 9 batch 200 train loss: 0.0073 test loss: 0.0068\n",
      "Epoch 9 batch 300 train loss: 0.0087 test loss: 0.0068\n",
      "Epoch 9 batch 400 train loss: 0.0055 test loss: 0.0064\n",
      "Epoch 9 batch 500 train loss: 0.0097 test loss: 0.0070\n",
      "Epoch 9 batch 600 train loss: 0.0078 test loss: 0.0071\n",
      "Epoch 9 batch 700 train loss: 0.0101 test loss: 0.0077\n",
      "Epoch 9 batch 800 train loss: 0.0079 test loss: 0.0075\n",
      "Epoch 9 batch 900 train loss: 0.0070 test loss: 0.0071\n",
      "Epoch 9 batch 1000 train loss: 0.0074 test loss: 0.0073\n",
      "Epoch 9 batch 1100 train loss: 0.0084 test loss: 0.0071\n",
      "Epoch 9 batch 1200 train loss: 0.0043 test loss: 0.0070\n",
      "Epoch 9 batch 1300 train loss: 0.0072 test loss: 0.0073\n",
      "Epoch 9 batch 1400 train loss: 0.0071 test loss: 0.0070\n",
      "Epoch 9 batch 1500 train loss: 0.0090 test loss: 0.0072\n",
      "Epoch 9 batch 1600 train loss: 0.0074 test loss: 0.0075\n",
      "Epoch 9 batch 1700 train loss: 0.0056 test loss: 0.0071\n",
      "Epoch 9 batch 1800 train loss: 0.0082 test loss: 0.0070\n",
      "Epoch 9 batch 1900 train loss: 0.0060 test loss: 0.0070\n",
      "Epoch 9 batch 2000 train loss: 0.0080 test loss: 0.0071\n",
      "Epoch 9 batch 2100 train loss: 0.0093 test loss: 0.0072\n",
      "Epoch 9 batch 2200 train loss: 0.0063 test loss: 0.0071\n",
      "Epoch 9 batch 2300 train loss: 0.0076 test loss: 0.0075\n",
      "Epoch 9 batch 2400 train loss: 0.0092 test loss: 0.0073\n",
      "Epoch 9 batch 2500 train loss: 0.0053 test loss: 0.0064\n",
      "Epoch 9 batch 2600 train loss: 0.0067 test loss: 0.0075\n",
      "Epoch 9 batch 2700 train loss: 0.0071 test loss: 0.0080\n",
      "Epoch 9 batch 2800 train loss: 0.0082 test loss: 0.0072\n",
      "Epoch 9 batch 2900 train loss: 0.0090 test loss: 0.0066\n",
      "Epoch 10 batch 0 train loss: 0.0066 test loss: 0.0067\n",
      "Epoch 10 batch 100 train loss: 0.0064 test loss: 0.0072\n",
      "Epoch 10 batch 200 train loss: 0.0063 test loss: 0.0073\n",
      "Epoch 10 batch 300 train loss: 0.0057 test loss: 0.0068\n",
      "Epoch 10 batch 400 train loss: 0.0073 test loss: 0.0074\n",
      "Epoch 10 batch 500 train loss: 0.0079 test loss: 0.0071\n",
      "Epoch 10 batch 600 train loss: 0.0059 test loss: 0.0072\n",
      "Epoch 10 batch 700 train loss: 0.0084 test loss: 0.0071\n",
      "Epoch 10 batch 800 train loss: 0.0086 test loss: 0.0076\n",
      "Epoch 10 batch 900 train loss: 0.0062 test loss: 0.0069\n",
      "Epoch 10 batch 1000 train loss: 0.0071 test loss: 0.0067\n",
      "Epoch 10 batch 1100 train loss: 0.0097 test loss: 0.0078\n",
      "Epoch 10 batch 1200 train loss: 0.0074 test loss: 0.0068\n",
      "Epoch 10 batch 1300 train loss: 0.0068 test loss: 0.0072\n",
      "Epoch 10 batch 1400 train loss: 0.0063 test loss: 0.0076\n",
      "Epoch 10 batch 1500 train loss: 0.0075 test loss: 0.0074\n",
      "Epoch 10 batch 1600 train loss: 0.0086 test loss: 0.0069\n",
      "Epoch 10 batch 1700 train loss: 0.0064 test loss: 0.0074\n",
      "Epoch 10 batch 1800 train loss: 0.0087 test loss: 0.0066\n",
      "Epoch 10 batch 1900 train loss: 0.0076 test loss: 0.0075\n",
      "Epoch 10 batch 2000 train loss: 0.0061 test loss: 0.0071\n",
      "Epoch 10 batch 2100 train loss: 0.0081 test loss: 0.0072\n",
      "Epoch 10 batch 2200 train loss: 0.0103 test loss: 0.0068\n",
      "Epoch 10 batch 2300 train loss: 0.0083 test loss: 0.0063\n",
      "Epoch 10 batch 2400 train loss: 0.0105 test loss: 0.0067\n",
      "Epoch 10 batch 2500 train loss: 0.0066 test loss: 0.0071\n",
      "Epoch 10 batch 2600 train loss: 0.0085 test loss: 0.0070\n",
      "Epoch 10 batch 2700 train loss: 0.0095 test loss: 0.0071\n",
      "Epoch 10 batch 2800 train loss: 0.0070 test loss: 0.0065\n",
      "Epoch 10 batch 2900 train loss: 0.0067 test loss: 0.0063\n",
      "Epoch 11 batch 0 train loss: 0.0134 test loss: 0.0067\n",
      "Epoch 11 batch 100 train loss: 0.0085 test loss: 0.0067\n",
      "Epoch 11 batch 200 train loss: 0.0075 test loss: 0.0067\n",
      "Epoch 11 batch 300 train loss: 0.0053 test loss: 0.0067\n",
      "Epoch 11 batch 400 train loss: 0.0064 test loss: 0.0066\n",
      "Epoch 11 batch 500 train loss: 0.0061 test loss: 0.0071\n",
      "Epoch 11 batch 600 train loss: 0.0053 test loss: 0.0070\n",
      "Epoch 11 batch 700 train loss: 0.0082 test loss: 0.0067\n",
      "Epoch 11 batch 800 train loss: 0.0069 test loss: 0.0068\n",
      "Epoch 11 batch 900 train loss: 0.0070 test loss: 0.0071\n",
      "Epoch 11 batch 1000 train loss: 0.0075 test loss: 0.0071\n",
      "Epoch 11 batch 1100 train loss: 0.0071 test loss: 0.0070\n",
      "Epoch 11 batch 1200 train loss: 0.0077 test loss: 0.0067\n",
      "Epoch 11 batch 1300 train loss: 0.0057 test loss: 0.0073\n",
      "Epoch 11 batch 1400 train loss: 0.0090 test loss: 0.0069\n",
      "Epoch 11 batch 1500 train loss: 0.0064 test loss: 0.0068\n",
      "Epoch 11 batch 1600 train loss: 0.0061 test loss: 0.0071\n",
      "Epoch 11 batch 1700 train loss: 0.0092 test loss: 0.0064\n",
      "Epoch 11 batch 1800 train loss: 0.0067 test loss: 0.0070\n",
      "Epoch 11 batch 1900 train loss: 0.0071 test loss: 0.0070\n",
      "Epoch 11 batch 2000 train loss: 0.0049 test loss: 0.0068\n",
      "Epoch 11 batch 2100 train loss: 0.0114 test loss: 0.0063\n",
      "Epoch 11 batch 2200 train loss: 0.0071 test loss: 0.0070\n",
      "Epoch 11 batch 2300 train loss: 0.0067 test loss: 0.0072\n",
      "Epoch 11 batch 2400 train loss: 0.0096 test loss: 0.0068\n",
      "Epoch 11 batch 2500 train loss: 0.0070 test loss: 0.0070\n",
      "Epoch 11 batch 2600 train loss: 0.0082 test loss: 0.0074\n",
      "Epoch 11 batch 2700 train loss: 0.0092 test loss: 0.0073\n",
      "Epoch 11 batch 2800 train loss: 0.0083 test loss: 0.0065\n",
      "Epoch 11 batch 2900 train loss: 0.0072 test loss: 0.0066\n",
      "Epoch 12 batch 0 train loss: 0.0066 test loss: 0.0065\n",
      "Epoch 12 batch 100 train loss: 0.0073 test loss: 0.0068\n",
      "Epoch 12 batch 200 train loss: 0.0068 test loss: 0.0068\n",
      "Epoch 12 batch 300 train loss: 0.0076 test loss: 0.0065\n",
      "Epoch 12 batch 400 train loss: 0.0064 test loss: 0.0066\n",
      "Epoch 12 batch 500 train loss: 0.0063 test loss: 0.0073\n",
      "Epoch 12 batch 600 train loss: 0.0073 test loss: 0.0066\n",
      "Epoch 12 batch 700 train loss: 0.0042 test loss: 0.0069\n",
      "Epoch 12 batch 800 train loss: 0.0109 test loss: 0.0075\n",
      "Epoch 12 batch 900 train loss: 0.0086 test loss: 0.0070\n",
      "Epoch 12 batch 1000 train loss: 0.0079 test loss: 0.0071\n",
      "Epoch 12 batch 1100 train loss: 0.0060 test loss: 0.0071\n",
      "Epoch 12 batch 1200 train loss: 0.0065 test loss: 0.0070\n",
      "Epoch 12 batch 1300 train loss: 0.0062 test loss: 0.0073\n",
      "Epoch 12 batch 1400 train loss: 0.0068 test loss: 0.0072\n",
      "Epoch 12 batch 1500 train loss: 0.0062 test loss: 0.0068\n",
      "Epoch 12 batch 1600 train loss: 0.0091 test loss: 0.0072\n",
      "Epoch 12 batch 1700 train loss: 0.0082 test loss: 0.0070\n",
      "Epoch 12 batch 1800 train loss: 0.0081 test loss: 0.0069\n",
      "Epoch 12 batch 1900 train loss: 0.0067 test loss: 0.0074\n",
      "Epoch 12 batch 2000 train loss: 0.0083 test loss: 0.0065\n",
      "Epoch 12 batch 2100 train loss: 0.0073 test loss: 0.0064\n",
      "Epoch 12 batch 2200 train loss: 0.0099 test loss: 0.0069\n",
      "Epoch 12 batch 2300 train loss: 0.0052 test loss: 0.0069\n",
      "Epoch 12 batch 2400 train loss: 0.0073 test loss: 0.0069\n",
      "Epoch 12 batch 2500 train loss: 0.0064 test loss: 0.0068\n",
      "Epoch 12 batch 2600 train loss: 0.0067 test loss: 0.0068\n",
      "Epoch 12 batch 2700 train loss: 0.0087 test loss: 0.0071\n",
      "Epoch 12 batch 2800 train loss: 0.0065 test loss: 0.0066\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.20p/ckpt-36\n",
      "Epoch 12 batch 2900 train loss: 0.0080 test loss: 0.0060\n",
      "Epoch 13 batch 0 train loss: 0.0073 test loss: 0.0069\n",
      "Epoch 13 batch 100 train loss: 0.0057 test loss: 0.0071\n",
      "Epoch 13 batch 200 train loss: 0.0060 test loss: 0.0069\n",
      "Epoch 13 batch 300 train loss: 0.0053 test loss: 0.0065\n",
      "Epoch 13 batch 400 train loss: 0.0080 test loss: 0.0067\n",
      "Epoch 13 batch 500 train loss: 0.0076 test loss: 0.0065\n",
      "Epoch 13 batch 600 train loss: 0.0062 test loss: 0.0068\n",
      "Epoch 13 batch 700 train loss: 0.0056 test loss: 0.0066\n",
      "Epoch 13 batch 800 train loss: 0.0049 test loss: 0.0068\n",
      "Epoch 13 batch 900 train loss: 0.0093 test loss: 0.0069\n",
      "Epoch 13 batch 1000 train loss: 0.0077 test loss: 0.0067\n",
      "Epoch 13 batch 1100 train loss: 0.0075 test loss: 0.0069\n",
      "Epoch 13 batch 1200 train loss: 0.0081 test loss: 0.0065\n",
      "Epoch 13 batch 1300 train loss: 0.0084 test loss: 0.0070\n",
      "Epoch 13 batch 1400 train loss: 0.0062 test loss: 0.0070\n",
      "Epoch 13 batch 1500 train loss: 0.0052 test loss: 0.0072\n",
      "Epoch 13 batch 1600 train loss: 0.0103 test loss: 0.0070\n",
      "Epoch 13 batch 1700 train loss: 0.0079 test loss: 0.0070\n",
      "Epoch 13 batch 1800 train loss: 0.0077 test loss: 0.0067\n",
      "Epoch 13 batch 1900 train loss: 0.0063 test loss: 0.0064\n",
      "Epoch 13 batch 2000 train loss: 0.0081 test loss: 0.0070\n",
      "Epoch 13 batch 2100 train loss: 0.0097 test loss: 0.0070\n",
      "Epoch 13 batch 2200 train loss: 0.0076 test loss: 0.0073\n",
      "Epoch 13 batch 2300 train loss: 0.0059 test loss: 0.0075\n",
      "Epoch 13 batch 2400 train loss: 0.0068 test loss: 0.0073\n",
      "Epoch 13 batch 2500 train loss: 0.0068 test loss: 0.0067\n",
      "Epoch 13 batch 2600 train loss: 0.0067 test loss: 0.0064\n",
      "Epoch 13 batch 2700 train loss: 0.0073 test loss: 0.0068\n",
      "Epoch 13 batch 2800 train loss: 0.0082 test loss: 0.0061\n",
      "Epoch 13 batch 2900 train loss: 0.0073 test loss: 0.0067\n",
      "Epoch 14 batch 0 train loss: 0.0087 test loss: 0.0067\n",
      "Epoch 14 batch 100 train loss: 0.0087 test loss: 0.0071\n",
      "Epoch 14 batch 200 train loss: 0.0077 test loss: 0.0069\n",
      "Epoch 14 batch 300 train loss: 0.0075 test loss: 0.0065\n",
      "Epoch 14 batch 400 train loss: 0.0060 test loss: 0.0070\n",
      "Epoch 14 batch 500 train loss: 0.0078 test loss: 0.0074\n",
      "Epoch 14 batch 600 train loss: 0.0060 test loss: 0.0069\n",
      "Epoch 14 batch 700 train loss: 0.0101 test loss: 0.0064\n",
      "Epoch 14 batch 800 train loss: 0.0049 test loss: 0.0066\n",
      "Epoch 14 batch 900 train loss: 0.0070 test loss: 0.0070\n",
      "Epoch 14 batch 1000 train loss: 0.0076 test loss: 0.0070\n",
      "Epoch 14 batch 1100 train loss: 0.0071 test loss: 0.0072\n",
      "Epoch 14 batch 1200 train loss: 0.0082 test loss: 0.0068\n",
      "Epoch 14 batch 1300 train loss: 0.0082 test loss: 0.0070\n",
      "Epoch 14 batch 1400 train loss: 0.0074 test loss: 0.0072\n",
      "Epoch 14 batch 1500 train loss: 0.0063 test loss: 0.0065\n",
      "Epoch 14 batch 1600 train loss: 0.0073 test loss: 0.0073\n",
      "Epoch 14 batch 1700 train loss: 0.0066 test loss: 0.0070\n",
      "Epoch 14 batch 1800 train loss: 0.0084 test loss: 0.0069\n",
      "Epoch 14 batch 1900 train loss: 0.0100 test loss: 0.0072\n",
      "Epoch 14 batch 2000 train loss: 0.0084 test loss: 0.0072\n",
      "Epoch 14 batch 2100 train loss: 0.0069 test loss: 0.0064\n",
      "Epoch 14 batch 2200 train loss: 0.0055 test loss: 0.0070\n",
      "Epoch 14 batch 2300 train loss: 0.0084 test loss: 0.0073\n",
      "Epoch 14 batch 2400 train loss: 0.0083 test loss: 0.0067\n",
      "Epoch 14 batch 2500 train loss: 0.0075 test loss: 0.0068\n",
      "Epoch 14 batch 2600 train loss: 0.0073 test loss: 0.0066\n",
      "Epoch 14 batch 2700 train loss: 0.0072 test loss: 0.0072\n",
      "Epoch 14 batch 2800 train loss: 0.0082 test loss: 0.0064\n",
      "Epoch 14 batch 2900 train loss: 0.0131 test loss: 0.0065\n",
      "Epoch 15 batch 0 train loss: 0.0076 test loss: 0.0064\n",
      "Epoch 15 batch 100 train loss: 0.0059 test loss: 0.0067\n",
      "Epoch 15 batch 200 train loss: 0.0067 test loss: 0.0066\n",
      "Epoch 15 batch 300 train loss: 0.0078 test loss: 0.0069\n",
      "Epoch 15 batch 400 train loss: 0.0101 test loss: 0.0068\n",
      "Epoch 15 batch 500 train loss: 0.0056 test loss: 0.0070\n",
      "Epoch 15 batch 600 train loss: 0.0064 test loss: 0.0072\n",
      "Epoch 15 batch 700 train loss: 0.0062 test loss: 0.0068\n",
      "Epoch 15 batch 800 train loss: 0.0107 test loss: 0.0071\n",
      "Epoch 15 batch 900 train loss: 0.0060 test loss: 0.0069\n",
      "Epoch 15 batch 1000 train loss: 0.0066 test loss: 0.0071\n",
      "Epoch 15 batch 1100 train loss: 0.0085 test loss: 0.0069\n",
      "Epoch 15 batch 1200 train loss: 0.0064 test loss: 0.0071\n",
      "Epoch 15 batch 1300 train loss: 0.0079 test loss: 0.0065\n",
      "Epoch 15 batch 1400 train loss: 0.0081 test loss: 0.0069\n",
      "Epoch 15 batch 1500 train loss: 0.0075 test loss: 0.0070\n",
      "Epoch 15 batch 1600 train loss: 0.0076 test loss: 0.0071\n",
      "Epoch 15 batch 1700 train loss: 0.0085 test loss: 0.0070\n",
      "Epoch 15 batch 1800 train loss: 0.0069 test loss: 0.0070\n",
      "Epoch 15 batch 1900 train loss: 0.0066 test loss: 0.0065\n",
      "Epoch 15 batch 2000 train loss: 0.0075 test loss: 0.0071\n",
      "Epoch 15 batch 2100 train loss: 0.0088 test loss: 0.0065\n",
      "Epoch 15 batch 2200 train loss: 0.0068 test loss: 0.0068\n",
      "Epoch 15 batch 2300 train loss: 0.0076 test loss: 0.0064\n",
      "Epoch 15 batch 2400 train loss: 0.0069 test loss: 0.0074\n",
      "Epoch 15 batch 2500 train loss: 0.0069 test loss: 0.0063\n",
      "Epoch 15 batch 2600 train loss: 0.0077 test loss: 0.0067\n",
      "Epoch 15 batch 2700 train loss: 0.0089 test loss: 0.0068\n",
      "Epoch 15 batch 2800 train loss: 0.0106 test loss: 0.0069\n",
      "Epoch 15 batch 2900 train loss: 0.0066 test loss: 0.0065\n",
      "Epoch 16 batch 0 train loss: 0.0088 test loss: 0.0066\n",
      "Epoch 16 batch 100 train loss: 0.0091 test loss: 0.0066\n",
      "Epoch 16 batch 200 train loss: 0.0078 test loss: 0.0068\n",
      "Epoch 16 batch 300 train loss: 0.0065 test loss: 0.0067\n",
      "Epoch 16 batch 400 train loss: 0.0068 test loss: 0.0071\n",
      "Epoch 16 batch 500 train loss: 0.0065 test loss: 0.0070\n",
      "Epoch 16 batch 600 train loss: 0.0063 test loss: 0.0073\n",
      "Epoch 16 batch 700 train loss: 0.0059 test loss: 0.0067\n",
      "Epoch 16 batch 800 train loss: 0.0079 test loss: 0.0069\n",
      "Epoch 16 batch 900 train loss: 0.0071 test loss: 0.0064\n",
      "Epoch 16 batch 1000 train loss: 0.0070 test loss: 0.0074\n",
      "Epoch 16 batch 1100 train loss: 0.0077 test loss: 0.0064\n",
      "Epoch 16 batch 1200 train loss: 0.0100 test loss: 0.0068\n",
      "Epoch 16 batch 1300 train loss: 0.0067 test loss: 0.0069\n",
      "Epoch 16 batch 1400 train loss: 0.0071 test loss: 0.0070\n",
      "Epoch 16 batch 1500 train loss: 0.0065 test loss: 0.0068\n",
      "Epoch 16 batch 1600 train loss: 0.0070 test loss: 0.0069\n",
      "Epoch 16 batch 1700 train loss: 0.0072 test loss: 0.0068\n",
      "Epoch 16 batch 1800 train loss: 0.0089 test loss: 0.0066\n",
      "Epoch 16 batch 1900 train loss: 0.0080 test loss: 0.0071\n",
      "Epoch 16 batch 2000 train loss: 0.0078 test loss: 0.0069\n",
      "Epoch 16 batch 2100 train loss: 0.0048 test loss: 0.0071\n",
      "Epoch 16 batch 2200 train loss: 0.0075 test loss: 0.0069\n",
      "Epoch 16 batch 2300 train loss: 0.0063 test loss: 0.0072\n",
      "Epoch 16 batch 2400 train loss: 0.0079 test loss: 0.0066\n",
      "Epoch 16 batch 2500 train loss: 0.0089 test loss: 0.0072\n",
      "Epoch 16 batch 2600 train loss: 0.0063 test loss: 0.0068\n",
      "Epoch 16 batch 2700 train loss: 0.0073 test loss: 0.0075\n",
      "Epoch 16 batch 2800 train loss: 0.0085 test loss: 0.0063\n",
      "Epoch 16 batch 2900 train loss: 0.0094 test loss: 0.0064\n",
      "Epoch 17 batch 0 train loss: 0.0061 test loss: 0.0069\n",
      "Epoch 17 batch 100 train loss: 0.0052 test loss: 0.0070\n",
      "Epoch 17 batch 200 train loss: 0.0080 test loss: 0.0069\n",
      "Epoch 17 batch 300 train loss: 0.0092 test loss: 0.0066\n",
      "Epoch 17 batch 400 train loss: 0.0058 test loss: 0.0067\n",
      "Epoch 17 batch 500 train loss: 0.0071 test loss: 0.0072\n",
      "Epoch 17 batch 600 train loss: 0.0063 test loss: 0.0072\n",
      "Epoch 17 batch 700 train loss: 0.0074 test loss: 0.0066\n",
      "Epoch 17 batch 800 train loss: 0.0060 test loss: 0.0074\n",
      "Epoch 17 batch 900 train loss: 0.0077 test loss: 0.0067\n",
      "Epoch 17 batch 1000 train loss: 0.0066 test loss: 0.0070\n",
      "Epoch 17 batch 1100 train loss: 0.0081 test loss: 0.0071\n",
      "Epoch 17 batch 1200 train loss: 0.0071 test loss: 0.0066\n",
      "Epoch 17 batch 1300 train loss: 0.0068 test loss: 0.0069\n",
      "Epoch 17 batch 1400 train loss: 0.0076 test loss: 0.0080\n",
      "Epoch 17 batch 1500 train loss: 0.0063 test loss: 0.0068\n",
      "Epoch 17 batch 1600 train loss: 0.0081 test loss: 0.0066\n",
      "Epoch 17 batch 1700 train loss: 0.0076 test loss: 0.0071\n",
      "Epoch 17 batch 1800 train loss: 0.0084 test loss: 0.0067\n",
      "Epoch 17 batch 1900 train loss: 0.0074 test loss: 0.0067\n",
      "Epoch 17 batch 2000 train loss: 0.0071 test loss: 0.0069\n",
      "Epoch 17 batch 2100 train loss: 0.0081 test loss: 0.0070\n",
      "Epoch 17 batch 2200 train loss: 0.0063 test loss: 0.0073\n",
      "Epoch 17 batch 2300 train loss: 0.0064 test loss: 0.0070\n",
      "Epoch 17 batch 2400 train loss: 0.0071 test loss: 0.0070\n",
      "Epoch 17 batch 2500 train loss: 0.0091 test loss: 0.0074\n",
      "early stop.\n",
      "Checkpoint 36 restored!!\n",
      "Training for loss rate 0.30 start.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['bi_lstm_2/bi_lstm_block_4/bidirectional_4/forward_lstm_4/lstm_cell_13/kernel:0', 'bi_lstm_2/bi_lstm_block_4/bidirectional_4/forward_lstm_4/lstm_cell_13/recurrent_kernel:0', 'bi_lstm_2/bi_lstm_block_4/bidirectional_4/forward_lstm_4/lstm_cell_13/bias:0', 'bi_lstm_2/bi_lstm_block_4/bidirectional_4/backward_lstm_4/lstm_cell_14/kernel:0', 'bi_lstm_2/bi_lstm_block_4/bidirectional_4/backward_lstm_4/lstm_cell_14/recurrent_kernel:0', 'bi_lstm_2/bi_lstm_block_4/bidirectional_4/backward_lstm_4/lstm_cell_14/bias:0', 'bi_lstm_2/bi_lstm_block_4/layer_normalization_4/gamma:0', 'bi_lstm_2/bi_lstm_block_4/layer_normalization_4/beta:0', 'bi_lstm_2/bi_lstm_block_5/bidirectional_5/forward_lstm_5/lstm_cell_16/kernel:0', 'bi_lstm_2/bi_lstm_block_5/bidirectional_5/forward_lstm_5/lstm_cell_16/recurrent_kernel:0', 'bi_lstm_2/bi_lstm_block_5/bidirectional_5/forward_lstm_5/lstm_cell_16/bias:0', 'bi_lstm_2/bi_lstm_block_5/bidirectional_5/backward_lstm_5/lstm_cell_17/kernel:0', 'bi_lstm_2/bi_lstm_block_5/bidirectional_5/backward_lstm_5/lstm_cell_17/recurrent_kernel:0', 'bi_lstm_2/bi_lstm_block_5/bidirectional_5/backward_lstm_5/lstm_cell_17/bias:0', 'bi_lstm_2/bi_lstm_block_5/layer_normalization_5/gamma:0', 'bi_lstm_2/bi_lstm_block_5/layer_normalization_5/beta:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['bi_lstm_2/bi_lstm_block_4/bidirectional_4/forward_lstm_4/lstm_cell_13/kernel:0', 'bi_lstm_2/bi_lstm_block_4/bidirectional_4/forward_lstm_4/lstm_cell_13/recurrent_kernel:0', 'bi_lstm_2/bi_lstm_block_4/bidirectional_4/forward_lstm_4/lstm_cell_13/bias:0', 'bi_lstm_2/bi_lstm_block_4/bidirectional_4/backward_lstm_4/lstm_cell_14/kernel:0', 'bi_lstm_2/bi_lstm_block_4/bidirectional_4/backward_lstm_4/lstm_cell_14/recurrent_kernel:0', 'bi_lstm_2/bi_lstm_block_4/bidirectional_4/backward_lstm_4/lstm_cell_14/bias:0', 'bi_lstm_2/bi_lstm_block_4/layer_normalization_4/gamma:0', 'bi_lstm_2/bi_lstm_block_4/layer_normalization_4/beta:0', 'bi_lstm_2/bi_lstm_block_5/bidirectional_5/forward_lstm_5/lstm_cell_16/kernel:0', 'bi_lstm_2/bi_lstm_block_5/bidirectional_5/forward_lstm_5/lstm_cell_16/recurrent_kernel:0', 'bi_lstm_2/bi_lstm_block_5/bidirectional_5/forward_lstm_5/lstm_cell_16/bias:0', 'bi_lstm_2/bi_lstm_block_5/bidirectional_5/backward_lstm_5/lstm_cell_17/kernel:0', 'bi_lstm_2/bi_lstm_block_5/bidirectional_5/backward_lstm_5/lstm_cell_17/recurrent_kernel:0', 'bi_lstm_2/bi_lstm_block_5/bidirectional_5/backward_lstm_5/lstm_cell_17/bias:0', 'bi_lstm_2/bi_lstm_block_5/layer_normalization_5/gamma:0', 'bi_lstm_2/bi_lstm_block_5/layer_normalization_5/beta:0'] when minimizing the loss.\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.30p/ckpt-1\n",
      "Epoch 0 batch 0 train loss: 0.5392 test loss: 0.6939\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.30p/ckpt-2\n",
      "Epoch 0 batch 100 train loss: 0.2873 test loss: 0.2835\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.30p/ckpt-3\n",
      "Epoch 0 batch 200 train loss: 0.1321 test loss: 0.1479\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.30p/ckpt-4\n",
      "Epoch 0 batch 300 train loss: 0.0913 test loss: 0.0866\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.30p/ckpt-5\n",
      "Epoch 0 batch 400 train loss: 0.0570 test loss: 0.0547\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.30p/ckpt-6\n",
      "Epoch 0 batch 500 train loss: 0.0338 test loss: 0.0362\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.30p/ckpt-7\n",
      "Epoch 0 batch 600 train loss: 0.0185 test loss: 0.0248\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.30p/ckpt-8\n",
      "Epoch 0 batch 700 train loss: 0.0155 test loss: 0.0181\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.30p/ckpt-9\n",
      "Epoch 0 batch 800 train loss: 0.0117 test loss: 0.0144\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.30p/ckpt-10\n",
      "Epoch 0 batch 900 train loss: 0.0113 test loss: 0.0123\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.30p/ckpt-11\n",
      "Epoch 0 batch 1000 train loss: 0.0109 test loss: 0.0114\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.30p/ckpt-12\n",
      "Epoch 0 batch 1100 train loss: 0.0080 test loss: 0.0101\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.30p/ckpt-13\n",
      "Epoch 0 batch 1200 train loss: 0.0131 test loss: 0.0098\n",
      "Epoch 0 batch 1300 train loss: 0.0104 test loss: 0.0098\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.30p/ckpt-14\n",
      "Epoch 0 batch 1400 train loss: 0.0068 test loss: 0.0095\n",
      "Epoch 0 batch 1500 train loss: 0.0080 test loss: 0.0097\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.30p/ckpt-15\n",
      "Epoch 0 batch 1600 train loss: 0.0104 test loss: 0.0088\n",
      "Epoch 0 batch 1700 train loss: 0.0084 test loss: 0.0088\n",
      "Epoch 0 batch 1800 train loss: 0.0089 test loss: 0.0089\n",
      "Epoch 0 batch 1900 train loss: 0.0079 test loss: 0.0090\n",
      "Epoch 0 batch 2000 train loss: 0.0069 test loss: 0.0092\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.30p/ckpt-16\n",
      "Epoch 0 batch 2100 train loss: 0.0091 test loss: 0.0087\n",
      "Epoch 0 batch 2200 train loss: 0.0080 test loss: 0.0095\n",
      "Epoch 0 batch 2300 train loss: 0.0086 test loss: 0.0089\n",
      "Epoch 0 batch 2400 train loss: 0.0075 test loss: 0.0093\n",
      "Epoch 0 batch 2500 train loss: 0.0088 test loss: 0.0092\n",
      "Epoch 0 batch 2600 train loss: 0.0075 test loss: 0.0090\n",
      "Epoch 0 batch 2700 train loss: 0.0099 test loss: 0.0088\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.30p/ckpt-17\n",
      "Epoch 0 batch 2800 train loss: 0.0077 test loss: 0.0086\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.30p/ckpt-18\n",
      "Epoch 0 batch 2900 train loss: 0.0076 test loss: 0.0085\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['bi_lstm_2/bi_lstm_block_4/bidirectional_4/forward_lstm_4/lstm_cell_13/kernel:0', 'bi_lstm_2/bi_lstm_block_4/bidirectional_4/forward_lstm_4/lstm_cell_13/recurrent_kernel:0', 'bi_lstm_2/bi_lstm_block_4/bidirectional_4/forward_lstm_4/lstm_cell_13/bias:0', 'bi_lstm_2/bi_lstm_block_4/bidirectional_4/backward_lstm_4/lstm_cell_14/kernel:0', 'bi_lstm_2/bi_lstm_block_4/bidirectional_4/backward_lstm_4/lstm_cell_14/recurrent_kernel:0', 'bi_lstm_2/bi_lstm_block_4/bidirectional_4/backward_lstm_4/lstm_cell_14/bias:0', 'bi_lstm_2/bi_lstm_block_4/layer_normalization_4/gamma:0', 'bi_lstm_2/bi_lstm_block_4/layer_normalization_4/beta:0', 'bi_lstm_2/bi_lstm_block_5/bidirectional_5/forward_lstm_5/lstm_cell_16/kernel:0', 'bi_lstm_2/bi_lstm_block_5/bidirectional_5/forward_lstm_5/lstm_cell_16/recurrent_kernel:0', 'bi_lstm_2/bi_lstm_block_5/bidirectional_5/forward_lstm_5/lstm_cell_16/bias:0', 'bi_lstm_2/bi_lstm_block_5/bidirectional_5/backward_lstm_5/lstm_cell_17/kernel:0', 'bi_lstm_2/bi_lstm_block_5/bidirectional_5/backward_lstm_5/lstm_cell_17/recurrent_kernel:0', 'bi_lstm_2/bi_lstm_block_5/bidirectional_5/backward_lstm_5/lstm_cell_17/bias:0', 'bi_lstm_2/bi_lstm_block_5/layer_normalization_5/gamma:0', 'bi_lstm_2/bi_lstm_block_5/layer_normalization_5/beta:0'] when minimizing the loss.\n",
      "Epoch 1 batch 0 train loss: 0.0082 test loss: 0.0086\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.30p/ckpt-19\n",
      "Epoch 1 batch 100 train loss: 0.0073 test loss: 0.0083\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.30p/ckpt-20\n",
      "Epoch 1 batch 200 train loss: 0.0083 test loss: 0.0082\n",
      "Epoch 1 batch 300 train loss: 0.0097 test loss: 0.0086\n",
      "Epoch 1 batch 400 train loss: 0.0087 test loss: 0.0093\n",
      "Epoch 1 batch 500 train loss: 0.0074 test loss: 0.0091\n",
      "Epoch 1 batch 600 train loss: 0.0098 test loss: 0.0086\n",
      "Epoch 1 batch 700 train loss: 0.0075 test loss: 0.0086\n",
      "Epoch 1 batch 800 train loss: 0.0073 test loss: 0.0088\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.30p/ckpt-21\n",
      "Epoch 1 batch 900 train loss: 0.0094 test loss: 0.0080\n",
      "Epoch 1 batch 1000 train loss: 0.0068 test loss: 0.0090\n",
      "Epoch 1 batch 1100 train loss: 0.0081 test loss: 0.0086\n",
      "Epoch 1 batch 1200 train loss: 0.0070 test loss: 0.0091\n",
      "Epoch 1 batch 1300 train loss: 0.0076 test loss: 0.0089\n",
      "Epoch 1 batch 1400 train loss: 0.0093 test loss: 0.0085\n",
      "Epoch 1 batch 1500 train loss: 0.0066 test loss: 0.0086\n",
      "Epoch 1 batch 1600 train loss: 0.0071 test loss: 0.0085\n",
      "Epoch 1 batch 1700 train loss: 0.0072 test loss: 0.0089\n",
      "Epoch 1 batch 1800 train loss: 0.0065 test loss: 0.0089\n",
      "Epoch 1 batch 1900 train loss: 0.0066 test loss: 0.0092\n",
      "Epoch 1 batch 2000 train loss: 0.0090 test loss: 0.0083\n",
      "Epoch 1 batch 2100 train loss: 0.0106 test loss: 0.0089\n",
      "Epoch 1 batch 2200 train loss: 0.0071 test loss: 0.0084\n",
      "Epoch 1 batch 2300 train loss: 0.0067 test loss: 0.0088\n",
      "Epoch 1 batch 2400 train loss: 0.0075 test loss: 0.0088\n",
      "Epoch 1 batch 2500 train loss: 0.0068 test loss: 0.0091\n",
      "Epoch 1 batch 2600 train loss: 0.0092 test loss: 0.0087\n",
      "Epoch 1 batch 2700 train loss: 0.0075 test loss: 0.0087\n",
      "Epoch 1 batch 2800 train loss: 0.0062 test loss: 0.0085\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.30p/ckpt-22\n",
      "Epoch 1 batch 2900 train loss: 0.0069 test loss: 0.0080\n",
      "Epoch 2 batch 0 train loss: 0.0063 test loss: 0.0080\n",
      "Epoch 2 batch 100 train loss: 0.0059 test loss: 0.0084\n",
      "Epoch 2 batch 200 train loss: 0.0077 test loss: 0.0085\n",
      "Epoch 2 batch 300 train loss: 0.0074 test loss: 0.0086\n",
      "Epoch 2 batch 400 train loss: 0.0061 test loss: 0.0083\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.30p/ckpt-23\n",
      "Epoch 2 batch 500 train loss: 0.0074 test loss: 0.0076\n",
      "Epoch 2 batch 600 train loss: 0.0091 test loss: 0.0083\n",
      "Epoch 2 batch 700 train loss: 0.0071 test loss: 0.0087\n",
      "Epoch 2 batch 800 train loss: 0.0069 test loss: 0.0086\n",
      "Epoch 2 batch 900 train loss: 0.0088 test loss: 0.0083\n",
      "Epoch 2 batch 1000 train loss: 0.0065 test loss: 0.0084\n",
      "Epoch 2 batch 1100 train loss: 0.0069 test loss: 0.0078\n",
      "Epoch 2 batch 1200 train loss: 0.0085 test loss: 0.0082\n",
      "Epoch 2 batch 1300 train loss: 0.0104 test loss: 0.0085\n",
      "Epoch 2 batch 1400 train loss: 0.0076 test loss: 0.0089\n",
      "Epoch 2 batch 1500 train loss: 0.0057 test loss: 0.0086\n",
      "Epoch 2 batch 1600 train loss: 0.0082 test loss: 0.0083\n",
      "Epoch 2 batch 1700 train loss: 0.0064 test loss: 0.0092\n",
      "Epoch 2 batch 1800 train loss: 0.0068 test loss: 0.0087\n",
      "Epoch 2 batch 1900 train loss: 0.0061 test loss: 0.0085\n",
      "Epoch 2 batch 2000 train loss: 0.0081 test loss: 0.0088\n",
      "Epoch 2 batch 2100 train loss: 0.0073 test loss: 0.0082\n",
      "Epoch 2 batch 2200 train loss: 0.0079 test loss: 0.0079\n",
      "Epoch 2 batch 2300 train loss: 0.0072 test loss: 0.0081\n",
      "Epoch 2 batch 2400 train loss: 0.0070 test loss: 0.0079\n",
      "Epoch 2 batch 2500 train loss: 0.0093 test loss: 0.0085\n",
      "Epoch 2 batch 2600 train loss: 0.0082 test loss: 0.0086\n",
      "Epoch 2 batch 2700 train loss: 0.0066 test loss: 0.0082\n",
      "Epoch 2 batch 2800 train loss: 0.0098 test loss: 0.0079\n",
      "Epoch 2 batch 2900 train loss: 0.0080 test loss: 0.0085\n",
      "Epoch 3 batch 0 train loss: 0.0071 test loss: 0.0080\n",
      "Epoch 3 batch 100 train loss: 0.0068 test loss: 0.0082\n",
      "Epoch 3 batch 200 train loss: 0.0082 test loss: 0.0079\n",
      "Epoch 3 batch 300 train loss: 0.0062 test loss: 0.0079\n",
      "Epoch 3 batch 400 train loss: 0.0067 test loss: 0.0083\n",
      "Epoch 3 batch 500 train loss: 0.0072 test loss: 0.0082\n",
      "Epoch 3 batch 600 train loss: 0.0086 test loss: 0.0089\n",
      "Epoch 3 batch 700 train loss: 0.0072 test loss: 0.0081\n",
      "Epoch 3 batch 800 train loss: 0.0059 test loss: 0.0084\n",
      "Epoch 3 batch 900 train loss: 0.0071 test loss: 0.0092\n",
      "Epoch 3 batch 1000 train loss: 0.0065 test loss: 0.0087\n",
      "Epoch 3 batch 1100 train loss: 0.0071 test loss: 0.0085\n",
      "Epoch 3 batch 1200 train loss: 0.0075 test loss: 0.0081\n",
      "Epoch 3 batch 1300 train loss: 0.0059 test loss: 0.0087\n",
      "Epoch 3 batch 1400 train loss: 0.0072 test loss: 0.0085\n",
      "Epoch 3 batch 1500 train loss: 0.0077 test loss: 0.0081\n",
      "Epoch 3 batch 1600 train loss: 0.0121 test loss: 0.0081\n",
      "Epoch 3 batch 1700 train loss: 0.0095 test loss: 0.0083\n",
      "Epoch 3 batch 1800 train loss: 0.0075 test loss: 0.0087\n",
      "Epoch 3 batch 1900 train loss: 0.0078 test loss: 0.0089\n",
      "Epoch 3 batch 2000 train loss: 0.0080 test loss: 0.0085\n",
      "Epoch 3 batch 2100 train loss: 0.0082 test loss: 0.0091\n",
      "Epoch 3 batch 2200 train loss: 0.0073 test loss: 0.0083\n",
      "Epoch 3 batch 2300 train loss: 0.0072 test loss: 0.0082\n",
      "Epoch 3 batch 2400 train loss: 0.0067 test loss: 0.0084\n",
      "Epoch 3 batch 2500 train loss: 0.0087 test loss: 0.0083\n",
      "Epoch 3 batch 2600 train loss: 0.0082 test loss: 0.0091\n",
      "Epoch 3 batch 2700 train loss: 0.0062 test loss: 0.0080\n",
      "Epoch 3 batch 2800 train loss: 0.0081 test loss: 0.0077\n",
      "Epoch 3 batch 2900 train loss: 0.0067 test loss: 0.0082\n",
      "Epoch 4 batch 0 train loss: 0.0091 test loss: 0.0079\n",
      "Epoch 4 batch 100 train loss: 0.0075 test loss: 0.0085\n",
      "Epoch 4 batch 200 train loss: 0.0061 test loss: 0.0083\n",
      "Epoch 4 batch 300 train loss: 0.0082 test loss: 0.0077\n",
      "Epoch 4 batch 400 train loss: 0.0092 test loss: 0.0092\n",
      "Epoch 4 batch 500 train loss: 0.0088 test loss: 0.0085\n",
      "Epoch 4 batch 600 train loss: 0.0094 test loss: 0.0087\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.30p/ckpt-24\n",
      "Epoch 4 batch 700 train loss: 0.0065 test loss: 0.0075\n",
      "Epoch 4 batch 800 train loss: 0.0070 test loss: 0.0089\n",
      "Epoch 4 batch 900 train loss: 0.0063 test loss: 0.0077\n",
      "Epoch 4 batch 1000 train loss: 0.0080 test loss: 0.0085\n",
      "Epoch 4 batch 1100 train loss: 0.0069 test loss: 0.0084\n",
      "Epoch 4 batch 1200 train loss: 0.0087 test loss: 0.0084\n",
      "Epoch 4 batch 1300 train loss: 0.0087 test loss: 0.0088\n",
      "Epoch 4 batch 1400 train loss: 0.0108 test loss: 0.0082\n",
      "Epoch 4 batch 1500 train loss: 0.0078 test loss: 0.0079\n",
      "Epoch 4 batch 1600 train loss: 0.0089 test loss: 0.0083\n",
      "Epoch 4 batch 1700 train loss: 0.0076 test loss: 0.0090\n",
      "Epoch 4 batch 1800 train loss: 0.0064 test loss: 0.0085\n",
      "Epoch 4 batch 1900 train loss: 0.0060 test loss: 0.0079\n",
      "Epoch 4 batch 2000 train loss: 0.0093 test loss: 0.0086\n",
      "Epoch 4 batch 2100 train loss: 0.0074 test loss: 0.0090\n",
      "Epoch 4 batch 2200 train loss: 0.0055 test loss: 0.0085\n",
      "Epoch 4 batch 2300 train loss: 0.0077 test loss: 0.0079\n",
      "Epoch 4 batch 2400 train loss: 0.0075 test loss: 0.0077\n",
      "Epoch 4 batch 2500 train loss: 0.0063 test loss: 0.0087\n",
      "Epoch 4 batch 2600 train loss: 0.0073 test loss: 0.0077\n",
      "Epoch 4 batch 2700 train loss: 0.0073 test loss: 0.0083\n",
      "Epoch 4 batch 2800 train loss: 0.0086 test loss: 0.0078\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.30p/ckpt-25\n",
      "Epoch 4 batch 2900 train loss: 0.0087 test loss: 0.0074\n",
      "Epoch 5 batch 0 train loss: 0.0066 test loss: 0.0079\n",
      "Epoch 5 batch 100 train loss: 0.0076 test loss: 0.0087\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.30p/ckpt-26\n",
      "Epoch 5 batch 200 train loss: 0.0059 test loss: 0.0074\n",
      "Epoch 5 batch 300 train loss: 0.0090 test loss: 0.0075\n",
      "Epoch 5 batch 400 train loss: 0.0060 test loss: 0.0085\n",
      "Epoch 5 batch 500 train loss: 0.0064 test loss: 0.0086\n",
      "Epoch 5 batch 600 train loss: 0.0072 test loss: 0.0082\n",
      "Epoch 5 batch 700 train loss: 0.0083 test loss: 0.0080\n",
      "Epoch 5 batch 800 train loss: 0.0076 test loss: 0.0078\n",
      "Epoch 5 batch 900 train loss: 0.0071 test loss: 0.0082\n",
      "Epoch 5 batch 1000 train loss: 0.0065 test loss: 0.0087\n",
      "Epoch 5 batch 1100 train loss: 0.0088 test loss: 0.0090\n",
      "Epoch 5 batch 1200 train loss: 0.0066 test loss: 0.0081\n",
      "Epoch 5 batch 1300 train loss: 0.0071 test loss: 0.0081\n",
      "Epoch 5 batch 1400 train loss: 0.0075 test loss: 0.0083\n",
      "Epoch 5 batch 1500 train loss: 0.0097 test loss: 0.0080\n",
      "Epoch 5 batch 1600 train loss: 0.0084 test loss: 0.0087\n",
      "Epoch 5 batch 1700 train loss: 0.0089 test loss: 0.0081\n",
      "Epoch 5 batch 1800 train loss: 0.0069 test loss: 0.0091\n",
      "Epoch 5 batch 1900 train loss: 0.0083 test loss: 0.0085\n",
      "Epoch 5 batch 2000 train loss: 0.0075 test loss: 0.0082\n",
      "Epoch 5 batch 2100 train loss: 0.0068 test loss: 0.0084\n",
      "Epoch 5 batch 2200 train loss: 0.0069 test loss: 0.0089\n",
      "Epoch 5 batch 2300 train loss: 0.0075 test loss: 0.0077\n",
      "Epoch 5 batch 2400 train loss: 0.0086 test loss: 0.0078\n",
      "Epoch 5 batch 2500 train loss: 0.0077 test loss: 0.0077\n",
      "Epoch 5 batch 2600 train loss: 0.0093 test loss: 0.0082\n",
      "Epoch 5 batch 2700 train loss: 0.0068 test loss: 0.0088\n",
      "Epoch 5 batch 2800 train loss: 0.0091 test loss: 0.0081\n",
      "Epoch 5 batch 2900 train loss: 0.0073 test loss: 0.0079\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.30p/ckpt-27\n",
      "Epoch 6 batch 0 train loss: 0.0069 test loss: 0.0071\n",
      "Epoch 6 batch 100 train loss: 0.0068 test loss: 0.0082\n",
      "Epoch 6 batch 200 train loss: 0.0073 test loss: 0.0082\n",
      "Epoch 6 batch 300 train loss: 0.0069 test loss: 0.0080\n",
      "Epoch 6 batch 400 train loss: 0.0098 test loss: 0.0081\n",
      "Epoch 6 batch 500 train loss: 0.0087 test loss: 0.0080\n",
      "Epoch 6 batch 600 train loss: 0.0065 test loss: 0.0085\n",
      "Epoch 6 batch 700 train loss: 0.0086 test loss: 0.0083\n",
      "Epoch 6 batch 800 train loss: 0.0067 test loss: 0.0083\n",
      "Epoch 6 batch 900 train loss: 0.0076 test loss: 0.0090\n",
      "Epoch 6 batch 1000 train loss: 0.0074 test loss: 0.0085\n",
      "Epoch 6 batch 1100 train loss: 0.0074 test loss: 0.0086\n",
      "Epoch 6 batch 1200 train loss: 0.0082 test loss: 0.0074\n",
      "Epoch 6 batch 1300 train loss: 0.0063 test loss: 0.0083\n",
      "Epoch 6 batch 1400 train loss: 0.0061 test loss: 0.0089\n",
      "Epoch 6 batch 1500 train loss: 0.0076 test loss: 0.0087\n",
      "Epoch 6 batch 1600 train loss: 0.0077 test loss: 0.0084\n",
      "Epoch 6 batch 1700 train loss: 0.0079 test loss: 0.0083\n",
      "Epoch 6 batch 1800 train loss: 0.0063 test loss: 0.0084\n",
      "Epoch 6 batch 1900 train loss: 0.0051 test loss: 0.0083\n",
      "Epoch 6 batch 2000 train loss: 0.0065 test loss: 0.0078\n",
      "Epoch 6 batch 2100 train loss: 0.0067 test loss: 0.0084\n",
      "Epoch 6 batch 2200 train loss: 0.0077 test loss: 0.0086\n",
      "Epoch 6 batch 2300 train loss: 0.0091 test loss: 0.0086\n",
      "Epoch 6 batch 2400 train loss: 0.0085 test loss: 0.0086\n",
      "Epoch 6 batch 2500 train loss: 0.0094 test loss: 0.0079\n",
      "Epoch 6 batch 2600 train loss: 0.0087 test loss: 0.0084\n",
      "Epoch 6 batch 2700 train loss: 0.0068 test loss: 0.0082\n",
      "Epoch 6 batch 2800 train loss: 0.0075 test loss: 0.0084\n",
      "Epoch 6 batch 2900 train loss: 0.0098 test loss: 0.0079\n",
      "Epoch 7 batch 0 train loss: 0.0076 test loss: 0.0086\n",
      "Epoch 7 batch 100 train loss: 0.0100 test loss: 0.0080\n",
      "Epoch 7 batch 200 train loss: 0.0070 test loss: 0.0081\n",
      "Epoch 7 batch 300 train loss: 0.0088 test loss: 0.0076\n",
      "Epoch 7 batch 400 train loss: 0.0086 test loss: 0.0084\n",
      "Epoch 7 batch 500 train loss: 0.0059 test loss: 0.0079\n",
      "Epoch 7 batch 600 train loss: 0.0074 test loss: 0.0084\n",
      "Epoch 7 batch 700 train loss: 0.0058 test loss: 0.0079\n",
      "Epoch 7 batch 800 train loss: 0.0069 test loss: 0.0086\n",
      "Epoch 7 batch 900 train loss: 0.0063 test loss: 0.0082\n",
      "Epoch 7 batch 1000 train loss: 0.0060 test loss: 0.0087\n",
      "Epoch 7 batch 1100 train loss: 0.0092 test loss: 0.0086\n",
      "Epoch 7 batch 1200 train loss: 0.0074 test loss: 0.0081\n",
      "Epoch 7 batch 1300 train loss: 0.0079 test loss: 0.0089\n",
      "Epoch 7 batch 1400 train loss: 0.0069 test loss: 0.0082\n",
      "Epoch 7 batch 1500 train loss: 0.0085 test loss: 0.0084\n",
      "Epoch 7 batch 1600 train loss: 0.0064 test loss: 0.0079\n",
      "Epoch 7 batch 1700 train loss: 0.0049 test loss: 0.0079\n",
      "Epoch 7 batch 1800 train loss: 0.0063 test loss: 0.0079\n",
      "Epoch 7 batch 1900 train loss: 0.0074 test loss: 0.0085\n",
      "Epoch 7 batch 2000 train loss: 0.0063 test loss: 0.0076\n",
      "Epoch 7 batch 2100 train loss: 0.0087 test loss: 0.0086\n",
      "Epoch 7 batch 2200 train loss: 0.0080 test loss: 0.0082\n",
      "Epoch 7 batch 2300 train loss: 0.0059 test loss: 0.0081\n",
      "Epoch 7 batch 2400 train loss: 0.0080 test loss: 0.0086\n",
      "Epoch 7 batch 2500 train loss: 0.0094 test loss: 0.0081\n",
      "Epoch 7 batch 2600 train loss: 0.0055 test loss: 0.0079\n",
      "Epoch 7 batch 2700 train loss: 0.0086 test loss: 0.0085\n",
      "Epoch 7 batch 2800 train loss: 0.0065 test loss: 0.0081\n",
      "Epoch 7 batch 2900 train loss: 0.0069 test loss: 0.0079\n",
      "Epoch 8 batch 0 train loss: 0.0081 test loss: 0.0080\n",
      "Epoch 8 batch 100 train loss: 0.0056 test loss: 0.0087\n",
      "Epoch 8 batch 200 train loss: 0.0063 test loss: 0.0081\n",
      "Epoch 8 batch 300 train loss: 0.0067 test loss: 0.0077\n",
      "Epoch 8 batch 400 train loss: 0.0066 test loss: 0.0082\n",
      "Epoch 8 batch 500 train loss: 0.0056 test loss: 0.0076\n",
      "Epoch 8 batch 600 train loss: 0.0084 test loss: 0.0082\n",
      "Epoch 8 batch 700 train loss: 0.0071 test loss: 0.0087\n",
      "Epoch 8 batch 800 train loss: 0.0061 test loss: 0.0086\n",
      "Epoch 8 batch 900 train loss: 0.0086 test loss: 0.0082\n",
      "Epoch 8 batch 1000 train loss: 0.0056 test loss: 0.0086\n",
      "Epoch 8 batch 1100 train loss: 0.0098 test loss: 0.0076\n",
      "Epoch 8 batch 1200 train loss: 0.0086 test loss: 0.0080\n",
      "Epoch 8 batch 1300 train loss: 0.0066 test loss: 0.0083\n",
      "Epoch 8 batch 1400 train loss: 0.0068 test loss: 0.0077\n",
      "Epoch 8 batch 1500 train loss: 0.0070 test loss: 0.0080\n",
      "Epoch 8 batch 1600 train loss: 0.0062 test loss: 0.0082\n",
      "Epoch 8 batch 1700 train loss: 0.0061 test loss: 0.0089\n",
      "Epoch 8 batch 1800 train loss: 0.0081 test loss: 0.0088\n",
      "Epoch 8 batch 1900 train loss: 0.0061 test loss: 0.0081\n",
      "Epoch 8 batch 2000 train loss: 0.0068 test loss: 0.0080\n",
      "Epoch 8 batch 2100 train loss: 0.0073 test loss: 0.0082\n",
      "Epoch 8 batch 2200 train loss: 0.0066 test loss: 0.0076\n",
      "Epoch 8 batch 2300 train loss: 0.0087 test loss: 0.0078\n",
      "Epoch 8 batch 2400 train loss: 0.0085 test loss: 0.0086\n",
      "Epoch 8 batch 2500 train loss: 0.0076 test loss: 0.0083\n",
      "Epoch 8 batch 2600 train loss: 0.0073 test loss: 0.0083\n",
      "Epoch 8 batch 2700 train loss: 0.0074 test loss: 0.0096\n",
      "Epoch 8 batch 2800 train loss: 0.0087 test loss: 0.0086\n",
      "Epoch 8 batch 2900 train loss: 0.0083 test loss: 0.0080\n",
      "Epoch 9 batch 0 train loss: 0.0064 test loss: 0.0081\n",
      "Epoch 9 batch 100 train loss: 0.0059 test loss: 0.0077\n",
      "Epoch 9 batch 200 train loss: 0.0099 test loss: 0.0080\n",
      "Epoch 9 batch 300 train loss: 0.0073 test loss: 0.0082\n",
      "Epoch 9 batch 400 train loss: 0.0087 test loss: 0.0080\n",
      "Epoch 9 batch 500 train loss: 0.0050 test loss: 0.0083\n",
      "Epoch 9 batch 600 train loss: 0.0077 test loss: 0.0082\n",
      "Epoch 9 batch 700 train loss: 0.0079 test loss: 0.0084\n",
      "Epoch 9 batch 800 train loss: 0.0085 test loss: 0.0085\n",
      "Epoch 9 batch 900 train loss: 0.0066 test loss: 0.0081\n",
      "Epoch 9 batch 1000 train loss: 0.0065 test loss: 0.0085\n",
      "Epoch 9 batch 1100 train loss: 0.0098 test loss: 0.0081\n",
      "Epoch 9 batch 1200 train loss: 0.0085 test loss: 0.0081\n",
      "Epoch 9 batch 1300 train loss: 0.0076 test loss: 0.0081\n",
      "Epoch 9 batch 1400 train loss: 0.0061 test loss: 0.0079\n",
      "Epoch 9 batch 1500 train loss: 0.0063 test loss: 0.0082\n",
      "Epoch 9 batch 1600 train loss: 0.0047 test loss: 0.0083\n",
      "Epoch 9 batch 1700 train loss: 0.0100 test loss: 0.0087\n",
      "Epoch 9 batch 1800 train loss: 0.0094 test loss: 0.0082\n",
      "Epoch 9 batch 1900 train loss: 0.0094 test loss: 0.0082\n",
      "Epoch 9 batch 2000 train loss: 0.0070 test loss: 0.0089\n",
      "Epoch 9 batch 2100 train loss: 0.0100 test loss: 0.0081\n",
      "Epoch 9 batch 2200 train loss: 0.0066 test loss: 0.0082\n",
      "Epoch 9 batch 2300 train loss: 0.0085 test loss: 0.0079\n",
      "Epoch 9 batch 2400 train loss: 0.0056 test loss: 0.0077\n",
      "Epoch 9 batch 2500 train loss: 0.0054 test loss: 0.0078\n",
      "Epoch 9 batch 2600 train loss: 0.0072 test loss: 0.0082\n",
      "Epoch 9 batch 2700 train loss: 0.0061 test loss: 0.0078\n",
      "Epoch 9 batch 2800 train loss: 0.0060 test loss: 0.0080\n",
      "Epoch 9 batch 2900 train loss: 0.0087 test loss: 0.0074\n",
      "Epoch 10 batch 0 train loss: 0.0079 test loss: 0.0076\n",
      "Epoch 10 batch 100 train loss: 0.0087 test loss: 0.0083\n",
      "Epoch 10 batch 200 train loss: 0.0066 test loss: 0.0074\n",
      "Epoch 10 batch 300 train loss: 0.0073 test loss: 0.0077\n",
      "Epoch 10 batch 400 train loss: 0.0085 test loss: 0.0081\n",
      "Epoch 10 batch 500 train loss: 0.0078 test loss: 0.0084\n",
      "Epoch 10 batch 600 train loss: 0.0050 test loss: 0.0082\n",
      "Epoch 10 batch 700 train loss: 0.0090 test loss: 0.0079\n",
      "Epoch 10 batch 800 train loss: 0.0077 test loss: 0.0093\n",
      "Epoch 10 batch 900 train loss: 0.0079 test loss: 0.0081\n",
      "Epoch 10 batch 1000 train loss: 0.0066 test loss: 0.0077\n",
      "Epoch 10 batch 1100 train loss: 0.0061 test loss: 0.0083\n",
      "Epoch 10 batch 1200 train loss: 0.0089 test loss: 0.0078\n",
      "Epoch 10 batch 1300 train loss: 0.0068 test loss: 0.0086\n",
      "Epoch 10 batch 1400 train loss: 0.0076 test loss: 0.0079\n",
      "Epoch 10 batch 1500 train loss: 0.0057 test loss: 0.0082\n",
      "Epoch 10 batch 1600 train loss: 0.0063 test loss: 0.0083\n",
      "Epoch 10 batch 1700 train loss: 0.0055 test loss: 0.0080\n",
      "Epoch 10 batch 1800 train loss: 0.0086 test loss: 0.0083\n",
      "Epoch 10 batch 1900 train loss: 0.0075 test loss: 0.0085\n",
      "Epoch 10 batch 2000 train loss: 0.0063 test loss: 0.0084\n",
      "Epoch 10 batch 2100 train loss: 0.0069 test loss: 0.0082\n",
      "Epoch 10 batch 2200 train loss: 0.0089 test loss: 0.0083\n",
      "Epoch 10 batch 2300 train loss: 0.0083 test loss: 0.0087\n",
      "Epoch 10 batch 2400 train loss: 0.0067 test loss: 0.0076\n",
      "Epoch 10 batch 2500 train loss: 0.0076 test loss: 0.0086\n",
      "Epoch 10 batch 2600 train loss: 0.0076 test loss: 0.0082\n",
      "Epoch 10 batch 2700 train loss: 0.0074 test loss: 0.0082\n",
      "Epoch 10 batch 2800 train loss: 0.0065 test loss: 0.0079\n",
      "Epoch 10 batch 2900 train loss: 0.0072 test loss: 0.0082\n",
      "Epoch 11 batch 0 train loss: 0.0068 test loss: 0.0078\n",
      "Epoch 11 batch 100 train loss: 0.0055 test loss: 0.0085\n",
      "Epoch 11 batch 200 train loss: 0.0061 test loss: 0.0090\n",
      "Epoch 11 batch 300 train loss: 0.0078 test loss: 0.0078\n",
      "Epoch 11 batch 400 train loss: 0.0098 test loss: 0.0087\n",
      "Epoch 11 batch 500 train loss: 0.0074 test loss: 0.0080\n",
      "Epoch 11 batch 600 train loss: 0.0082 test loss: 0.0079\n",
      "Epoch 11 batch 700 train loss: 0.0083 test loss: 0.0080\n",
      "Epoch 11 batch 800 train loss: 0.0079 test loss: 0.0078\n",
      "Epoch 11 batch 900 train loss: 0.0069 test loss: 0.0077\n",
      "Epoch 11 batch 1000 train loss: 0.0081 test loss: 0.0072\n",
      "Epoch 11 batch 1100 train loss: 0.0052 test loss: 0.0085\n",
      "Epoch 11 batch 1200 train loss: 0.0067 test loss: 0.0086\n",
      "Epoch 11 batch 1300 train loss: 0.0075 test loss: 0.0081\n",
      "Epoch 11 batch 1400 train loss: 0.0077 test loss: 0.0081\n",
      "Epoch 11 batch 1500 train loss: 0.0060 test loss: 0.0079\n",
      "Epoch 11 batch 1600 train loss: 0.0080 test loss: 0.0083\n",
      "Epoch 11 batch 1700 train loss: 0.0076 test loss: 0.0088\n",
      "Epoch 11 batch 1800 train loss: 0.0085 test loss: 0.0083\n",
      "Epoch 11 batch 1900 train loss: 0.0068 test loss: 0.0081\n",
      "Epoch 11 batch 2000 train loss: 0.0071 test loss: 0.0085\n",
      "Epoch 11 batch 2100 train loss: 0.0075 test loss: 0.0075\n",
      "Epoch 11 batch 2200 train loss: 0.0079 test loss: 0.0091\n",
      "Epoch 11 batch 2300 train loss: 0.0079 test loss: 0.0085\n",
      "Epoch 11 batch 2400 train loss: 0.0083 test loss: 0.0086\n",
      "Epoch 11 batch 2500 train loss: 0.0073 test loss: 0.0079\n",
      "Epoch 11 batch 2600 train loss: 0.0062 test loss: 0.0083\n",
      "Epoch 11 batch 2700 train loss: 0.0062 test loss: 0.0086\n",
      "Epoch 11 batch 2800 train loss: 0.0094 test loss: 0.0079\n",
      "Epoch 11 batch 2900 train loss: 0.0081 test loss: 0.0075\n",
      "Epoch 12 batch 0 train loss: 0.0091 test loss: 0.0073\n",
      "Epoch 12 batch 100 train loss: 0.0057 test loss: 0.0083\n",
      "Epoch 12 batch 200 train loss: 0.0060 test loss: 0.0078\n",
      "Epoch 12 batch 300 train loss: 0.0077 test loss: 0.0075\n",
      "Epoch 12 batch 400 train loss: 0.0066 test loss: 0.0077\n",
      "Epoch 12 batch 500 train loss: 0.0076 test loss: 0.0082\n",
      "Epoch 12 batch 600 train loss: 0.0083 test loss: 0.0085\n",
      "Epoch 12 batch 700 train loss: 0.0083 test loss: 0.0082\n",
      "Epoch 12 batch 800 train loss: 0.0072 test loss: 0.0076\n",
      "Epoch 12 batch 900 train loss: 0.0107 test loss: 0.0075\n",
      "Epoch 12 batch 1000 train loss: 0.0068 test loss: 0.0086\n",
      "Epoch 12 batch 1100 train loss: 0.0061 test loss: 0.0084\n",
      "Epoch 12 batch 1200 train loss: 0.0087 test loss: 0.0080\n",
      "Epoch 12 batch 1300 train loss: 0.0060 test loss: 0.0083\n",
      "Epoch 12 batch 1400 train loss: 0.0080 test loss: 0.0082\n",
      "Epoch 12 batch 1500 train loss: 0.0064 test loss: 0.0080\n",
      "Epoch 12 batch 1600 train loss: 0.0083 test loss: 0.0086\n",
      "Epoch 12 batch 1700 train loss: 0.0063 test loss: 0.0083\n",
      "Epoch 12 batch 1800 train loss: 0.0079 test loss: 0.0083\n",
      "Epoch 12 batch 1900 train loss: 0.0058 test loss: 0.0085\n",
      "Epoch 12 batch 2000 train loss: 0.0055 test loss: 0.0081\n",
      "Epoch 12 batch 2100 train loss: 0.0071 test loss: 0.0083\n",
      "Epoch 12 batch 2200 train loss: 0.0068 test loss: 0.0084\n",
      "Epoch 12 batch 2300 train loss: 0.0084 test loss: 0.0084\n",
      "Epoch 12 batch 2400 train loss: 0.0065 test loss: 0.0078\n",
      "Epoch 12 batch 2500 train loss: 0.0081 test loss: 0.0081\n",
      "Epoch 12 batch 2600 train loss: 0.0076 test loss: 0.0081\n",
      "Epoch 12 batch 2700 train loss: 0.0055 test loss: 0.0084\n",
      "Epoch 12 batch 2800 train loss: 0.0083 test loss: 0.0073\n",
      "Epoch 12 batch 2900 train loss: 0.0070 test loss: 0.0073\n",
      "Epoch 13 batch 0 train loss: 0.0066 test loss: 0.0087\n",
      "Epoch 13 batch 100 train loss: 0.0088 test loss: 0.0077\n",
      "Epoch 13 batch 200 train loss: 0.0079 test loss: 0.0087\n",
      "Epoch 13 batch 300 train loss: 0.0060 test loss: 0.0082\n",
      "Epoch 13 batch 400 train loss: 0.0094 test loss: 0.0082\n",
      "Epoch 13 batch 500 train loss: 0.0081 test loss: 0.0085\n",
      "Epoch 13 batch 600 train loss: 0.0069 test loss: 0.0083\n",
      "Epoch 13 batch 700 train loss: 0.0107 test loss: 0.0085\n",
      "Epoch 13 batch 800 train loss: 0.0097 test loss: 0.0086\n",
      "Epoch 13 batch 900 train loss: 0.0060 test loss: 0.0078\n",
      "Epoch 13 batch 1000 train loss: 0.0083 test loss: 0.0091\n",
      "Epoch 13 batch 1100 train loss: 0.0086 test loss: 0.0081\n",
      "Epoch 13 batch 1200 train loss: 0.0066 test loss: 0.0076\n",
      "Epoch 13 batch 1300 train loss: 0.0082 test loss: 0.0077\n",
      "Epoch 13 batch 1400 train loss: 0.0066 test loss: 0.0081\n",
      "Epoch 13 batch 1500 train loss: 0.0095 test loss: 0.0080\n",
      "Epoch 13 batch 1600 train loss: 0.0084 test loss: 0.0081\n",
      "Epoch 13 batch 1700 train loss: 0.0072 test loss: 0.0079\n",
      "Epoch 13 batch 1800 train loss: 0.0087 test loss: 0.0083\n",
      "Epoch 13 batch 1900 train loss: 0.0070 test loss: 0.0084\n",
      "Epoch 13 batch 2000 train loss: 0.0074 test loss: 0.0081\n",
      "Epoch 13 batch 2100 train loss: 0.0077 test loss: 0.0088\n",
      "Epoch 13 batch 2200 train loss: 0.0073 test loss: 0.0084\n",
      "Epoch 13 batch 2300 train loss: 0.0067 test loss: 0.0084\n",
      "Epoch 13 batch 2400 train loss: 0.0062 test loss: 0.0077\n",
      "Epoch 13 batch 2500 train loss: 0.0063 test loss: 0.0083\n",
      "Epoch 13 batch 2600 train loss: 0.0057 test loss: 0.0080\n",
      "Epoch 13 batch 2700 train loss: 0.0068 test loss: 0.0080\n",
      "Epoch 13 batch 2800 train loss: 0.0082 test loss: 0.0077\n",
      "Epoch 13 batch 2900 train loss: 0.0056 test loss: 0.0079\n",
      "Epoch 14 batch 0 train loss: 0.0069 test loss: 0.0080\n",
      "Epoch 14 batch 100 train loss: 0.0057 test loss: 0.0072\n",
      "Epoch 14 batch 200 train loss: 0.0085 test loss: 0.0080\n",
      "Epoch 14 batch 300 train loss: 0.0082 test loss: 0.0081\n",
      "Epoch 14 batch 400 train loss: 0.0066 test loss: 0.0077\n",
      "Epoch 14 batch 500 train loss: 0.0062 test loss: 0.0087\n",
      "Epoch 14 batch 600 train loss: 0.0090 test loss: 0.0083\n",
      "Epoch 14 batch 700 train loss: 0.0076 test loss: 0.0081\n",
      "Epoch 14 batch 800 train loss: 0.0075 test loss: 0.0083\n",
      "Epoch 14 batch 900 train loss: 0.0099 test loss: 0.0087\n",
      "Epoch 14 batch 1000 train loss: 0.0077 test loss: 0.0088\n",
      "Epoch 14 batch 1100 train loss: 0.0104 test loss: 0.0079\n",
      "Epoch 14 batch 1200 train loss: 0.0080 test loss: 0.0078\n",
      "Epoch 14 batch 1300 train loss: 0.0070 test loss: 0.0079\n",
      "Epoch 14 batch 1400 train loss: 0.0095 test loss: 0.0085\n",
      "Epoch 14 batch 1500 train loss: 0.0080 test loss: 0.0085\n",
      "Epoch 14 batch 1600 train loss: 0.0056 test loss: 0.0079\n",
      "Epoch 14 batch 1700 train loss: 0.0088 test loss: 0.0084\n",
      "Epoch 14 batch 1800 train loss: 0.0079 test loss: 0.0089\n",
      "Epoch 14 batch 1900 train loss: 0.0069 test loss: 0.0080\n",
      "Epoch 14 batch 2000 train loss: 0.0079 test loss: 0.0079\n",
      "Epoch 14 batch 2100 train loss: 0.0090 test loss: 0.0082\n",
      "Epoch 14 batch 2200 train loss: 0.0070 test loss: 0.0084\n",
      "Epoch 14 batch 2300 train loss: 0.0061 test loss: 0.0082\n",
      "Epoch 14 batch 2400 train loss: 0.0067 test loss: 0.0085\n",
      "Epoch 14 batch 2500 train loss: 0.0051 test loss: 0.0087\n",
      "Epoch 14 batch 2600 train loss: 0.0064 test loss: 0.0075\n",
      "Epoch 14 batch 2700 train loss: 0.0090 test loss: 0.0080\n",
      "Epoch 14 batch 2800 train loss: 0.0067 test loss: 0.0079\n",
      "Epoch 14 batch 2900 train loss: 0.0089 test loss: 0.0080\n",
      "Epoch 15 batch 0 train loss: 0.0089 test loss: 0.0081\n",
      "Epoch 15 batch 100 train loss: 0.0075 test loss: 0.0082\n",
      "Epoch 15 batch 200 train loss: 0.0076 test loss: 0.0082\n",
      "Epoch 15 batch 300 train loss: 0.0059 test loss: 0.0072\n",
      "Epoch 15 batch 400 train loss: 0.0078 test loss: 0.0086\n",
      "Epoch 15 batch 500 train loss: 0.0052 test loss: 0.0083\n",
      "Epoch 15 batch 600 train loss: 0.0078 test loss: 0.0087\n",
      "Epoch 15 batch 700 train loss: 0.0075 test loss: 0.0079\n",
      "Epoch 15 batch 800 train loss: 0.0067 test loss: 0.0085\n",
      "Epoch 15 batch 900 train loss: 0.0067 test loss: 0.0087\n",
      "Epoch 15 batch 1000 train loss: 0.0077 test loss: 0.0087\n",
      "Epoch 15 batch 1100 train loss: 0.0064 test loss: 0.0089\n",
      "Epoch 15 batch 1200 train loss: 0.0081 test loss: 0.0085\n",
      "Epoch 15 batch 1300 train loss: 0.0068 test loss: 0.0083\n",
      "Epoch 15 batch 1400 train loss: 0.0096 test loss: 0.0089\n",
      "Epoch 15 batch 1500 train loss: 0.0060 test loss: 0.0083\n",
      "Epoch 15 batch 1600 train loss: 0.0081 test loss: 0.0081\n",
      "Epoch 15 batch 1700 train loss: 0.0068 test loss: 0.0089\n",
      "Epoch 15 batch 1800 train loss: 0.0074 test loss: 0.0084\n",
      "Epoch 15 batch 1900 train loss: 0.0068 test loss: 0.0085\n",
      "Epoch 15 batch 2000 train loss: 0.0073 test loss: 0.0077\n",
      "Epoch 15 batch 2100 train loss: 0.0074 test loss: 0.0086\n",
      "Epoch 15 batch 2200 train loss: 0.0085 test loss: 0.0084\n",
      "Epoch 15 batch 2300 train loss: 0.0074 test loss: 0.0078\n",
      "Epoch 15 batch 2400 train loss: 0.0095 test loss: 0.0080\n",
      "Epoch 15 batch 2500 train loss: 0.0095 test loss: 0.0082\n",
      "Epoch 15 batch 2600 train loss: 0.0066 test loss: 0.0087\n",
      "Epoch 15 batch 2700 train loss: 0.0067 test loss: 0.0081\n",
      "Epoch 15 batch 2800 train loss: 0.0078 test loss: 0.0082\n",
      "Epoch 15 batch 2900 train loss: 0.0068 test loss: 0.0074\n",
      "Epoch 16 batch 0 train loss: 0.0056 test loss: 0.0076\n",
      "Epoch 16 batch 100 train loss: 0.0071 test loss: 0.0080\n",
      "Epoch 16 batch 200 train loss: 0.0073 test loss: 0.0080\n",
      "Epoch 16 batch 300 train loss: 0.0084 test loss: 0.0085\n",
      "Epoch 16 batch 400 train loss: 0.0040 test loss: 0.0082\n",
      "Epoch 16 batch 500 train loss: 0.0056 test loss: 0.0078\n",
      "Epoch 16 batch 600 train loss: 0.0057 test loss: 0.0085\n",
      "Epoch 16 batch 700 train loss: 0.0080 test loss: 0.0076\n",
      "Epoch 16 batch 800 train loss: 0.0060 test loss: 0.0086\n",
      "Epoch 16 batch 900 train loss: 0.0063 test loss: 0.0083\n",
      "Epoch 16 batch 1000 train loss: 0.0067 test loss: 0.0082\n",
      "Epoch 16 batch 1100 train loss: 0.0062 test loss: 0.0084\n",
      "Epoch 16 batch 1200 train loss: 0.0074 test loss: 0.0078\n",
      "Epoch 16 batch 1300 train loss: 0.0076 test loss: 0.0087\n",
      "Epoch 16 batch 1400 train loss: 0.0070 test loss: 0.0080\n",
      "Epoch 16 batch 1500 train loss: 0.0066 test loss: 0.0083\n",
      "Epoch 16 batch 1600 train loss: 0.0090 test loss: 0.0080\n",
      "Epoch 16 batch 1700 train loss: 0.0068 test loss: 0.0085\n",
      "Epoch 16 batch 1800 train loss: 0.0074 test loss: 0.0084\n",
      "Epoch 16 batch 1900 train loss: 0.0061 test loss: 0.0086\n",
      "Epoch 16 batch 2000 train loss: 0.0065 test loss: 0.0074\n",
      "Epoch 16 batch 2100 train loss: 0.0065 test loss: 0.0085\n",
      "Epoch 16 batch 2200 train loss: 0.0069 test loss: 0.0079\n",
      "Epoch 16 batch 2300 train loss: 0.0086 test loss: 0.0076\n",
      "Epoch 16 batch 2400 train loss: 0.0074 test loss: 0.0075\n",
      "Epoch 16 batch 2500 train loss: 0.0071 test loss: 0.0083\n",
      "Epoch 16 batch 2600 train loss: 0.0066 test loss: 0.0082\n",
      "Epoch 16 batch 2700 train loss: 0.0087 test loss: 0.0088\n",
      "Epoch 16 batch 2800 train loss: 0.0090 test loss: 0.0077\n",
      "Epoch 16 batch 2900 train loss: 0.0080 test loss: 0.0077\n",
      "Epoch 17 batch 0 train loss: 0.0072 test loss: 0.0077\n",
      "Epoch 17 batch 100 train loss: 0.0075 test loss: 0.0077\n",
      "Epoch 17 batch 200 train loss: 0.0078 test loss: 0.0088\n",
      "Epoch 17 batch 300 train loss: 0.0066 test loss: 0.0075\n",
      "Epoch 17 batch 400 train loss: 0.0065 test loss: 0.0080\n",
      "Epoch 17 batch 500 train loss: 0.0089 test loss: 0.0082\n",
      "Epoch 17 batch 600 train loss: 0.0066 test loss: 0.0087\n",
      "Epoch 17 batch 700 train loss: 0.0080 test loss: 0.0086\n",
      "Epoch 17 batch 800 train loss: 0.0094 test loss: 0.0078\n",
      "Epoch 17 batch 900 train loss: 0.0085 test loss: 0.0083\n",
      "Epoch 17 batch 1000 train loss: 0.0069 test loss: 0.0077\n",
      "Epoch 17 batch 1100 train loss: 0.0072 test loss: 0.0087\n",
      "Epoch 17 batch 1200 train loss: 0.0060 test loss: 0.0083\n",
      "Epoch 17 batch 1300 train loss: 0.0087 test loss: 0.0080\n",
      "Epoch 17 batch 1400 train loss: 0.0074 test loss: 0.0075\n",
      "Epoch 17 batch 1500 train loss: 0.0075 test loss: 0.0077\n",
      "Epoch 17 batch 1600 train loss: 0.0079 test loss: 0.0079\n",
      "early stop.\n",
      "Checkpoint 27 restored!!\n",
      "Training for loss rate 0.40 start.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['bi_lstm_3/bi_lstm_block_6/bidirectional_6/forward_lstm_6/lstm_cell_19/kernel:0', 'bi_lstm_3/bi_lstm_block_6/bidirectional_6/forward_lstm_6/lstm_cell_19/recurrent_kernel:0', 'bi_lstm_3/bi_lstm_block_6/bidirectional_6/forward_lstm_6/lstm_cell_19/bias:0', 'bi_lstm_3/bi_lstm_block_6/bidirectional_6/backward_lstm_6/lstm_cell_20/kernel:0', 'bi_lstm_3/bi_lstm_block_6/bidirectional_6/backward_lstm_6/lstm_cell_20/recurrent_kernel:0', 'bi_lstm_3/bi_lstm_block_6/bidirectional_6/backward_lstm_6/lstm_cell_20/bias:0', 'bi_lstm_3/bi_lstm_block_6/layer_normalization_6/gamma:0', 'bi_lstm_3/bi_lstm_block_6/layer_normalization_6/beta:0', 'bi_lstm_3/bi_lstm_block_7/bidirectional_7/forward_lstm_7/lstm_cell_22/kernel:0', 'bi_lstm_3/bi_lstm_block_7/bidirectional_7/forward_lstm_7/lstm_cell_22/recurrent_kernel:0', 'bi_lstm_3/bi_lstm_block_7/bidirectional_7/forward_lstm_7/lstm_cell_22/bias:0', 'bi_lstm_3/bi_lstm_block_7/bidirectional_7/backward_lstm_7/lstm_cell_23/kernel:0', 'bi_lstm_3/bi_lstm_block_7/bidirectional_7/backward_lstm_7/lstm_cell_23/recurrent_kernel:0', 'bi_lstm_3/bi_lstm_block_7/bidirectional_7/backward_lstm_7/lstm_cell_23/bias:0', 'bi_lstm_3/bi_lstm_block_7/layer_normalization_7/gamma:0', 'bi_lstm_3/bi_lstm_block_7/layer_normalization_7/beta:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['bi_lstm_3/bi_lstm_block_6/bidirectional_6/forward_lstm_6/lstm_cell_19/kernel:0', 'bi_lstm_3/bi_lstm_block_6/bidirectional_6/forward_lstm_6/lstm_cell_19/recurrent_kernel:0', 'bi_lstm_3/bi_lstm_block_6/bidirectional_6/forward_lstm_6/lstm_cell_19/bias:0', 'bi_lstm_3/bi_lstm_block_6/bidirectional_6/backward_lstm_6/lstm_cell_20/kernel:0', 'bi_lstm_3/bi_lstm_block_6/bidirectional_6/backward_lstm_6/lstm_cell_20/recurrent_kernel:0', 'bi_lstm_3/bi_lstm_block_6/bidirectional_6/backward_lstm_6/lstm_cell_20/bias:0', 'bi_lstm_3/bi_lstm_block_6/layer_normalization_6/gamma:0', 'bi_lstm_3/bi_lstm_block_6/layer_normalization_6/beta:0', 'bi_lstm_3/bi_lstm_block_7/bidirectional_7/forward_lstm_7/lstm_cell_22/kernel:0', 'bi_lstm_3/bi_lstm_block_7/bidirectional_7/forward_lstm_7/lstm_cell_22/recurrent_kernel:0', 'bi_lstm_3/bi_lstm_block_7/bidirectional_7/forward_lstm_7/lstm_cell_22/bias:0', 'bi_lstm_3/bi_lstm_block_7/bidirectional_7/backward_lstm_7/lstm_cell_23/kernel:0', 'bi_lstm_3/bi_lstm_block_7/bidirectional_7/backward_lstm_7/lstm_cell_23/recurrent_kernel:0', 'bi_lstm_3/bi_lstm_block_7/bidirectional_7/backward_lstm_7/lstm_cell_23/bias:0', 'bi_lstm_3/bi_lstm_block_7/layer_normalization_7/gamma:0', 'bi_lstm_3/bi_lstm_block_7/layer_normalization_7/beta:0'] when minimizing the loss.\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.40p/ckpt-1\n",
      "Epoch 0 batch 0 train loss: 0.6608 test loss: 1.0345\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.40p/ckpt-2\n",
      "Epoch 0 batch 100 train loss: 0.2521 test loss: 0.3275\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.40p/ckpt-3\n",
      "Epoch 0 batch 200 train loss: 0.1315 test loss: 0.1408\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.40p/ckpt-4\n",
      "Epoch 0 batch 300 train loss: 0.0762 test loss: 0.0793\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.40p/ckpt-5\n",
      "Epoch 0 batch 400 train loss: 0.0490 test loss: 0.0528\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.40p/ckpt-6\n",
      "Epoch 0 batch 500 train loss: 0.0340 test loss: 0.0381\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.40p/ckpt-7\n",
      "Epoch 0 batch 600 train loss: 0.0270 test loss: 0.0289\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.40p/ckpt-8\n",
      "Epoch 0 batch 700 train loss: 0.0200 test loss: 0.0232\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.40p/ckpt-9\n",
      "Epoch 0 batch 800 train loss: 0.0155 test loss: 0.0197\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.40p/ckpt-10\n",
      "Epoch 0 batch 900 train loss: 0.0131 test loss: 0.0161\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.40p/ckpt-11\n",
      "Epoch 0 batch 1000 train loss: 0.0113 test loss: 0.0146\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.40p/ckpt-12\n",
      "Epoch 0 batch 1100 train loss: 0.0143 test loss: 0.0140\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.40p/ckpt-13\n",
      "Epoch 0 batch 1200 train loss: 0.0114 test loss: 0.0125\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.40p/ckpt-14\n",
      "Epoch 0 batch 1300 train loss: 0.0096 test loss: 0.0117\n",
      "Epoch 0 batch 1400 train loss: 0.0109 test loss: 0.0119\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.40p/ckpt-15\n",
      "Epoch 0 batch 1500 train loss: 0.0083 test loss: 0.0111\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.40p/ckpt-16\n",
      "Epoch 0 batch 1600 train loss: 0.0094 test loss: 0.0110\n",
      "Epoch 0 batch 1700 train loss: 0.0081 test loss: 0.0121\n",
      "Epoch 0 batch 1800 train loss: 0.0082 test loss: 0.0112\n",
      "Epoch 0 batch 1900 train loss: 0.0061 test loss: 0.0110\n",
      "Epoch 0 batch 2000 train loss: 0.0075 test loss: 0.0110\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.40p/ckpt-17\n",
      "Epoch 0 batch 2100 train loss: 0.0084 test loss: 0.0102\n",
      "Epoch 0 batch 2200 train loss: 0.0076 test loss: 0.0120\n",
      "Epoch 0 batch 2300 train loss: 0.0087 test loss: 0.0108\n",
      "Epoch 0 batch 2400 train loss: 0.0097 test loss: 0.0115\n",
      "Epoch 0 batch 2500 train loss: 0.0078 test loss: 0.0120\n",
      "Epoch 0 batch 2600 train loss: 0.0100 test loss: 0.0107\n",
      "Epoch 0 batch 2700 train loss: 0.0080 test loss: 0.0116\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.40p/ckpt-18\n",
      "Epoch 0 batch 2800 train loss: 0.0135 test loss: 0.0101\n",
      "Epoch 0 batch 2900 train loss: 0.0089 test loss: 0.0102\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['bi_lstm_3/bi_lstm_block_6/bidirectional_6/forward_lstm_6/lstm_cell_19/kernel:0', 'bi_lstm_3/bi_lstm_block_6/bidirectional_6/forward_lstm_6/lstm_cell_19/recurrent_kernel:0', 'bi_lstm_3/bi_lstm_block_6/bidirectional_6/forward_lstm_6/lstm_cell_19/bias:0', 'bi_lstm_3/bi_lstm_block_6/bidirectional_6/backward_lstm_6/lstm_cell_20/kernel:0', 'bi_lstm_3/bi_lstm_block_6/bidirectional_6/backward_lstm_6/lstm_cell_20/recurrent_kernel:0', 'bi_lstm_3/bi_lstm_block_6/bidirectional_6/backward_lstm_6/lstm_cell_20/bias:0', 'bi_lstm_3/bi_lstm_block_6/layer_normalization_6/gamma:0', 'bi_lstm_3/bi_lstm_block_6/layer_normalization_6/beta:0', 'bi_lstm_3/bi_lstm_block_7/bidirectional_7/forward_lstm_7/lstm_cell_22/kernel:0', 'bi_lstm_3/bi_lstm_block_7/bidirectional_7/forward_lstm_7/lstm_cell_22/recurrent_kernel:0', 'bi_lstm_3/bi_lstm_block_7/bidirectional_7/forward_lstm_7/lstm_cell_22/bias:0', 'bi_lstm_3/bi_lstm_block_7/bidirectional_7/backward_lstm_7/lstm_cell_23/kernel:0', 'bi_lstm_3/bi_lstm_block_7/bidirectional_7/backward_lstm_7/lstm_cell_23/recurrent_kernel:0', 'bi_lstm_3/bi_lstm_block_7/bidirectional_7/backward_lstm_7/lstm_cell_23/bias:0', 'bi_lstm_3/bi_lstm_block_7/layer_normalization_7/gamma:0', 'bi_lstm_3/bi_lstm_block_7/layer_normalization_7/beta:0'] when minimizing the loss.\n",
      "Epoch 1 batch 0 train loss: 0.0096 test loss: 0.0110\n",
      "Epoch 1 batch 100 train loss: 0.0088 test loss: 0.0105\n",
      "Epoch 1 batch 200 train loss: 0.0069 test loss: 0.0104\n",
      "Epoch 1 batch 300 train loss: 0.0075 test loss: 0.0108\n",
      "Epoch 1 batch 400 train loss: 0.0098 test loss: 0.0106\n",
      "Epoch 1 batch 500 train loss: 0.0110 test loss: 0.0107\n",
      "Epoch 1 batch 600 train loss: 0.0084 test loss: 0.0103\n",
      "Epoch 1 batch 700 train loss: 0.0093 test loss: 0.0101\n",
      "Epoch 1 batch 800 train loss: 0.0109 test loss: 0.0106\n",
      "Epoch 1 batch 900 train loss: 0.0081 test loss: 0.0112\n",
      "Epoch 1 batch 1000 train loss: 0.0090 test loss: 0.0113\n",
      "Epoch 1 batch 1100 train loss: 0.0072 test loss: 0.0112\n",
      "Epoch 1 batch 1200 train loss: 0.0064 test loss: 0.0105\n",
      "Epoch 1 batch 1300 train loss: 0.0076 test loss: 0.0113\n",
      "Epoch 1 batch 1400 train loss: 0.0080 test loss: 0.0108\n",
      "Epoch 1 batch 1500 train loss: 0.0076 test loss: 0.0117\n",
      "Epoch 1 batch 1600 train loss: 0.0090 test loss: 0.0104\n",
      "Epoch 1 batch 1700 train loss: 0.0096 test loss: 0.0107\n",
      "Epoch 1 batch 1800 train loss: 0.0064 test loss: 0.0103\n",
      "Epoch 1 batch 1900 train loss: 0.0088 test loss: 0.0114\n",
      "Epoch 1 batch 2000 train loss: 0.0100 test loss: 0.0107\n",
      "Epoch 1 batch 2100 train loss: 0.0089 test loss: 0.0104\n",
      "Epoch 1 batch 2200 train loss: 0.0077 test loss: 0.0106\n",
      "Epoch 1 batch 2300 train loss: 0.0073 test loss: 0.0108\n",
      "Epoch 1 batch 2400 train loss: 0.0078 test loss: 0.0110\n",
      "Epoch 1 batch 2500 train loss: 0.0095 test loss: 0.0111\n",
      "Epoch 1 batch 2600 train loss: 0.0084 test loss: 0.0110\n",
      "Epoch 1 batch 2700 train loss: 0.0058 test loss: 0.0102\n",
      "Epoch 1 batch 2800 train loss: 0.0086 test loss: 0.0110\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.40p/ckpt-19\n",
      "Epoch 1 batch 2900 train loss: 0.0068 test loss: 0.0099\n",
      "Epoch 2 batch 0 train loss: 0.0080 test loss: 0.0104\n",
      "Epoch 2 batch 100 train loss: 0.0062 test loss: 0.0108\n",
      "Epoch 2 batch 200 train loss: 0.0077 test loss: 0.0112\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.40p/ckpt-20\n",
      "Epoch 2 batch 300 train loss: 0.0096 test loss: 0.0097\n",
      "Epoch 2 batch 400 train loss: 0.0080 test loss: 0.0105\n",
      "Epoch 2 batch 500 train loss: 0.0067 test loss: 0.0106\n",
      "Epoch 2 batch 600 train loss: 0.0077 test loss: 0.0106\n",
      "Epoch 2 batch 700 train loss: 0.0067 test loss: 0.0107\n",
      "Epoch 2 batch 800 train loss: 0.0085 test loss: 0.0105\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.40p/ckpt-21\n",
      "Epoch 2 batch 900 train loss: 0.0085 test loss: 0.0096\n",
      "Epoch 2 batch 1000 train loss: 0.0073 test loss: 0.0106\n",
      "Epoch 2 batch 1100 train loss: 0.0070 test loss: 0.0106\n",
      "Epoch 2 batch 1200 train loss: 0.0073 test loss: 0.0105\n",
      "Epoch 2 batch 1300 train loss: 0.0102 test loss: 0.0109\n",
      "Epoch 2 batch 1400 train loss: 0.0077 test loss: 0.0102\n",
      "Epoch 2 batch 1500 train loss: 0.0070 test loss: 0.0105\n",
      "Epoch 2 batch 1600 train loss: 0.0087 test loss: 0.0104\n",
      "Epoch 2 batch 1700 train loss: 0.0066 test loss: 0.0108\n",
      "Epoch 2 batch 1800 train loss: 0.0076 test loss: 0.0103\n",
      "Epoch 2 batch 1900 train loss: 0.0087 test loss: 0.0107\n",
      "Epoch 2 batch 2000 train loss: 0.0087 test loss: 0.0096\n",
      "Epoch 2 batch 2100 train loss: 0.0092 test loss: 0.0101\n",
      "Epoch 2 batch 2200 train loss: 0.0091 test loss: 0.0105\n",
      "Epoch 2 batch 2300 train loss: 0.0090 test loss: 0.0103\n",
      "Epoch 2 batch 2400 train loss: 0.0065 test loss: 0.0100\n",
      "Epoch 2 batch 2500 train loss: 0.0086 test loss: 0.0115\n",
      "Epoch 2 batch 2600 train loss: 0.0083 test loss: 0.0101\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.40p/ckpt-22\n",
      "Epoch 2 batch 2700 train loss: 0.0085 test loss: 0.0096\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.40p/ckpt-23\n",
      "Epoch 2 batch 2800 train loss: 0.0098 test loss: 0.0095\n",
      "Epoch 2 batch 2900 train loss: 0.0068 test loss: 0.0099\n",
      "Epoch 3 batch 0 train loss: 0.0067 test loss: 0.0096\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.40p/ckpt-24\n",
      "Epoch 3 batch 100 train loss: 0.0077 test loss: 0.0092\n",
      "Epoch 3 batch 200 train loss: 0.0074 test loss: 0.0101\n",
      "Epoch 3 batch 300 train loss: 0.0081 test loss: 0.0097\n",
      "Epoch 3 batch 400 train loss: 0.0055 test loss: 0.0106\n",
      "Epoch 3 batch 500 train loss: 0.0064 test loss: 0.0106\n",
      "Epoch 3 batch 600 train loss: 0.0091 test loss: 0.0104\n",
      "Epoch 3 batch 700 train loss: 0.0064 test loss: 0.0108\n",
      "Epoch 3 batch 800 train loss: 0.0066 test loss: 0.0104\n",
      "Epoch 3 batch 900 train loss: 0.0093 test loss: 0.0101\n",
      "Epoch 3 batch 1000 train loss: 0.0068 test loss: 0.0105\n",
      "Epoch 3 batch 1100 train loss: 0.0077 test loss: 0.0107\n",
      "Epoch 3 batch 1200 train loss: 0.0091 test loss: 0.0097\n",
      "Epoch 3 batch 1300 train loss: 0.0067 test loss: 0.0100\n",
      "Epoch 3 batch 1400 train loss: 0.0075 test loss: 0.0105\n",
      "Epoch 3 batch 1500 train loss: 0.0090 test loss: 0.0098\n",
      "Epoch 3 batch 1600 train loss: 0.0092 test loss: 0.0112\n",
      "Epoch 3 batch 1700 train loss: 0.0069 test loss: 0.0104\n",
      "Epoch 3 batch 1800 train loss: 0.0057 test loss: 0.0097\n",
      "Epoch 3 batch 1900 train loss: 0.0060 test loss: 0.0106\n",
      "Epoch 3 batch 2000 train loss: 0.0061 test loss: 0.0110\n",
      "Epoch 3 batch 2100 train loss: 0.0086 test loss: 0.0101\n",
      "Epoch 3 batch 2200 train loss: 0.0073 test loss: 0.0104\n",
      "Epoch 3 batch 2300 train loss: 0.0076 test loss: 0.0106\n",
      "Epoch 3 batch 2400 train loss: 0.0061 test loss: 0.0100\n",
      "Epoch 3 batch 2500 train loss: 0.0059 test loss: 0.0098\n",
      "Epoch 3 batch 2600 train loss: 0.0070 test loss: 0.0100\n",
      "Epoch 3 batch 2700 train loss: 0.0091 test loss: 0.0104\n",
      "Epoch 3 batch 2800 train loss: 0.0071 test loss: 0.0101\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.40p/ckpt-25\n",
      "Epoch 3 batch 2900 train loss: 0.0082 test loss: 0.0092\n",
      "Epoch 4 batch 0 train loss: 0.0059 test loss: 0.0094\n",
      "Epoch 4 batch 100 train loss: 0.0052 test loss: 0.0097\n",
      "Epoch 4 batch 200 train loss: 0.0073 test loss: 0.0097\n",
      "Epoch 4 batch 300 train loss: 0.0066 test loss: 0.0098\n",
      "Epoch 4 batch 400 train loss: 0.0062 test loss: 0.0099\n",
      "Epoch 4 batch 500 train loss: 0.0062 test loss: 0.0101\n",
      "Epoch 4 batch 600 train loss: 0.0072 test loss: 0.0107\n",
      "Epoch 4 batch 700 train loss: 0.0082 test loss: 0.0107\n",
      "Epoch 4 batch 800 train loss: 0.0067 test loss: 0.0114\n",
      "Epoch 4 batch 900 train loss: 0.0099 test loss: 0.0096\n",
      "Epoch 4 batch 1000 train loss: 0.0074 test loss: 0.0100\n",
      "Epoch 4 batch 1100 train loss: 0.0066 test loss: 0.0104\n",
      "Epoch 4 batch 1200 train loss: 0.0074 test loss: 0.0103\n",
      "Epoch 4 batch 1300 train loss: 0.0072 test loss: 0.0092\n",
      "Epoch 4 batch 1400 train loss: 0.0051 test loss: 0.0100\n",
      "Epoch 4 batch 1500 train loss: 0.0081 test loss: 0.0099\n",
      "Epoch 4 batch 1600 train loss: 0.0078 test loss: 0.0099\n",
      "Epoch 4 batch 1700 train loss: 0.0064 test loss: 0.0100\n",
      "Epoch 4 batch 1800 train loss: 0.0066 test loss: 0.0098\n",
      "Epoch 4 batch 1900 train loss: 0.0069 test loss: 0.0106\n",
      "Epoch 4 batch 2000 train loss: 0.0089 test loss: 0.0102\n",
      "Epoch 4 batch 2100 train loss: 0.0079 test loss: 0.0102\n",
      "Epoch 4 batch 2200 train loss: 0.0063 test loss: 0.0110\n",
      "Epoch 4 batch 2300 train loss: 0.0082 test loss: 0.0104\n",
      "Epoch 4 batch 2400 train loss: 0.0096 test loss: 0.0097\n",
      "Epoch 4 batch 2500 train loss: 0.0068 test loss: 0.0107\n",
      "Epoch 4 batch 2600 train loss: 0.0072 test loss: 0.0106\n",
      "Epoch 4 batch 2700 train loss: 0.0067 test loss: 0.0104\n",
      "Epoch 4 batch 2800 train loss: 0.0068 test loss: 0.0099\n",
      "Epoch 4 batch 2900 train loss: 0.0069 test loss: 0.0093\n",
      "Epoch 5 batch 0 train loss: 0.0083 test loss: 0.0099\n",
      "Epoch 5 batch 100 train loss: 0.0095 test loss: 0.0117\n",
      "Epoch 5 batch 200 train loss: 0.0080 test loss: 0.0110\n",
      "Epoch 5 batch 300 train loss: 0.0083 test loss: 0.0104\n",
      "Epoch 5 batch 400 train loss: 0.0073 test loss: 0.0097\n",
      "Epoch 5 batch 500 train loss: 0.0066 test loss: 0.0100\n",
      "Epoch 5 batch 600 train loss: 0.0073 test loss: 0.0096\n",
      "Epoch 5 batch 700 train loss: 0.0082 test loss: 0.0100\n",
      "Epoch 5 batch 800 train loss: 0.0085 test loss: 0.0106\n",
      "Epoch 5 batch 900 train loss: 0.0100 test loss: 0.0098\n",
      "Epoch 5 batch 1000 train loss: 0.0052 test loss: 0.0103\n",
      "Epoch 5 batch 1100 train loss: 0.0065 test loss: 0.0103\n",
      "Epoch 5 batch 1200 train loss: 0.0075 test loss: 0.0098\n",
      "Epoch 5 batch 1300 train loss: 0.0086 test loss: 0.0098\n",
      "Epoch 5 batch 1400 train loss: 0.0065 test loss: 0.0105\n",
      "Epoch 5 batch 1500 train loss: 0.0078 test loss: 0.0102\n",
      "Epoch 5 batch 1600 train loss: 0.0105 test loss: 0.0099\n",
      "Epoch 5 batch 1700 train loss: 0.0068 test loss: 0.0096\n",
      "Epoch 5 batch 1800 train loss: 0.0072 test loss: 0.0103\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.40p/ckpt-26\n",
      "Epoch 5 batch 1900 train loss: 0.0067 test loss: 0.0090\n",
      "Epoch 5 batch 2000 train loss: 0.0068 test loss: 0.0091\n",
      "Epoch 5 batch 2100 train loss: 0.0077 test loss: 0.0096\n",
      "Epoch 5 batch 2200 train loss: 0.0103 test loss: 0.0105\n",
      "Epoch 5 batch 2300 train loss: 0.0076 test loss: 0.0092\n",
      "Epoch 5 batch 2400 train loss: 0.0071 test loss: 0.0107\n",
      "Epoch 5 batch 2500 train loss: 0.0064 test loss: 0.0107\n",
      "Epoch 5 batch 2600 train loss: 0.0055 test loss: 0.0102\n",
      "Epoch 5 batch 2700 train loss: 0.0059 test loss: 0.0104\n",
      "Epoch 5 batch 2800 train loss: 0.0079 test loss: 0.0092\n",
      "Epoch 5 batch 2900 train loss: 0.0098 test loss: 0.0093\n",
      "Epoch 6 batch 0 train loss: 0.0065 test loss: 0.0096\n",
      "Epoch 6 batch 100 train loss: 0.0086 test loss: 0.0098\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.40p/ckpt-27\n",
      "Epoch 6 batch 200 train loss: 0.0096 test loss: 0.0090\n",
      "Epoch 6 batch 300 train loss: 0.0065 test loss: 0.0093\n",
      "Epoch 6 batch 400 train loss: 0.0087 test loss: 0.0099\n",
      "Epoch 6 batch 500 train loss: 0.0064 test loss: 0.0092\n",
      "Epoch 6 batch 600 train loss: 0.0087 test loss: 0.0098\n",
      "Epoch 6 batch 700 train loss: 0.0079 test loss: 0.0102\n",
      "Epoch 6 batch 800 train loss: 0.0068 test loss: 0.0097\n",
      "Epoch 6 batch 900 train loss: 0.0064 test loss: 0.0102\n",
      "Epoch 6 batch 1000 train loss: 0.0075 test loss: 0.0102\n",
      "Epoch 6 batch 1100 train loss: 0.0101 test loss: 0.0105\n",
      "Epoch 6 batch 1200 train loss: 0.0077 test loss: 0.0101\n",
      "Epoch 6 batch 1300 train loss: 0.0064 test loss: 0.0094\n",
      "Epoch 6 batch 1400 train loss: 0.0079 test loss: 0.0108\n",
      "Epoch 6 batch 1500 train loss: 0.0064 test loss: 0.0100\n",
      "Epoch 6 batch 1600 train loss: 0.0066 test loss: 0.0114\n",
      "Epoch 6 batch 1700 train loss: 0.0078 test loss: 0.0101\n",
      "Epoch 6 batch 1800 train loss: 0.0105 test loss: 0.0101\n",
      "Epoch 6 batch 1900 train loss: 0.0075 test loss: 0.0092\n",
      "Epoch 6 batch 2000 train loss: 0.0069 test loss: 0.0099\n",
      "Epoch 6 batch 2100 train loss: 0.0088 test loss: 0.0104\n",
      "Epoch 6 batch 2200 train loss: 0.0065 test loss: 0.0106\n",
      "Epoch 6 batch 2300 train loss: 0.0089 test loss: 0.0108\n",
      "Epoch 6 batch 2400 train loss: 0.0079 test loss: 0.0098\n",
      "Epoch 6 batch 2500 train loss: 0.0063 test loss: 0.0108\n",
      "Epoch 6 batch 2600 train loss: 0.0092 test loss: 0.0093\n",
      "Epoch 6 batch 2700 train loss: 0.0076 test loss: 0.0111\n",
      "Epoch 6 batch 2800 train loss: 0.0081 test loss: 0.0094\n",
      "Epoch 6 batch 2900 train loss: 0.0057 test loss: 0.0094\n",
      "Epoch 7 batch 0 train loss: 0.0073 test loss: 0.0097\n",
      "Epoch 7 batch 100 train loss: 0.0087 test loss: 0.0098\n",
      "Epoch 7 batch 200 train loss: 0.0110 test loss: 0.0097\n",
      "Epoch 7 batch 300 train loss: 0.0077 test loss: 0.0105\n",
      "Epoch 7 batch 400 train loss: 0.0066 test loss: 0.0105\n",
      "Epoch 7 batch 500 train loss: 0.0117 test loss: 0.0105\n",
      "Epoch 7 batch 600 train loss: 0.0103 test loss: 0.0109\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.40p/ckpt-28\n",
      "Epoch 7 batch 700 train loss: 0.0071 test loss: 0.0088\n",
      "Epoch 7 batch 800 train loss: 0.0094 test loss: 0.0093\n",
      "Epoch 7 batch 900 train loss: 0.0057 test loss: 0.0094\n",
      "Epoch 7 batch 1000 train loss: 0.0066 test loss: 0.0104\n",
      "Epoch 7 batch 1100 train loss: 0.0079 test loss: 0.0099\n",
      "Epoch 7 batch 1200 train loss: 0.0086 test loss: 0.0102\n",
      "Epoch 7 batch 1300 train loss: 0.0091 test loss: 0.0101\n",
      "Epoch 7 batch 1400 train loss: 0.0081 test loss: 0.0100\n",
      "Epoch 7 batch 1500 train loss: 0.0068 test loss: 0.0109\n",
      "Epoch 7 batch 1600 train loss: 0.0072 test loss: 0.0094\n",
      "Epoch 7 batch 1700 train loss: 0.0072 test loss: 0.0104\n",
      "Epoch 7 batch 1800 train loss: 0.0081 test loss: 0.0105\n",
      "Epoch 7 batch 1900 train loss: 0.0056 test loss: 0.0094\n",
      "Epoch 7 batch 2000 train loss: 0.0065 test loss: 0.0107\n",
      "Epoch 7 batch 2100 train loss: 0.0077 test loss: 0.0096\n",
      "Epoch 7 batch 2200 train loss: 0.0083 test loss: 0.0113\n",
      "Epoch 7 batch 2300 train loss: 0.0085 test loss: 0.0103\n",
      "Epoch 7 batch 2400 train loss: 0.0089 test loss: 0.0112\n",
      "Epoch 7 batch 2500 train loss: 0.0083 test loss: 0.0100\n",
      "Epoch 7 batch 2600 train loss: 0.0064 test loss: 0.0099\n",
      "Epoch 7 batch 2700 train loss: 0.0064 test loss: 0.0098\n",
      "Epoch 7 batch 2800 train loss: 0.0109 test loss: 0.0097\n",
      "Epoch 7 batch 2900 train loss: 0.0061 test loss: 0.0097\n",
      "Epoch 8 batch 0 train loss: 0.0066 test loss: 0.0093\n",
      "Epoch 8 batch 100 train loss: 0.0072 test loss: 0.0098\n",
      "Epoch 8 batch 200 train loss: 0.0072 test loss: 0.0094\n",
      "Epoch 8 batch 300 train loss: 0.0071 test loss: 0.0098\n",
      "Epoch 8 batch 400 train loss: 0.0099 test loss: 0.0106\n",
      "Epoch 8 batch 500 train loss: 0.0096 test loss: 0.0098\n",
      "Epoch 8 batch 600 train loss: 0.0083 test loss: 0.0097\n",
      "Epoch 8 batch 700 train loss: 0.0059 test loss: 0.0098\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.40p/ckpt-29\n",
      "Epoch 8 batch 800 train loss: 0.0093 test loss: 0.0088\n",
      "Epoch 8 batch 900 train loss: 0.0062 test loss: 0.0099\n",
      "Epoch 8 batch 1000 train loss: 0.0075 test loss: 0.0110\n",
      "Epoch 8 batch 1100 train loss: 0.0070 test loss: 0.0100\n",
      "Epoch 8 batch 1200 train loss: 0.0098 test loss: 0.0096\n",
      "Epoch 8 batch 1300 train loss: 0.0054 test loss: 0.0114\n",
      "Epoch 8 batch 1400 train loss: 0.0088 test loss: 0.0098\n",
      "Epoch 8 batch 1500 train loss: 0.0078 test loss: 0.0102\n",
      "Epoch 8 batch 1600 train loss: 0.0079 test loss: 0.0104\n",
      "Epoch 8 batch 1700 train loss: 0.0071 test loss: 0.0108\n",
      "Epoch 8 batch 1800 train loss: 0.0055 test loss: 0.0101\n",
      "Epoch 8 batch 1900 train loss: 0.0075 test loss: 0.0107\n",
      "Epoch 8 batch 2000 train loss: 0.0062 test loss: 0.0102\n",
      "Epoch 8 batch 2100 train loss: 0.0078 test loss: 0.0100\n",
      "Epoch 8 batch 2200 train loss: 0.0072 test loss: 0.0097\n",
      "Epoch 8 batch 2300 train loss: 0.0074 test loss: 0.0098\n",
      "Epoch 8 batch 2400 train loss: 0.0076 test loss: 0.0104\n",
      "Epoch 8 batch 2500 train loss: 0.0071 test loss: 0.0110\n",
      "Epoch 8 batch 2600 train loss: 0.0089 test loss: 0.0100\n",
      "Epoch 8 batch 2700 train loss: 0.0061 test loss: 0.0103\n",
      "Epoch 8 batch 2800 train loss: 0.0061 test loss: 0.0094\n",
      "Epoch 8 batch 2900 train loss: 0.0070 test loss: 0.0094\n",
      "Epoch 9 batch 0 train loss: 0.0064 test loss: 0.0094\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.40p/ckpt-30\n",
      "Epoch 9 batch 100 train loss: 0.0069 test loss: 0.0086\n",
      "Epoch 9 batch 200 train loss: 0.0076 test loss: 0.0101\n",
      "Epoch 9 batch 300 train loss: 0.0079 test loss: 0.0103\n",
      "Epoch 9 batch 400 train loss: 0.0062 test loss: 0.0097\n",
      "Epoch 9 batch 500 train loss: 0.0091 test loss: 0.0101\n",
      "Epoch 9 batch 600 train loss: 0.0059 test loss: 0.0103\n",
      "Epoch 9 batch 700 train loss: 0.0072 test loss: 0.0099\n",
      "Epoch 9 batch 800 train loss: 0.0079 test loss: 0.0092\n",
      "Epoch 9 batch 900 train loss: 0.0086 test loss: 0.0099\n",
      "Epoch 9 batch 1000 train loss: 0.0084 test loss: 0.0100\n",
      "Epoch 9 batch 1100 train loss: 0.0072 test loss: 0.0104\n",
      "Epoch 9 batch 1200 train loss: 0.0063 test loss: 0.0097\n",
      "Epoch 9 batch 1300 train loss: 0.0086 test loss: 0.0100\n",
      "Epoch 9 batch 1400 train loss: 0.0053 test loss: 0.0100\n",
      "Epoch 9 batch 1500 train loss: 0.0085 test loss: 0.0111\n",
      "Epoch 9 batch 1600 train loss: 0.0063 test loss: 0.0108\n",
      "Epoch 9 batch 1700 train loss: 0.0081 test loss: 0.0099\n",
      "Epoch 9 batch 1800 train loss: 0.0087 test loss: 0.0091\n",
      "Epoch 9 batch 1900 train loss: 0.0073 test loss: 0.0098\n",
      "Epoch 9 batch 2000 train loss: 0.0067 test loss: 0.0097\n",
      "Epoch 9 batch 2100 train loss: 0.0093 test loss: 0.0094\n",
      "Epoch 9 batch 2200 train loss: 0.0102 test loss: 0.0095\n",
      "Epoch 9 batch 2300 train loss: 0.0048 test loss: 0.0105\n",
      "Epoch 9 batch 2400 train loss: 0.0064 test loss: 0.0100\n",
      "Epoch 9 batch 2500 train loss: 0.0075 test loss: 0.0096\n",
      "Epoch 9 batch 2600 train loss: 0.0069 test loss: 0.0096\n",
      "Epoch 9 batch 2700 train loss: 0.0087 test loss: 0.0104\n",
      "Epoch 9 batch 2800 train loss: 0.0072 test loss: 0.0098\n",
      "Epoch 9 batch 2900 train loss: 0.0065 test loss: 0.0091\n",
      "Epoch 10 batch 0 train loss: 0.0075 test loss: 0.0095\n",
      "Epoch 10 batch 100 train loss: 0.0094 test loss: 0.0095\n",
      "Epoch 10 batch 200 train loss: 0.0088 test loss: 0.0093\n",
      "Epoch 10 batch 300 train loss: 0.0059 test loss: 0.0100\n",
      "Epoch 10 batch 400 train loss: 0.0070 test loss: 0.0097\n",
      "Epoch 10 batch 500 train loss: 0.0094 test loss: 0.0113\n",
      "Epoch 10 batch 600 train loss: 0.0067 test loss: 0.0095\n",
      "Epoch 10 batch 700 train loss: 0.0053 test loss: 0.0103\n",
      "Epoch 10 batch 800 train loss: 0.0079 test loss: 0.0100\n",
      "Epoch 10 batch 900 train loss: 0.0080 test loss: 0.0096\n",
      "Epoch 10 batch 1000 train loss: 0.0072 test loss: 0.0092\n",
      "Epoch 10 batch 1100 train loss: 0.0061 test loss: 0.0101\n",
      "Epoch 10 batch 1200 train loss: 0.0069 test loss: 0.0105\n",
      "Epoch 10 batch 1300 train loss: 0.0068 test loss: 0.0108\n",
      "Epoch 10 batch 1400 train loss: 0.0067 test loss: 0.0099\n",
      "Epoch 10 batch 1500 train loss: 0.0067 test loss: 0.0104\n",
      "Epoch 10 batch 1600 train loss: 0.0075 test loss: 0.0100\n",
      "Epoch 10 batch 1700 train loss: 0.0061 test loss: 0.0104\n",
      "Epoch 10 batch 1800 train loss: 0.0073 test loss: 0.0111\n",
      "Epoch 10 batch 1900 train loss: 0.0078 test loss: 0.0109\n",
      "Epoch 10 batch 2000 train loss: 0.0059 test loss: 0.0099\n",
      "Epoch 10 batch 2100 train loss: 0.0086 test loss: 0.0103\n",
      "Epoch 10 batch 2200 train loss: 0.0061 test loss: 0.0098\n",
      "Epoch 10 batch 2300 train loss: 0.0059 test loss: 0.0106\n",
      "Epoch 10 batch 2400 train loss: 0.0060 test loss: 0.0105\n",
      "Epoch 10 batch 2500 train loss: 0.0077 test loss: 0.0100\n",
      "Epoch 10 batch 2600 train loss: 0.0088 test loss: 0.0096\n",
      "Epoch 10 batch 2700 train loss: 0.0076 test loss: 0.0104\n",
      "Epoch 10 batch 2800 train loss: 0.0067 test loss: 0.0093\n",
      "Epoch 10 batch 2900 train loss: 0.0078 test loss: 0.0087\n",
      "Epoch 11 batch 0 train loss: 0.0096 test loss: 0.0097\n",
      "Epoch 11 batch 100 train loss: 0.0049 test loss: 0.0095\n",
      "Epoch 11 batch 200 train loss: 0.0060 test loss: 0.0102\n",
      "Epoch 11 batch 300 train loss: 0.0076 test loss: 0.0094\n",
      "Epoch 11 batch 400 train loss: 0.0086 test loss: 0.0106\n",
      "Epoch 11 batch 500 train loss: 0.0075 test loss: 0.0102\n",
      "Epoch 11 batch 600 train loss: 0.0069 test loss: 0.0105\n",
      "Epoch 11 batch 700 train loss: 0.0080 test loss: 0.0096\n",
      "Epoch 11 batch 800 train loss: 0.0079 test loss: 0.0100\n",
      "Epoch 11 batch 900 train loss: 0.0077 test loss: 0.0099\n",
      "Epoch 11 batch 1000 train loss: 0.0073 test loss: 0.0104\n",
      "Epoch 11 batch 1100 train loss: 0.0062 test loss: 0.0096\n",
      "Epoch 11 batch 1200 train loss: 0.0075 test loss: 0.0093\n",
      "Epoch 11 batch 1300 train loss: 0.0057 test loss: 0.0103\n",
      "Epoch 11 batch 1400 train loss: 0.0054 test loss: 0.0097\n",
      "Epoch 11 batch 1500 train loss: 0.0088 test loss: 0.0101\n",
      "Epoch 11 batch 1600 train loss: 0.0052 test loss: 0.0095\n",
      "Epoch 11 batch 1700 train loss: 0.0081 test loss: 0.0097\n",
      "Epoch 11 batch 1800 train loss: 0.0073 test loss: 0.0092\n",
      "Epoch 11 batch 1900 train loss: 0.0064 test loss: 0.0105\n",
      "Epoch 11 batch 2000 train loss: 0.0078 test loss: 0.0091\n",
      "Epoch 11 batch 2100 train loss: 0.0068 test loss: 0.0112\n",
      "Epoch 11 batch 2200 train loss: 0.0082 test loss: 0.0106\n",
      "Epoch 11 batch 2300 train loss: 0.0077 test loss: 0.0102\n",
      "Epoch 11 batch 2400 train loss: 0.0075 test loss: 0.0104\n",
      "Epoch 11 batch 2500 train loss: 0.0080 test loss: 0.0103\n",
      "Epoch 11 batch 2600 train loss: 0.0062 test loss: 0.0095\n",
      "Epoch 11 batch 2700 train loss: 0.0063 test loss: 0.0100\n",
      "Epoch 11 batch 2800 train loss: 0.0089 test loss: 0.0087\n",
      "Epoch 11 batch 2900 train loss: 0.0069 test loss: 0.0101\n",
      "Epoch 12 batch 0 train loss: 0.0066 test loss: 0.0096\n",
      "Epoch 12 batch 100 train loss: 0.0066 test loss: 0.0105\n",
      "Epoch 12 batch 200 train loss: 0.0054 test loss: 0.0092\n",
      "Epoch 12 batch 300 train loss: 0.0082 test loss: 0.0096\n",
      "Epoch 12 batch 400 train loss: 0.0065 test loss: 0.0103\n",
      "Epoch 12 batch 500 train loss: 0.0057 test loss: 0.0097\n",
      "Epoch 12 batch 600 train loss: 0.0078 test loss: 0.0091\n",
      "Epoch 12 batch 700 train loss: 0.0081 test loss: 0.0096\n",
      "Epoch 12 batch 800 train loss: 0.0094 test loss: 0.0102\n",
      "Epoch 12 batch 900 train loss: 0.0076 test loss: 0.0093\n",
      "Epoch 12 batch 1000 train loss: 0.0062 test loss: 0.0103\n",
      "Epoch 12 batch 1100 train loss: 0.0076 test loss: 0.0102\n",
      "Epoch 12 batch 1200 train loss: 0.0074 test loss: 0.0099\n",
      "Epoch 12 batch 1300 train loss: 0.0066 test loss: 0.0099\n",
      "Epoch 12 batch 1400 train loss: 0.0059 test loss: 0.0101\n",
      "Epoch 12 batch 1500 train loss: 0.0071 test loss: 0.0102\n",
      "Epoch 12 batch 1600 train loss: 0.0096 test loss: 0.0095\n",
      "Epoch 12 batch 1700 train loss: 0.0068 test loss: 0.0102\n",
      "Epoch 12 batch 1800 train loss: 0.0054 test loss: 0.0100\n",
      "Epoch 12 batch 1900 train loss: 0.0076 test loss: 0.0093\n",
      "Epoch 12 batch 2000 train loss: 0.0077 test loss: 0.0099\n",
      "Epoch 12 batch 2100 train loss: 0.0056 test loss: 0.0098\n",
      "Epoch 12 batch 2200 train loss: 0.0068 test loss: 0.0094\n",
      "Epoch 12 batch 2300 train loss: 0.0066 test loss: 0.0107\n",
      "Epoch 12 batch 2400 train loss: 0.0083 test loss: 0.0099\n",
      "Epoch 12 batch 2500 train loss: 0.0067 test loss: 0.0107\n",
      "Epoch 12 batch 2600 train loss: 0.0064 test loss: 0.0091\n",
      "Epoch 12 batch 2700 train loss: 0.0062 test loss: 0.0096\n",
      "Epoch 12 batch 2800 train loss: 0.0070 test loss: 0.0089\n",
      "Epoch 12 batch 2900 train loss: 0.0069 test loss: 0.0094\n",
      "Epoch 13 batch 0 train loss: 0.0071 test loss: 0.0097\n",
      "Epoch 13 batch 100 train loss: 0.0060 test loss: 0.0097\n",
      "Epoch 13 batch 200 train loss: 0.0065 test loss: 0.0094\n",
      "Epoch 13 batch 300 train loss: 0.0087 test loss: 0.0103\n",
      "Epoch 13 batch 400 train loss: 0.0078 test loss: 0.0100\n",
      "Epoch 13 batch 500 train loss: 0.0070 test loss: 0.0098\n",
      "Epoch 13 batch 600 train loss: 0.0071 test loss: 0.0110\n",
      "Epoch 13 batch 700 train loss: 0.0085 test loss: 0.0101\n",
      "Epoch 13 batch 800 train loss: 0.0063 test loss: 0.0098\n",
      "Epoch 13 batch 900 train loss: 0.0066 test loss: 0.0099\n",
      "Epoch 13 batch 1000 train loss: 0.0088 test loss: 0.0100\n",
      "Epoch 13 batch 1100 train loss: 0.0066 test loss: 0.0103\n",
      "Epoch 13 batch 1200 train loss: 0.0068 test loss: 0.0099\n",
      "Epoch 13 batch 1300 train loss: 0.0105 test loss: 0.0100\n",
      "Epoch 13 batch 1400 train loss: 0.0053 test loss: 0.0101\n",
      "Epoch 13 batch 1500 train loss: 0.0070 test loss: 0.0091\n",
      "Epoch 13 batch 1600 train loss: 0.0065 test loss: 0.0100\n",
      "Epoch 13 batch 1700 train loss: 0.0063 test loss: 0.0101\n",
      "Epoch 13 batch 1800 train loss: 0.0097 test loss: 0.0093\n",
      "Epoch 13 batch 1900 train loss: 0.0059 test loss: 0.0102\n",
      "Epoch 13 batch 2000 train loss: 0.0073 test loss: 0.0101\n",
      "Epoch 13 batch 2100 train loss: 0.0079 test loss: 0.0097\n",
      "Epoch 13 batch 2200 train loss: 0.0079 test loss: 0.0101\n",
      "Epoch 13 batch 2300 train loss: 0.0078 test loss: 0.0112\n",
      "Epoch 13 batch 2400 train loss: 0.0081 test loss: 0.0102\n",
      "Epoch 13 batch 2500 train loss: 0.0084 test loss: 0.0102\n",
      "Epoch 13 batch 2600 train loss: 0.0079 test loss: 0.0094\n",
      "Epoch 13 batch 2700 train loss: 0.0091 test loss: 0.0101\n",
      "Epoch 13 batch 2800 train loss: 0.0081 test loss: 0.0091\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.40p/ckpt-31\n",
      "Epoch 13 batch 2900 train loss: 0.0073 test loss: 0.0086\n",
      "Epoch 14 batch 0 train loss: 0.0068 test loss: 0.0087\n",
      "Epoch 14 batch 100 train loss: 0.0072 test loss: 0.0103\n",
      "Epoch 14 batch 200 train loss: 0.0076 test loss: 0.0097\n",
      "Epoch 14 batch 300 train loss: 0.0061 test loss: 0.0099\n",
      "Epoch 14 batch 400 train loss: 0.0075 test loss: 0.0093\n",
      "Epoch 14 batch 500 train loss: 0.0077 test loss: 0.0103\n",
      "Epoch 14 batch 600 train loss: 0.0080 test loss: 0.0104\n",
      "Epoch 14 batch 700 train loss: 0.0064 test loss: 0.0088\n",
      "Epoch 14 batch 800 train loss: 0.0083 test loss: 0.0103\n",
      "Epoch 14 batch 900 train loss: 0.0066 test loss: 0.0095\n",
      "Epoch 14 batch 1000 train loss: 0.0060 test loss: 0.0099\n",
      "Epoch 14 batch 1100 train loss: 0.0084 test loss: 0.0091\n",
      "Epoch 14 batch 1200 train loss: 0.0090 test loss: 0.0104\n",
      "Epoch 14 batch 1300 train loss: 0.0083 test loss: 0.0101\n",
      "Epoch 14 batch 1400 train loss: 0.0060 test loss: 0.0092\n",
      "Epoch 14 batch 1500 train loss: 0.0078 test loss: 0.0100\n",
      "Epoch 14 batch 1600 train loss: 0.0064 test loss: 0.0091\n",
      "Epoch 14 batch 1700 train loss: 0.0058 test loss: 0.0100\n",
      "Epoch 14 batch 1800 train loss: 0.0077 test loss: 0.0096\n",
      "Epoch 14 batch 1900 train loss: 0.0073 test loss: 0.0100\n",
      "Epoch 14 batch 2000 train loss: 0.0067 test loss: 0.0098\n",
      "Epoch 14 batch 2100 train loss: 0.0087 test loss: 0.0105\n",
      "Epoch 14 batch 2200 train loss: 0.0046 test loss: 0.0108\n",
      "Epoch 14 batch 2300 train loss: 0.0079 test loss: 0.0103\n",
      "Epoch 14 batch 2400 train loss: 0.0082 test loss: 0.0095\n",
      "Epoch 14 batch 2500 train loss: 0.0085 test loss: 0.0112\n",
      "Epoch 14 batch 2600 train loss: 0.0080 test loss: 0.0098\n",
      "Epoch 14 batch 2700 train loss: 0.0047 test loss: 0.0101\n",
      "Epoch 14 batch 2800 train loss: 0.0076 test loss: 0.0095\n",
      "Epoch 14 batch 2900 train loss: 0.0071 test loss: 0.0095\n",
      "Epoch 15 batch 0 train loss: 0.0077 test loss: 0.0093\n",
      "Epoch 15 batch 100 train loss: 0.0067 test loss: 0.0098\n",
      "Epoch 15 batch 200 train loss: 0.0078 test loss: 0.0092\n",
      "Epoch 15 batch 300 train loss: 0.0063 test loss: 0.0094\n",
      "Epoch 15 batch 400 train loss: 0.0056 test loss: 0.0105\n",
      "Epoch 15 batch 500 train loss: 0.0078 test loss: 0.0107\n",
      "Epoch 15 batch 600 train loss: 0.0053 test loss: 0.0103\n",
      "Epoch 15 batch 700 train loss: 0.0060 test loss: 0.0098\n",
      "Epoch 15 batch 800 train loss: 0.0101 test loss: 0.0098\n",
      "Epoch 15 batch 900 train loss: 0.0058 test loss: 0.0095\n",
      "Epoch 15 batch 1000 train loss: 0.0078 test loss: 0.0101\n",
      "Epoch 15 batch 1100 train loss: 0.0090 test loss: 0.0103\n",
      "Epoch 15 batch 1200 train loss: 0.0088 test loss: 0.0097\n",
      "Epoch 15 batch 1300 train loss: 0.0072 test loss: 0.0096\n",
      "Epoch 15 batch 1400 train loss: 0.0076 test loss: 0.0109\n",
      "Epoch 15 batch 1500 train loss: 0.0076 test loss: 0.0103\n",
      "Epoch 15 batch 1600 train loss: 0.0076 test loss: 0.0097\n",
      "Epoch 15 batch 1700 train loss: 0.0082 test loss: 0.0103\n",
      "Epoch 15 batch 1800 train loss: 0.0094 test loss: 0.0094\n",
      "Epoch 15 batch 1900 train loss: 0.0085 test loss: 0.0100\n",
      "Epoch 15 batch 2000 train loss: 0.0090 test loss: 0.0108\n",
      "Epoch 15 batch 2100 train loss: 0.0073 test loss: 0.0108\n",
      "Epoch 15 batch 2200 train loss: 0.0056 test loss: 0.0106\n",
      "Epoch 15 batch 2300 train loss: 0.0068 test loss: 0.0094\n",
      "Epoch 15 batch 2400 train loss: 0.0072 test loss: 0.0107\n",
      "Epoch 15 batch 2500 train loss: 0.0078 test loss: 0.0095\n",
      "Epoch 15 batch 2600 train loss: 0.0071 test loss: 0.0100\n",
      "Epoch 15 batch 2700 train loss: 0.0091 test loss: 0.0101\n",
      "Epoch 15 batch 2800 train loss: 0.0096 test loss: 0.0096\n",
      "Epoch 15 batch 2900 train loss: 0.0081 test loss: 0.0090\n",
      "Epoch 16 batch 0 train loss: 0.0059 test loss: 0.0089\n",
      "Epoch 16 batch 100 train loss: 0.0059 test loss: 0.0098\n",
      "Epoch 16 batch 200 train loss: 0.0064 test loss: 0.0100\n",
      "Epoch 16 batch 300 train loss: 0.0072 test loss: 0.0096\n",
      "Epoch 16 batch 400 train loss: 0.0083 test loss: 0.0099\n",
      "Epoch 16 batch 500 train loss: 0.0057 test loss: 0.0094\n",
      "Epoch 16 batch 600 train loss: 0.0082 test loss: 0.0099\n",
      "Epoch 16 batch 700 train loss: 0.0068 test loss: 0.0104\n",
      "Epoch 16 batch 800 train loss: 0.0076 test loss: 0.0102\n",
      "Epoch 16 batch 900 train loss: 0.0053 test loss: 0.0093\n",
      "Epoch 16 batch 1000 train loss: 0.0063 test loss: 0.0097\n",
      "Epoch 16 batch 1100 train loss: 0.0081 test loss: 0.0095\n",
      "Epoch 16 batch 1200 train loss: 0.0091 test loss: 0.0095\n",
      "Epoch 16 batch 1300 train loss: 0.0080 test loss: 0.0101\n",
      "Epoch 16 batch 1400 train loss: 0.0058 test loss: 0.0093\n",
      "Epoch 16 batch 1500 train loss: 0.0072 test loss: 0.0107\n",
      "Epoch 16 batch 1600 train loss: 0.0080 test loss: 0.0099\n",
      "Epoch 16 batch 1700 train loss: 0.0087 test loss: 0.0102\n",
      "Epoch 16 batch 1800 train loss: 0.0091 test loss: 0.0102\n",
      "Epoch 16 batch 1900 train loss: 0.0090 test loss: 0.0101\n",
      "Epoch 16 batch 2000 train loss: 0.0086 test loss: 0.0103\n",
      "Epoch 16 batch 2100 train loss: 0.0082 test loss: 0.0099\n",
      "Epoch 16 batch 2200 train loss: 0.0074 test loss: 0.0100\n",
      "Epoch 16 batch 2300 train loss: 0.0061 test loss: 0.0097\n",
      "Epoch 16 batch 2400 train loss: 0.0088 test loss: 0.0094\n",
      "Epoch 16 batch 2500 train loss: 0.0082 test loss: 0.0109\n",
      "Epoch 16 batch 2600 train loss: 0.0102 test loss: 0.0090\n",
      "Epoch 16 batch 2700 train loss: 0.0059 test loss: 0.0095\n",
      "Epoch 16 batch 2800 train loss: 0.0066 test loss: 0.0092\n",
      "Epoch 16 batch 2900 train loss: 0.0088 test loss: 0.0094\n",
      "Epoch 17 batch 0 train loss: 0.0060 test loss: 0.0094\n",
      "Epoch 17 batch 100 train loss: 0.0070 test loss: 0.0094\n",
      "Epoch 17 batch 200 train loss: 0.0083 test loss: 0.0100\n",
      "Epoch 17 batch 300 train loss: 0.0052 test loss: 0.0094\n",
      "Epoch 17 batch 400 train loss: 0.0097 test loss: 0.0103\n",
      "Epoch 17 batch 500 train loss: 0.0074 test loss: 0.0116\n",
      "Epoch 17 batch 600 train loss: 0.0054 test loss: 0.0098\n",
      "Epoch 17 batch 700 train loss: 0.0059 test loss: 0.0102\n",
      "Epoch 17 batch 800 train loss: 0.0077 test loss: 0.0102\n",
      "Epoch 17 batch 900 train loss: 0.0066 test loss: 0.0104\n",
      "Epoch 17 batch 1000 train loss: 0.0071 test loss: 0.0108\n",
      "Epoch 17 batch 1100 train loss: 0.0080 test loss: 0.0099\n",
      "Epoch 17 batch 1200 train loss: 0.0081 test loss: 0.0088\n",
      "Epoch 17 batch 1300 train loss: 0.0092 test loss: 0.0099\n",
      "Epoch 17 batch 1400 train loss: 0.0061 test loss: 0.0099\n",
      "Epoch 17 batch 1500 train loss: 0.0052 test loss: 0.0101\n",
      "Epoch 17 batch 1600 train loss: 0.0086 test loss: 0.0100\n",
      "Epoch 17 batch 1700 train loss: 0.0073 test loss: 0.0106\n",
      "Epoch 17 batch 1800 train loss: 0.0069 test loss: 0.0100\n",
      "Epoch 17 batch 1900 train loss: 0.0069 test loss: 0.0119\n",
      "Epoch 17 batch 2000 train loss: 0.0071 test loss: 0.0094\n",
      "early stop.\n",
      "Checkpoint 31 restored!!\n",
      "Training for loss rate 0.50 start.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['bi_lstm_4/bi_lstm_block_8/bidirectional_8/forward_lstm_8/lstm_cell_25/kernel:0', 'bi_lstm_4/bi_lstm_block_8/bidirectional_8/forward_lstm_8/lstm_cell_25/recurrent_kernel:0', 'bi_lstm_4/bi_lstm_block_8/bidirectional_8/forward_lstm_8/lstm_cell_25/bias:0', 'bi_lstm_4/bi_lstm_block_8/bidirectional_8/backward_lstm_8/lstm_cell_26/kernel:0', 'bi_lstm_4/bi_lstm_block_8/bidirectional_8/backward_lstm_8/lstm_cell_26/recurrent_kernel:0', 'bi_lstm_4/bi_lstm_block_8/bidirectional_8/backward_lstm_8/lstm_cell_26/bias:0', 'bi_lstm_4/bi_lstm_block_8/layer_normalization_8/gamma:0', 'bi_lstm_4/bi_lstm_block_8/layer_normalization_8/beta:0', 'bi_lstm_4/bi_lstm_block_9/bidirectional_9/forward_lstm_9/lstm_cell_28/kernel:0', 'bi_lstm_4/bi_lstm_block_9/bidirectional_9/forward_lstm_9/lstm_cell_28/recurrent_kernel:0', 'bi_lstm_4/bi_lstm_block_9/bidirectional_9/forward_lstm_9/lstm_cell_28/bias:0', 'bi_lstm_4/bi_lstm_block_9/bidirectional_9/backward_lstm_9/lstm_cell_29/kernel:0', 'bi_lstm_4/bi_lstm_block_9/bidirectional_9/backward_lstm_9/lstm_cell_29/recurrent_kernel:0', 'bi_lstm_4/bi_lstm_block_9/bidirectional_9/backward_lstm_9/lstm_cell_29/bias:0', 'bi_lstm_4/bi_lstm_block_9/layer_normalization_9/gamma:0', 'bi_lstm_4/bi_lstm_block_9/layer_normalization_9/beta:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['bi_lstm_4/bi_lstm_block_8/bidirectional_8/forward_lstm_8/lstm_cell_25/kernel:0', 'bi_lstm_4/bi_lstm_block_8/bidirectional_8/forward_lstm_8/lstm_cell_25/recurrent_kernel:0', 'bi_lstm_4/bi_lstm_block_8/bidirectional_8/forward_lstm_8/lstm_cell_25/bias:0', 'bi_lstm_4/bi_lstm_block_8/bidirectional_8/backward_lstm_8/lstm_cell_26/kernel:0', 'bi_lstm_4/bi_lstm_block_8/bidirectional_8/backward_lstm_8/lstm_cell_26/recurrent_kernel:0', 'bi_lstm_4/bi_lstm_block_8/bidirectional_8/backward_lstm_8/lstm_cell_26/bias:0', 'bi_lstm_4/bi_lstm_block_8/layer_normalization_8/gamma:0', 'bi_lstm_4/bi_lstm_block_8/layer_normalization_8/beta:0', 'bi_lstm_4/bi_lstm_block_9/bidirectional_9/forward_lstm_9/lstm_cell_28/kernel:0', 'bi_lstm_4/bi_lstm_block_9/bidirectional_9/forward_lstm_9/lstm_cell_28/recurrent_kernel:0', 'bi_lstm_4/bi_lstm_block_9/bidirectional_9/forward_lstm_9/lstm_cell_28/bias:0', 'bi_lstm_4/bi_lstm_block_9/bidirectional_9/backward_lstm_9/lstm_cell_29/kernel:0', 'bi_lstm_4/bi_lstm_block_9/bidirectional_9/backward_lstm_9/lstm_cell_29/recurrent_kernel:0', 'bi_lstm_4/bi_lstm_block_9/bidirectional_9/backward_lstm_9/lstm_cell_29/bias:0', 'bi_lstm_4/bi_lstm_block_9/layer_normalization_9/gamma:0', 'bi_lstm_4/bi_lstm_block_9/layer_normalization_9/beta:0'] when minimizing the loss.\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.50p/ckpt-1\n",
      "Epoch 0 batch 0 train loss: 0.6401 test loss: 0.9095\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.50p/ckpt-2\n",
      "Epoch 0 batch 100 train loss: 0.2386 test loss: 0.2825\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.50p/ckpt-3\n",
      "Epoch 0 batch 200 train loss: 0.1159 test loss: 0.1152\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.50p/ckpt-4\n",
      "Epoch 0 batch 300 train loss: 0.0555 test loss: 0.0606\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.50p/ckpt-5\n",
      "Epoch 0 batch 400 train loss: 0.0359 test loss: 0.0387\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.50p/ckpt-6\n",
      "Epoch 0 batch 500 train loss: 0.0283 test loss: 0.0280\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.50p/ckpt-7\n",
      "Epoch 0 batch 600 train loss: 0.0210 test loss: 0.0218\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.50p/ckpt-8\n",
      "Epoch 0 batch 700 train loss: 0.0160 test loss: 0.0187\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.50p/ckpt-9\n",
      "Epoch 0 batch 800 train loss: 0.0144 test loss: 0.0162\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.50p/ckpt-10\n",
      "Epoch 0 batch 900 train loss: 0.0119 test loss: 0.0152\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.50p/ckpt-11\n",
      "Epoch 0 batch 1000 train loss: 0.0107 test loss: 0.0137\n",
      "Epoch 0 batch 1100 train loss: 0.0088 test loss: 0.0140\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.50p/ckpt-12\n",
      "Epoch 0 batch 1200 train loss: 0.0132 test loss: 0.0129\n",
      "Epoch 0 batch 1300 train loss: 0.0090 test loss: 0.0139\n",
      "Epoch 0 batch 1400 train loss: 0.0112 test loss: 0.0132\n",
      "Epoch 0 batch 1500 train loss: 0.0080 test loss: 0.0133\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.50p/ckpt-13\n",
      "Epoch 0 batch 1600 train loss: 0.0124 test loss: 0.0122\n",
      "Epoch 0 batch 1700 train loss: 0.0104 test loss: 0.0133\n",
      "Epoch 0 batch 1800 train loss: 0.0098 test loss: 0.0128\n",
      "Epoch 0 batch 1900 train loss: 0.0090 test loss: 0.0129\n",
      "Epoch 0 batch 2000 train loss: 0.0083 test loss: 0.0128\n",
      "Epoch 0 batch 2100 train loss: 0.0083 test loss: 0.0128\n",
      "Epoch 0 batch 2200 train loss: 0.0085 test loss: 0.0132\n",
      "Epoch 0 batch 2300 train loss: 0.0094 test loss: 0.0127\n",
      "Epoch 0 batch 2400 train loss: 0.0073 test loss: 0.0132\n",
      "Epoch 0 batch 2500 train loss: 0.0080 test loss: 0.0125\n",
      "Epoch 0 batch 2600 train loss: 0.0115 test loss: 0.0140\n",
      "Epoch 0 batch 2700 train loss: 0.0082 test loss: 0.0122\n",
      "Epoch 0 batch 2800 train loss: 0.0106 test loss: 0.0126\n",
      "Epoch 0 batch 2900 train loss: 0.0085 test loss: 0.0134\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['bi_lstm_4/bi_lstm_block_8/bidirectional_8/forward_lstm_8/lstm_cell_25/kernel:0', 'bi_lstm_4/bi_lstm_block_8/bidirectional_8/forward_lstm_8/lstm_cell_25/recurrent_kernel:0', 'bi_lstm_4/bi_lstm_block_8/bidirectional_8/forward_lstm_8/lstm_cell_25/bias:0', 'bi_lstm_4/bi_lstm_block_8/bidirectional_8/backward_lstm_8/lstm_cell_26/kernel:0', 'bi_lstm_4/bi_lstm_block_8/bidirectional_8/backward_lstm_8/lstm_cell_26/recurrent_kernel:0', 'bi_lstm_4/bi_lstm_block_8/bidirectional_8/backward_lstm_8/lstm_cell_26/bias:0', 'bi_lstm_4/bi_lstm_block_8/layer_normalization_8/gamma:0', 'bi_lstm_4/bi_lstm_block_8/layer_normalization_8/beta:0', 'bi_lstm_4/bi_lstm_block_9/bidirectional_9/forward_lstm_9/lstm_cell_28/kernel:0', 'bi_lstm_4/bi_lstm_block_9/bidirectional_9/forward_lstm_9/lstm_cell_28/recurrent_kernel:0', 'bi_lstm_4/bi_lstm_block_9/bidirectional_9/forward_lstm_9/lstm_cell_28/bias:0', 'bi_lstm_4/bi_lstm_block_9/bidirectional_9/backward_lstm_9/lstm_cell_29/kernel:0', 'bi_lstm_4/bi_lstm_block_9/bidirectional_9/backward_lstm_9/lstm_cell_29/recurrent_kernel:0', 'bi_lstm_4/bi_lstm_block_9/bidirectional_9/backward_lstm_9/lstm_cell_29/bias:0', 'bi_lstm_4/bi_lstm_block_9/layer_normalization_9/gamma:0', 'bi_lstm_4/bi_lstm_block_9/layer_normalization_9/beta:0'] when minimizing the loss.\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.50p/ckpt-14\n",
      "Epoch 1 batch 0 train loss: 0.0069 test loss: 0.0116\n",
      "Epoch 1 batch 100 train loss: 0.0085 test loss: 0.0119\n",
      "Epoch 1 batch 200 train loss: 0.0080 test loss: 0.0125\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.50p/ckpt-15\n",
      "Epoch 1 batch 300 train loss: 0.0087 test loss: 0.0113\n",
      "Epoch 1 batch 400 train loss: 0.0077 test loss: 0.0134\n",
      "Epoch 1 batch 500 train loss: 0.0092 test loss: 0.0125\n",
      "Epoch 1 batch 600 train loss: 0.0088 test loss: 0.0124\n",
      "Epoch 1 batch 700 train loss: 0.0101 test loss: 0.0124\n",
      "Epoch 1 batch 800 train loss: 0.0078 test loss: 0.0140\n",
      "Epoch 1 batch 900 train loss: 0.0080 test loss: 0.0121\n",
      "Epoch 1 batch 1000 train loss: 0.0062 test loss: 0.0129\n",
      "Epoch 1 batch 1100 train loss: 0.0085 test loss: 0.0126\n",
      "Epoch 1 batch 1200 train loss: 0.0069 test loss: 0.0118\n",
      "Epoch 1 batch 1300 train loss: 0.0087 test loss: 0.0127\n",
      "Epoch 1 batch 1400 train loss: 0.0084 test loss: 0.0121\n",
      "Epoch 1 batch 1500 train loss: 0.0098 test loss: 0.0134\n",
      "Epoch 1 batch 1600 train loss: 0.0083 test loss: 0.0120\n",
      "Epoch 1 batch 1700 train loss: 0.0104 test loss: 0.0120\n",
      "Epoch 1 batch 1800 train loss: 0.0077 test loss: 0.0122\n",
      "Epoch 1 batch 1900 train loss: 0.0061 test loss: 0.0127\n",
      "Epoch 1 batch 2000 train loss: 0.0088 test loss: 0.0138\n",
      "Epoch 1 batch 2100 train loss: 0.0086 test loss: 0.0140\n",
      "Epoch 1 batch 2200 train loss: 0.0099 test loss: 0.0135\n",
      "Epoch 1 batch 2300 train loss: 0.0099 test loss: 0.0116\n",
      "Epoch 1 batch 2400 train loss: 0.0069 test loss: 0.0115\n",
      "Epoch 1 batch 2500 train loss: 0.0094 test loss: 0.0139\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.50p/ckpt-16\n",
      "Epoch 1 batch 2600 train loss: 0.0060 test loss: 0.0112\n",
      "Epoch 1 batch 2700 train loss: 0.0119 test loss: 0.0136\n",
      "Epoch 1 batch 2800 train loss: 0.0116 test loss: 0.0120\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.50p/ckpt-17\n",
      "Epoch 1 batch 2900 train loss: 0.0069 test loss: 0.0111\n",
      "Epoch 2 batch 0 train loss: 0.0087 test loss: 0.0114\n",
      "Epoch 2 batch 100 train loss: 0.0081 test loss: 0.0114\n",
      "Epoch 2 batch 200 train loss: 0.0074 test loss: 0.0117\n",
      "Epoch 2 batch 300 train loss: 0.0078 test loss: 0.0118\n",
      "Epoch 2 batch 400 train loss: 0.0086 test loss: 0.0123\n",
      "Epoch 2 batch 500 train loss: 0.0085 test loss: 0.0120\n",
      "Epoch 2 batch 600 train loss: 0.0071 test loss: 0.0119\n",
      "Epoch 2 batch 700 train loss: 0.0081 test loss: 0.0115\n",
      "Epoch 2 batch 800 train loss: 0.0076 test loss: 0.0118\n",
      "Epoch 2 batch 900 train loss: 0.0083 test loss: 0.0119\n",
      "Epoch 2 batch 1000 train loss: 0.0087 test loss: 0.0124\n",
      "Epoch 2 batch 1100 train loss: 0.0072 test loss: 0.0119\n",
      "Epoch 2 batch 1200 train loss: 0.0091 test loss: 0.0128\n",
      "Epoch 2 batch 1300 train loss: 0.0100 test loss: 0.0128\n",
      "Epoch 2 batch 1400 train loss: 0.0087 test loss: 0.0124\n",
      "Epoch 2 batch 1500 train loss: 0.0065 test loss: 0.0116\n",
      "Epoch 2 batch 1600 train loss: 0.0062 test loss: 0.0119\n",
      "Epoch 2 batch 1700 train loss: 0.0060 test loss: 0.0112\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.50p/ckpt-18\n",
      "Epoch 2 batch 1800 train loss: 0.0078 test loss: 0.0107\n",
      "Epoch 2 batch 1900 train loss: 0.0074 test loss: 0.0135\n",
      "Epoch 2 batch 2000 train loss: 0.0082 test loss: 0.0125\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.50p/ckpt-19\n",
      "Epoch 2 batch 2100 train loss: 0.0077 test loss: 0.0100\n",
      "Epoch 2 batch 2200 train loss: 0.0067 test loss: 0.0114\n",
      "Epoch 2 batch 2300 train loss: 0.0077 test loss: 0.0122\n",
      "Epoch 2 batch 2400 train loss: 0.0062 test loss: 0.0124\n",
      "Epoch 2 batch 2500 train loss: 0.0072 test loss: 0.0127\n",
      "Epoch 2 batch 2600 train loss: 0.0082 test loss: 0.0111\n",
      "Epoch 2 batch 2700 train loss: 0.0069 test loss: 0.0118\n",
      "Epoch 2 batch 2800 train loss: 0.0081 test loss: 0.0112\n",
      "Epoch 2 batch 2900 train loss: 0.0076 test loss: 0.0113\n",
      "Epoch 3 batch 0 train loss: 0.0085 test loss: 0.0104\n",
      "Epoch 3 batch 100 train loss: 0.0067 test loss: 0.0125\n",
      "Epoch 3 batch 200 train loss: 0.0102 test loss: 0.0106\n",
      "Epoch 3 batch 300 train loss: 0.0087 test loss: 0.0117\n",
      "Epoch 3 batch 400 train loss: 0.0067 test loss: 0.0118\n",
      "Epoch 3 batch 500 train loss: 0.0077 test loss: 0.0136\n",
      "Epoch 3 batch 600 train loss: 0.0093 test loss: 0.0115\n",
      "Epoch 3 batch 700 train loss: 0.0060 test loss: 0.0123\n",
      "Epoch 3 batch 800 train loss: 0.0064 test loss: 0.0105\n",
      "Epoch 3 batch 900 train loss: 0.0087 test loss: 0.0114\n",
      "Epoch 3 batch 1000 train loss: 0.0097 test loss: 0.0123\n",
      "Epoch 3 batch 1100 train loss: 0.0057 test loss: 0.0110\n",
      "Epoch 3 batch 1200 train loss: 0.0084 test loss: 0.0116\n",
      "Epoch 3 batch 1300 train loss: 0.0069 test loss: 0.0122\n",
      "Epoch 3 batch 1400 train loss: 0.0067 test loss: 0.0129\n",
      "Epoch 3 batch 1500 train loss: 0.0074 test loss: 0.0122\n",
      "Epoch 3 batch 1600 train loss: 0.0071 test loss: 0.0115\n",
      "Epoch 3 batch 1700 train loss: 0.0069 test loss: 0.0119\n",
      "Epoch 3 batch 1800 train loss: 0.0045 test loss: 0.0113\n",
      "Epoch 3 batch 1900 train loss: 0.0098 test loss: 0.0119\n",
      "Epoch 3 batch 2000 train loss: 0.0081 test loss: 0.0111\n",
      "Epoch 3 batch 2100 train loss: 0.0049 test loss: 0.0111\n",
      "Epoch 3 batch 2200 train loss: 0.0093 test loss: 0.0114\n",
      "Epoch 3 batch 2300 train loss: 0.0088 test loss: 0.0121\n",
      "Epoch 3 batch 2400 train loss: 0.0072 test loss: 0.0109\n",
      "Epoch 3 batch 2500 train loss: 0.0073 test loss: 0.0125\n",
      "Epoch 3 batch 2600 train loss: 0.0076 test loss: 0.0110\n",
      "Epoch 3 batch 2700 train loss: 0.0067 test loss: 0.0115\n",
      "Epoch 3 batch 2800 train loss: 0.0078 test loss: 0.0110\n",
      "Epoch 3 batch 2900 train loss: 0.0079 test loss: 0.0116\n",
      "Epoch 4 batch 0 train loss: 0.0061 test loss: 0.0102\n",
      "Epoch 4 batch 100 train loss: 0.0088 test loss: 0.0120\n",
      "Epoch 4 batch 200 train loss: 0.0079 test loss: 0.0124\n",
      "Epoch 4 batch 300 train loss: 0.0095 test loss: 0.0114\n",
      "Epoch 4 batch 400 train loss: 0.0077 test loss: 0.0116\n",
      "Epoch 4 batch 500 train loss: 0.0066 test loss: 0.0116\n",
      "Epoch 4 batch 600 train loss: 0.0067 test loss: 0.0120\n",
      "Epoch 4 batch 700 train loss: 0.0076 test loss: 0.0116\n",
      "Epoch 4 batch 800 train loss: 0.0087 test loss: 0.0109\n",
      "Epoch 4 batch 900 train loss: 0.0080 test loss: 0.0113\n",
      "Epoch 4 batch 1000 train loss: 0.0081 test loss: 0.0115\n",
      "Epoch 4 batch 1100 train loss: 0.0065 test loss: 0.0109\n",
      "Epoch 4 batch 1200 train loss: 0.0071 test loss: 0.0129\n",
      "Epoch 4 batch 1300 train loss: 0.0076 test loss: 0.0102\n",
      "Epoch 4 batch 1400 train loss: 0.0054 test loss: 0.0112\n",
      "Epoch 4 batch 1500 train loss: 0.0071 test loss: 0.0116\n",
      "Epoch 4 batch 1600 train loss: 0.0060 test loss: 0.0118\n",
      "Epoch 4 batch 1700 train loss: 0.0070 test loss: 0.0118\n",
      "Epoch 4 batch 1800 train loss: 0.0071 test loss: 0.0129\n",
      "Epoch 4 batch 1900 train loss: 0.0055 test loss: 0.0122\n",
      "Epoch 4 batch 2000 train loss: 0.0080 test loss: 0.0128\n",
      "Epoch 4 batch 2100 train loss: 0.0067 test loss: 0.0117\n",
      "Epoch 4 batch 2200 train loss: 0.0066 test loss: 0.0123\n",
      "Epoch 4 batch 2300 train loss: 0.0064 test loss: 0.0111\n",
      "Epoch 4 batch 2400 train loss: 0.0086 test loss: 0.0114\n",
      "Epoch 4 batch 2500 train loss: 0.0069 test loss: 0.0124\n",
      "Epoch 4 batch 2600 train loss: 0.0077 test loss: 0.0110\n",
      "Epoch 4 batch 2700 train loss: 0.0062 test loss: 0.0119\n",
      "Epoch 4 batch 2800 train loss: 0.0083 test loss: 0.0119\n",
      "Epoch 4 batch 2900 train loss: 0.0078 test loss: 0.0104\n",
      "Epoch 5 batch 0 train loss: 0.0085 test loss: 0.0106\n",
      "Epoch 5 batch 100 train loss: 0.0101 test loss: 0.0112\n",
      "Epoch 5 batch 200 train loss: 0.0070 test loss: 0.0122\n",
      "Epoch 5 batch 300 train loss: 0.0071 test loss: 0.0122\n",
      "Epoch 5 batch 400 train loss: 0.0059 test loss: 0.0119\n",
      "Epoch 5 batch 500 train loss: 0.0088 test loss: 0.0115\n",
      "Epoch 5 batch 600 train loss: 0.0065 test loss: 0.0108\n",
      "Epoch 5 batch 700 train loss: 0.0068 test loss: 0.0114\n",
      "Epoch 5 batch 800 train loss: 0.0067 test loss: 0.0113\n",
      "Epoch 5 batch 900 train loss: 0.0062 test loss: 0.0109\n",
      "Epoch 5 batch 1000 train loss: 0.0075 test loss: 0.0114\n",
      "Epoch 5 batch 1100 train loss: 0.0061 test loss: 0.0124\n",
      "Epoch 5 batch 1200 train loss: 0.0103 test loss: 0.0102\n",
      "Epoch 5 batch 1300 train loss: 0.0071 test loss: 0.0124\n",
      "Epoch 5 batch 1400 train loss: 0.0081 test loss: 0.0114\n",
      "Epoch 5 batch 1500 train loss: 0.0087 test loss: 0.0127\n",
      "Epoch 5 batch 1600 train loss: 0.0071 test loss: 0.0109\n",
      "Epoch 5 batch 1700 train loss: 0.0072 test loss: 0.0124\n",
      "Epoch 5 batch 1800 train loss: 0.0071 test loss: 0.0111\n",
      "Epoch 5 batch 1900 train loss: 0.0063 test loss: 0.0111\n",
      "Epoch 5 batch 2000 train loss: 0.0062 test loss: 0.0113\n",
      "Epoch 5 batch 2100 train loss: 0.0070 test loss: 0.0124\n",
      "Epoch 5 batch 2200 train loss: 0.0062 test loss: 0.0125\n",
      "Epoch 5 batch 2300 train loss: 0.0093 test loss: 0.0116\n",
      "Epoch 5 batch 2400 train loss: 0.0082 test loss: 0.0104\n",
      "Epoch 5 batch 2500 train loss: 0.0072 test loss: 0.0125\n",
      "Epoch 5 batch 2600 train loss: 0.0053 test loss: 0.0119\n",
      "Epoch 5 batch 2700 train loss: 0.0094 test loss: 0.0126\n",
      "Epoch 5 batch 2800 train loss: 0.0061 test loss: 0.0117\n",
      "Epoch 5 batch 2900 train loss: 0.0072 test loss: 0.0111\n",
      "Epoch 6 batch 0 train loss: 0.0086 test loss: 0.0119\n",
      "Epoch 6 batch 100 train loss: 0.0079 test loss: 0.0106\n",
      "Epoch 6 batch 200 train loss: 0.0062 test loss: 0.0102\n",
      "Epoch 6 batch 300 train loss: 0.0080 test loss: 0.0107\n",
      "Epoch 6 batch 400 train loss: 0.0064 test loss: 0.0106\n",
      "Epoch 6 batch 500 train loss: 0.0094 test loss: 0.0115\n",
      "Epoch 6 batch 600 train loss: 0.0093 test loss: 0.0123\n",
      "Epoch 6 batch 700 train loss: 0.0093 test loss: 0.0120\n",
      "Epoch 6 batch 800 train loss: 0.0060 test loss: 0.0114\n",
      "Epoch 6 batch 900 train loss: 0.0075 test loss: 0.0114\n",
      "Epoch 6 batch 1000 train loss: 0.0068 test loss: 0.0116\n",
      "Epoch 6 batch 1100 train loss: 0.0073 test loss: 0.0115\n",
      "Epoch 6 batch 1200 train loss: 0.0075 test loss: 0.0110\n",
      "Epoch 6 batch 1300 train loss: 0.0078 test loss: 0.0120\n",
      "Epoch 6 batch 1400 train loss: 0.0059 test loss: 0.0130\n",
      "Epoch 6 batch 1500 train loss: 0.0085 test loss: 0.0117\n",
      "Epoch 6 batch 1600 train loss: 0.0099 test loss: 0.0107\n",
      "Epoch 6 batch 1700 train loss: 0.0069 test loss: 0.0120\n",
      "Epoch 6 batch 1800 train loss: 0.0085 test loss: 0.0104\n",
      "Epoch 6 batch 1900 train loss: 0.0060 test loss: 0.0112\n",
      "Epoch 6 batch 2000 train loss: 0.0057 test loss: 0.0116\n",
      "Epoch 6 batch 2100 train loss: 0.0057 test loss: 0.0123\n",
      "Epoch 6 batch 2200 train loss: 0.0071 test loss: 0.0114\n",
      "Epoch 6 batch 2300 train loss: 0.0066 test loss: 0.0121\n",
      "Epoch 6 batch 2400 train loss: 0.0064 test loss: 0.0114\n",
      "Epoch 6 batch 2500 train loss: 0.0081 test loss: 0.0111\n",
      "Epoch 6 batch 2600 train loss: 0.0073 test loss: 0.0117\n",
      "Epoch 6 batch 2700 train loss: 0.0093 test loss: 0.0123\n",
      "Epoch 6 batch 2800 train loss: 0.0061 test loss: 0.0111\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.50p/ckpt-20\n",
      "Epoch 6 batch 2900 train loss: 0.0058 test loss: 0.0099\n",
      "Epoch 7 batch 0 train loss: 0.0088 test loss: 0.0116\n",
      "Epoch 7 batch 100 train loss: 0.0060 test loss: 0.0102\n",
      "Epoch 7 batch 200 train loss: 0.0089 test loss: 0.0124\n",
      "Epoch 7 batch 300 train loss: 0.0072 test loss: 0.0104\n",
      "Epoch 7 batch 400 train loss: 0.0072 test loss: 0.0115\n",
      "Epoch 7 batch 500 train loss: 0.0070 test loss: 0.0108\n",
      "Epoch 7 batch 600 train loss: 0.0098 test loss: 0.0102\n",
      "Epoch 7 batch 700 train loss: 0.0075 test loss: 0.0118\n",
      "Epoch 7 batch 800 train loss: 0.0074 test loss: 0.0123\n",
      "Epoch 7 batch 900 train loss: 0.0062 test loss: 0.0111\n",
      "Epoch 7 batch 1000 train loss: 0.0058 test loss: 0.0108\n",
      "Epoch 7 batch 1100 train loss: 0.0063 test loss: 0.0113\n",
      "Epoch 7 batch 1200 train loss: 0.0082 test loss: 0.0106\n",
      "Epoch 7 batch 1300 train loss: 0.0092 test loss: 0.0117\n",
      "Epoch 7 batch 1400 train loss: 0.0069 test loss: 0.0110\n",
      "Epoch 7 batch 1500 train loss: 0.0078 test loss: 0.0114\n",
      "Epoch 7 batch 1600 train loss: 0.0077 test loss: 0.0115\n",
      "Epoch 7 batch 1700 train loss: 0.0062 test loss: 0.0112\n",
      "Epoch 7 batch 1800 train loss: 0.0079 test loss: 0.0115\n",
      "Epoch 7 batch 1900 train loss: 0.0053 test loss: 0.0126\n",
      "Epoch 7 batch 2000 train loss: 0.0076 test loss: 0.0121\n",
      "Epoch 7 batch 2100 train loss: 0.0084 test loss: 0.0110\n",
      "Epoch 7 batch 2200 train loss: 0.0055 test loss: 0.0122\n",
      "Epoch 7 batch 2300 train loss: 0.0067 test loss: 0.0113\n",
      "Epoch 7 batch 2400 train loss: 0.0068 test loss: 0.0113\n",
      "Epoch 7 batch 2500 train loss: 0.0066 test loss: 0.0126\n",
      "Epoch 7 batch 2600 train loss: 0.0074 test loss: 0.0112\n",
      "Epoch 7 batch 2700 train loss: 0.0082 test loss: 0.0118\n",
      "Epoch 7 batch 2800 train loss: 0.0075 test loss: 0.0117\n",
      "Epoch 7 batch 2900 train loss: 0.0082 test loss: 0.0105\n",
      "Epoch 8 batch 0 train loss: 0.0076 test loss: 0.0101\n",
      "Epoch 8 batch 100 train loss: 0.0098 test loss: 0.0108\n",
      "Epoch 8 batch 200 train loss: 0.0058 test loss: 0.0104\n",
      "Epoch 8 batch 300 train loss: 0.0074 test loss: 0.0121\n",
      "Epoch 8 batch 400 train loss: 0.0069 test loss: 0.0112\n",
      "Epoch 8 batch 500 train loss: 0.0085 test loss: 0.0126\n",
      "Epoch 8 batch 600 train loss: 0.0111 test loss: 0.0106\n",
      "Epoch 8 batch 700 train loss: 0.0075 test loss: 0.0122\n",
      "Epoch 8 batch 800 train loss: 0.0092 test loss: 0.0111\n",
      "Epoch 8 batch 900 train loss: 0.0070 test loss: 0.0106\n",
      "Epoch 8 batch 1000 train loss: 0.0071 test loss: 0.0112\n",
      "Epoch 8 batch 1100 train loss: 0.0080 test loss: 0.0115\n",
      "Epoch 8 batch 1200 train loss: 0.0078 test loss: 0.0121\n",
      "Epoch 8 batch 1300 train loss: 0.0091 test loss: 0.0131\n",
      "Epoch 8 batch 1400 train loss: 0.0089 test loss: 0.0115\n",
      "Epoch 8 batch 1500 train loss: 0.0088 test loss: 0.0112\n",
      "Epoch 8 batch 1600 train loss: 0.0083 test loss: 0.0114\n",
      "Epoch 8 batch 1700 train loss: 0.0077 test loss: 0.0127\n",
      "Epoch 8 batch 1800 train loss: 0.0072 test loss: 0.0114\n",
      "Epoch 8 batch 1900 train loss: 0.0077 test loss: 0.0114\n",
      "Epoch 8 batch 2000 train loss: 0.0076 test loss: 0.0122\n",
      "Epoch 8 batch 2100 train loss: 0.0080 test loss: 0.0112\n",
      "Epoch 8 batch 2200 train loss: 0.0074 test loss: 0.0113\n",
      "Epoch 8 batch 2300 train loss: 0.0084 test loss: 0.0110\n",
      "Epoch 8 batch 2400 train loss: 0.0063 test loss: 0.0122\n",
      "Epoch 8 batch 2500 train loss: 0.0091 test loss: 0.0118\n",
      "Epoch 8 batch 2600 train loss: 0.0060 test loss: 0.0114\n",
      "Epoch 8 batch 2700 train loss: 0.0073 test loss: 0.0121\n",
      "Epoch 8 batch 2800 train loss: 0.0081 test loss: 0.0108\n",
      "Epoch 8 batch 2900 train loss: 0.0099 test loss: 0.0107\n",
      "Epoch 9 batch 0 train loss: 0.0083 test loss: 0.0116\n",
      "Epoch 9 batch 100 train loss: 0.0084 test loss: 0.0117\n",
      "Epoch 9 batch 200 train loss: 0.0075 test loss: 0.0112\n",
      "Epoch 9 batch 300 train loss: 0.0075 test loss: 0.0106\n",
      "Epoch 9 batch 400 train loss: 0.0070 test loss: 0.0129\n",
      "Epoch 9 batch 500 train loss: 0.0070 test loss: 0.0122\n",
      "Epoch 9 batch 600 train loss: 0.0057 test loss: 0.0123\n",
      "Epoch 9 batch 700 train loss: 0.0073 test loss: 0.0114\n",
      "Epoch 9 batch 800 train loss: 0.0066 test loss: 0.0117\n",
      "Epoch 9 batch 900 train loss: 0.0091 test loss: 0.0102\n",
      "Epoch 9 batch 1000 train loss: 0.0083 test loss: 0.0127\n",
      "Epoch 9 batch 1100 train loss: 0.0054 test loss: 0.0124\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.50p/ckpt-21\n",
      "Epoch 9 batch 1200 train loss: 0.0092 test loss: 0.0096\n",
      "Epoch 9 batch 1300 train loss: 0.0065 test loss: 0.0113\n",
      "Epoch 9 batch 1400 train loss: 0.0072 test loss: 0.0118\n",
      "Epoch 9 batch 1500 train loss: 0.0076 test loss: 0.0111\n",
      "Epoch 9 batch 1600 train loss: 0.0064 test loss: 0.0111\n",
      "Epoch 9 batch 1700 train loss: 0.0075 test loss: 0.0123\n",
      "Epoch 9 batch 1800 train loss: 0.0084 test loss: 0.0104\n",
      "Epoch 9 batch 1900 train loss: 0.0066 test loss: 0.0125\n",
      "Epoch 9 batch 2000 train loss: 0.0062 test loss: 0.0112\n",
      "Epoch 9 batch 2100 train loss: 0.0069 test loss: 0.0110\n",
      "Epoch 9 batch 2200 train loss: 0.0085 test loss: 0.0105\n",
      "Epoch 9 batch 2300 train loss: 0.0069 test loss: 0.0113\n",
      "Epoch 9 batch 2400 train loss: 0.0075 test loss: 0.0114\n",
      "Epoch 9 batch 2500 train loss: 0.0083 test loss: 0.0120\n",
      "Epoch 9 batch 2600 train loss: 0.0091 test loss: 0.0113\n",
      "Epoch 9 batch 2700 train loss: 0.0077 test loss: 0.0122\n",
      "Epoch 9 batch 2800 train loss: 0.0070 test loss: 0.0102\n",
      "Epoch 9 batch 2900 train loss: 0.0073 test loss: 0.0110\n",
      "Epoch 10 batch 0 train loss: 0.0082 test loss: 0.0107\n",
      "Epoch 10 batch 100 train loss: 0.0062 test loss: 0.0111\n",
      "Epoch 10 batch 200 train loss: 0.0063 test loss: 0.0116\n",
      "Epoch 10 batch 300 train loss: 0.0079 test loss: 0.0122\n",
      "Epoch 10 batch 400 train loss: 0.0061 test loss: 0.0104\n",
      "Epoch 10 batch 500 train loss: 0.0071 test loss: 0.0112\n",
      "Epoch 10 batch 600 train loss: 0.0070 test loss: 0.0105\n",
      "Epoch 10 batch 700 train loss: 0.0053 test loss: 0.0122\n",
      "Epoch 10 batch 800 train loss: 0.0077 test loss: 0.0114\n",
      "Epoch 10 batch 900 train loss: 0.0067 test loss: 0.0109\n",
      "Epoch 10 batch 1000 train loss: 0.0068 test loss: 0.0114\n",
      "Epoch 10 batch 1100 train loss: 0.0060 test loss: 0.0111\n",
      "Epoch 10 batch 1200 train loss: 0.0084 test loss: 0.0101\n",
      "Epoch 10 batch 1300 train loss: 0.0078 test loss: 0.0119\n",
      "Epoch 10 batch 1400 train loss: 0.0075 test loss: 0.0102\n",
      "Epoch 10 batch 1500 train loss: 0.0087 test loss: 0.0119\n",
      "Epoch 10 batch 1600 train loss: 0.0090 test loss: 0.0112\n",
      "Epoch 10 batch 1700 train loss: 0.0070 test loss: 0.0113\n",
      "Epoch 10 batch 1800 train loss: 0.0088 test loss: 0.0107\n",
      "Epoch 10 batch 1900 train loss: 0.0098 test loss: 0.0121\n",
      "Epoch 10 batch 2000 train loss: 0.0083 test loss: 0.0117\n",
      "Epoch 10 batch 2100 train loss: 0.0064 test loss: 0.0122\n",
      "Epoch 10 batch 2200 train loss: 0.0062 test loss: 0.0113\n",
      "Epoch 10 batch 2300 train loss: 0.0104 test loss: 0.0115\n",
      "Epoch 10 batch 2400 train loss: 0.0068 test loss: 0.0120\n",
      "Epoch 10 batch 2500 train loss: 0.0113 test loss: 0.0119\n",
      "Epoch 10 batch 2600 train loss: 0.0064 test loss: 0.0118\n",
      "Epoch 10 batch 2700 train loss: 0.0083 test loss: 0.0116\n",
      "Epoch 10 batch 2800 train loss: 0.0095 test loss: 0.0107\n",
      "Epoch 10 batch 2900 train loss: 0.0078 test loss: 0.0111\n",
      "Epoch 11 batch 0 train loss: 0.0070 test loss: 0.0108\n",
      "Epoch 11 batch 100 train loss: 0.0080 test loss: 0.0108\n",
      "Epoch 11 batch 200 train loss: 0.0077 test loss: 0.0110\n",
      "Epoch 11 batch 300 train loss: 0.0087 test loss: 0.0118\n",
      "Epoch 11 batch 400 train loss: 0.0058 test loss: 0.0105\n",
      "Epoch 11 batch 500 train loss: 0.0098 test loss: 0.0111\n",
      "Epoch 11 batch 600 train loss: 0.0077 test loss: 0.0114\n",
      "Epoch 11 batch 700 train loss: 0.0070 test loss: 0.0120\n",
      "Epoch 11 batch 800 train loss: 0.0073 test loss: 0.0116\n",
      "Epoch 11 batch 900 train loss: 0.0065 test loss: 0.0104\n",
      "Epoch 11 batch 1000 train loss: 0.0090 test loss: 0.0109\n",
      "Epoch 11 batch 1100 train loss: 0.0092 test loss: 0.0112\n",
      "Epoch 11 batch 1200 train loss: 0.0098 test loss: 0.0104\n",
      "Epoch 11 batch 1300 train loss: 0.0068 test loss: 0.0111\n",
      "Epoch 11 batch 1400 train loss: 0.0063 test loss: 0.0131\n",
      "Epoch 11 batch 1500 train loss: 0.0075 test loss: 0.0128\n",
      "Epoch 11 batch 1600 train loss: 0.0088 test loss: 0.0116\n",
      "Epoch 11 batch 1700 train loss: 0.0080 test loss: 0.0116\n",
      "Epoch 11 batch 1800 train loss: 0.0061 test loss: 0.0107\n",
      "Epoch 11 batch 1900 train loss: 0.0075 test loss: 0.0112\n",
      "Epoch 11 batch 2000 train loss: 0.0079 test loss: 0.0126\n",
      "Epoch 11 batch 2100 train loss: 0.0076 test loss: 0.0113\n",
      "Epoch 11 batch 2200 train loss: 0.0057 test loss: 0.0111\n",
      "Epoch 11 batch 2300 train loss: 0.0072 test loss: 0.0113\n",
      "Epoch 11 batch 2400 train loss: 0.0072 test loss: 0.0102\n",
      "Epoch 11 batch 2500 train loss: 0.0072 test loss: 0.0121\n",
      "Epoch 11 batch 2600 train loss: 0.0063 test loss: 0.0101\n",
      "Epoch 11 batch 2700 train loss: 0.0060 test loss: 0.0113\n",
      "Epoch 11 batch 2800 train loss: 0.0098 test loss: 0.0111\n",
      "Epoch 11 batch 2900 train loss: 0.0088 test loss: 0.0109\n",
      "Epoch 12 batch 0 train loss: 0.0084 test loss: 0.0106\n",
      "Epoch 12 batch 100 train loss: 0.0064 test loss: 0.0111\n",
      "Epoch 12 batch 200 train loss: 0.0073 test loss: 0.0106\n",
      "Epoch 12 batch 300 train loss: 0.0068 test loss: 0.0110\n",
      "Epoch 12 batch 400 train loss: 0.0107 test loss: 0.0110\n",
      "Epoch 12 batch 500 train loss: 0.0057 test loss: 0.0110\n",
      "Epoch 12 batch 600 train loss: 0.0054 test loss: 0.0123\n",
      "Epoch 12 batch 700 train loss: 0.0091 test loss: 0.0117\n",
      "Epoch 12 batch 800 train loss: 0.0083 test loss: 0.0123\n",
      "Epoch 12 batch 900 train loss: 0.0071 test loss: 0.0116\n",
      "Epoch 12 batch 1000 train loss: 0.0055 test loss: 0.0132\n",
      "Epoch 12 batch 1100 train loss: 0.0061 test loss: 0.0128\n",
      "Epoch 12 batch 1200 train loss: 0.0075 test loss: 0.0116\n",
      "Epoch 12 batch 1300 train loss: 0.0044 test loss: 0.0120\n",
      "Epoch 12 batch 1400 train loss: 0.0071 test loss: 0.0123\n",
      "Epoch 12 batch 1500 train loss: 0.0063 test loss: 0.0124\n",
      "Epoch 12 batch 1600 train loss: 0.0094 test loss: 0.0112\n",
      "Epoch 12 batch 1700 train loss: 0.0066 test loss: 0.0108\n",
      "Epoch 12 batch 1800 train loss: 0.0085 test loss: 0.0118\n",
      "Epoch 12 batch 1900 train loss: 0.0064 test loss: 0.0114\n",
      "Epoch 12 batch 2000 train loss: 0.0092 test loss: 0.0114\n",
      "Epoch 12 batch 2100 train loss: 0.0079 test loss: 0.0104\n",
      "Epoch 12 batch 2200 train loss: 0.0065 test loss: 0.0111\n",
      "Epoch 12 batch 2300 train loss: 0.0076 test loss: 0.0115\n",
      "Epoch 12 batch 2400 train loss: 0.0064 test loss: 0.0129\n",
      "Epoch 12 batch 2500 train loss: 0.0098 test loss: 0.0112\n",
      "Epoch 12 batch 2600 train loss: 0.0083 test loss: 0.0111\n",
      "Epoch 12 batch 2700 train loss: 0.0068 test loss: 0.0127\n",
      "Epoch 12 batch 2800 train loss: 0.0082 test loss: 0.0115\n",
      "Epoch 12 batch 2900 train loss: 0.0063 test loss: 0.0115\n",
      "Epoch 13 batch 0 train loss: 0.0093 test loss: 0.0101\n",
      "Epoch 13 batch 100 train loss: 0.0065 test loss: 0.0115\n",
      "Epoch 13 batch 200 train loss: 0.0077 test loss: 0.0119\n",
      "Epoch 13 batch 300 train loss: 0.0088 test loss: 0.0116\n",
      "Epoch 13 batch 400 train loss: 0.0075 test loss: 0.0113\n",
      "Epoch 13 batch 500 train loss: 0.0067 test loss: 0.0116\n",
      "Epoch 13 batch 600 train loss: 0.0082 test loss: 0.0113\n",
      "Epoch 13 batch 700 train loss: 0.0069 test loss: 0.0119\n",
      "Epoch 13 batch 800 train loss: 0.0061 test loss: 0.0116\n",
      "Epoch 13 batch 900 train loss: 0.0059 test loss: 0.0116\n",
      "Epoch 13 batch 1000 train loss: 0.0069 test loss: 0.0111\n",
      "Epoch 13 batch 1100 train loss: 0.0076 test loss: 0.0126\n",
      "Epoch 13 batch 1200 train loss: 0.0104 test loss: 0.0116\n",
      "Epoch 13 batch 1300 train loss: 0.0090 test loss: 0.0112\n",
      "Epoch 13 batch 1400 train loss: 0.0092 test loss: 0.0114\n",
      "Epoch 13 batch 1500 train loss: 0.0065 test loss: 0.0110\n",
      "Epoch 13 batch 1600 train loss: 0.0086 test loss: 0.0103\n",
      "Epoch 13 batch 1700 train loss: 0.0074 test loss: 0.0109\n",
      "Epoch 13 batch 1800 train loss: 0.0064 test loss: 0.0116\n",
      "Epoch 13 batch 1900 train loss: 0.0080 test loss: 0.0124\n",
      "Epoch 13 batch 2000 train loss: 0.0086 test loss: 0.0121\n",
      "Epoch 13 batch 2100 train loss: 0.0077 test loss: 0.0118\n",
      "Epoch 13 batch 2200 train loss: 0.0075 test loss: 0.0105\n",
      "Epoch 13 batch 2300 train loss: 0.0091 test loss: 0.0112\n",
      "Epoch 13 batch 2400 train loss: 0.0062 test loss: 0.0113\n",
      "Epoch 13 batch 2500 train loss: 0.0078 test loss: 0.0124\n",
      "Epoch 13 batch 2600 train loss: 0.0060 test loss: 0.0108\n",
      "Epoch 13 batch 2700 train loss: 0.0082 test loss: 0.0123\n",
      "Epoch 13 batch 2800 train loss: 0.0047 test loss: 0.0111\n",
      "Epoch 13 batch 2900 train loss: 0.0083 test loss: 0.0119\n",
      "Epoch 14 batch 0 train loss: 0.0081 test loss: 0.0113\n",
      "Epoch 14 batch 100 train loss: 0.0098 test loss: 0.0112\n",
      "Epoch 14 batch 200 train loss: 0.0096 test loss: 0.0113\n",
      "Epoch 14 batch 300 train loss: 0.0049 test loss: 0.0115\n",
      "Epoch 14 batch 400 train loss: 0.0055 test loss: 0.0113\n",
      "Epoch 14 batch 500 train loss: 0.0074 test loss: 0.0120\n",
      "Epoch 14 batch 600 train loss: 0.0065 test loss: 0.0109\n",
      "Epoch 14 batch 700 train loss: 0.0047 test loss: 0.0109\n",
      "Epoch 14 batch 800 train loss: 0.0085 test loss: 0.0117\n",
      "Epoch 14 batch 900 train loss: 0.0078 test loss: 0.0112\n",
      "Epoch 14 batch 1000 train loss: 0.0075 test loss: 0.0111\n",
      "Epoch 14 batch 1100 train loss: 0.0069 test loss: 0.0111\n",
      "Epoch 14 batch 1200 train loss: 0.0083 test loss: 0.0113\n",
      "Epoch 14 batch 1300 train loss: 0.0101 test loss: 0.0112\n",
      "Epoch 14 batch 1400 train loss: 0.0065 test loss: 0.0116\n",
      "Epoch 14 batch 1500 train loss: 0.0087 test loss: 0.0112\n",
      "Epoch 14 batch 1600 train loss: 0.0073 test loss: 0.0118\n",
      "Epoch 14 batch 1700 train loss: 0.0068 test loss: 0.0108\n",
      "Epoch 14 batch 1800 train loss: 0.0057 test loss: 0.0105\n",
      "Epoch 14 batch 1900 train loss: 0.0080 test loss: 0.0110\n",
      "Epoch 14 batch 2000 train loss: 0.0076 test loss: 0.0102\n",
      "Epoch 14 batch 2100 train loss: 0.0080 test loss: 0.0123\n",
      "Epoch 14 batch 2200 train loss: 0.0080 test loss: 0.0108\n",
      "Epoch 14 batch 2300 train loss: 0.0053 test loss: 0.0112\n",
      "Epoch 14 batch 2400 train loss: 0.0066 test loss: 0.0121\n",
      "Epoch 14 batch 2500 train loss: 0.0078 test loss: 0.0124\n",
      "Epoch 14 batch 2600 train loss: 0.0075 test loss: 0.0104\n",
      "Epoch 14 batch 2700 train loss: 0.0066 test loss: 0.0122\n",
      "Epoch 14 batch 2800 train loss: 0.0083 test loss: 0.0110\n",
      "Epoch 14 batch 2900 train loss: 0.0070 test loss: 0.0110\n",
      "Epoch 15 batch 0 train loss: 0.0104 test loss: 0.0101\n",
      "Epoch 15 batch 100 train loss: 0.0060 test loss: 0.0110\n",
      "Epoch 15 batch 200 train loss: 0.0073 test loss: 0.0107\n",
      "Epoch 15 batch 300 train loss: 0.0082 test loss: 0.0103\n",
      "Epoch 15 batch 400 train loss: 0.0065 test loss: 0.0112\n",
      "Epoch 15 batch 500 train loss: 0.0057 test loss: 0.0109\n",
      "Epoch 15 batch 600 train loss: 0.0082 test loss: 0.0115\n",
      "Epoch 15 batch 700 train loss: 0.0087 test loss: 0.0114\n",
      "Epoch 15 batch 800 train loss: 0.0079 test loss: 0.0116\n",
      "Epoch 15 batch 900 train loss: 0.0062 test loss: 0.0113\n",
      "Epoch 15 batch 1000 train loss: 0.0075 test loss: 0.0113\n",
      "Epoch 15 batch 1100 train loss: 0.0080 test loss: 0.0135\n",
      "Epoch 15 batch 1200 train loss: 0.0068 test loss: 0.0108\n",
      "Epoch 15 batch 1300 train loss: 0.0081 test loss: 0.0119\n",
      "Epoch 15 batch 1400 train loss: 0.0077 test loss: 0.0111\n",
      "Epoch 15 batch 1500 train loss: 0.0077 test loss: 0.0104\n",
      "Epoch 15 batch 1600 train loss: 0.0090 test loss: 0.0113\n",
      "Epoch 15 batch 1700 train loss: 0.0078 test loss: 0.0102\n",
      "Epoch 15 batch 1800 train loss: 0.0082 test loss: 0.0123\n",
      "Epoch 15 batch 1900 train loss: 0.0089 test loss: 0.0109\n",
      "Epoch 15 batch 2000 train loss: 0.0087 test loss: 0.0114\n",
      "Epoch 15 batch 2100 train loss: 0.0061 test loss: 0.0115\n",
      "Epoch 15 batch 2200 train loss: 0.0086 test loss: 0.0118\n",
      "Epoch 15 batch 2300 train loss: 0.0065 test loss: 0.0111\n",
      "Epoch 15 batch 2400 train loss: 0.0073 test loss: 0.0120\n",
      "Epoch 15 batch 2500 train loss: 0.0056 test loss: 0.0103\n",
      "Epoch 15 batch 2600 train loss: 0.0095 test loss: 0.0109\n",
      "Epoch 15 batch 2700 train loss: 0.0065 test loss: 0.0105\n",
      "Epoch 15 batch 2800 train loss: 0.0086 test loss: 0.0112\n",
      "Epoch 15 batch 2900 train loss: 0.0068 test loss: 0.0119\n",
      "Epoch 16 batch 0 train loss: 0.0086 test loss: 0.0097\n",
      "Epoch 16 batch 100 train loss: 0.0084 test loss: 0.0105\n",
      "Epoch 16 batch 200 train loss: 0.0089 test loss: 0.0106\n",
      "Epoch 16 batch 300 train loss: 0.0065 test loss: 0.0111\n",
      "Epoch 16 batch 400 train loss: 0.0067 test loss: 0.0106\n",
      "Epoch 16 batch 500 train loss: 0.0071 test loss: 0.0109\n",
      "Epoch 16 batch 600 train loss: 0.0059 test loss: 0.0114\n",
      "Epoch 16 batch 700 train loss: 0.0069 test loss: 0.0113\n",
      "Epoch 16 batch 800 train loss: 0.0088 test loss: 0.0106\n",
      "Epoch 16 batch 900 train loss: 0.0075 test loss: 0.0116\n",
      "Epoch 16 batch 1000 train loss: 0.0082 test loss: 0.0112\n",
      "Epoch 16 batch 1100 train loss: 0.0071 test loss: 0.0110\n",
      "Epoch 16 batch 1200 train loss: 0.0096 test loss: 0.0108\n",
      "Epoch 16 batch 1300 train loss: 0.0076 test loss: 0.0126\n",
      "Epoch 16 batch 1400 train loss: 0.0097 test loss: 0.0105\n",
      "Epoch 16 batch 1500 train loss: 0.0090 test loss: 0.0113\n",
      "Epoch 16 batch 1600 train loss: 0.0062 test loss: 0.0111\n",
      "Epoch 16 batch 1700 train loss: 0.0051 test loss: 0.0115\n",
      "Epoch 16 batch 1800 train loss: 0.0065 test loss: 0.0102\n",
      "Epoch 16 batch 1900 train loss: 0.0066 test loss: 0.0120\n",
      "Epoch 16 batch 2000 train loss: 0.0074 test loss: 0.0133\n",
      "Epoch 16 batch 2100 train loss: 0.0092 test loss: 0.0113\n",
      "Epoch 16 batch 2200 train loss: 0.0092 test loss: 0.0114\n",
      "Epoch 16 batch 2300 train loss: 0.0055 test loss: 0.0103\n",
      "Epoch 16 batch 2400 train loss: 0.0086 test loss: 0.0113\n",
      "Epoch 16 batch 2500 train loss: 0.0085 test loss: 0.0124\n",
      "Epoch 16 batch 2600 train loss: 0.0066 test loss: 0.0116\n",
      "Epoch 16 batch 2700 train loss: 0.0066 test loss: 0.0112\n",
      "Epoch 16 batch 2800 train loss: 0.0086 test loss: 0.0104\n",
      "Epoch 16 batch 2900 train loss: 0.0064 test loss: 0.0101\n",
      "Epoch 17 batch 0 train loss: 0.0091 test loss: 0.0105\n",
      "Epoch 17 batch 100 train loss: 0.0086 test loss: 0.0105\n",
      "Epoch 17 batch 200 train loss: 0.0079 test loss: 0.0107\n",
      "Epoch 17 batch 300 train loss: 0.0072 test loss: 0.0118\n",
      "Epoch 17 batch 400 train loss: 0.0081 test loss: 0.0109\n",
      "Epoch 17 batch 500 train loss: 0.0079 test loss: 0.0109\n",
      "Epoch 17 batch 600 train loss: 0.0072 test loss: 0.0114\n",
      "Epoch 17 batch 700 train loss: 0.0061 test loss: 0.0108\n",
      "Epoch 17 batch 800 train loss: 0.0063 test loss: 0.0113\n",
      "Epoch 17 batch 900 train loss: 0.0091 test loss: 0.0111\n",
      "Epoch 17 batch 1000 train loss: 0.0083 test loss: 0.0106\n",
      "early stop.\n",
      "Checkpoint 21 restored!!\n",
      "Training for loss rate 0.60 start.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['bi_lstm_5/bi_lstm_block_10/bidirectional_10/forward_lstm_10/lstm_cell_31/kernel:0', 'bi_lstm_5/bi_lstm_block_10/bidirectional_10/forward_lstm_10/lstm_cell_31/recurrent_kernel:0', 'bi_lstm_5/bi_lstm_block_10/bidirectional_10/forward_lstm_10/lstm_cell_31/bias:0', 'bi_lstm_5/bi_lstm_block_10/bidirectional_10/backward_lstm_10/lstm_cell_32/kernel:0', 'bi_lstm_5/bi_lstm_block_10/bidirectional_10/backward_lstm_10/lstm_cell_32/recurrent_kernel:0', 'bi_lstm_5/bi_lstm_block_10/bidirectional_10/backward_lstm_10/lstm_cell_32/bias:0', 'bi_lstm_5/bi_lstm_block_10/layer_normalization_10/gamma:0', 'bi_lstm_5/bi_lstm_block_10/layer_normalization_10/beta:0', 'bi_lstm_5/bi_lstm_block_11/bidirectional_11/forward_lstm_11/lstm_cell_34/kernel:0', 'bi_lstm_5/bi_lstm_block_11/bidirectional_11/forward_lstm_11/lstm_cell_34/recurrent_kernel:0', 'bi_lstm_5/bi_lstm_block_11/bidirectional_11/forward_lstm_11/lstm_cell_34/bias:0', 'bi_lstm_5/bi_lstm_block_11/bidirectional_11/backward_lstm_11/lstm_cell_35/kernel:0', 'bi_lstm_5/bi_lstm_block_11/bidirectional_11/backward_lstm_11/lstm_cell_35/recurrent_kernel:0', 'bi_lstm_5/bi_lstm_block_11/bidirectional_11/backward_lstm_11/lstm_cell_35/bias:0', 'bi_lstm_5/bi_lstm_block_11/layer_normalization_11/gamma:0', 'bi_lstm_5/bi_lstm_block_11/layer_normalization_11/beta:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['bi_lstm_5/bi_lstm_block_10/bidirectional_10/forward_lstm_10/lstm_cell_31/kernel:0', 'bi_lstm_5/bi_lstm_block_10/bidirectional_10/forward_lstm_10/lstm_cell_31/recurrent_kernel:0', 'bi_lstm_5/bi_lstm_block_10/bidirectional_10/forward_lstm_10/lstm_cell_31/bias:0', 'bi_lstm_5/bi_lstm_block_10/bidirectional_10/backward_lstm_10/lstm_cell_32/kernel:0', 'bi_lstm_5/bi_lstm_block_10/bidirectional_10/backward_lstm_10/lstm_cell_32/recurrent_kernel:0', 'bi_lstm_5/bi_lstm_block_10/bidirectional_10/backward_lstm_10/lstm_cell_32/bias:0', 'bi_lstm_5/bi_lstm_block_10/layer_normalization_10/gamma:0', 'bi_lstm_5/bi_lstm_block_10/layer_normalization_10/beta:0', 'bi_lstm_5/bi_lstm_block_11/bidirectional_11/forward_lstm_11/lstm_cell_34/kernel:0', 'bi_lstm_5/bi_lstm_block_11/bidirectional_11/forward_lstm_11/lstm_cell_34/recurrent_kernel:0', 'bi_lstm_5/bi_lstm_block_11/bidirectional_11/forward_lstm_11/lstm_cell_34/bias:0', 'bi_lstm_5/bi_lstm_block_11/bidirectional_11/backward_lstm_11/lstm_cell_35/kernel:0', 'bi_lstm_5/bi_lstm_block_11/bidirectional_11/backward_lstm_11/lstm_cell_35/recurrent_kernel:0', 'bi_lstm_5/bi_lstm_block_11/bidirectional_11/backward_lstm_11/lstm_cell_35/bias:0', 'bi_lstm_5/bi_lstm_block_11/layer_normalization_11/gamma:0', 'bi_lstm_5/bi_lstm_block_11/layer_normalization_11/beta:0'] when minimizing the loss.\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.60p/ckpt-1\n",
      "Epoch 0 batch 0 train loss: 1.1630 test loss: 2.3547\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.60p/ckpt-2\n",
      "Epoch 0 batch 100 train loss: 0.3862 test loss: 0.7375\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.60p/ckpt-3\n",
      "Epoch 0 batch 200 train loss: 0.1758 test loss: 0.2424\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.60p/ckpt-4\n",
      "Epoch 0 batch 300 train loss: 0.0963 test loss: 0.1004\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.60p/ckpt-5\n",
      "Epoch 0 batch 400 train loss: 0.0699 test loss: 0.0605\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.60p/ckpt-6\n",
      "Epoch 0 batch 500 train loss: 0.0479 test loss: 0.0449\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.60p/ckpt-7\n",
      "Epoch 0 batch 600 train loss: 0.0391 test loss: 0.0361\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.60p/ckpt-8\n",
      "Epoch 0 batch 700 train loss: 0.0291 test loss: 0.0303\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.60p/ckpt-9\n",
      "Epoch 0 batch 800 train loss: 0.0235 test loss: 0.0269\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.60p/ckpt-10\n",
      "Epoch 0 batch 900 train loss: 0.0212 test loss: 0.0238\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.60p/ckpt-11\n",
      "Epoch 0 batch 1000 train loss: 0.0182 test loss: 0.0223\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.60p/ckpt-12\n",
      "Epoch 0 batch 1100 train loss: 0.0144 test loss: 0.0197\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.60p/ckpt-13\n",
      "Epoch 0 batch 1200 train loss: 0.0170 test loss: 0.0184\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.60p/ckpt-14\n",
      "Epoch 0 batch 1300 train loss: 0.0129 test loss: 0.0181\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.60p/ckpt-15\n",
      "Epoch 0 batch 1400 train loss: 0.0112 test loss: 0.0159\n",
      "Epoch 0 batch 1500 train loss: 0.0097 test loss: 0.0161\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.60p/ckpt-16\n",
      "Epoch 0 batch 1600 train loss: 0.0094 test loss: 0.0145\n",
      "Epoch 0 batch 1700 train loss: 0.0089 test loss: 0.0152\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.60p/ckpt-17\n",
      "Epoch 0 batch 1800 train loss: 0.0096 test loss: 0.0143\n",
      "Epoch 0 batch 1900 train loss: 0.0090 test loss: 0.0146\n",
      "Epoch 0 batch 2000 train loss: 0.0081 test loss: 0.0147\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.60p/ckpt-18\n",
      "Epoch 0 batch 2100 train loss: 0.0083 test loss: 0.0143\n",
      "Epoch 0 batch 2200 train loss: 0.0110 test loss: 0.0152\n",
      "Epoch 0 batch 2300 train loss: 0.0060 test loss: 0.0152\n",
      "Epoch 0 batch 2400 train loss: 0.0089 test loss: 0.0148\n",
      "Epoch 0 batch 2500 train loss: 0.0097 test loss: 0.0146\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.60p/ckpt-19\n",
      "Epoch 0 batch 2600 train loss: 0.0076 test loss: 0.0138\n",
      "Epoch 0 batch 2700 train loss: 0.0086 test loss: 0.0153\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.60p/ckpt-20\n",
      "Epoch 0 batch 2800 train loss: 0.0087 test loss: 0.0136\n",
      "Epoch 0 batch 2900 train loss: 0.0070 test loss: 0.0138\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['bi_lstm_5/bi_lstm_block_10/bidirectional_10/forward_lstm_10/lstm_cell_31/kernel:0', 'bi_lstm_5/bi_lstm_block_10/bidirectional_10/forward_lstm_10/lstm_cell_31/recurrent_kernel:0', 'bi_lstm_5/bi_lstm_block_10/bidirectional_10/forward_lstm_10/lstm_cell_31/bias:0', 'bi_lstm_5/bi_lstm_block_10/bidirectional_10/backward_lstm_10/lstm_cell_32/kernel:0', 'bi_lstm_5/bi_lstm_block_10/bidirectional_10/backward_lstm_10/lstm_cell_32/recurrent_kernel:0', 'bi_lstm_5/bi_lstm_block_10/bidirectional_10/backward_lstm_10/lstm_cell_32/bias:0', 'bi_lstm_5/bi_lstm_block_10/layer_normalization_10/gamma:0', 'bi_lstm_5/bi_lstm_block_10/layer_normalization_10/beta:0', 'bi_lstm_5/bi_lstm_block_11/bidirectional_11/forward_lstm_11/lstm_cell_34/kernel:0', 'bi_lstm_5/bi_lstm_block_11/bidirectional_11/forward_lstm_11/lstm_cell_34/recurrent_kernel:0', 'bi_lstm_5/bi_lstm_block_11/bidirectional_11/forward_lstm_11/lstm_cell_34/bias:0', 'bi_lstm_5/bi_lstm_block_11/bidirectional_11/backward_lstm_11/lstm_cell_35/kernel:0', 'bi_lstm_5/bi_lstm_block_11/bidirectional_11/backward_lstm_11/lstm_cell_35/recurrent_kernel:0', 'bi_lstm_5/bi_lstm_block_11/bidirectional_11/backward_lstm_11/lstm_cell_35/bias:0', 'bi_lstm_5/bi_lstm_block_11/layer_normalization_11/gamma:0', 'bi_lstm_5/bi_lstm_block_11/layer_normalization_11/beta:0'] when minimizing the loss.\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.60p/ckpt-21\n",
      "Epoch 1 batch 0 train loss: 0.0102 test loss: 0.0135\n",
      "Epoch 1 batch 100 train loss: 0.0087 test loss: 0.0144\n",
      "Epoch 1 batch 200 train loss: 0.0105 test loss: 0.0144\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.60p/ckpt-22\n",
      "Epoch 1 batch 300 train loss: 0.0095 test loss: 0.0129\n",
      "Epoch 1 batch 400 train loss: 0.0090 test loss: 0.0153\n",
      "Epoch 1 batch 500 train loss: 0.0059 test loss: 0.0138\n",
      "Epoch 1 batch 600 train loss: 0.0070 test loss: 0.0149\n",
      "Epoch 1 batch 700 train loss: 0.0076 test loss: 0.0142\n",
      "Epoch 1 batch 800 train loss: 0.0068 test loss: 0.0148\n",
      "Epoch 1 batch 900 train loss: 0.0078 test loss: 0.0140\n",
      "Epoch 1 batch 1000 train loss: 0.0076 test loss: 0.0155\n",
      "Epoch 1 batch 1100 train loss: 0.0099 test loss: 0.0136\n",
      "Epoch 1 batch 1200 train loss: 0.0098 test loss: 0.0132\n",
      "Epoch 1 batch 1300 train loss: 0.0080 test loss: 0.0171\n",
      "Epoch 1 batch 1400 train loss: 0.0092 test loss: 0.0142\n",
      "Epoch 1 batch 1500 train loss: 0.0066 test loss: 0.0142\n",
      "Epoch 1 batch 1600 train loss: 0.0082 test loss: 0.0137\n",
      "Epoch 1 batch 1700 train loss: 0.0083 test loss: 0.0171\n",
      "Epoch 1 batch 1800 train loss: 0.0081 test loss: 0.0142\n",
      "Epoch 1 batch 1900 train loss: 0.0087 test loss: 0.0151\n",
      "Epoch 1 batch 2000 train loss: 0.0074 test loss: 0.0145\n",
      "Epoch 1 batch 2100 train loss: 0.0062 test loss: 0.0130\n",
      "Epoch 1 batch 2200 train loss: 0.0075 test loss: 0.0150\n",
      "Epoch 1 batch 2300 train loss: 0.0082 test loss: 0.0147\n",
      "Epoch 1 batch 2400 train loss: 0.0106 test loss: 0.0164\n",
      "Epoch 1 batch 2500 train loss: 0.0072 test loss: 0.0135\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.60p/ckpt-23\n",
      "Epoch 1 batch 2600 train loss: 0.0089 test loss: 0.0122\n",
      "Epoch 1 batch 2700 train loss: 0.0073 test loss: 0.0144\n",
      "Epoch 1 batch 2800 train loss: 0.0075 test loss: 0.0140\n",
      "Epoch 1 batch 2900 train loss: 0.0084 test loss: 0.0151\n",
      "Epoch 2 batch 0 train loss: 0.0091 test loss: 0.0123\n",
      "Epoch 2 batch 100 train loss: 0.0071 test loss: 0.0132\n",
      "Epoch 2 batch 200 train loss: 0.0068 test loss: 0.0135\n",
      "Epoch 2 batch 300 train loss: 0.0077 test loss: 0.0130\n",
      "Epoch 2 batch 400 train loss: 0.0069 test loss: 0.0150\n",
      "Epoch 2 batch 500 train loss: 0.0080 test loss: 0.0127\n",
      "Epoch 2 batch 600 train loss: 0.0092 test loss: 0.0131\n",
      "Epoch 2 batch 700 train loss: 0.0087 test loss: 0.0130\n",
      "Epoch 2 batch 800 train loss: 0.0099 test loss: 0.0144\n",
      "Epoch 2 batch 900 train loss: 0.0074 test loss: 0.0138\n",
      "Epoch 2 batch 1000 train loss: 0.0079 test loss: 0.0130\n",
      "Epoch 2 batch 1100 train loss: 0.0097 test loss: 0.0130\n",
      "Epoch 2 batch 1200 train loss: 0.0092 test loss: 0.0125\n",
      "Epoch 2 batch 1300 train loss: 0.0084 test loss: 0.0144\n",
      "Epoch 2 batch 1400 train loss: 0.0123 test loss: 0.0152\n",
      "Epoch 2 batch 1500 train loss: 0.0083 test loss: 0.0148\n",
      "Epoch 2 batch 1600 train loss: 0.0076 test loss: 0.0135\n",
      "Epoch 2 batch 1700 train loss: 0.0112 test loss: 0.0152\n",
      "Epoch 2 batch 1800 train loss: 0.0082 test loss: 0.0139\n",
      "Epoch 2 batch 1900 train loss: 0.0139 test loss: 0.0140\n",
      "Epoch 2 batch 2000 train loss: 0.0085 test loss: 0.0142\n",
      "Epoch 2 batch 2100 train loss: 0.0061 test loss: 0.0146\n",
      "Epoch 2 batch 2200 train loss: 0.0089 test loss: 0.0156\n",
      "Epoch 2 batch 2300 train loss: 0.0074 test loss: 0.0138\n",
      "Epoch 2 batch 2400 train loss: 0.0079 test loss: 0.0147\n",
      "Epoch 2 batch 2500 train loss: 0.0083 test loss: 0.0151\n",
      "Epoch 2 batch 2600 train loss: 0.0087 test loss: 0.0128\n",
      "Epoch 2 batch 2700 train loss: 0.0091 test loss: 0.0144\n",
      "Epoch 2 batch 2800 train loss: 0.0085 test loss: 0.0127\n",
      "Epoch 2 batch 2900 train loss: 0.0082 test loss: 0.0132\n",
      "Epoch 3 batch 0 train loss: 0.0080 test loss: 0.0148\n",
      "Epoch 3 batch 100 train loss: 0.0091 test loss: 0.0135\n",
      "Epoch 3 batch 200 train loss: 0.0088 test loss: 0.0138\n",
      "Epoch 3 batch 300 train loss: 0.0102 test loss: 0.0149\n",
      "Epoch 3 batch 400 train loss: 0.0076 test loss: 0.0142\n",
      "Epoch 3 batch 500 train loss: 0.0091 test loss: 0.0139\n",
      "Epoch 3 batch 600 train loss: 0.0076 test loss: 0.0147\n",
      "Epoch 3 batch 700 train loss: 0.0069 test loss: 0.0157\n",
      "Epoch 3 batch 800 train loss: 0.0079 test loss: 0.0142\n",
      "Epoch 3 batch 900 train loss: 0.0087 test loss: 0.0136\n",
      "Epoch 3 batch 1000 train loss: 0.0065 test loss: 0.0146\n",
      "Epoch 3 batch 1100 train loss: 0.0091 test loss: 0.0156\n",
      "Epoch 3 batch 1200 train loss: 0.0069 test loss: 0.0137\n",
      "Epoch 3 batch 1300 train loss: 0.0078 test loss: 0.0130\n",
      "Epoch 3 batch 1400 train loss: 0.0063 test loss: 0.0124\n",
      "Epoch 3 batch 1500 train loss: 0.0082 test loss: 0.0130\n",
      "Epoch 3 batch 1600 train loss: 0.0109 test loss: 0.0154\n",
      "Epoch 3 batch 1700 train loss: 0.0063 test loss: 0.0143\n",
      "Epoch 3 batch 1800 train loss: 0.0062 test loss: 0.0131\n",
      "Epoch 3 batch 1900 train loss: 0.0070 test loss: 0.0144\n",
      "Epoch 3 batch 2000 train loss: 0.0070 test loss: 0.0141\n",
      "Epoch 3 batch 2100 train loss: 0.0069 test loss: 0.0144\n",
      "Epoch 3 batch 2200 train loss: 0.0081 test loss: 0.0134\n",
      "Epoch 3 batch 2300 train loss: 0.0072 test loss: 0.0144\n",
      "Epoch 3 batch 2400 train loss: 0.0056 test loss: 0.0144\n",
      "Epoch 3 batch 2500 train loss: 0.0065 test loss: 0.0135\n",
      "Epoch 3 batch 2600 train loss: 0.0064 test loss: 0.0138\n",
      "Epoch 3 batch 2700 train loss: 0.0069 test loss: 0.0141\n",
      "Epoch 3 batch 2800 train loss: 0.0089 test loss: 0.0154\n",
      "Epoch 3 batch 2900 train loss: 0.0084 test loss: 0.0133\n",
      "Epoch 4 batch 0 train loss: 0.0070 test loss: 0.0134\n",
      "Epoch 4 batch 100 train loss: 0.0076 test loss: 0.0143\n",
      "Epoch 4 batch 200 train loss: 0.0062 test loss: 0.0141\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.60p/ckpt-24\n",
      "Epoch 4 batch 300 train loss: 0.0075 test loss: 0.0118\n",
      "Epoch 4 batch 400 train loss: 0.0073 test loss: 0.0122\n",
      "Epoch 4 batch 500 train loss: 0.0086 test loss: 0.0148\n",
      "Epoch 4 batch 600 train loss: 0.0067 test loss: 0.0124\n",
      "Epoch 4 batch 700 train loss: 0.0089 test loss: 0.0148\n",
      "Epoch 4 batch 800 train loss: 0.0083 test loss: 0.0145\n",
      "Epoch 4 batch 900 train loss: 0.0077 test loss: 0.0132\n",
      "Epoch 4 batch 1000 train loss: 0.0052 test loss: 0.0148\n",
      "Epoch 4 batch 1100 train loss: 0.0084 test loss: 0.0125\n",
      "Epoch 4 batch 1200 train loss: 0.0075 test loss: 0.0144\n",
      "Epoch 4 batch 1300 train loss: 0.0058 test loss: 0.0148\n",
      "Epoch 4 batch 1400 train loss: 0.0072 test loss: 0.0148\n",
      "Epoch 4 batch 1500 train loss: 0.0066 test loss: 0.0132\n",
      "Epoch 4 batch 1600 train loss: 0.0084 test loss: 0.0138\n",
      "Epoch 4 batch 1700 train loss: 0.0083 test loss: 0.0135\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.60p/ckpt-25\n",
      "Epoch 4 batch 1800 train loss: 0.0057 test loss: 0.0113\n",
      "Epoch 4 batch 1900 train loss: 0.0088 test loss: 0.0127\n",
      "Epoch 4 batch 2000 train loss: 0.0077 test loss: 0.0141\n",
      "Epoch 4 batch 2100 train loss: 0.0058 test loss: 0.0154\n",
      "Epoch 4 batch 2200 train loss: 0.0086 test loss: 0.0148\n",
      "Epoch 4 batch 2300 train loss: 0.0053 test loss: 0.0153\n",
      "Epoch 4 batch 2400 train loss: 0.0058 test loss: 0.0153\n",
      "Epoch 4 batch 2500 train loss: 0.0081 test loss: 0.0139\n",
      "Epoch 4 batch 2600 train loss: 0.0071 test loss: 0.0129\n",
      "Epoch 4 batch 2700 train loss: 0.0084 test loss: 0.0144\n",
      "Epoch 4 batch 2800 train loss: 0.0065 test loss: 0.0143\n",
      "Epoch 4 batch 2900 train loss: 0.0064 test loss: 0.0130\n",
      "Epoch 5 batch 0 train loss: 0.0065 test loss: 0.0123\n",
      "Epoch 5 batch 100 train loss: 0.0076 test loss: 0.0147\n",
      "Epoch 5 batch 200 train loss: 0.0090 test loss: 0.0128\n",
      "Epoch 5 batch 300 train loss: 0.0075 test loss: 0.0133\n",
      "Epoch 5 batch 400 train loss: 0.0072 test loss: 0.0138\n",
      "Epoch 5 batch 500 train loss: 0.0064 test loss: 0.0132\n",
      "Epoch 5 batch 600 train loss: 0.0067 test loss: 0.0140\n",
      "Epoch 5 batch 700 train loss: 0.0065 test loss: 0.0143\n",
      "Epoch 5 batch 800 train loss: 0.0062 test loss: 0.0130\n",
      "Epoch 5 batch 900 train loss: 0.0084 test loss: 0.0138\n",
      "Epoch 5 batch 1000 train loss: 0.0073 test loss: 0.0143\n",
      "Epoch 5 batch 1100 train loss: 0.0055 test loss: 0.0133\n",
      "Epoch 5 batch 1200 train loss: 0.0092 test loss: 0.0139\n",
      "Epoch 5 batch 1300 train loss: 0.0088 test loss: 0.0145\n",
      "Epoch 5 batch 1400 train loss: 0.0089 test loss: 0.0140\n",
      "Epoch 5 batch 1500 train loss: 0.0079 test loss: 0.0135\n",
      "Epoch 5 batch 1600 train loss: 0.0063 test loss: 0.0132\n",
      "Epoch 5 batch 1700 train loss: 0.0056 test loss: 0.0146\n",
      "Epoch 5 batch 1800 train loss: 0.0073 test loss: 0.0129\n",
      "Epoch 5 batch 1900 train loss: 0.0089 test loss: 0.0136\n",
      "Epoch 5 batch 2000 train loss: 0.0114 test loss: 0.0137\n",
      "Epoch 5 batch 2100 train loss: 0.0055 test loss: 0.0138\n",
      "Epoch 5 batch 2200 train loss: 0.0075 test loss: 0.0144\n",
      "Epoch 5 batch 2300 train loss: 0.0084 test loss: 0.0152\n",
      "Epoch 5 batch 2400 train loss: 0.0082 test loss: 0.0131\n",
      "Epoch 5 batch 2500 train loss: 0.0060 test loss: 0.0134\n",
      "Epoch 5 batch 2600 train loss: 0.0054 test loss: 0.0143\n",
      "Epoch 5 batch 2700 train loss: 0.0054 test loss: 0.0136\n",
      "Epoch 5 batch 2800 train loss: 0.0064 test loss: 0.0136\n",
      "Epoch 5 batch 2900 train loss: 0.0083 test loss: 0.0126\n",
      "Epoch 6 batch 0 train loss: 0.0093 test loss: 0.0124\n",
      "Epoch 6 batch 100 train loss: 0.0052 test loss: 0.0135\n",
      "Epoch 6 batch 200 train loss: 0.0077 test loss: 0.0131\n",
      "Epoch 6 batch 300 train loss: 0.0066 test loss: 0.0128\n",
      "Epoch 6 batch 400 train loss: 0.0064 test loss: 0.0137\n",
      "Epoch 6 batch 500 train loss: 0.0088 test loss: 0.0129\n",
      "Epoch 6 batch 600 train loss: 0.0065 test loss: 0.0139\n",
      "Epoch 6 batch 700 train loss: 0.0058 test loss: 0.0137\n",
      "Epoch 6 batch 800 train loss: 0.0080 test loss: 0.0138\n",
      "Epoch 6 batch 900 train loss: 0.0088 test loss: 0.0134\n",
      "Epoch 6 batch 1000 train loss: 0.0087 test loss: 0.0145\n",
      "Epoch 6 batch 1100 train loss: 0.0078 test loss: 0.0147\n",
      "Epoch 6 batch 1200 train loss: 0.0098 test loss: 0.0131\n",
      "Epoch 6 batch 1300 train loss: 0.0103 test loss: 0.0151\n",
      "Epoch 6 batch 1400 train loss: 0.0067 test loss: 0.0146\n",
      "Epoch 6 batch 1500 train loss: 0.0066 test loss: 0.0129\n",
      "Epoch 6 batch 1600 train loss: 0.0078 test loss: 0.0143\n",
      "Epoch 6 batch 1700 train loss: 0.0072 test loss: 0.0139\n",
      "Epoch 6 batch 1800 train loss: 0.0086 test loss: 0.0148\n",
      "Epoch 6 batch 1900 train loss: 0.0088 test loss: 0.0132\n",
      "Epoch 6 batch 2000 train loss: 0.0086 test loss: 0.0146\n",
      "Epoch 6 batch 2100 train loss: 0.0084 test loss: 0.0143\n",
      "Epoch 6 batch 2200 train loss: 0.0086 test loss: 0.0137\n",
      "Epoch 6 batch 2300 train loss: 0.0072 test loss: 0.0146\n",
      "Epoch 6 batch 2400 train loss: 0.0073 test loss: 0.0128\n",
      "Epoch 6 batch 2500 train loss: 0.0095 test loss: 0.0133\n",
      "Epoch 6 batch 2600 train loss: 0.0073 test loss: 0.0128\n",
      "Epoch 6 batch 2700 train loss: 0.0072 test loss: 0.0137\n",
      "Epoch 6 batch 2800 train loss: 0.0078 test loss: 0.0118\n",
      "Epoch 6 batch 2900 train loss: 0.0088 test loss: 0.0127\n",
      "Epoch 7 batch 0 train loss: 0.0080 test loss: 0.0120\n",
      "Epoch 7 batch 100 train loss: 0.0075 test loss: 0.0137\n",
      "Epoch 7 batch 200 train loss: 0.0056 test loss: 0.0143\n",
      "Epoch 7 batch 300 train loss: 0.0067 test loss: 0.0131\n",
      "Epoch 7 batch 400 train loss: 0.0055 test loss: 0.0139\n",
      "Epoch 7 batch 500 train loss: 0.0077 test loss: 0.0133\n",
      "Epoch 7 batch 600 train loss: 0.0070 test loss: 0.0132\n",
      "Epoch 7 batch 700 train loss: 0.0067 test loss: 0.0125\n",
      "Epoch 7 batch 800 train loss: 0.0071 test loss: 0.0129\n",
      "Epoch 7 batch 900 train loss: 0.0066 test loss: 0.0128\n",
      "Epoch 7 batch 1000 train loss: 0.0066 test loss: 0.0139\n",
      "Epoch 7 batch 1100 train loss: 0.0069 test loss: 0.0129\n",
      "Epoch 7 batch 1200 train loss: 0.0083 test loss: 0.0152\n",
      "Epoch 7 batch 1300 train loss: 0.0062 test loss: 0.0131\n",
      "Epoch 7 batch 1400 train loss: 0.0112 test loss: 0.0139\n",
      "Epoch 7 batch 1500 train loss: 0.0074 test loss: 0.0140\n",
      "Epoch 7 batch 1600 train loss: 0.0058 test loss: 0.0138\n",
      "Epoch 7 batch 1700 train loss: 0.0060 test loss: 0.0142\n",
      "Epoch 7 batch 1800 train loss: 0.0072 test loss: 0.0131\n",
      "Epoch 7 batch 1900 train loss: 0.0083 test loss: 0.0123\n",
      "Epoch 7 batch 2000 train loss: 0.0068 test loss: 0.0153\n",
      "Epoch 7 batch 2100 train loss: 0.0086 test loss: 0.0150\n",
      "Epoch 7 batch 2200 train loss: 0.0062 test loss: 0.0143\n",
      "Epoch 7 batch 2300 train loss: 0.0065 test loss: 0.0142\n",
      "Epoch 7 batch 2400 train loss: 0.0071 test loss: 0.0151\n",
      "Epoch 7 batch 2500 train loss: 0.0062 test loss: 0.0146\n",
      "Epoch 7 batch 2600 train loss: 0.0083 test loss: 0.0129\n",
      "Epoch 7 batch 2700 train loss: 0.0077 test loss: 0.0146\n",
      "Epoch 7 batch 2800 train loss: 0.0068 test loss: 0.0138\n",
      "Epoch 7 batch 2900 train loss: 0.0060 test loss: 0.0141\n",
      "Epoch 8 batch 0 train loss: 0.0054 test loss: 0.0119\n",
      "Epoch 8 batch 100 train loss: 0.0097 test loss: 0.0134\n",
      "Epoch 8 batch 200 train loss: 0.0079 test loss: 0.0136\n",
      "Epoch 8 batch 300 train loss: 0.0063 test loss: 0.0136\n",
      "Epoch 8 batch 400 train loss: 0.0059 test loss: 0.0133\n",
      "Epoch 8 batch 500 train loss: 0.0060 test loss: 0.0136\n",
      "Epoch 8 batch 600 train loss: 0.0101 test loss: 0.0132\n",
      "Epoch 8 batch 700 train loss: 0.0100 test loss: 0.0145\n",
      "Epoch 8 batch 800 train loss: 0.0052 test loss: 0.0142\n",
      "Epoch 8 batch 900 train loss: 0.0092 test loss: 0.0124\n",
      "Epoch 8 batch 1000 train loss: 0.0047 test loss: 0.0130\n",
      "Epoch 8 batch 1100 train loss: 0.0069 test loss: 0.0137\n",
      "Epoch 8 batch 1200 train loss: 0.0079 test loss: 0.0125\n",
      "Epoch 8 batch 1300 train loss: 0.0064 test loss: 0.0153\n",
      "Epoch 8 batch 1400 train loss: 0.0080 test loss: 0.0135\n",
      "Epoch 8 batch 1500 train loss: 0.0079 test loss: 0.0145\n",
      "Epoch 8 batch 1600 train loss: 0.0060 test loss: 0.0145\n",
      "Epoch 8 batch 1700 train loss: 0.0074 test loss: 0.0134\n",
      "Epoch 8 batch 1800 train loss: 0.0086 test loss: 0.0145\n",
      "Epoch 8 batch 1900 train loss: 0.0067 test loss: 0.0149\n",
      "Epoch 8 batch 2000 train loss: 0.0074 test loss: 0.0167\n",
      "Epoch 8 batch 2100 train loss: 0.0094 test loss: 0.0136\n",
      "Epoch 8 batch 2200 train loss: 0.0071 test loss: 0.0135\n",
      "Epoch 8 batch 2300 train loss: 0.0071 test loss: 0.0128\n",
      "Epoch 8 batch 2400 train loss: 0.0078 test loss: 0.0137\n",
      "Epoch 8 batch 2500 train loss: 0.0066 test loss: 0.0144\n",
      "Epoch 8 batch 2600 train loss: 0.0058 test loss: 0.0127\n",
      "Epoch 8 batch 2700 train loss: 0.0053 test loss: 0.0143\n",
      "Epoch 8 batch 2800 train loss: 0.0074 test loss: 0.0144\n",
      "Epoch 8 batch 2900 train loss: 0.0079 test loss: 0.0129\n",
      "Epoch 9 batch 0 train loss: 0.0071 test loss: 0.0144\n",
      "Epoch 9 batch 100 train loss: 0.0102 test loss: 0.0143\n",
      "Epoch 9 batch 200 train loss: 0.0074 test loss: 0.0130\n",
      "Epoch 9 batch 300 train loss: 0.0069 test loss: 0.0142\n",
      "Epoch 9 batch 400 train loss: 0.0045 test loss: 0.0149\n",
      "Epoch 9 batch 500 train loss: 0.0062 test loss: 0.0132\n",
      "Epoch 9 batch 600 train loss: 0.0069 test loss: 0.0137\n",
      "Epoch 9 batch 700 train loss: 0.0077 test loss: 0.0135\n",
      "Epoch 9 batch 800 train loss: 0.0089 test loss: 0.0145\n",
      "Epoch 9 batch 900 train loss: 0.0072 test loss: 0.0142\n",
      "Epoch 9 batch 1000 train loss: 0.0062 test loss: 0.0129\n",
      "Epoch 9 batch 1100 train loss: 0.0080 test loss: 0.0139\n",
      "Epoch 9 batch 1200 train loss: 0.0084 test loss: 0.0122\n",
      "Epoch 9 batch 1300 train loss: 0.0087 test loss: 0.0130\n",
      "Epoch 9 batch 1400 train loss: 0.0076 test loss: 0.0148\n",
      "Epoch 9 batch 1500 train loss: 0.0072 test loss: 0.0146\n",
      "Epoch 9 batch 1600 train loss: 0.0065 test loss: 0.0133\n",
      "Epoch 9 batch 1700 train loss: 0.0078 test loss: 0.0144\n",
      "Epoch 9 batch 1800 train loss: 0.0072 test loss: 0.0120\n",
      "Epoch 9 batch 1900 train loss: 0.0068 test loss: 0.0149\n",
      "Epoch 9 batch 2000 train loss: 0.0072 test loss: 0.0149\n",
      "Epoch 9 batch 2100 train loss: 0.0060 test loss: 0.0140\n",
      "Epoch 9 batch 2200 train loss: 0.0076 test loss: 0.0143\n",
      "Epoch 9 batch 2300 train loss: 0.0071 test loss: 0.0125\n",
      "Epoch 9 batch 2400 train loss: 0.0059 test loss: 0.0162\n",
      "Epoch 9 batch 2500 train loss: 0.0078 test loss: 0.0143\n",
      "Epoch 9 batch 2600 train loss: 0.0066 test loss: 0.0129\n",
      "Epoch 9 batch 2700 train loss: 0.0064 test loss: 0.0147\n",
      "Epoch 9 batch 2800 train loss: 0.0074 test loss: 0.0128\n",
      "Epoch 9 batch 2900 train loss: 0.0097 test loss: 0.0123\n",
      "Epoch 10 batch 0 train loss: 0.0059 test loss: 0.0133\n",
      "Epoch 10 batch 100 train loss: 0.0084 test loss: 0.0157\n",
      "Epoch 10 batch 200 train loss: 0.0055 test loss: 0.0125\n",
      "Epoch 10 batch 300 train loss: 0.0072 test loss: 0.0118\n",
      "Epoch 10 batch 400 train loss: 0.0065 test loss: 0.0130\n",
      "Epoch 10 batch 500 train loss: 0.0103 test loss: 0.0141\n",
      "Epoch 10 batch 600 train loss: 0.0082 test loss: 0.0134\n",
      "Epoch 10 batch 700 train loss: 0.0080 test loss: 0.0135\n",
      "Epoch 10 batch 800 train loss: 0.0077 test loss: 0.0146\n",
      "Epoch 10 batch 900 train loss: 0.0078 test loss: 0.0131\n",
      "Epoch 10 batch 1000 train loss: 0.0067 test loss: 0.0137\n",
      "Epoch 10 batch 1100 train loss: 0.0050 test loss: 0.0143\n",
      "Epoch 10 batch 1200 train loss: 0.0068 test loss: 0.0135\n",
      "Epoch 10 batch 1300 train loss: 0.0060 test loss: 0.0141\n",
      "Epoch 10 batch 1400 train loss: 0.0065 test loss: 0.0143\n",
      "Epoch 10 batch 1500 train loss: 0.0059 test loss: 0.0146\n",
      "Epoch 10 batch 1600 train loss: 0.0062 test loss: 0.0130\n",
      "Epoch 10 batch 1700 train loss: 0.0088 test loss: 0.0138\n",
      "Epoch 10 batch 1800 train loss: 0.0078 test loss: 0.0131\n",
      "Epoch 10 batch 1900 train loss: 0.0071 test loss: 0.0143\n",
      "Epoch 10 batch 2000 train loss: 0.0053 test loss: 0.0138\n",
      "Epoch 10 batch 2100 train loss: 0.0066 test loss: 0.0140\n",
      "Epoch 10 batch 2200 train loss: 0.0077 test loss: 0.0131\n",
      "Epoch 10 batch 2300 train loss: 0.0082 test loss: 0.0139\n",
      "Epoch 10 batch 2400 train loss: 0.0066 test loss: 0.0146\n",
      "Epoch 10 batch 2500 train loss: 0.0098 test loss: 0.0139\n",
      "Epoch 10 batch 2600 train loss: 0.0079 test loss: 0.0145\n",
      "Epoch 10 batch 2700 train loss: 0.0083 test loss: 0.0136\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.60p/ckpt-26\n",
      "Epoch 10 batch 2800 train loss: 0.0064 test loss: 0.0110\n",
      "Epoch 10 batch 2900 train loss: 0.0080 test loss: 0.0136\n",
      "Epoch 11 batch 0 train loss: 0.0053 test loss: 0.0130\n",
      "Epoch 11 batch 100 train loss: 0.0064 test loss: 0.0143\n",
      "Epoch 11 batch 200 train loss: 0.0063 test loss: 0.0126\n",
      "Epoch 11 batch 300 train loss: 0.0053 test loss: 0.0133\n",
      "Epoch 11 batch 400 train loss: 0.0082 test loss: 0.0144\n",
      "Epoch 11 batch 500 train loss: 0.0082 test loss: 0.0144\n",
      "Epoch 11 batch 600 train loss: 0.0074 test loss: 0.0132\n",
      "Epoch 11 batch 700 train loss: 0.0057 test loss: 0.0135\n",
      "Epoch 11 batch 800 train loss: 0.0088 test loss: 0.0151\n",
      "Epoch 11 batch 900 train loss: 0.0053 test loss: 0.0119\n",
      "Epoch 11 batch 1000 train loss: 0.0055 test loss: 0.0141\n",
      "Epoch 11 batch 1100 train loss: 0.0061 test loss: 0.0133\n",
      "Epoch 11 batch 1200 train loss: 0.0081 test loss: 0.0133\n",
      "Epoch 11 batch 1300 train loss: 0.0064 test loss: 0.0148\n",
      "Epoch 11 batch 1400 train loss: 0.0087 test loss: 0.0133\n",
      "Epoch 11 batch 1500 train loss: 0.0084 test loss: 0.0130\n",
      "Epoch 11 batch 1600 train loss: 0.0067 test loss: 0.0133\n",
      "Epoch 11 batch 1700 train loss: 0.0065 test loss: 0.0130\n",
      "Epoch 11 batch 1800 train loss: 0.0074 test loss: 0.0135\n",
      "Epoch 11 batch 1900 train loss: 0.0089 test loss: 0.0143\n",
      "Epoch 11 batch 2000 train loss: 0.0072 test loss: 0.0138\n",
      "Epoch 11 batch 2100 train loss: 0.0058 test loss: 0.0131\n",
      "Epoch 11 batch 2200 train loss: 0.0064 test loss: 0.0157\n",
      "Epoch 11 batch 2300 train loss: 0.0073 test loss: 0.0135\n",
      "Epoch 11 batch 2400 train loss: 0.0096 test loss: 0.0117\n",
      "Epoch 11 batch 2500 train loss: 0.0075 test loss: 0.0137\n",
      "Epoch 11 batch 2600 train loss: 0.0079 test loss: 0.0116\n",
      "Epoch 11 batch 2700 train loss: 0.0103 test loss: 0.0131\n",
      "Epoch 11 batch 2800 train loss: 0.0084 test loss: 0.0137\n",
      "Epoch 11 batch 2900 train loss: 0.0090 test loss: 0.0117\n",
      "Epoch 12 batch 0 train loss: 0.0082 test loss: 0.0115\n",
      "Epoch 12 batch 100 train loss: 0.0077 test loss: 0.0143\n",
      "Epoch 12 batch 200 train loss: 0.0080 test loss: 0.0127\n",
      "Epoch 12 batch 300 train loss: 0.0052 test loss: 0.0146\n",
      "Epoch 12 batch 400 train loss: 0.0068 test loss: 0.0138\n",
      "Epoch 12 batch 500 train loss: 0.0074 test loss: 0.0147\n",
      "Epoch 12 batch 600 train loss: 0.0086 test loss: 0.0143\n",
      "Epoch 12 batch 700 train loss: 0.0084 test loss: 0.0160\n",
      "Epoch 12 batch 800 train loss: 0.0068 test loss: 0.0131\n",
      "Epoch 12 batch 900 train loss: 0.0075 test loss: 0.0130\n",
      "Epoch 12 batch 1000 train loss: 0.0040 test loss: 0.0150\n",
      "Epoch 12 batch 1100 train loss: 0.0066 test loss: 0.0150\n",
      "Epoch 12 batch 1200 train loss: 0.0081 test loss: 0.0133\n",
      "Epoch 12 batch 1300 train loss: 0.0072 test loss: 0.0127\n",
      "Epoch 12 batch 1400 train loss: 0.0077 test loss: 0.0138\n",
      "Epoch 12 batch 1500 train loss: 0.0077 test loss: 0.0130\n",
      "Epoch 12 batch 1600 train loss: 0.0089 test loss: 0.0134\n",
      "Epoch 12 batch 1700 train loss: 0.0076 test loss: 0.0145\n",
      "Epoch 12 batch 1800 train loss: 0.0064 test loss: 0.0130\n",
      "Epoch 12 batch 1900 train loss: 0.0069 test loss: 0.0139\n",
      "Epoch 12 batch 2000 train loss: 0.0071 test loss: 0.0135\n",
      "Epoch 12 batch 2100 train loss: 0.0074 test loss: 0.0129\n",
      "Epoch 12 batch 2200 train loss: 0.0066 test loss: 0.0148\n",
      "Epoch 12 batch 2300 train loss: 0.0081 test loss: 0.0130\n",
      "Epoch 12 batch 2400 train loss: 0.0054 test loss: 0.0137\n",
      "Epoch 12 batch 2500 train loss: 0.0063 test loss: 0.0132\n",
      "Epoch 12 batch 2600 train loss: 0.0087 test loss: 0.0128\n",
      "Epoch 12 batch 2700 train loss: 0.0064 test loss: 0.0123\n",
      "Epoch 12 batch 2800 train loss: 0.0085 test loss: 0.0138\n",
      "Epoch 12 batch 2900 train loss: 0.0054 test loss: 0.0144\n",
      "Epoch 13 batch 0 train loss: 0.0110 test loss: 0.0122\n",
      "Epoch 13 batch 100 train loss: 0.0076 test loss: 0.0124\n",
      "Epoch 13 batch 200 train loss: 0.0069 test loss: 0.0146\n",
      "Epoch 13 batch 300 train loss: 0.0057 test loss: 0.0131\n",
      "Epoch 13 batch 400 train loss: 0.0069 test loss: 0.0123\n",
      "Epoch 13 batch 500 train loss: 0.0085 test loss: 0.0144\n",
      "Epoch 13 batch 600 train loss: 0.0064 test loss: 0.0143\n",
      "Epoch 13 batch 700 train loss: 0.0056 test loss: 0.0142\n",
      "Epoch 13 batch 800 train loss: 0.0089 test loss: 0.0132\n",
      "Epoch 13 batch 900 train loss: 0.0069 test loss: 0.0129\n",
      "Epoch 13 batch 1000 train loss: 0.0059 test loss: 0.0141\n",
      "Epoch 13 batch 1100 train loss: 0.0057 test loss: 0.0132\n",
      "Epoch 13 batch 1200 train loss: 0.0089 test loss: 0.0128\n",
      "Epoch 13 batch 1300 train loss: 0.0072 test loss: 0.0143\n",
      "Epoch 13 batch 1400 train loss: 0.0060 test loss: 0.0140\n",
      "Epoch 13 batch 1500 train loss: 0.0056 test loss: 0.0133\n",
      "Epoch 13 batch 1600 train loss: 0.0075 test loss: 0.0133\n",
      "Epoch 13 batch 1700 train loss: 0.0052 test loss: 0.0137\n",
      "Epoch 13 batch 1800 train loss: 0.0063 test loss: 0.0128\n",
      "Epoch 13 batch 1900 train loss: 0.0076 test loss: 0.0145\n",
      "Epoch 13 batch 2000 train loss: 0.0066 test loss: 0.0154\n",
      "Epoch 13 batch 2100 train loss: 0.0070 test loss: 0.0149\n",
      "Epoch 13 batch 2200 train loss: 0.0079 test loss: 0.0139\n",
      "Epoch 13 batch 2300 train loss: 0.0078 test loss: 0.0132\n",
      "Epoch 13 batch 2400 train loss: 0.0101 test loss: 0.0139\n",
      "Epoch 13 batch 2500 train loss: 0.0114 test loss: 0.0139\n",
      "Epoch 13 batch 2600 train loss: 0.0060 test loss: 0.0133\n",
      "Epoch 13 batch 2700 train loss: 0.0080 test loss: 0.0141\n",
      "Epoch 13 batch 2800 train loss: 0.0077 test loss: 0.0139\n",
      "Epoch 13 batch 2900 train loss: 0.0090 test loss: 0.0124\n",
      "Epoch 14 batch 0 train loss: 0.0093 test loss: 0.0123\n",
      "Epoch 14 batch 100 train loss: 0.0099 test loss: 0.0133\n",
      "Epoch 14 batch 200 train loss: 0.0058 test loss: 0.0139\n",
      "Epoch 14 batch 300 train loss: 0.0065 test loss: 0.0120\n",
      "Epoch 14 batch 400 train loss: 0.0073 test loss: 0.0142\n",
      "Epoch 14 batch 500 train loss: 0.0062 test loss: 0.0129\n",
      "Epoch 14 batch 600 train loss: 0.0055 test loss: 0.0124\n",
      "Epoch 14 batch 700 train loss: 0.0072 test loss: 0.0133\n",
      "Epoch 14 batch 800 train loss: 0.0065 test loss: 0.0132\n",
      "Epoch 14 batch 900 train loss: 0.0063 test loss: 0.0132\n",
      "Epoch 14 batch 1000 train loss: 0.0080 test loss: 0.0135\n",
      "Epoch 14 batch 1100 train loss: 0.0073 test loss: 0.0140\n",
      "Epoch 14 batch 1200 train loss: 0.0076 test loss: 0.0124\n",
      "Epoch 14 batch 1300 train loss: 0.0083 test loss: 0.0134\n",
      "Epoch 14 batch 1400 train loss: 0.0068 test loss: 0.0142\n",
      "Epoch 14 batch 1500 train loss: 0.0072 test loss: 0.0126\n",
      "Epoch 14 batch 1600 train loss: 0.0119 test loss: 0.0135\n",
      "Epoch 14 batch 1700 train loss: 0.0065 test loss: 0.0137\n",
      "Epoch 14 batch 1800 train loss: 0.0069 test loss: 0.0140\n",
      "Epoch 14 batch 1900 train loss: 0.0087 test loss: 0.0139\n",
      "Epoch 14 batch 2000 train loss: 0.0074 test loss: 0.0132\n",
      "Epoch 14 batch 2100 train loss: 0.0078 test loss: 0.0135\n",
      "Epoch 14 batch 2200 train loss: 0.0072 test loss: 0.0149\n",
      "Epoch 14 batch 2300 train loss: 0.0070 test loss: 0.0143\n",
      "Epoch 14 batch 2400 train loss: 0.0066 test loss: 0.0130\n",
      "Epoch 14 batch 2500 train loss: 0.0063 test loss: 0.0125\n",
      "Epoch 14 batch 2600 train loss: 0.0059 test loss: 0.0137\n",
      "Epoch 14 batch 2700 train loss: 0.0063 test loss: 0.0137\n",
      "Epoch 14 batch 2800 train loss: 0.0085 test loss: 0.0140\n",
      "Epoch 14 batch 2900 train loss: 0.0076 test loss: 0.0119\n",
      "Epoch 15 batch 0 train loss: 0.0079 test loss: 0.0133\n",
      "Epoch 15 batch 100 train loss: 0.0054 test loss: 0.0140\n",
      "Epoch 15 batch 200 train loss: 0.0078 test loss: 0.0131\n",
      "Epoch 15 batch 300 train loss: 0.0075 test loss: 0.0130\n",
      "Epoch 15 batch 400 train loss: 0.0075 test loss: 0.0134\n",
      "Epoch 15 batch 500 train loss: 0.0076 test loss: 0.0137\n",
      "Epoch 15 batch 600 train loss: 0.0081 test loss: 0.0123\n",
      "Epoch 15 batch 700 train loss: 0.0069 test loss: 0.0136\n",
      "Epoch 15 batch 800 train loss: 0.0068 test loss: 0.0133\n",
      "Epoch 15 batch 900 train loss: 0.0069 test loss: 0.0125\n",
      "Epoch 15 batch 1000 train loss: 0.0071 test loss: 0.0138\n",
      "Epoch 15 batch 1100 train loss: 0.0070 test loss: 0.0140\n",
      "Epoch 15 batch 1200 train loss: 0.0066 test loss: 0.0117\n",
      "Epoch 15 batch 1300 train loss: 0.0066 test loss: 0.0147\n",
      "Epoch 15 batch 1400 train loss: 0.0070 test loss: 0.0140\n",
      "Epoch 15 batch 1500 train loss: 0.0062 test loss: 0.0132\n",
      "Epoch 15 batch 1600 train loss: 0.0083 test loss: 0.0132\n",
      "Epoch 15 batch 1700 train loss: 0.0081 test loss: 0.0132\n",
      "Epoch 15 batch 1800 train loss: 0.0078 test loss: 0.0129\n",
      "Epoch 15 batch 1900 train loss: 0.0089 test loss: 0.0139\n",
      "Epoch 15 batch 2000 train loss: 0.0071 test loss: 0.0127\n",
      "Epoch 15 batch 2100 train loss: 0.0072 test loss: 0.0124\n",
      "Epoch 15 batch 2200 train loss: 0.0042 test loss: 0.0138\n",
      "Epoch 15 batch 2300 train loss: 0.0066 test loss: 0.0136\n",
      "Epoch 15 batch 2400 train loss: 0.0095 test loss: 0.0132\n",
      "Epoch 15 batch 2500 train loss: 0.0064 test loss: 0.0130\n",
      "Epoch 15 batch 2600 train loss: 0.0078 test loss: 0.0132\n",
      "Epoch 15 batch 2700 train loss: 0.0070 test loss: 0.0137\n",
      "Epoch 15 batch 2800 train loss: 0.0063 test loss: 0.0135\n",
      "Epoch 15 batch 2900 train loss: 0.0109 test loss: 0.0116\n",
      "Epoch 16 batch 0 train loss: 0.0066 test loss: 0.0143\n",
      "Epoch 16 batch 100 train loss: 0.0064 test loss: 0.0119\n",
      "Epoch 16 batch 200 train loss: 0.0080 test loss: 0.0126\n",
      "Epoch 16 batch 300 train loss: 0.0053 test loss: 0.0128\n",
      "Epoch 16 batch 400 train loss: 0.0072 test loss: 0.0131\n",
      "Epoch 16 batch 500 train loss: 0.0081 test loss: 0.0126\n",
      "Epoch 16 batch 600 train loss: 0.0064 test loss: 0.0139\n",
      "Epoch 16 batch 700 train loss: 0.0095 test loss: 0.0133\n",
      "Epoch 16 batch 800 train loss: 0.0070 test loss: 0.0136\n",
      "Epoch 16 batch 900 train loss: 0.0095 test loss: 0.0128\n",
      "Epoch 16 batch 1000 train loss: 0.0091 test loss: 0.0134\n",
      "Epoch 16 batch 1100 train loss: 0.0071 test loss: 0.0129\n",
      "Epoch 16 batch 1200 train loss: 0.0077 test loss: 0.0128\n",
      "Epoch 16 batch 1300 train loss: 0.0077 test loss: 0.0143\n",
      "Epoch 16 batch 1400 train loss: 0.0081 test loss: 0.0132\n",
      "Epoch 16 batch 1500 train loss: 0.0064 test loss: 0.0133\n",
      "Epoch 16 batch 1600 train loss: 0.0072 test loss: 0.0135\n",
      "Epoch 16 batch 1700 train loss: 0.0063 test loss: 0.0154\n",
      "Epoch 16 batch 1800 train loss: 0.0071 test loss: 0.0131\n",
      "Epoch 16 batch 1900 train loss: 0.0083 test loss: 0.0129\n",
      "Epoch 16 batch 2000 train loss: 0.0059 test loss: 0.0133\n",
      "Epoch 16 batch 2100 train loss: 0.0071 test loss: 0.0133\n",
      "Epoch 16 batch 2200 train loss: 0.0065 test loss: 0.0140\n",
      "Epoch 16 batch 2300 train loss: 0.0070 test loss: 0.0145\n",
      "Epoch 16 batch 2400 train loss: 0.0084 test loss: 0.0150\n",
      "Epoch 16 batch 2500 train loss: 0.0079 test loss: 0.0127\n",
      "Epoch 16 batch 2600 train loss: 0.0085 test loss: 0.0145\n",
      "Epoch 16 batch 2700 train loss: 0.0068 test loss: 0.0143\n",
      "Epoch 16 batch 2800 train loss: 0.0065 test loss: 0.0139\n",
      "Epoch 16 batch 2900 train loss: 0.0075 test loss: 0.0122\n",
      "Epoch 17 batch 0 train loss: 0.0091 test loss: 0.0150\n",
      "Epoch 17 batch 100 train loss: 0.0099 test loss: 0.0124\n",
      "Epoch 17 batch 200 train loss: 0.0047 test loss: 0.0126\n",
      "Epoch 17 batch 300 train loss: 0.0083 test loss: 0.0138\n",
      "Epoch 17 batch 400 train loss: 0.0064 test loss: 0.0137\n",
      "Epoch 17 batch 500 train loss: 0.0086 test loss: 0.0135\n",
      "Epoch 17 batch 600 train loss: 0.0073 test loss: 0.0131\n",
      "Epoch 17 batch 700 train loss: 0.0078 test loss: 0.0140\n",
      "Epoch 17 batch 800 train loss: 0.0066 test loss: 0.0139\n",
      "Epoch 17 batch 900 train loss: 0.0065 test loss: 0.0124\n",
      "Epoch 17 batch 1000 train loss: 0.0086 test loss: 0.0135\n",
      "Epoch 17 batch 1100 train loss: 0.0071 test loss: 0.0133\n",
      "Epoch 17 batch 1200 train loss: 0.0084 test loss: 0.0132\n",
      "Epoch 17 batch 1300 train loss: 0.0076 test loss: 0.0126\n",
      "Epoch 17 batch 1400 train loss: 0.0066 test loss: 0.0146\n",
      "Epoch 17 batch 1500 train loss: 0.0072 test loss: 0.0151\n",
      "early stop.\n",
      "Checkpoint 26 restored!!\n",
      "Training for loss rate 0.70 start.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['bi_lstm_6/bi_lstm_block_12/bidirectional_12/forward_lstm_12/lstm_cell_37/kernel:0', 'bi_lstm_6/bi_lstm_block_12/bidirectional_12/forward_lstm_12/lstm_cell_37/recurrent_kernel:0', 'bi_lstm_6/bi_lstm_block_12/bidirectional_12/forward_lstm_12/lstm_cell_37/bias:0', 'bi_lstm_6/bi_lstm_block_12/bidirectional_12/backward_lstm_12/lstm_cell_38/kernel:0', 'bi_lstm_6/bi_lstm_block_12/bidirectional_12/backward_lstm_12/lstm_cell_38/recurrent_kernel:0', 'bi_lstm_6/bi_lstm_block_12/bidirectional_12/backward_lstm_12/lstm_cell_38/bias:0', 'bi_lstm_6/bi_lstm_block_12/layer_normalization_12/gamma:0', 'bi_lstm_6/bi_lstm_block_12/layer_normalization_12/beta:0', 'bi_lstm_6/bi_lstm_block_13/bidirectional_13/forward_lstm_13/lstm_cell_40/kernel:0', 'bi_lstm_6/bi_lstm_block_13/bidirectional_13/forward_lstm_13/lstm_cell_40/recurrent_kernel:0', 'bi_lstm_6/bi_lstm_block_13/bidirectional_13/forward_lstm_13/lstm_cell_40/bias:0', 'bi_lstm_6/bi_lstm_block_13/bidirectional_13/backward_lstm_13/lstm_cell_41/kernel:0', 'bi_lstm_6/bi_lstm_block_13/bidirectional_13/backward_lstm_13/lstm_cell_41/recurrent_kernel:0', 'bi_lstm_6/bi_lstm_block_13/bidirectional_13/backward_lstm_13/lstm_cell_41/bias:0', 'bi_lstm_6/bi_lstm_block_13/layer_normalization_13/gamma:0', 'bi_lstm_6/bi_lstm_block_13/layer_normalization_13/beta:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['bi_lstm_6/bi_lstm_block_12/bidirectional_12/forward_lstm_12/lstm_cell_37/kernel:0', 'bi_lstm_6/bi_lstm_block_12/bidirectional_12/forward_lstm_12/lstm_cell_37/recurrent_kernel:0', 'bi_lstm_6/bi_lstm_block_12/bidirectional_12/forward_lstm_12/lstm_cell_37/bias:0', 'bi_lstm_6/bi_lstm_block_12/bidirectional_12/backward_lstm_12/lstm_cell_38/kernel:0', 'bi_lstm_6/bi_lstm_block_12/bidirectional_12/backward_lstm_12/lstm_cell_38/recurrent_kernel:0', 'bi_lstm_6/bi_lstm_block_12/bidirectional_12/backward_lstm_12/lstm_cell_38/bias:0', 'bi_lstm_6/bi_lstm_block_12/layer_normalization_12/gamma:0', 'bi_lstm_6/bi_lstm_block_12/layer_normalization_12/beta:0', 'bi_lstm_6/bi_lstm_block_13/bidirectional_13/forward_lstm_13/lstm_cell_40/kernel:0', 'bi_lstm_6/bi_lstm_block_13/bidirectional_13/forward_lstm_13/lstm_cell_40/recurrent_kernel:0', 'bi_lstm_6/bi_lstm_block_13/bidirectional_13/forward_lstm_13/lstm_cell_40/bias:0', 'bi_lstm_6/bi_lstm_block_13/bidirectional_13/backward_lstm_13/lstm_cell_41/kernel:0', 'bi_lstm_6/bi_lstm_block_13/bidirectional_13/backward_lstm_13/lstm_cell_41/recurrent_kernel:0', 'bi_lstm_6/bi_lstm_block_13/bidirectional_13/backward_lstm_13/lstm_cell_41/bias:0', 'bi_lstm_6/bi_lstm_block_13/layer_normalization_13/gamma:0', 'bi_lstm_6/bi_lstm_block_13/layer_normalization_13/beta:0'] when minimizing the loss.\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.70p/ckpt-1\n",
      "Epoch 0 batch 0 train loss: 0.5425 test loss: 0.7200\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.70p/ckpt-2\n",
      "Epoch 0 batch 100 train loss: 0.1739 test loss: 0.1910\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.70p/ckpt-3\n",
      "Epoch 0 batch 200 train loss: 0.0934 test loss: 0.0909\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.70p/ckpt-4\n",
      "Epoch 0 batch 300 train loss: 0.0548 test loss: 0.0511\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.70p/ckpt-5\n",
      "Epoch 0 batch 400 train loss: 0.0371 test loss: 0.0366\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.70p/ckpt-6\n",
      "Epoch 0 batch 500 train loss: 0.0230 test loss: 0.0269\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.70p/ckpt-7\n",
      "Epoch 0 batch 600 train loss: 0.0157 test loss: 0.0218\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.70p/ckpt-8\n",
      "Epoch 0 batch 700 train loss: 0.0125 test loss: 0.0194\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.70p/ckpt-9\n",
      "Epoch 0 batch 800 train loss: 0.0123 test loss: 0.0182\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.70p/ckpt-10\n",
      "Epoch 0 batch 900 train loss: 0.0098 test loss: 0.0167\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.70p/ckpt-11\n",
      "Epoch 0 batch 1000 train loss: 0.0110 test loss: 0.0154\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.70p/ckpt-12\n",
      "Epoch 0 batch 1100 train loss: 0.0093 test loss: 0.0152\n",
      "Epoch 0 batch 1200 train loss: 0.0107 test loss: 0.0159\n",
      "Epoch 0 batch 1300 train loss: 0.0087 test loss: 0.0174\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.70p/ckpt-13\n",
      "Epoch 0 batch 1400 train loss: 0.0087 test loss: 0.0148\n",
      "Epoch 0 batch 1500 train loss: 0.0082 test loss: 0.0154\n",
      "Epoch 0 batch 1600 train loss: 0.0082 test loss: 0.0157\n",
      "Epoch 0 batch 1700 train loss: 0.0097 test loss: 0.0166\n",
      "Epoch 0 batch 1800 train loss: 0.0084 test loss: 0.0150\n",
      "Epoch 0 batch 1900 train loss: 0.0120 test loss: 0.0156\n",
      "Epoch 0 batch 2000 train loss: 0.0080 test loss: 0.0165\n",
      "Epoch 0 batch 2100 train loss: 0.0073 test loss: 0.0165\n",
      "Epoch 0 batch 2200 train loss: 0.0080 test loss: 0.0158\n",
      "Epoch 0 batch 2300 train loss: 0.0098 test loss: 0.0155\n",
      "Epoch 0 batch 2400 train loss: 0.0086 test loss: 0.0149\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.70p/ckpt-14\n",
      "Epoch 0 batch 2500 train loss: 0.0096 test loss: 0.0142\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.70p/ckpt-15\n",
      "Epoch 0 batch 2600 train loss: 0.0098 test loss: 0.0140\n",
      "Epoch 0 batch 2700 train loss: 0.0062 test loss: 0.0146\n",
      "Epoch 0 batch 2800 train loss: 0.0064 test loss: 0.0144\n",
      "Epoch 0 batch 2900 train loss: 0.0062 test loss: 0.0146\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['bi_lstm_6/bi_lstm_block_12/bidirectional_12/forward_lstm_12/lstm_cell_37/kernel:0', 'bi_lstm_6/bi_lstm_block_12/bidirectional_12/forward_lstm_12/lstm_cell_37/recurrent_kernel:0', 'bi_lstm_6/bi_lstm_block_12/bidirectional_12/forward_lstm_12/lstm_cell_37/bias:0', 'bi_lstm_6/bi_lstm_block_12/bidirectional_12/backward_lstm_12/lstm_cell_38/kernel:0', 'bi_lstm_6/bi_lstm_block_12/bidirectional_12/backward_lstm_12/lstm_cell_38/recurrent_kernel:0', 'bi_lstm_6/bi_lstm_block_12/bidirectional_12/backward_lstm_12/lstm_cell_38/bias:0', 'bi_lstm_6/bi_lstm_block_12/layer_normalization_12/gamma:0', 'bi_lstm_6/bi_lstm_block_12/layer_normalization_12/beta:0', 'bi_lstm_6/bi_lstm_block_13/bidirectional_13/forward_lstm_13/lstm_cell_40/kernel:0', 'bi_lstm_6/bi_lstm_block_13/bidirectional_13/forward_lstm_13/lstm_cell_40/recurrent_kernel:0', 'bi_lstm_6/bi_lstm_block_13/bidirectional_13/forward_lstm_13/lstm_cell_40/bias:0', 'bi_lstm_6/bi_lstm_block_13/bidirectional_13/backward_lstm_13/lstm_cell_41/kernel:0', 'bi_lstm_6/bi_lstm_block_13/bidirectional_13/backward_lstm_13/lstm_cell_41/recurrent_kernel:0', 'bi_lstm_6/bi_lstm_block_13/bidirectional_13/backward_lstm_13/lstm_cell_41/bias:0', 'bi_lstm_6/bi_lstm_block_13/layer_normalization_13/gamma:0', 'bi_lstm_6/bi_lstm_block_13/layer_normalization_13/beta:0'] when minimizing the loss.\n",
      "Epoch 1 batch 0 train loss: 0.0093 test loss: 0.0157\n",
      "Epoch 1 batch 100 train loss: 0.0088 test loss: 0.0146\n",
      "Epoch 1 batch 200 train loss: 0.0074 test loss: 0.0151\n",
      "Epoch 1 batch 300 train loss: 0.0077 test loss: 0.0148\n",
      "Epoch 1 batch 400 train loss: 0.0068 test loss: 0.0150\n",
      "Epoch 1 batch 500 train loss: 0.0078 test loss: 0.0165\n",
      "Epoch 1 batch 600 train loss: 0.0103 test loss: 0.0155\n",
      "Epoch 1 batch 700 train loss: 0.0070 test loss: 0.0165\n",
      "Epoch 1 batch 800 train loss: 0.0082 test loss: 0.0168\n",
      "Epoch 1 batch 900 train loss: 0.0089 test loss: 0.0158\n",
      "Epoch 1 batch 1000 train loss: 0.0089 test loss: 0.0184\n",
      "Epoch 1 batch 1100 train loss: 0.0100 test loss: 0.0161\n",
      "Epoch 1 batch 1200 train loss: 0.0073 test loss: 0.0147\n",
      "Epoch 1 batch 1300 train loss: 0.0077 test loss: 0.0161\n",
      "Epoch 1 batch 1400 train loss: 0.0071 test loss: 0.0146\n",
      "Epoch 1 batch 1500 train loss: 0.0063 test loss: 0.0147\n",
      "Epoch 1 batch 1600 train loss: 0.0065 test loss: 0.0147\n",
      "Epoch 1 batch 1700 train loss: 0.0067 test loss: 0.0172\n",
      "Epoch 1 batch 1800 train loss: 0.0070 test loss: 0.0147\n",
      "Epoch 1 batch 1900 train loss: 0.0091 test loss: 0.0147\n",
      "Epoch 1 batch 2000 train loss: 0.0073 test loss: 0.0150\n",
      "Epoch 1 batch 2100 train loss: 0.0074 test loss: 0.0156\n",
      "Epoch 1 batch 2200 train loss: 0.0059 test loss: 0.0145\n",
      "Epoch 1 batch 2300 train loss: 0.0081 test loss: 0.0167\n",
      "Epoch 1 batch 2400 train loss: 0.0068 test loss: 0.0153\n",
      "Epoch 1 batch 2500 train loss: 0.0104 test loss: 0.0150\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.70p/ckpt-16\n",
      "Epoch 1 batch 2600 train loss: 0.0071 test loss: 0.0136\n",
      "Epoch 1 batch 2700 train loss: 0.0057 test loss: 0.0164\n",
      "Epoch 1 batch 2800 train loss: 0.0074 test loss: 0.0157\n",
      "Epoch 1 batch 2900 train loss: 0.0083 test loss: 0.0139\n",
      "Epoch 2 batch 0 train loss: 0.0086 test loss: 0.0142\n",
      "Epoch 2 batch 100 train loss: 0.0091 test loss: 0.0142\n",
      "Epoch 2 batch 200 train loss: 0.0090 test loss: 0.0145\n",
      "Epoch 2 batch 300 train loss: 0.0070 test loss: 0.0140\n",
      "Epoch 2 batch 400 train loss: 0.0078 test loss: 0.0157\n",
      "Epoch 2 batch 500 train loss: 0.0093 test loss: 0.0142\n",
      "Epoch 2 batch 600 train loss: 0.0103 test loss: 0.0153\n",
      "Epoch 2 batch 700 train loss: 0.0073 test loss: 0.0157\n",
      "Epoch 2 batch 800 train loss: 0.0086 test loss: 0.0144\n",
      "Epoch 2 batch 900 train loss: 0.0060 test loss: 0.0156\n",
      "Epoch 2 batch 1000 train loss: 0.0097 test loss: 0.0165\n",
      "Epoch 2 batch 1100 train loss: 0.0090 test loss: 0.0158\n",
      "Epoch 2 batch 1200 train loss: 0.0078 test loss: 0.0158\n",
      "Epoch 2 batch 1300 train loss: 0.0097 test loss: 0.0149\n",
      "Epoch 2 batch 1400 train loss: 0.0060 test loss: 0.0156\n",
      "Epoch 2 batch 1500 train loss: 0.0095 test loss: 0.0172\n",
      "Epoch 2 batch 1600 train loss: 0.0061 test loss: 0.0157\n",
      "Epoch 2 batch 1700 train loss: 0.0078 test loss: 0.0159\n",
      "Epoch 2 batch 1800 train loss: 0.0093 test loss: 0.0152\n",
      "Epoch 2 batch 1900 train loss: 0.0068 test loss: 0.0158\n",
      "Epoch 2 batch 2000 train loss: 0.0062 test loss: 0.0155\n",
      "Epoch 2 batch 2100 train loss: 0.0080 test loss: 0.0173\n",
      "Epoch 2 batch 2200 train loss: 0.0079 test loss: 0.0162\n",
      "Epoch 2 batch 2300 train loss: 0.0091 test loss: 0.0174\n",
      "Epoch 2 batch 2400 train loss: 0.0070 test loss: 0.0162\n",
      "Epoch 2 batch 2500 train loss: 0.0065 test loss: 0.0142\n",
      "Epoch 2 batch 2600 train loss: 0.0067 test loss: 0.0155\n",
      "Epoch 2 batch 2700 train loss: 0.0088 test loss: 0.0158\n",
      "Epoch 2 batch 2800 train loss: 0.0072 test loss: 0.0154\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.70p/ckpt-17\n",
      "Epoch 2 batch 2900 train loss: 0.0054 test loss: 0.0129\n",
      "Epoch 3 batch 0 train loss: 0.0077 test loss: 0.0156\n",
      "Epoch 3 batch 100 train loss: 0.0080 test loss: 0.0157\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.70p/ckpt-18\n",
      "Epoch 3 batch 200 train loss: 0.0072 test loss: 0.0126\n",
      "Epoch 3 batch 300 train loss: 0.0064 test loss: 0.0155\n",
      "Epoch 3 batch 400 train loss: 0.0070 test loss: 0.0149\n",
      "Epoch 3 batch 500 train loss: 0.0086 test loss: 0.0170\n",
      "Epoch 3 batch 600 train loss: 0.0071 test loss: 0.0152\n",
      "Epoch 3 batch 700 train loss: 0.0065 test loss: 0.0142\n",
      "Epoch 3 batch 800 train loss: 0.0082 test loss: 0.0148\n",
      "Epoch 3 batch 900 train loss: 0.0075 test loss: 0.0160\n",
      "Epoch 3 batch 1000 train loss: 0.0078 test loss: 0.0154\n",
      "Epoch 3 batch 1100 train loss: 0.0115 test loss: 0.0161\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.70p/ckpt-19\n",
      "Epoch 3 batch 1200 train loss: 0.0068 test loss: 0.0125\n",
      "Epoch 3 batch 1300 train loss: 0.0073 test loss: 0.0167\n",
      "Epoch 3 batch 1400 train loss: 0.0081 test loss: 0.0151\n",
      "Epoch 3 batch 1500 train loss: 0.0082 test loss: 0.0147\n",
      "Epoch 3 batch 1600 train loss: 0.0079 test loss: 0.0164\n",
      "Epoch 3 batch 1700 train loss: 0.0086 test loss: 0.0164\n",
      "Epoch 3 batch 1800 train loss: 0.0072 test loss: 0.0152\n",
      "Epoch 3 batch 1900 train loss: 0.0081 test loss: 0.0168\n",
      "Epoch 3 batch 2000 train loss: 0.0063 test loss: 0.0175\n",
      "Epoch 3 batch 2100 train loss: 0.0085 test loss: 0.0168\n",
      "Epoch 3 batch 2200 train loss: 0.0066 test loss: 0.0149\n",
      "Epoch 3 batch 2300 train loss: 0.0053 test loss: 0.0150\n",
      "Epoch 3 batch 2400 train loss: 0.0065 test loss: 0.0157\n",
      "Epoch 3 batch 2500 train loss: 0.0060 test loss: 0.0153\n",
      "Epoch 3 batch 2600 train loss: 0.0101 test loss: 0.0147\n",
      "Epoch 3 batch 2700 train loss: 0.0059 test loss: 0.0164\n",
      "Epoch 3 batch 2800 train loss: 0.0073 test loss: 0.0152\n",
      "Epoch 3 batch 2900 train loss: 0.0068 test loss: 0.0149\n",
      "Epoch 4 batch 0 train loss: 0.0058 test loss: 0.0145\n",
      "Epoch 4 batch 100 train loss: 0.0065 test loss: 0.0153\n",
      "Epoch 4 batch 200 train loss: 0.0085 test loss: 0.0129\n",
      "Epoch 4 batch 300 train loss: 0.0089 test loss: 0.0140\n",
      "Epoch 4 batch 400 train loss: 0.0086 test loss: 0.0142\n",
      "Epoch 4 batch 500 train loss: 0.0078 test loss: 0.0162\n",
      "Epoch 4 batch 600 train loss: 0.0075 test loss: 0.0143\n",
      "Epoch 4 batch 700 train loss: 0.0075 test loss: 0.0172\n",
      "Epoch 4 batch 800 train loss: 0.0048 test loss: 0.0169\n",
      "Epoch 4 batch 900 train loss: 0.0091 test loss: 0.0152\n",
      "Epoch 4 batch 1000 train loss: 0.0066 test loss: 0.0165\n",
      "Epoch 4 batch 1100 train loss: 0.0057 test loss: 0.0169\n",
      "Epoch 4 batch 1200 train loss: 0.0094 test loss: 0.0147\n",
      "Epoch 4 batch 1300 train loss: 0.0081 test loss: 0.0177\n",
      "Epoch 4 batch 1400 train loss: 0.0078 test loss: 0.0154\n",
      "Epoch 4 batch 1500 train loss: 0.0089 test loss: 0.0136\n",
      "Epoch 4 batch 1600 train loss: 0.0065 test loss: 0.0152\n",
      "Epoch 4 batch 1700 train loss: 0.0082 test loss: 0.0157\n",
      "Epoch 4 batch 1800 train loss: 0.0080 test loss: 0.0154\n",
      "Epoch 4 batch 1900 train loss: 0.0077 test loss: 0.0165\n",
      "Epoch 4 batch 2000 train loss: 0.0081 test loss: 0.0169\n",
      "Epoch 4 batch 2100 train loss: 0.0078 test loss: 0.0171\n",
      "Epoch 4 batch 2200 train loss: 0.0077 test loss: 0.0141\n",
      "Epoch 4 batch 2300 train loss: 0.0083 test loss: 0.0149\n",
      "Epoch 4 batch 2400 train loss: 0.0088 test loss: 0.0172\n",
      "Epoch 4 batch 2500 train loss: 0.0068 test loss: 0.0159\n",
      "Epoch 4 batch 2600 train loss: 0.0070 test loss: 0.0130\n",
      "Epoch 4 batch 2700 train loss: 0.0065 test loss: 0.0150\n",
      "Epoch 4 batch 2800 train loss: 0.0075 test loss: 0.0147\n",
      "Epoch 4 batch 2900 train loss: 0.0078 test loss: 0.0142\n",
      "Epoch 5 batch 0 train loss: 0.0086 test loss: 0.0148\n",
      "Epoch 5 batch 100 train loss: 0.0070 test loss: 0.0135\n",
      "Epoch 5 batch 200 train loss: 0.0068 test loss: 0.0140\n",
      "Epoch 5 batch 300 train loss: 0.0104 test loss: 0.0140\n",
      "Epoch 5 batch 400 train loss: 0.0093 test loss: 0.0148\n",
      "Epoch 5 batch 500 train loss: 0.0059 test loss: 0.0167\n",
      "Epoch 5 batch 600 train loss: 0.0072 test loss: 0.0152\n",
      "Epoch 5 batch 700 train loss: 0.0067 test loss: 0.0156\n",
      "Epoch 5 batch 800 train loss: 0.0077 test loss: 0.0139\n",
      "Epoch 5 batch 900 train loss: 0.0066 test loss: 0.0148\n",
      "Epoch 5 batch 1000 train loss: 0.0085 test loss: 0.0155\n",
      "Epoch 5 batch 1100 train loss: 0.0083 test loss: 0.0135\n",
      "Epoch 5 batch 1200 train loss: 0.0090 test loss: 0.0145\n",
      "Epoch 5 batch 1300 train loss: 0.0073 test loss: 0.0166\n",
      "Epoch 5 batch 1400 train loss: 0.0061 test loss: 0.0150\n",
      "Epoch 5 batch 1500 train loss: 0.0073 test loss: 0.0149\n",
      "Epoch 5 batch 1600 train loss: 0.0087 test loss: 0.0144\n",
      "Epoch 5 batch 1700 train loss: 0.0072 test loss: 0.0160\n",
      "Epoch 5 batch 1800 train loss: 0.0064 test loss: 0.0154\n",
      "Epoch 5 batch 1900 train loss: 0.0072 test loss: 0.0146\n",
      "Epoch 5 batch 2000 train loss: 0.0057 test loss: 0.0153\n",
      "Epoch 5 batch 2100 train loss: 0.0048 test loss: 0.0153\n",
      "Epoch 5 batch 2200 train loss: 0.0061 test loss: 0.0152\n",
      "Epoch 5 batch 2300 train loss: 0.0077 test loss: 0.0143\n",
      "Epoch 5 batch 2400 train loss: 0.0084 test loss: 0.0175\n",
      "Epoch 5 batch 2500 train loss: 0.0053 test loss: 0.0148\n",
      "Epoch 5 batch 2600 train loss: 0.0068 test loss: 0.0147\n",
      "Epoch 5 batch 2700 train loss: 0.0076 test loss: 0.0163\n",
      "Epoch 5 batch 2800 train loss: 0.0070 test loss: 0.0165\n",
      "Epoch 5 batch 2900 train loss: 0.0065 test loss: 0.0143\n",
      "Epoch 6 batch 0 train loss: 0.0083 test loss: 0.0148\n",
      "Epoch 6 batch 100 train loss: 0.0073 test loss: 0.0162\n",
      "Epoch 6 batch 200 train loss: 0.0067 test loss: 0.0136\n",
      "Epoch 6 batch 300 train loss: 0.0081 test loss: 0.0147\n",
      "Epoch 6 batch 400 train loss: 0.0067 test loss: 0.0160\n",
      "Epoch 6 batch 500 train loss: 0.0068 test loss: 0.0147\n",
      "Epoch 6 batch 600 train loss: 0.0076 test loss: 0.0140\n",
      "Epoch 6 batch 700 train loss: 0.0061 test loss: 0.0134\n",
      "Epoch 6 batch 800 train loss: 0.0086 test loss: 0.0131\n",
      "Epoch 6 batch 900 train loss: 0.0070 test loss: 0.0148\n",
      "Epoch 6 batch 1000 train loss: 0.0074 test loss: 0.0164\n",
      "Epoch 6 batch 1100 train loss: 0.0067 test loss: 0.0161\n",
      "Epoch 6 batch 1200 train loss: 0.0087 test loss: 0.0144\n",
      "Epoch 6 batch 1300 train loss: 0.0081 test loss: 0.0146\n",
      "Epoch 6 batch 1400 train loss: 0.0068 test loss: 0.0160\n",
      "Epoch 6 batch 1500 train loss: 0.0070 test loss: 0.0171\n",
      "Epoch 6 batch 1600 train loss: 0.0079 test loss: 0.0149\n",
      "Epoch 6 batch 1700 train loss: 0.0070 test loss: 0.0143\n",
      "Epoch 6 batch 1800 train loss: 0.0070 test loss: 0.0160\n",
      "Epoch 6 batch 1900 train loss: 0.0069 test loss: 0.0149\n",
      "Epoch 6 batch 2000 train loss: 0.0074 test loss: 0.0148\n",
      "Epoch 6 batch 2100 train loss: 0.0064 test loss: 0.0161\n",
      "Epoch 6 batch 2200 train loss: 0.0070 test loss: 0.0166\n",
      "Epoch 6 batch 2300 train loss: 0.0068 test loss: 0.0153\n",
      "Epoch 6 batch 2400 train loss: 0.0055 test loss: 0.0154\n",
      "Epoch 6 batch 2500 train loss: 0.0076 test loss: 0.0154\n",
      "Epoch 6 batch 2600 train loss: 0.0071 test loss: 0.0151\n",
      "Epoch 6 batch 2700 train loss: 0.0078 test loss: 0.0149\n",
      "Epoch 6 batch 2800 train loss: 0.0092 test loss: 0.0145\n",
      "Epoch 6 batch 2900 train loss: 0.0061 test loss: 0.0136\n",
      "Epoch 7 batch 0 train loss: 0.0072 test loss: 0.0132\n",
      "Epoch 7 batch 100 train loss: 0.0067 test loss: 0.0162\n",
      "Epoch 7 batch 200 train loss: 0.0069 test loss: 0.0143\n",
      "Epoch 7 batch 300 train loss: 0.0081 test loss: 0.0155\n",
      "Epoch 7 batch 400 train loss: 0.0067 test loss: 0.0143\n",
      "Epoch 7 batch 500 train loss: 0.0074 test loss: 0.0139\n",
      "Epoch 7 batch 600 train loss: 0.0067 test loss: 0.0136\n",
      "Epoch 7 batch 700 train loss: 0.0063 test loss: 0.0163\n",
      "Epoch 7 batch 800 train loss: 0.0059 test loss: 0.0144\n",
      "Epoch 7 batch 900 train loss: 0.0075 test loss: 0.0135\n",
      "Epoch 7 batch 1000 train loss: 0.0070 test loss: 0.0157\n",
      "Epoch 7 batch 1100 train loss: 0.0069 test loss: 0.0152\n",
      "Epoch 7 batch 1200 train loss: 0.0088 test loss: 0.0164\n",
      "Epoch 7 batch 1300 train loss: 0.0063 test loss: 0.0128\n",
      "Epoch 7 batch 1400 train loss: 0.0081 test loss: 0.0167\n",
      "Epoch 7 batch 1500 train loss: 0.0082 test loss: 0.0139\n",
      "Epoch 7 batch 1600 train loss: 0.0081 test loss: 0.0147\n",
      "Epoch 7 batch 1700 train loss: 0.0059 test loss: 0.0157\n",
      "Epoch 7 batch 1800 train loss: 0.0084 test loss: 0.0143\n",
      "Epoch 7 batch 1900 train loss: 0.0076 test loss: 0.0173\n",
      "Epoch 7 batch 2000 train loss: 0.0063 test loss: 0.0146\n",
      "Epoch 7 batch 2100 train loss: 0.0060 test loss: 0.0145\n",
      "Epoch 7 batch 2200 train loss: 0.0067 test loss: 0.0150\n",
      "Epoch 7 batch 2300 train loss: 0.0070 test loss: 0.0156\n",
      "Epoch 7 batch 2400 train loss: 0.0064 test loss: 0.0144\n",
      "Epoch 7 batch 2500 train loss: 0.0085 test loss: 0.0166\n",
      "Epoch 7 batch 2600 train loss: 0.0065 test loss: 0.0163\n",
      "Epoch 7 batch 2700 train loss: 0.0083 test loss: 0.0154\n",
      "Epoch 7 batch 2800 train loss: 0.0070 test loss: 0.0142\n",
      "Epoch 7 batch 2900 train loss: 0.0064 test loss: 0.0165\n",
      "Epoch 8 batch 0 train loss: 0.0076 test loss: 0.0130\n",
      "Epoch 8 batch 100 train loss: 0.0063 test loss: 0.0144\n",
      "Epoch 8 batch 200 train loss: 0.0076 test loss: 0.0145\n",
      "Epoch 8 batch 300 train loss: 0.0080 test loss: 0.0154\n",
      "Epoch 8 batch 400 train loss: 0.0065 test loss: 0.0136\n",
      "Epoch 8 batch 500 train loss: 0.0072 test loss: 0.0156\n",
      "Epoch 8 batch 600 train loss: 0.0102 test loss: 0.0161\n",
      "Epoch 8 batch 700 train loss: 0.0078 test loss: 0.0142\n",
      "Epoch 8 batch 800 train loss: 0.0067 test loss: 0.0156\n",
      "Epoch 8 batch 900 train loss: 0.0079 test loss: 0.0161\n",
      "Epoch 8 batch 1000 train loss: 0.0076 test loss: 0.0137\n",
      "Epoch 8 batch 1100 train loss: 0.0085 test loss: 0.0157\n",
      "Epoch 8 batch 1200 train loss: 0.0061 test loss: 0.0132\n",
      "Epoch 8 batch 1300 train loss: 0.0047 test loss: 0.0152\n",
      "Epoch 8 batch 1400 train loss: 0.0061 test loss: 0.0143\n",
      "Epoch 8 batch 1500 train loss: 0.0066 test loss: 0.0173\n",
      "Epoch 8 batch 1600 train loss: 0.0091 test loss: 0.0148\n",
      "Epoch 8 batch 1700 train loss: 0.0076 test loss: 0.0135\n",
      "Epoch 8 batch 1800 train loss: 0.0061 test loss: 0.0145\n",
      "Epoch 8 batch 1900 train loss: 0.0086 test loss: 0.0154\n",
      "Epoch 8 batch 2000 train loss: 0.0078 test loss: 0.0146\n",
      "Epoch 8 batch 2100 train loss: 0.0063 test loss: 0.0142\n",
      "Epoch 8 batch 2200 train loss: 0.0085 test loss: 0.0143\n",
      "Epoch 8 batch 2300 train loss: 0.0078 test loss: 0.0146\n",
      "Epoch 8 batch 2400 train loss: 0.0070 test loss: 0.0150\n",
      "Epoch 8 batch 2500 train loss: 0.0087 test loss: 0.0162\n",
      "Epoch 8 batch 2600 train loss: 0.0074 test loss: 0.0147\n",
      "Epoch 8 batch 2700 train loss: 0.0069 test loss: 0.0145\n",
      "Epoch 8 batch 2800 train loss: 0.0060 test loss: 0.0152\n",
      "Epoch 8 batch 2900 train loss: 0.0056 test loss: 0.0129\n",
      "Epoch 9 batch 0 train loss: 0.0073 test loss: 0.0148\n",
      "Epoch 9 batch 100 train loss: 0.0065 test loss: 0.0142\n",
      "Epoch 9 batch 200 train loss: 0.0073 test loss: 0.0143\n",
      "Epoch 9 batch 300 train loss: 0.0070 test loss: 0.0147\n",
      "Epoch 9 batch 400 train loss: 0.0051 test loss: 0.0157\n",
      "Epoch 9 batch 500 train loss: 0.0092 test loss: 0.0173\n",
      "Epoch 9 batch 600 train loss: 0.0084 test loss: 0.0152\n",
      "Epoch 9 batch 700 train loss: 0.0059 test loss: 0.0157\n",
      "Epoch 9 batch 800 train loss: 0.0082 test loss: 0.0147\n",
      "Epoch 9 batch 900 train loss: 0.0057 test loss: 0.0145\n",
      "Epoch 9 batch 1000 train loss: 0.0057 test loss: 0.0179\n",
      "Epoch 9 batch 1100 train loss: 0.0057 test loss: 0.0134\n",
      "Epoch 9 batch 1200 train loss: 0.0059 test loss: 0.0145\n",
      "Epoch 9 batch 1300 train loss: 0.0080 test loss: 0.0150\n",
      "Epoch 9 batch 1400 train loss: 0.0071 test loss: 0.0152\n",
      "Epoch 9 batch 1500 train loss: 0.0066 test loss: 0.0135\n",
      "Epoch 9 batch 1600 train loss: 0.0083 test loss: 0.0148\n",
      "Epoch 9 batch 1700 train loss: 0.0062 test loss: 0.0154\n",
      "Epoch 9 batch 1800 train loss: 0.0071 test loss: 0.0152\n",
      "Epoch 9 batch 1900 train loss: 0.0076 test loss: 0.0139\n",
      "Epoch 9 batch 2000 train loss: 0.0079 test loss: 0.0170\n",
      "Epoch 9 batch 2100 train loss: 0.0084 test loss: 0.0155\n",
      "Epoch 9 batch 2200 train loss: 0.0067 test loss: 0.0155\n",
      "Epoch 9 batch 2300 train loss: 0.0092 test loss: 0.0151\n",
      "Epoch 9 batch 2400 train loss: 0.0072 test loss: 0.0154\n",
      "Epoch 9 batch 2500 train loss: 0.0075 test loss: 0.0164\n",
      "Epoch 9 batch 2600 train loss: 0.0093 test loss: 0.0140\n",
      "Epoch 9 batch 2700 train loss: 0.0065 test loss: 0.0144\n",
      "Epoch 9 batch 2800 train loss: 0.0060 test loss: 0.0147\n",
      "Epoch 9 batch 2900 train loss: 0.0076 test loss: 0.0134\n",
      "Epoch 10 batch 0 train loss: 0.0069 test loss: 0.0144\n",
      "Epoch 10 batch 100 train loss: 0.0071 test loss: 0.0145\n",
      "Epoch 10 batch 200 train loss: 0.0086 test loss: 0.0143\n",
      "Epoch 10 batch 300 train loss: 0.0065 test loss: 0.0166\n",
      "Epoch 10 batch 400 train loss: 0.0083 test loss: 0.0171\n",
      "Epoch 10 batch 500 train loss: 0.0072 test loss: 0.0155\n",
      "Epoch 10 batch 600 train loss: 0.0073 test loss: 0.0147\n",
      "Epoch 10 batch 700 train loss: 0.0067 test loss: 0.0148\n",
      "Epoch 10 batch 800 train loss: 0.0066 test loss: 0.0147\n",
      "Epoch 10 batch 900 train loss: 0.0069 test loss: 0.0146\n",
      "Epoch 10 batch 1000 train loss: 0.0088 test loss: 0.0160\n",
      "Epoch 10 batch 1100 train loss: 0.0080 test loss: 0.0143\n",
      "Epoch 10 batch 1200 train loss: 0.0107 test loss: 0.0133\n",
      "Epoch 10 batch 1300 train loss: 0.0063 test loss: 0.0148\n",
      "Epoch 10 batch 1400 train loss: 0.0063 test loss: 0.0165\n",
      "Epoch 10 batch 1500 train loss: 0.0076 test loss: 0.0164\n",
      "Epoch 10 batch 1600 train loss: 0.0074 test loss: 0.0155\n",
      "Epoch 10 batch 1700 train loss: 0.0107 test loss: 0.0150\n",
      "Epoch 10 batch 1800 train loss: 0.0063 test loss: 0.0162\n",
      "Epoch 10 batch 1900 train loss: 0.0076 test loss: 0.0156\n",
      "Epoch 10 batch 2000 train loss: 0.0076 test loss: 0.0161\n",
      "Epoch 10 batch 2100 train loss: 0.0076 test loss: 0.0152\n",
      "Epoch 10 batch 2200 train loss: 0.0061 test loss: 0.0140\n",
      "Epoch 10 batch 2300 train loss: 0.0084 test loss: 0.0137\n",
      "Epoch 10 batch 2400 train loss: 0.0076 test loss: 0.0137\n",
      "Epoch 10 batch 2500 train loss: 0.0086 test loss: 0.0138\n",
      "Epoch 10 batch 2600 train loss: 0.0067 test loss: 0.0155\n",
      "Epoch 10 batch 2700 train loss: 0.0087 test loss: 0.0152\n",
      "Epoch 10 batch 2800 train loss: 0.0069 test loss: 0.0149\n",
      "Epoch 10 batch 2900 train loss: 0.0072 test loss: 0.0156\n",
      "Epoch 11 batch 0 train loss: 0.0081 test loss: 0.0150\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.70p/ckpt-20\n",
      "Epoch 11 batch 100 train loss: 0.0078 test loss: 0.0125\n",
      "Epoch 11 batch 200 train loss: 0.0063 test loss: 0.0142\n",
      "Epoch 11 batch 300 train loss: 0.0078 test loss: 0.0150\n",
      "Epoch 11 batch 400 train loss: 0.0078 test loss: 0.0156\n",
      "Epoch 11 batch 500 train loss: 0.0086 test loss: 0.0166\n",
      "Epoch 11 batch 600 train loss: 0.0079 test loss: 0.0156\n",
      "Epoch 11 batch 700 train loss: 0.0087 test loss: 0.0155\n",
      "Epoch 11 batch 800 train loss: 0.0075 test loss: 0.0154\n",
      "Epoch 11 batch 900 train loss: 0.0074 test loss: 0.0150\n",
      "Epoch 11 batch 1000 train loss: 0.0085 test loss: 0.0136\n",
      "Epoch 11 batch 1100 train loss: 0.0054 test loss: 0.0168\n",
      "Epoch 11 batch 1200 train loss: 0.0091 test loss: 0.0136\n",
      "Epoch 11 batch 1300 train loss: 0.0093 test loss: 0.0151\n",
      "Epoch 11 batch 1400 train loss: 0.0111 test loss: 0.0164\n",
      "Epoch 11 batch 1500 train loss: 0.0056 test loss: 0.0166\n",
      "Epoch 11 batch 1600 train loss: 0.0081 test loss: 0.0138\n",
      "Epoch 11 batch 1700 train loss: 0.0087 test loss: 0.0150\n",
      "Epoch 11 batch 1800 train loss: 0.0078 test loss: 0.0142\n",
      "Epoch 11 batch 1900 train loss: 0.0057 test loss: 0.0156\n",
      "Epoch 11 batch 2000 train loss: 0.0072 test loss: 0.0167\n",
      "Epoch 11 batch 2100 train loss: 0.0078 test loss: 0.0162\n",
      "Epoch 11 batch 2200 train loss: 0.0082 test loss: 0.0157\n",
      "Epoch 11 batch 2300 train loss: 0.0091 test loss: 0.0153\n",
      "Epoch 11 batch 2400 train loss: 0.0060 test loss: 0.0145\n",
      "Epoch 11 batch 2500 train loss: 0.0089 test loss: 0.0129\n",
      "Epoch 11 batch 2600 train loss: 0.0076 test loss: 0.0148\n",
      "Epoch 11 batch 2700 train loss: 0.0070 test loss: 0.0157\n",
      "Epoch 11 batch 2800 train loss: 0.0060 test loss: 0.0130\n",
      "Epoch 11 batch 2900 train loss: 0.0082 test loss: 0.0141\n",
      "Epoch 12 batch 0 train loss: 0.0072 test loss: 0.0143\n",
      "Epoch 12 batch 100 train loss: 0.0073 test loss: 0.0138\n",
      "Epoch 12 batch 200 train loss: 0.0093 test loss: 0.0142\n",
      "Epoch 12 batch 300 train loss: 0.0065 test loss: 0.0144\n",
      "Epoch 12 batch 400 train loss: 0.0069 test loss: 0.0160\n",
      "Epoch 12 batch 500 train loss: 0.0079 test loss: 0.0170\n",
      "Epoch 12 batch 600 train loss: 0.0060 test loss: 0.0138\n",
      "Epoch 12 batch 700 train loss: 0.0084 test loss: 0.0150\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.70p/ckpt-21\n",
      "Epoch 12 batch 800 train loss: 0.0072 test loss: 0.0124\n",
      "Epoch 12 batch 900 train loss: 0.0070 test loss: 0.0144\n",
      "Epoch 12 batch 1000 train loss: 0.0055 test loss: 0.0158\n",
      "Epoch 12 batch 1100 train loss: 0.0064 test loss: 0.0140\n",
      "Epoch 12 batch 1200 train loss: 0.0079 test loss: 0.0137\n",
      "Epoch 12 batch 1300 train loss: 0.0059 test loss: 0.0146\n",
      "Epoch 12 batch 1400 train loss: 0.0076 test loss: 0.0139\n",
      "Epoch 12 batch 1500 train loss: 0.0064 test loss: 0.0145\n",
      "Epoch 12 batch 1600 train loss: 0.0053 test loss: 0.0137\n",
      "Epoch 12 batch 1700 train loss: 0.0071 test loss: 0.0150\n",
      "Epoch 12 batch 1800 train loss: 0.0085 test loss: 0.0138\n",
      "Epoch 12 batch 1900 train loss: 0.0086 test loss: 0.0150\n",
      "Epoch 12 batch 2000 train loss: 0.0068 test loss: 0.0172\n",
      "Epoch 12 batch 2100 train loss: 0.0080 test loss: 0.0163\n",
      "Epoch 12 batch 2200 train loss: 0.0067 test loss: 0.0143\n",
      "Epoch 12 batch 2300 train loss: 0.0070 test loss: 0.0151\n",
      "Epoch 12 batch 2400 train loss: 0.0100 test loss: 0.0156\n",
      "Epoch 12 batch 2500 train loss: 0.0083 test loss: 0.0148\n",
      "Epoch 12 batch 2600 train loss: 0.0070 test loss: 0.0146\n",
      "Epoch 12 batch 2700 train loss: 0.0071 test loss: 0.0147\n",
      "Epoch 12 batch 2800 train loss: 0.0072 test loss: 0.0134\n",
      "Epoch 12 batch 2900 train loss: 0.0050 test loss: 0.0145\n",
      "Epoch 13 batch 0 train loss: 0.0058 test loss: 0.0129\n",
      "Epoch 13 batch 100 train loss: 0.0101 test loss: 0.0155\n",
      "Epoch 13 batch 200 train loss: 0.0079 test loss: 0.0127\n",
      "Epoch 13 batch 300 train loss: 0.0058 test loss: 0.0129\n",
      "Epoch 13 batch 400 train loss: 0.0076 test loss: 0.0160\n",
      "Epoch 13 batch 500 train loss: 0.0083 test loss: 0.0161\n",
      "Epoch 13 batch 600 train loss: 0.0076 test loss: 0.0156\n",
      "Epoch 13 batch 700 train loss: 0.0069 test loss: 0.0151\n",
      "Epoch 13 batch 800 train loss: 0.0067 test loss: 0.0153\n",
      "Epoch 13 batch 900 train loss: 0.0069 test loss: 0.0153\n",
      "Epoch 13 batch 1000 train loss: 0.0077 test loss: 0.0161\n",
      "Epoch 13 batch 1100 train loss: 0.0078 test loss: 0.0133\n",
      "Epoch 13 batch 1200 train loss: 0.0087 test loss: 0.0148\n",
      "Epoch 13 batch 1300 train loss: 0.0083 test loss: 0.0159\n",
      "Epoch 13 batch 1400 train loss: 0.0072 test loss: 0.0158\n",
      "Epoch 13 batch 1500 train loss: 0.0078 test loss: 0.0163\n",
      "Epoch 13 batch 1600 train loss: 0.0086 test loss: 0.0139\n",
      "Epoch 13 batch 1700 train loss: 0.0064 test loss: 0.0152\n",
      "Epoch 13 batch 1800 train loss: 0.0070 test loss: 0.0154\n",
      "Epoch 13 batch 1900 train loss: 0.0077 test loss: 0.0150\n",
      "Epoch 13 batch 2000 train loss: 0.0081 test loss: 0.0132\n",
      "Epoch 13 batch 2100 train loss: 0.0084 test loss: 0.0160\n",
      "Epoch 13 batch 2200 train loss: 0.0045 test loss: 0.0148\n",
      "Epoch 13 batch 2300 train loss: 0.0078 test loss: 0.0155\n",
      "Epoch 13 batch 2400 train loss: 0.0060 test loss: 0.0144\n",
      "Epoch 13 batch 2500 train loss: 0.0070 test loss: 0.0167\n",
      "Epoch 13 batch 2600 train loss: 0.0069 test loss: 0.0154\n",
      "Epoch 13 batch 2700 train loss: 0.0056 test loss: 0.0154\n",
      "Epoch 13 batch 2800 train loss: 0.0047 test loss: 0.0146\n",
      "Epoch 13 batch 2900 train loss: 0.0067 test loss: 0.0149\n",
      "Epoch 14 batch 0 train loss: 0.0084 test loss: 0.0135\n",
      "Epoch 14 batch 100 train loss: 0.0081 test loss: 0.0153\n",
      "Epoch 14 batch 200 train loss: 0.0083 test loss: 0.0135\n",
      "Epoch 14 batch 300 train loss: 0.0059 test loss: 0.0141\n",
      "Epoch 14 batch 400 train loss: 0.0053 test loss: 0.0148\n",
      "Epoch 14 batch 500 train loss: 0.0081 test loss: 0.0158\n",
      "Epoch 14 batch 600 train loss: 0.0092 test loss: 0.0168\n",
      "Epoch 14 batch 700 train loss: 0.0063 test loss: 0.0145\n",
      "Epoch 14 batch 800 train loss: 0.0058 test loss: 0.0138\n",
      "Epoch 14 batch 900 train loss: 0.0046 test loss: 0.0163\n",
      "Epoch 14 batch 1000 train loss: 0.0072 test loss: 0.0147\n",
      "Epoch 14 batch 1100 train loss: 0.0078 test loss: 0.0174\n",
      "Epoch 14 batch 1200 train loss: 0.0085 test loss: 0.0152\n",
      "Epoch 14 batch 1300 train loss: 0.0063 test loss: 0.0151\n",
      "Epoch 14 batch 1400 train loss: 0.0071 test loss: 0.0142\n",
      "Epoch 14 batch 1500 train loss: 0.0062 test loss: 0.0145\n",
      "Epoch 14 batch 1600 train loss: 0.0068 test loss: 0.0148\n",
      "Epoch 14 batch 1700 train loss: 0.0074 test loss: 0.0142\n",
      "Epoch 14 batch 1800 train loss: 0.0072 test loss: 0.0166\n",
      "Epoch 14 batch 1900 train loss: 0.0073 test loss: 0.0161\n",
      "Epoch 14 batch 2000 train loss: 0.0070 test loss: 0.0163\n",
      "Epoch 14 batch 2100 train loss: 0.0065 test loss: 0.0138\n",
      "Epoch 14 batch 2200 train loss: 0.0062 test loss: 0.0163\n",
      "Epoch 14 batch 2300 train loss: 0.0073 test loss: 0.0149\n",
      "Epoch 14 batch 2400 train loss: 0.0069 test loss: 0.0159\n",
      "Epoch 14 batch 2500 train loss: 0.0106 test loss: 0.0148\n",
      "Epoch 14 batch 2600 train loss: 0.0088 test loss: 0.0142\n",
      "Epoch 14 batch 2700 train loss: 0.0047 test loss: 0.0159\n",
      "Epoch 14 batch 2800 train loss: 0.0072 test loss: 0.0159\n",
      "Epoch 14 batch 2900 train loss: 0.0091 test loss: 0.0138\n",
      "Epoch 15 batch 0 train loss: 0.0081 test loss: 0.0151\n",
      "Epoch 15 batch 100 train loss: 0.0066 test loss: 0.0134\n",
      "Epoch 15 batch 200 train loss: 0.0070 test loss: 0.0140\n",
      "Epoch 15 batch 300 train loss: 0.0076 test loss: 0.0136\n",
      "Epoch 15 batch 400 train loss: 0.0053 test loss: 0.0156\n",
      "Epoch 15 batch 500 train loss: 0.0073 test loss: 0.0135\n",
      "Epoch 15 batch 600 train loss: 0.0076 test loss: 0.0161\n",
      "Epoch 15 batch 700 train loss: 0.0067 test loss: 0.0158\n",
      "Epoch 15 batch 800 train loss: 0.0079 test loss: 0.0142\n",
      "Epoch 15 batch 900 train loss: 0.0065 test loss: 0.0148\n",
      "Epoch 15 batch 1000 train loss: 0.0076 test loss: 0.0158\n",
      "Epoch 15 batch 1100 train loss: 0.0071 test loss: 0.0160\n",
      "Epoch 15 batch 1200 train loss: 0.0067 test loss: 0.0142\n",
      "Epoch 15 batch 1300 train loss: 0.0066 test loss: 0.0170\n",
      "Epoch 15 batch 1400 train loss: 0.0088 test loss: 0.0135\n",
      "Epoch 15 batch 1500 train loss: 0.0070 test loss: 0.0179\n",
      "Epoch 15 batch 1600 train loss: 0.0081 test loss: 0.0133\n",
      "Epoch 15 batch 1700 train loss: 0.0063 test loss: 0.0132\n",
      "Epoch 15 batch 1800 train loss: 0.0072 test loss: 0.0163\n",
      "Epoch 15 batch 1900 train loss: 0.0080 test loss: 0.0145\n",
      "Epoch 15 batch 2000 train loss: 0.0061 test loss: 0.0167\n",
      "Epoch 15 batch 2100 train loss: 0.0070 test loss: 0.0139\n",
      "Epoch 15 batch 2200 train loss: 0.0074 test loss: 0.0159\n",
      "Epoch 15 batch 2300 train loss: 0.0060 test loss: 0.0160\n",
      "Epoch 15 batch 2400 train loss: 0.0054 test loss: 0.0159\n",
      "Epoch 15 batch 2500 train loss: 0.0067 test loss: 0.0171\n",
      "Epoch 15 batch 2600 train loss: 0.0077 test loss: 0.0134\n",
      "Epoch 15 batch 2700 train loss: 0.0054 test loss: 0.0160\n",
      "Epoch 15 batch 2800 train loss: 0.0072 test loss: 0.0141\n",
      "Epoch 15 batch 2900 train loss: 0.0075 test loss: 0.0151\n",
      "Epoch 16 batch 0 train loss: 0.0066 test loss: 0.0147\n",
      "Epoch 16 batch 100 train loss: 0.0078 test loss: 0.0148\n",
      "Epoch 16 batch 200 train loss: 0.0070 test loss: 0.0144\n",
      "Epoch 16 batch 300 train loss: 0.0058 test loss: 0.0144\n",
      "Epoch 16 batch 400 train loss: 0.0075 test loss: 0.0170\n",
      "Epoch 16 batch 500 train loss: 0.0074 test loss: 0.0155\n",
      "Epoch 16 batch 600 train loss: 0.0069 test loss: 0.0152\n",
      "Epoch 16 batch 700 train loss: 0.0079 test loss: 0.0165\n",
      "Epoch 16 batch 800 train loss: 0.0067 test loss: 0.0144\n",
      "Epoch 16 batch 900 train loss: 0.0080 test loss: 0.0153\n",
      "Epoch 16 batch 1000 train loss: 0.0072 test loss: 0.0150\n",
      "Epoch 16 batch 1100 train loss: 0.0061 test loss: 0.0169\n",
      "Epoch 16 batch 1200 train loss: 0.0078 test loss: 0.0144\n",
      "Epoch 16 batch 1300 train loss: 0.0098 test loss: 0.0159\n",
      "Epoch 16 batch 1400 train loss: 0.0069 test loss: 0.0166\n",
      "Epoch 16 batch 1500 train loss: 0.0066 test loss: 0.0157\n",
      "Epoch 16 batch 1600 train loss: 0.0061 test loss: 0.0143\n",
      "Epoch 16 batch 1700 train loss: 0.0067 test loss: 0.0151\n",
      "Epoch 16 batch 1800 train loss: 0.0077 test loss: 0.0133\n",
      "Epoch 16 batch 1900 train loss: 0.0073 test loss: 0.0145\n",
      "Epoch 16 batch 2000 train loss: 0.0089 test loss: 0.0160\n",
      "Epoch 16 batch 2100 train loss: 0.0087 test loss: 0.0159\n",
      "Epoch 16 batch 2200 train loss: 0.0068 test loss: 0.0159\n",
      "Epoch 16 batch 2300 train loss: 0.0067 test loss: 0.0163\n",
      "Epoch 16 batch 2400 train loss: 0.0074 test loss: 0.0150\n",
      "Epoch 16 batch 2500 train loss: 0.0075 test loss: 0.0153\n",
      "Epoch 16 batch 2600 train loss: 0.0071 test loss: 0.0143\n",
      "Epoch 16 batch 2700 train loss: 0.0085 test loss: 0.0164\n",
      "Epoch 16 batch 2800 train loss: 0.0076 test loss: 0.0139\n",
      "Epoch 16 batch 2900 train loss: 0.0070 test loss: 0.0145\n",
      "Epoch 17 batch 0 train loss: 0.0055 test loss: 0.0145\n",
      "Epoch 17 batch 100 train loss: 0.0074 test loss: 0.0146\n",
      "Epoch 17 batch 200 train loss: 0.0065 test loss: 0.0136\n",
      "Epoch 17 batch 300 train loss: 0.0059 test loss: 0.0148\n",
      "Epoch 17 batch 400 train loss: 0.0095 test loss: 0.0143\n",
      "Epoch 17 batch 500 train loss: 0.0063 test loss: 0.0150\n",
      "Epoch 17 batch 600 train loss: 0.0056 test loss: 0.0138\n",
      "Epoch 17 batch 700 train loss: 0.0081 test loss: 0.0167\n",
      "Epoch 17 batch 800 train loss: 0.0059 test loss: 0.0153\n",
      "Epoch 17 batch 900 train loss: 0.0057 test loss: 0.0143\n",
      "Epoch 17 batch 1000 train loss: 0.0064 test loss: 0.0171\n",
      "early stop.\n",
      "Checkpoint 21 restored!!\n",
      "Training for loss rate 0.80 start.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['bi_lstm_7/bi_lstm_block_14/bidirectional_14/forward_lstm_14/lstm_cell_43/kernel:0', 'bi_lstm_7/bi_lstm_block_14/bidirectional_14/forward_lstm_14/lstm_cell_43/recurrent_kernel:0', 'bi_lstm_7/bi_lstm_block_14/bidirectional_14/forward_lstm_14/lstm_cell_43/bias:0', 'bi_lstm_7/bi_lstm_block_14/bidirectional_14/backward_lstm_14/lstm_cell_44/kernel:0', 'bi_lstm_7/bi_lstm_block_14/bidirectional_14/backward_lstm_14/lstm_cell_44/recurrent_kernel:0', 'bi_lstm_7/bi_lstm_block_14/bidirectional_14/backward_lstm_14/lstm_cell_44/bias:0', 'bi_lstm_7/bi_lstm_block_14/layer_normalization_14/gamma:0', 'bi_lstm_7/bi_lstm_block_14/layer_normalization_14/beta:0', 'bi_lstm_7/bi_lstm_block_15/bidirectional_15/forward_lstm_15/lstm_cell_46/kernel:0', 'bi_lstm_7/bi_lstm_block_15/bidirectional_15/forward_lstm_15/lstm_cell_46/recurrent_kernel:0', 'bi_lstm_7/bi_lstm_block_15/bidirectional_15/forward_lstm_15/lstm_cell_46/bias:0', 'bi_lstm_7/bi_lstm_block_15/bidirectional_15/backward_lstm_15/lstm_cell_47/kernel:0', 'bi_lstm_7/bi_lstm_block_15/bidirectional_15/backward_lstm_15/lstm_cell_47/recurrent_kernel:0', 'bi_lstm_7/bi_lstm_block_15/bidirectional_15/backward_lstm_15/lstm_cell_47/bias:0', 'bi_lstm_7/bi_lstm_block_15/layer_normalization_15/gamma:0', 'bi_lstm_7/bi_lstm_block_15/layer_normalization_15/beta:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['bi_lstm_7/bi_lstm_block_14/bidirectional_14/forward_lstm_14/lstm_cell_43/kernel:0', 'bi_lstm_7/bi_lstm_block_14/bidirectional_14/forward_lstm_14/lstm_cell_43/recurrent_kernel:0', 'bi_lstm_7/bi_lstm_block_14/bidirectional_14/forward_lstm_14/lstm_cell_43/bias:0', 'bi_lstm_7/bi_lstm_block_14/bidirectional_14/backward_lstm_14/lstm_cell_44/kernel:0', 'bi_lstm_7/bi_lstm_block_14/bidirectional_14/backward_lstm_14/lstm_cell_44/recurrent_kernel:0', 'bi_lstm_7/bi_lstm_block_14/bidirectional_14/backward_lstm_14/lstm_cell_44/bias:0', 'bi_lstm_7/bi_lstm_block_14/layer_normalization_14/gamma:0', 'bi_lstm_7/bi_lstm_block_14/layer_normalization_14/beta:0', 'bi_lstm_7/bi_lstm_block_15/bidirectional_15/forward_lstm_15/lstm_cell_46/kernel:0', 'bi_lstm_7/bi_lstm_block_15/bidirectional_15/forward_lstm_15/lstm_cell_46/recurrent_kernel:0', 'bi_lstm_7/bi_lstm_block_15/bidirectional_15/forward_lstm_15/lstm_cell_46/bias:0', 'bi_lstm_7/bi_lstm_block_15/bidirectional_15/backward_lstm_15/lstm_cell_47/kernel:0', 'bi_lstm_7/bi_lstm_block_15/bidirectional_15/backward_lstm_15/lstm_cell_47/recurrent_kernel:0', 'bi_lstm_7/bi_lstm_block_15/bidirectional_15/backward_lstm_15/lstm_cell_47/bias:0', 'bi_lstm_7/bi_lstm_block_15/layer_normalization_15/gamma:0', 'bi_lstm_7/bi_lstm_block_15/layer_normalization_15/beta:0'] when minimizing the loss.\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.80p/ckpt-1\n",
      "Epoch 0 batch 0 train loss: 0.4710 test loss: 0.7075\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.80p/ckpt-2\n",
      "Epoch 0 batch 100 train loss: 0.1887 test loss: 0.1225\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.80p/ckpt-3\n",
      "Epoch 0 batch 200 train loss: 0.0971 test loss: 0.0597\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.80p/ckpt-4\n",
      "Epoch 0 batch 300 train loss: 0.0747 test loss: 0.0357\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.80p/ckpt-5\n",
      "Epoch 0 batch 400 train loss: 0.0450 test loss: 0.0259\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.80p/ckpt-6\n",
      "Epoch 0 batch 500 train loss: 0.0338 test loss: 0.0214\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.80p/ckpt-7\n",
      "Epoch 0 batch 600 train loss: 0.0233 test loss: 0.0194\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.80p/ckpt-8\n",
      "Epoch 0 batch 700 train loss: 0.0185 test loss: 0.0191\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.80p/ckpt-9\n",
      "Epoch 0 batch 800 train loss: 0.0120 test loss: 0.0180\n",
      "Epoch 0 batch 900 train loss: 0.0081 test loss: 0.0188\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.80p/ckpt-10\n",
      "Epoch 0 batch 1000 train loss: 0.0119 test loss: 0.0174\n",
      "Epoch 0 batch 1100 train loss: 0.0078 test loss: 0.0183\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.80p/ckpt-11\n",
      "Epoch 0 batch 1200 train loss: 0.0085 test loss: 0.0170\n",
      "Epoch 0 batch 1300 train loss: 0.0107 test loss: 0.0180\n",
      "Epoch 0 batch 1400 train loss: 0.0086 test loss: 0.0180\n",
      "Epoch 0 batch 1500 train loss: 0.0106 test loss: 0.0180\n",
      "Epoch 0 batch 1600 train loss: 0.0117 test loss: 0.0176\n",
      "Epoch 0 batch 1700 train loss: 0.0073 test loss: 0.0175\n",
      "Epoch 0 batch 1800 train loss: 0.0084 test loss: 0.0183\n",
      "Epoch 0 batch 1900 train loss: 0.0080 test loss: 0.0186\n",
      "Epoch 0 batch 2000 train loss: 0.0081 test loss: 0.0204\n",
      "Epoch 0 batch 2100 train loss: 0.0069 test loss: 0.0176\n",
      "Epoch 0 batch 2200 train loss: 0.0083 test loss: 0.0186\n",
      "Epoch 0 batch 2300 train loss: 0.0085 test loss: 0.0196\n",
      "Epoch 0 batch 2400 train loss: 0.0091 test loss: 0.0183\n",
      "Epoch 0 batch 2500 train loss: 0.0082 test loss: 0.0192\n",
      "Epoch 0 batch 2600 train loss: 0.0081 test loss: 0.0171\n",
      "Epoch 0 batch 2700 train loss: 0.0098 test loss: 0.0184\n",
      "Epoch 0 batch 2800 train loss: 0.0098 test loss: 0.0181\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.80p/ckpt-12\n",
      "Epoch 0 batch 2900 train loss: 0.0102 test loss: 0.0161\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['bi_lstm_7/bi_lstm_block_14/bidirectional_14/forward_lstm_14/lstm_cell_43/kernel:0', 'bi_lstm_7/bi_lstm_block_14/bidirectional_14/forward_lstm_14/lstm_cell_43/recurrent_kernel:0', 'bi_lstm_7/bi_lstm_block_14/bidirectional_14/forward_lstm_14/lstm_cell_43/bias:0', 'bi_lstm_7/bi_lstm_block_14/bidirectional_14/backward_lstm_14/lstm_cell_44/kernel:0', 'bi_lstm_7/bi_lstm_block_14/bidirectional_14/backward_lstm_14/lstm_cell_44/recurrent_kernel:0', 'bi_lstm_7/bi_lstm_block_14/bidirectional_14/backward_lstm_14/lstm_cell_44/bias:0', 'bi_lstm_7/bi_lstm_block_14/layer_normalization_14/gamma:0', 'bi_lstm_7/bi_lstm_block_14/layer_normalization_14/beta:0', 'bi_lstm_7/bi_lstm_block_15/bidirectional_15/forward_lstm_15/lstm_cell_46/kernel:0', 'bi_lstm_7/bi_lstm_block_15/bidirectional_15/forward_lstm_15/lstm_cell_46/recurrent_kernel:0', 'bi_lstm_7/bi_lstm_block_15/bidirectional_15/forward_lstm_15/lstm_cell_46/bias:0', 'bi_lstm_7/bi_lstm_block_15/bidirectional_15/backward_lstm_15/lstm_cell_47/kernel:0', 'bi_lstm_7/bi_lstm_block_15/bidirectional_15/backward_lstm_15/lstm_cell_47/recurrent_kernel:0', 'bi_lstm_7/bi_lstm_block_15/bidirectional_15/backward_lstm_15/lstm_cell_47/bias:0', 'bi_lstm_7/bi_lstm_block_15/layer_normalization_15/gamma:0', 'bi_lstm_7/bi_lstm_block_15/layer_normalization_15/beta:0'] when minimizing the loss.\n",
      "Epoch 1 batch 0 train loss: 0.0099 test loss: 0.0174\n",
      "Epoch 1 batch 100 train loss: 0.0079 test loss: 0.0182\n",
      "Epoch 1 batch 200 train loss: 0.0095 test loss: 0.0184\n",
      "Epoch 1 batch 300 train loss: 0.0069 test loss: 0.0163\n",
      "Epoch 1 batch 400 train loss: 0.0079 test loss: 0.0187\n",
      "Epoch 1 batch 500 train loss: 0.0089 test loss: 0.0178\n",
      "Epoch 1 batch 600 train loss: 0.0126 test loss: 0.0172\n",
      "Epoch 1 batch 700 train loss: 0.0095 test loss: 0.0189\n",
      "Epoch 1 batch 800 train loss: 0.0098 test loss: 0.0193\n",
      "Epoch 1 batch 900 train loss: 0.0091 test loss: 0.0164\n",
      "Epoch 1 batch 1000 train loss: 0.0077 test loss: 0.0169\n",
      "Epoch 1 batch 1100 train loss: 0.0077 test loss: 0.0175\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.80p/ckpt-13\n",
      "Epoch 1 batch 1200 train loss: 0.0070 test loss: 0.0151\n",
      "Epoch 1 batch 1300 train loss: 0.0072 test loss: 0.0187\n",
      "Epoch 1 batch 1400 train loss: 0.0068 test loss: 0.0179\n",
      "Epoch 1 batch 1500 train loss: 0.0074 test loss: 0.0178\n",
      "Epoch 1 batch 1600 train loss: 0.0070 test loss: 0.0177\n",
      "Epoch 1 batch 1700 train loss: 0.0082 test loss: 0.0177\n",
      "Epoch 1 batch 1800 train loss: 0.0100 test loss: 0.0162\n",
      "Epoch 1 batch 1900 train loss: 0.0056 test loss: 0.0190\n",
      "Epoch 1 batch 2000 train loss: 0.0092 test loss: 0.0190\n",
      "Epoch 1 batch 2100 train loss: 0.0066 test loss: 0.0188\n",
      "Epoch 1 batch 2200 train loss: 0.0090 test loss: 0.0190\n",
      "Epoch 1 batch 2300 train loss: 0.0081 test loss: 0.0204\n",
      "Epoch 1 batch 2400 train loss: 0.0081 test loss: 0.0191\n",
      "Epoch 1 batch 2500 train loss: 0.0068 test loss: 0.0171\n",
      "Epoch 1 batch 2600 train loss: 0.0101 test loss: 0.0170\n",
      "Epoch 1 batch 2700 train loss: 0.0075 test loss: 0.0196\n",
      "Epoch 1 batch 2800 train loss: 0.0068 test loss: 0.0191\n",
      "Epoch 1 batch 2900 train loss: 0.0091 test loss: 0.0160\n",
      "Epoch 2 batch 0 train loss: 0.0060 test loss: 0.0179\n",
      "Epoch 2 batch 100 train loss: 0.0059 test loss: 0.0206\n",
      "Epoch 2 batch 200 train loss: 0.0080 test loss: 0.0153\n",
      "Epoch 2 batch 300 train loss: 0.0096 test loss: 0.0172\n",
      "Epoch 2 batch 400 train loss: 0.0083 test loss: 0.0193\n",
      "Epoch 2 batch 500 train loss: 0.0064 test loss: 0.0184\n",
      "Epoch 2 batch 600 train loss: 0.0077 test loss: 0.0176\n",
      "Epoch 2 batch 700 train loss: 0.0069 test loss: 0.0207\n",
      "Epoch 2 batch 800 train loss: 0.0087 test loss: 0.0177\n",
      "Epoch 2 batch 900 train loss: 0.0077 test loss: 0.0161\n",
      "Epoch 2 batch 1000 train loss: 0.0085 test loss: 0.0175\n",
      "Epoch 2 batch 1100 train loss: 0.0084 test loss: 0.0167\n",
      "Epoch 2 batch 1200 train loss: 0.0092 test loss: 0.0170\n",
      "Epoch 2 batch 1300 train loss: 0.0086 test loss: 0.0175\n",
      "Epoch 2 batch 1400 train loss: 0.0079 test loss: 0.0180\n",
      "Epoch 2 batch 1500 train loss: 0.0102 test loss: 0.0192\n",
      "Epoch 2 batch 1600 train loss: 0.0075 test loss: 0.0185\n",
      "Epoch 2 batch 1700 train loss: 0.0090 test loss: 0.0170\n",
      "Epoch 2 batch 1800 train loss: 0.0092 test loss: 0.0154\n",
      "Epoch 2 batch 1900 train loss: 0.0095 test loss: 0.0181\n",
      "Epoch 2 batch 2000 train loss: 0.0089 test loss: 0.0171\n",
      "Epoch 2 batch 2100 train loss: 0.0062 test loss: 0.0194\n",
      "Epoch 2 batch 2200 train loss: 0.0080 test loss: 0.0158\n",
      "Epoch 2 batch 2300 train loss: 0.0064 test loss: 0.0168\n",
      "Epoch 2 batch 2400 train loss: 0.0093 test loss: 0.0183\n",
      "Epoch 2 batch 2500 train loss: 0.0076 test loss: 0.0181\n",
      "Epoch 2 batch 2600 train loss: 0.0078 test loss: 0.0156\n",
      "Epoch 2 batch 2700 train loss: 0.0071 test loss: 0.0158\n",
      "Epoch 2 batch 2800 train loss: 0.0063 test loss: 0.0156\n",
      "Epoch 2 batch 2900 train loss: 0.0070 test loss: 0.0195\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.80p/ckpt-14\n",
      "Epoch 3 batch 0 train loss: 0.0063 test loss: 0.0136\n",
      "Epoch 3 batch 100 train loss: 0.0081 test loss: 0.0175\n",
      "Epoch 3 batch 200 train loss: 0.0070 test loss: 0.0181\n",
      "Epoch 3 batch 300 train loss: 0.0066 test loss: 0.0164\n",
      "Epoch 3 batch 400 train loss: 0.0079 test loss: 0.0191\n",
      "Epoch 3 batch 500 train loss: 0.0068 test loss: 0.0196\n",
      "Epoch 3 batch 600 train loss: 0.0080 test loss: 0.0166\n",
      "Epoch 3 batch 700 train loss: 0.0064 test loss: 0.0181\n",
      "Epoch 3 batch 800 train loss: 0.0089 test loss: 0.0172\n",
      "Epoch 3 batch 900 train loss: 0.0085 test loss: 0.0156\n",
      "Epoch 3 batch 1000 train loss: 0.0077 test loss: 0.0186\n",
      "Epoch 3 batch 1100 train loss: 0.0074 test loss: 0.0184\n",
      "Epoch 3 batch 1200 train loss: 0.0080 test loss: 0.0174\n",
      "Epoch 3 batch 1300 train loss: 0.0047 test loss: 0.0172\n",
      "Epoch 3 batch 1400 train loss: 0.0061 test loss: 0.0163\n",
      "Epoch 3 batch 1500 train loss: 0.0077 test loss: 0.0216\n",
      "Epoch 3 batch 1600 train loss: 0.0072 test loss: 0.0189\n",
      "Epoch 3 batch 1700 train loss: 0.0078 test loss: 0.0209\n",
      "Epoch 3 batch 1800 train loss: 0.0078 test loss: 0.0193\n",
      "Epoch 3 batch 1900 train loss: 0.0082 test loss: 0.0162\n",
      "Epoch 3 batch 2000 train loss: 0.0051 test loss: 0.0166\n",
      "Epoch 3 batch 2100 train loss: 0.0077 test loss: 0.0145\n",
      "Epoch 3 batch 2200 train loss: 0.0086 test loss: 0.0200\n",
      "Epoch 3 batch 2300 train loss: 0.0066 test loss: 0.0170\n",
      "Epoch 3 batch 2400 train loss: 0.0065 test loss: 0.0182\n",
      "Epoch 3 batch 2500 train loss: 0.0103 test loss: 0.0164\n",
      "Epoch 3 batch 2600 train loss: 0.0075 test loss: 0.0160\n",
      "Epoch 3 batch 2700 train loss: 0.0085 test loss: 0.0184\n",
      "Epoch 3 batch 2800 train loss: 0.0068 test loss: 0.0157\n",
      "Epoch 3 batch 2900 train loss: 0.0083 test loss: 0.0180\n",
      "Epoch 4 batch 0 train loss: 0.0060 test loss: 0.0175\n",
      "Epoch 4 batch 100 train loss: 0.0081 test loss: 0.0168\n",
      "Epoch 4 batch 200 train loss: 0.0076 test loss: 0.0165\n",
      "Epoch 4 batch 300 train loss: 0.0070 test loss: 0.0159\n",
      "Epoch 4 batch 400 train loss: 0.0081 test loss: 0.0163\n",
      "Epoch 4 batch 500 train loss: 0.0077 test loss: 0.0183\n",
      "Epoch 4 batch 600 train loss: 0.0070 test loss: 0.0157\n",
      "Epoch 4 batch 700 train loss: 0.0082 test loss: 0.0164\n",
      "Epoch 4 batch 800 train loss: 0.0072 test loss: 0.0163\n",
      "Epoch 4 batch 900 train loss: 0.0085 test loss: 0.0174\n",
      "Epoch 4 batch 1000 train loss: 0.0088 test loss: 0.0194\n",
      "Epoch 4 batch 1100 train loss: 0.0059 test loss: 0.0170\n",
      "Epoch 4 batch 1200 train loss: 0.0056 test loss: 0.0189\n",
      "Epoch 4 batch 1300 train loss: 0.0071 test loss: 0.0184\n",
      "Epoch 4 batch 1400 train loss: 0.0092 test loss: 0.0189\n",
      "Epoch 4 batch 1500 train loss: 0.0066 test loss: 0.0182\n",
      "Epoch 4 batch 1600 train loss: 0.0089 test loss: 0.0193\n",
      "Epoch 4 batch 1700 train loss: 0.0061 test loss: 0.0190\n",
      "Epoch 4 batch 1800 train loss: 0.0081 test loss: 0.0203\n",
      "Epoch 4 batch 1900 train loss: 0.0065 test loss: 0.0168\n",
      "Epoch 4 batch 2000 train loss: 0.0080 test loss: 0.0172\n",
      "Epoch 4 batch 2100 train loss: 0.0083 test loss: 0.0146\n",
      "Epoch 4 batch 2200 train loss: 0.0079 test loss: 0.0189\n",
      "Epoch 4 batch 2300 train loss: 0.0086 test loss: 0.0169\n",
      "Epoch 4 batch 2400 train loss: 0.0086 test loss: 0.0167\n",
      "Epoch 4 batch 2500 train loss: 0.0076 test loss: 0.0187\n",
      "Epoch 4 batch 2600 train loss: 0.0088 test loss: 0.0164\n",
      "Epoch 4 batch 2700 train loss: 0.0077 test loss: 0.0174\n",
      "Epoch 4 batch 2800 train loss: 0.0081 test loss: 0.0167\n",
      "Epoch 4 batch 2900 train loss: 0.0074 test loss: 0.0188\n",
      "Epoch 5 batch 0 train loss: 0.0058 test loss: 0.0160\n",
      "Epoch 5 batch 100 train loss: 0.0069 test loss: 0.0174\n",
      "Epoch 5 batch 200 train loss: 0.0061 test loss: 0.0161\n",
      "Epoch 5 batch 300 train loss: 0.0108 test loss: 0.0164\n",
      "Epoch 5 batch 400 train loss: 0.0071 test loss: 0.0163\n",
      "Epoch 5 batch 500 train loss: 0.0090 test loss: 0.0169\n",
      "Epoch 5 batch 600 train loss: 0.0072 test loss: 0.0171\n",
      "Epoch 5 batch 700 train loss: 0.0066 test loss: 0.0177\n",
      "Epoch 5 batch 800 train loss: 0.0076 test loss: 0.0152\n",
      "Epoch 5 batch 900 train loss: 0.0076 test loss: 0.0182\n",
      "Epoch 5 batch 1000 train loss: 0.0064 test loss: 0.0183\n",
      "Epoch 5 batch 1100 train loss: 0.0087 test loss: 0.0191\n",
      "Epoch 5 batch 1200 train loss: 0.0065 test loss: 0.0191\n",
      "Epoch 5 batch 1300 train loss: 0.0067 test loss: 0.0182\n",
      "Epoch 5 batch 1400 train loss: 0.0081 test loss: 0.0179\n",
      "Epoch 5 batch 1500 train loss: 0.0077 test loss: 0.0177\n",
      "Epoch 5 batch 1600 train loss: 0.0067 test loss: 0.0173\n",
      "Epoch 5 batch 1700 train loss: 0.0085 test loss: 0.0165\n",
      "Epoch 5 batch 1800 train loss: 0.0095 test loss: 0.0165\n",
      "Epoch 5 batch 1900 train loss: 0.0096 test loss: 0.0188\n",
      "Epoch 5 batch 2000 train loss: 0.0075 test loss: 0.0196\n",
      "Epoch 5 batch 2100 train loss: 0.0077 test loss: 0.0176\n",
      "Epoch 5 batch 2200 train loss: 0.0079 test loss: 0.0160\n",
      "Epoch 5 batch 2300 train loss: 0.0109 test loss: 0.0160\n",
      "Epoch 5 batch 2400 train loss: 0.0076 test loss: 0.0165\n",
      "Epoch 5 batch 2500 train loss: 0.0061 test loss: 0.0175\n",
      "Epoch 5 batch 2600 train loss: 0.0086 test loss: 0.0165\n",
      "Epoch 5 batch 2700 train loss: 0.0101 test loss: 0.0167\n",
      "Epoch 5 batch 2800 train loss: 0.0096 test loss: 0.0153\n",
      "Epoch 5 batch 2900 train loss: 0.0067 test loss: 0.0156\n",
      "Epoch 6 batch 0 train loss: 0.0106 test loss: 0.0170\n",
      "Epoch 6 batch 100 train loss: 0.0071 test loss: 0.0201\n",
      "Epoch 6 batch 200 train loss: 0.0085 test loss: 0.0161\n",
      "Epoch 6 batch 300 train loss: 0.0051 test loss: 0.0156\n",
      "Epoch 6 batch 400 train loss: 0.0072 test loss: 0.0179\n",
      "Epoch 6 batch 500 train loss: 0.0067 test loss: 0.0187\n",
      "Epoch 6 batch 600 train loss: 0.0074 test loss: 0.0164\n",
      "Epoch 6 batch 700 train loss: 0.0099 test loss: 0.0176\n",
      "Epoch 6 batch 800 train loss: 0.0078 test loss: 0.0155\n",
      "Epoch 6 batch 900 train loss: 0.0081 test loss: 0.0167\n",
      "Epoch 6 batch 1000 train loss: 0.0067 test loss: 0.0163\n",
      "Epoch 6 batch 1100 train loss: 0.0075 test loss: 0.0182\n",
      "Epoch 6 batch 1200 train loss: 0.0101 test loss: 0.0156\n",
      "Epoch 6 batch 1300 train loss: 0.0103 test loss: 0.0192\n",
      "Epoch 6 batch 1400 train loss: 0.0075 test loss: 0.0166\n",
      "Epoch 6 batch 1500 train loss: 0.0068 test loss: 0.0178\n",
      "Epoch 6 batch 1600 train loss: 0.0081 test loss: 0.0163\n",
      "Epoch 6 batch 1700 train loss: 0.0066 test loss: 0.0206\n",
      "Epoch 6 batch 1800 train loss: 0.0056 test loss: 0.0158\n",
      "Epoch 6 batch 1900 train loss: 0.0083 test loss: 0.0164\n",
      "Epoch 6 batch 2000 train loss: 0.0081 test loss: 0.0160\n",
      "Epoch 6 batch 2100 train loss: 0.0079 test loss: 0.0170\n",
      "Epoch 6 batch 2200 train loss: 0.0087 test loss: 0.0167\n",
      "Epoch 6 batch 2300 train loss: 0.0072 test loss: 0.0174\n",
      "Epoch 6 batch 2400 train loss: 0.0083 test loss: 0.0186\n",
      "Epoch 6 batch 2500 train loss: 0.0071 test loss: 0.0206\n",
      "Epoch 6 batch 2600 train loss: 0.0063 test loss: 0.0177\n",
      "Epoch 6 batch 2700 train loss: 0.0072 test loss: 0.0174\n",
      "Epoch 6 batch 2800 train loss: 0.0078 test loss: 0.0184\n",
      "Epoch 6 batch 2900 train loss: 0.0099 test loss: 0.0171\n",
      "Epoch 7 batch 0 train loss: 0.0084 test loss: 0.0149\n",
      "Epoch 7 batch 100 train loss: 0.0056 test loss: 0.0178\n",
      "Epoch 7 batch 200 train loss: 0.0089 test loss: 0.0163\n",
      "Epoch 7 batch 300 train loss: 0.0068 test loss: 0.0152\n",
      "Epoch 7 batch 400 train loss: 0.0076 test loss: 0.0174\n",
      "Epoch 7 batch 500 train loss: 0.0057 test loss: 0.0175\n",
      "Epoch 7 batch 600 train loss: 0.0075 test loss: 0.0191\n",
      "Epoch 7 batch 700 train loss: 0.0075 test loss: 0.0174\n",
      "Epoch 7 batch 800 train loss: 0.0062 test loss: 0.0155\n",
      "Epoch 7 batch 900 train loss: 0.0064 test loss: 0.0159\n",
      "Epoch 7 batch 1000 train loss: 0.0065 test loss: 0.0152\n",
      "Epoch 7 batch 1100 train loss: 0.0054 test loss: 0.0184\n",
      "Epoch 7 batch 1200 train loss: 0.0064 test loss: 0.0142\n",
      "Epoch 7 batch 1300 train loss: 0.0059 test loss: 0.0181\n",
      "Epoch 7 batch 1400 train loss: 0.0069 test loss: 0.0176\n",
      "Epoch 7 batch 1500 train loss: 0.0073 test loss: 0.0165\n",
      "Epoch 7 batch 1600 train loss: 0.0072 test loss: 0.0157\n",
      "Epoch 7 batch 1700 train loss: 0.0073 test loss: 0.0182\n",
      "Epoch 7 batch 1800 train loss: 0.0083 test loss: 0.0175\n",
      "Epoch 7 batch 1900 train loss: 0.0071 test loss: 0.0155\n",
      "Epoch 7 batch 2000 train loss: 0.0078 test loss: 0.0199\n",
      "Epoch 7 batch 2100 train loss: 0.0070 test loss: 0.0177\n",
      "Epoch 7 batch 2200 train loss: 0.0065 test loss: 0.0173\n",
      "Epoch 7 batch 2300 train loss: 0.0082 test loss: 0.0175\n",
      "Epoch 7 batch 2400 train loss: 0.0099 test loss: 0.0166\n",
      "Epoch 7 batch 2500 train loss: 0.0052 test loss: 0.0192\n",
      "Epoch 7 batch 2600 train loss: 0.0085 test loss: 0.0140\n",
      "Epoch 7 batch 2700 train loss: 0.0044 test loss: 0.0178\n",
      "Epoch 7 batch 2800 train loss: 0.0060 test loss: 0.0159\n",
      "Epoch 7 batch 2900 train loss: 0.0097 test loss: 0.0163\n",
      "Epoch 8 batch 0 train loss: 0.0060 test loss: 0.0164\n",
      "Epoch 8 batch 100 train loss: 0.0050 test loss: 0.0153\n",
      "Epoch 8 batch 200 train loss: 0.0061 test loss: 0.0141\n",
      "Epoch 8 batch 300 train loss: 0.0057 test loss: 0.0182\n",
      "Epoch 8 batch 400 train loss: 0.0077 test loss: 0.0195\n",
      "Epoch 8 batch 500 train loss: 0.0109 test loss: 0.0175\n",
      "Epoch 8 batch 600 train loss: 0.0074 test loss: 0.0187\n",
      "Epoch 8 batch 700 train loss: 0.0070 test loss: 0.0200\n",
      "Epoch 8 batch 800 train loss: 0.0089 test loss: 0.0170\n",
      "Epoch 8 batch 900 train loss: 0.0083 test loss: 0.0189\n",
      "Epoch 8 batch 1000 train loss: 0.0059 test loss: 0.0197\n",
      "Epoch 8 batch 1100 train loss: 0.0078 test loss: 0.0178\n",
      "Epoch 8 batch 1200 train loss: 0.0070 test loss: 0.0164\n",
      "Epoch 8 batch 1300 train loss: 0.0079 test loss: 0.0195\n",
      "Epoch 8 batch 1400 train loss: 0.0065 test loss: 0.0168\n",
      "Epoch 8 batch 1500 train loss: 0.0073 test loss: 0.0189\n",
      "Epoch 8 batch 1600 train loss: 0.0077 test loss: 0.0160\n",
      "Epoch 8 batch 1700 train loss: 0.0060 test loss: 0.0158\n",
      "Epoch 8 batch 1800 train loss: 0.0069 test loss: 0.0180\n",
      "Epoch 8 batch 1900 train loss: 0.0074 test loss: 0.0180\n",
      "Epoch 8 batch 2000 train loss: 0.0063 test loss: 0.0173\n",
      "Epoch 8 batch 2100 train loss: 0.0099 test loss: 0.0194\n",
      "Epoch 8 batch 2200 train loss: 0.0067 test loss: 0.0172\n",
      "Epoch 8 batch 2300 train loss: 0.0077 test loss: 0.0187\n",
      "Epoch 8 batch 2400 train loss: 0.0064 test loss: 0.0175\n",
      "Epoch 8 batch 2500 train loss: 0.0074 test loss: 0.0186\n",
      "Epoch 8 batch 2600 train loss: 0.0071 test loss: 0.0163\n",
      "Epoch 8 batch 2700 train loss: 0.0077 test loss: 0.0161\n",
      "Epoch 8 batch 2800 train loss: 0.0090 test loss: 0.0163\n",
      "Epoch 8 batch 2900 train loss: 0.0056 test loss: 0.0162\n",
      "Epoch 9 batch 0 train loss: 0.0073 test loss: 0.0147\n",
      "Epoch 9 batch 100 train loss: 0.0059 test loss: 0.0153\n",
      "Epoch 9 batch 200 train loss: 0.0077 test loss: 0.0177\n",
      "Epoch 9 batch 300 train loss: 0.0063 test loss: 0.0164\n",
      "Epoch 9 batch 400 train loss: 0.0074 test loss: 0.0161\n",
      "Epoch 9 batch 500 train loss: 0.0067 test loss: 0.0184\n",
      "Epoch 9 batch 600 train loss: 0.0092 test loss: 0.0173\n",
      "Epoch 9 batch 700 train loss: 0.0073 test loss: 0.0168\n",
      "Epoch 9 batch 800 train loss: 0.0069 test loss: 0.0165\n",
      "Epoch 9 batch 900 train loss: 0.0084 test loss: 0.0180\n",
      "Epoch 9 batch 1000 train loss: 0.0063 test loss: 0.0174\n",
      "Epoch 9 batch 1100 train loss: 0.0077 test loss: 0.0179\n",
      "Epoch 9 batch 1200 train loss: 0.0077 test loss: 0.0183\n",
      "Epoch 9 batch 1300 train loss: 0.0072 test loss: 0.0176\n",
      "Epoch 9 batch 1400 train loss: 0.0080 test loss: 0.0192\n",
      "Epoch 9 batch 1500 train loss: 0.0075 test loss: 0.0163\n",
      "Epoch 9 batch 1600 train loss: 0.0066 test loss: 0.0183\n",
      "Epoch 9 batch 1700 train loss: 0.0067 test loss: 0.0164\n",
      "Epoch 9 batch 1800 train loss: 0.0090 test loss: 0.0182\n",
      "Epoch 9 batch 1900 train loss: 0.0080 test loss: 0.0174\n",
      "Epoch 9 batch 2000 train loss: 0.0098 test loss: 0.0182\n",
      "Epoch 9 batch 2100 train loss: 0.0079 test loss: 0.0171\n",
      "Epoch 9 batch 2200 train loss: 0.0075 test loss: 0.0182\n",
      "Epoch 9 batch 2300 train loss: 0.0072 test loss: 0.0162\n",
      "Epoch 9 batch 2400 train loss: 0.0070 test loss: 0.0166\n",
      "Epoch 9 batch 2500 train loss: 0.0097 test loss: 0.0187\n",
      "Epoch 9 batch 2600 train loss: 0.0079 test loss: 0.0168\n",
      "Epoch 9 batch 2700 train loss: 0.0080 test loss: 0.0182\n",
      "Epoch 9 batch 2800 train loss: 0.0069 test loss: 0.0156\n",
      "Epoch 9 batch 2900 train loss: 0.0080 test loss: 0.0166\n",
      "Epoch 10 batch 0 train loss: 0.0069 test loss: 0.0169\n",
      "Epoch 10 batch 100 train loss: 0.0092 test loss: 0.0186\n",
      "Epoch 10 batch 200 train loss: 0.0091 test loss: 0.0166\n",
      "Epoch 10 batch 300 train loss: 0.0079 test loss: 0.0162\n",
      "Epoch 10 batch 400 train loss: 0.0086 test loss: 0.0196\n",
      "Epoch 10 batch 500 train loss: 0.0075 test loss: 0.0159\n",
      "Epoch 10 batch 600 train loss: 0.0070 test loss: 0.0159\n",
      "Epoch 10 batch 700 train loss: 0.0079 test loss: 0.0163\n",
      "Epoch 10 batch 800 train loss: 0.0080 test loss: 0.0158\n",
      "Epoch 10 batch 900 train loss: 0.0087 test loss: 0.0168\n",
      "Epoch 10 batch 1000 train loss: 0.0069 test loss: 0.0161\n",
      "Epoch 10 batch 1100 train loss: 0.0081 test loss: 0.0233\n",
      "Epoch 10 batch 1200 train loss: 0.0100 test loss: 0.0186\n",
      "Epoch 10 batch 1300 train loss: 0.0090 test loss: 0.0155\n",
      "Epoch 10 batch 1400 train loss: 0.0053 test loss: 0.0164\n",
      "Epoch 10 batch 1500 train loss: 0.0070 test loss: 0.0197\n",
      "Epoch 10 batch 1600 train loss: 0.0081 test loss: 0.0155\n",
      "Epoch 10 batch 1700 train loss: 0.0077 test loss: 0.0198\n",
      "Epoch 10 batch 1800 train loss: 0.0072 test loss: 0.0180\n",
      "Epoch 10 batch 1900 train loss: 0.0057 test loss: 0.0183\n",
      "Epoch 10 batch 2000 train loss: 0.0077 test loss: 0.0180\n",
      "Epoch 10 batch 2100 train loss: 0.0069 test loss: 0.0171\n",
      "Epoch 10 batch 2200 train loss: 0.0078 test loss: 0.0183\n",
      "Epoch 10 batch 2300 train loss: 0.0072 test loss: 0.0191\n",
      "Epoch 10 batch 2400 train loss: 0.0068 test loss: 0.0203\n",
      "Epoch 10 batch 2500 train loss: 0.0084 test loss: 0.0167\n",
      "Epoch 10 batch 2600 train loss: 0.0060 test loss: 0.0167\n",
      "Epoch 10 batch 2700 train loss: 0.0050 test loss: 0.0152\n",
      "Epoch 10 batch 2800 train loss: 0.0066 test loss: 0.0160\n",
      "Epoch 10 batch 2900 train loss: 0.0075 test loss: 0.0166\n",
      "Epoch 11 batch 0 train loss: 0.0062 test loss: 0.0162\n",
      "Epoch 11 batch 100 train loss: 0.0059 test loss: 0.0173\n",
      "Epoch 11 batch 200 train loss: 0.0074 test loss: 0.0159\n",
      "Epoch 11 batch 300 train loss: 0.0077 test loss: 0.0166\n",
      "Epoch 11 batch 400 train loss: 0.0079 test loss: 0.0196\n",
      "Epoch 11 batch 500 train loss: 0.0074 test loss: 0.0145\n",
      "Epoch 11 batch 600 train loss: 0.0105 test loss: 0.0183\n",
      "Epoch 11 batch 700 train loss: 0.0083 test loss: 0.0178\n",
      "Epoch 11 batch 800 train loss: 0.0088 test loss: 0.0196\n",
      "Epoch 11 batch 900 train loss: 0.0059 test loss: 0.0160\n",
      "Epoch 11 batch 1000 train loss: 0.0081 test loss: 0.0173\n",
      "Epoch 11 batch 1100 train loss: 0.0079 test loss: 0.0190\n",
      "Epoch 11 batch 1200 train loss: 0.0091 test loss: 0.0186\n",
      "Epoch 11 batch 1300 train loss: 0.0075 test loss: 0.0170\n",
      "Epoch 11 batch 1400 train loss: 0.0060 test loss: 0.0169\n",
      "Epoch 11 batch 1500 train loss: 0.0052 test loss: 0.0173\n",
      "Epoch 11 batch 1600 train loss: 0.0053 test loss: 0.0175\n",
      "Epoch 11 batch 1700 train loss: 0.0070 test loss: 0.0153\n",
      "Epoch 11 batch 1800 train loss: 0.0054 test loss: 0.0177\n",
      "Epoch 11 batch 1900 train loss: 0.0096 test loss: 0.0204\n",
      "Epoch 11 batch 2000 train loss: 0.0059 test loss: 0.0187\n",
      "Epoch 11 batch 2100 train loss: 0.0070 test loss: 0.0176\n",
      "Epoch 11 batch 2200 train loss: 0.0071 test loss: 0.0165\n",
      "Epoch 11 batch 2300 train loss: 0.0084 test loss: 0.0179\n",
      "Epoch 11 batch 2400 train loss: 0.0070 test loss: 0.0183\n",
      "Epoch 11 batch 2500 train loss: 0.0086 test loss: 0.0168\n",
      "Epoch 11 batch 2600 train loss: 0.0088 test loss: 0.0165\n",
      "Epoch 11 batch 2700 train loss: 0.0052 test loss: 0.0158\n",
      "Epoch 11 batch 2800 train loss: 0.0071 test loss: 0.0156\n",
      "Epoch 11 batch 2900 train loss: 0.0102 test loss: 0.0160\n",
      "Epoch 12 batch 0 train loss: 0.0058 test loss: 0.0181\n",
      "Epoch 12 batch 100 train loss: 0.0070 test loss: 0.0172\n",
      "Epoch 12 batch 200 train loss: 0.0084 test loss: 0.0161\n",
      "Epoch 12 batch 300 train loss: 0.0087 test loss: 0.0178\n",
      "Epoch 12 batch 400 train loss: 0.0077 test loss: 0.0168\n",
      "Epoch 12 batch 500 train loss: 0.0054 test loss: 0.0170\n",
      "Epoch 12 batch 600 train loss: 0.0069 test loss: 0.0176\n",
      "Epoch 12 batch 700 train loss: 0.0070 test loss: 0.0185\n",
      "Epoch 12 batch 800 train loss: 0.0083 test loss: 0.0146\n",
      "Epoch 12 batch 900 train loss: 0.0065 test loss: 0.0156\n",
      "Epoch 12 batch 1000 train loss: 0.0092 test loss: 0.0182\n",
      "Epoch 12 batch 1100 train loss: 0.0081 test loss: 0.0168\n",
      "Epoch 12 batch 1200 train loss: 0.0057 test loss: 0.0156\n",
      "Epoch 12 batch 1300 train loss: 0.0087 test loss: 0.0177\n",
      "Epoch 12 batch 1400 train loss: 0.0054 test loss: 0.0152\n",
      "Epoch 12 batch 1500 train loss: 0.0079 test loss: 0.0171\n",
      "Epoch 12 batch 1600 train loss: 0.0071 test loss: 0.0153\n",
      "Epoch 12 batch 1700 train loss: 0.0079 test loss: 0.0170\n",
      "Epoch 12 batch 1800 train loss: 0.0063 test loss: 0.0190\n",
      "Epoch 12 batch 1900 train loss: 0.0077 test loss: 0.0177\n",
      "Epoch 12 batch 2000 train loss: 0.0088 test loss: 0.0181\n",
      "Epoch 12 batch 2100 train loss: 0.0067 test loss: 0.0178\n",
      "Epoch 12 batch 2200 train loss: 0.0090 test loss: 0.0198\n",
      "Epoch 12 batch 2300 train loss: 0.0081 test loss: 0.0176\n",
      "Epoch 12 batch 2400 train loss: 0.0061 test loss: 0.0178\n",
      "Epoch 12 batch 2500 train loss: 0.0082 test loss: 0.0195\n",
      "Epoch 12 batch 2600 train loss: 0.0093 test loss: 0.0173\n",
      "Epoch 12 batch 2700 train loss: 0.0084 test loss: 0.0188\n",
      "Epoch 12 batch 2800 train loss: 0.0083 test loss: 0.0153\n",
      "Epoch 12 batch 2900 train loss: 0.0072 test loss: 0.0175\n",
      "Epoch 13 batch 0 train loss: 0.0087 test loss: 0.0150\n",
      "Epoch 13 batch 100 train loss: 0.0056 test loss: 0.0183\n",
      "Epoch 13 batch 200 train loss: 0.0063 test loss: 0.0153\n",
      "Epoch 13 batch 300 train loss: 0.0077 test loss: 0.0171\n",
      "Epoch 13 batch 400 train loss: 0.0064 test loss: 0.0181\n",
      "Epoch 13 batch 500 train loss: 0.0070 test loss: 0.0152\n",
      "Epoch 13 batch 600 train loss: 0.0076 test loss: 0.0145\n",
      "Epoch 13 batch 700 train loss: 0.0084 test loss: 0.0153\n",
      "Epoch 13 batch 800 train loss: 0.0066 test loss: 0.0163\n",
      "Epoch 13 batch 900 train loss: 0.0064 test loss: 0.0177\n",
      "Epoch 13 batch 1000 train loss: 0.0080 test loss: 0.0156\n",
      "Epoch 13 batch 1100 train loss: 0.0074 test loss: 0.0162\n",
      "Epoch 13 batch 1200 train loss: 0.0125 test loss: 0.0174\n",
      "Epoch 13 batch 1300 train loss: 0.0095 test loss: 0.0163\n",
      "Epoch 13 batch 1400 train loss: 0.0087 test loss: 0.0168\n",
      "Epoch 13 batch 1500 train loss: 0.0069 test loss: 0.0178\n",
      "Epoch 13 batch 1600 train loss: 0.0111 test loss: 0.0168\n",
      "Epoch 13 batch 1700 train loss: 0.0084 test loss: 0.0196\n",
      "Epoch 13 batch 1800 train loss: 0.0066 test loss: 0.0174\n",
      "Epoch 13 batch 1900 train loss: 0.0073 test loss: 0.0163\n",
      "Epoch 13 batch 2000 train loss: 0.0076 test loss: 0.0154\n",
      "Epoch 13 batch 2100 train loss: 0.0069 test loss: 0.0157\n",
      "Epoch 13 batch 2200 train loss: 0.0085 test loss: 0.0176\n",
      "Epoch 13 batch 2300 train loss: 0.0064 test loss: 0.0174\n",
      "Epoch 13 batch 2400 train loss: 0.0075 test loss: 0.0187\n",
      "Epoch 13 batch 2500 train loss: 0.0050 test loss: 0.0178\n",
      "Epoch 13 batch 2600 train loss: 0.0074 test loss: 0.0176\n",
      "Epoch 13 batch 2700 train loss: 0.0065 test loss: 0.0191\n",
      "Epoch 13 batch 2800 train loss: 0.0084 test loss: 0.0185\n",
      "Epoch 13 batch 2900 train loss: 0.0062 test loss: 0.0169\n",
      "Epoch 14 batch 0 train loss: 0.0078 test loss: 0.0163\n",
      "Epoch 14 batch 100 train loss: 0.0056 test loss: 0.0184\n",
      "Epoch 14 batch 200 train loss: 0.0077 test loss: 0.0184\n",
      "Epoch 14 batch 300 train loss: 0.0074 test loss: 0.0208\n",
      "Epoch 14 batch 400 train loss: 0.0081 test loss: 0.0155\n",
      "Epoch 14 batch 500 train loss: 0.0072 test loss: 0.0153\n",
      "Epoch 14 batch 600 train loss: 0.0089 test loss: 0.0157\n",
      "Epoch 14 batch 700 train loss: 0.0086 test loss: 0.0176\n",
      "Epoch 14 batch 800 train loss: 0.0059 test loss: 0.0177\n",
      "Epoch 14 batch 900 train loss: 0.0060 test loss: 0.0174\n",
      "Epoch 14 batch 1000 train loss: 0.0067 test loss: 0.0177\n",
      "Epoch 14 batch 1100 train loss: 0.0070 test loss: 0.0162\n",
      "Epoch 14 batch 1200 train loss: 0.0067 test loss: 0.0146\n",
      "Epoch 14 batch 1300 train loss: 0.0087 test loss: 0.0155\n",
      "Epoch 14 batch 1400 train loss: 0.0077 test loss: 0.0171\n",
      "Epoch 14 batch 1500 train loss: 0.0058 test loss: 0.0187\n",
      "Epoch 14 batch 1600 train loss: 0.0070 test loss: 0.0142\n",
      "Epoch 14 batch 1700 train loss: 0.0082 test loss: 0.0160\n",
      "Epoch 14 batch 1800 train loss: 0.0070 test loss: 0.0165\n",
      "Epoch 14 batch 1900 train loss: 0.0088 test loss: 0.0180\n",
      "Epoch 14 batch 2000 train loss: 0.0063 test loss: 0.0186\n",
      "Epoch 14 batch 2100 train loss: 0.0075 test loss: 0.0171\n",
      "Epoch 14 batch 2200 train loss: 0.0078 test loss: 0.0159\n",
      "Epoch 14 batch 2300 train loss: 0.0063 test loss: 0.0165\n",
      "Epoch 14 batch 2400 train loss: 0.0060 test loss: 0.0176\n",
      "Epoch 14 batch 2500 train loss: 0.0114 test loss: 0.0189\n",
      "Epoch 14 batch 2600 train loss: 0.0103 test loss: 0.0177\n",
      "Epoch 14 batch 2700 train loss: 0.0075 test loss: 0.0190\n",
      "Epoch 14 batch 2800 train loss: 0.0054 test loss: 0.0175\n",
      "Epoch 14 batch 2900 train loss: 0.0064 test loss: 0.0158\n",
      "Epoch 15 batch 0 train loss: 0.0078 test loss: 0.0154\n",
      "Epoch 15 batch 100 train loss: 0.0079 test loss: 0.0173\n",
      "Epoch 15 batch 200 train loss: 0.0074 test loss: 0.0170\n",
      "Epoch 15 batch 300 train loss: 0.0097 test loss: 0.0154\n",
      "Epoch 15 batch 400 train loss: 0.0084 test loss: 0.0162\n",
      "Epoch 15 batch 500 train loss: 0.0076 test loss: 0.0180\n",
      "Epoch 15 batch 600 train loss: 0.0070 test loss: 0.0184\n",
      "Epoch 15 batch 700 train loss: 0.0047 test loss: 0.0155\n",
      "Epoch 15 batch 800 train loss: 0.0076 test loss: 0.0179\n",
      "Epoch 15 batch 900 train loss: 0.0070 test loss: 0.0138\n",
      "Epoch 15 batch 1000 train loss: 0.0056 test loss: 0.0162\n",
      "Epoch 15 batch 1100 train loss: 0.0086 test loss: 0.0160\n",
      "Epoch 15 batch 1200 train loss: 0.0106 test loss: 0.0170\n",
      "Epoch 15 batch 1300 train loss: 0.0062 test loss: 0.0165\n",
      "Epoch 15 batch 1400 train loss: 0.0060 test loss: 0.0197\n",
      "Epoch 15 batch 1500 train loss: 0.0061 test loss: 0.0181\n",
      "Epoch 15 batch 1600 train loss: 0.0075 test loss: 0.0173\n",
      "Epoch 15 batch 1700 train loss: 0.0077 test loss: 0.0169\n",
      "Epoch 15 batch 1800 train loss: 0.0080 test loss: 0.0166\n",
      "Epoch 15 batch 1900 train loss: 0.0076 test loss: 0.0175\n",
      "Epoch 15 batch 2000 train loss: 0.0091 test loss: 0.0163\n",
      "Epoch 15 batch 2100 train loss: 0.0093 test loss: 0.0158\n",
      "Epoch 15 batch 2200 train loss: 0.0060 test loss: 0.0189\n",
      "Epoch 15 batch 2300 train loss: 0.0056 test loss: 0.0198\n",
      "Epoch 15 batch 2400 train loss: 0.0058 test loss: 0.0176\n",
      "Epoch 15 batch 2500 train loss: 0.0084 test loss: 0.0179\n",
      "Epoch 15 batch 2600 train loss: 0.0081 test loss: 0.0148\n",
      "Epoch 15 batch 2700 train loss: 0.0071 test loss: 0.0180\n",
      "Epoch 15 batch 2800 train loss: 0.0068 test loss: 0.0172\n",
      "Epoch 15 batch 2900 train loss: 0.0051 test loss: 0.0178\n",
      "Epoch 16 batch 0 train loss: 0.0076 test loss: 0.0159\n",
      "Epoch 16 batch 100 train loss: 0.0068 test loss: 0.0188\n",
      "Epoch 16 batch 200 train loss: 0.0092 test loss: 0.0158\n",
      "Epoch 16 batch 300 train loss: 0.0086 test loss: 0.0147\n",
      "Epoch 16 batch 400 train loss: 0.0099 test loss: 0.0172\n",
      "Epoch 16 batch 500 train loss: 0.0049 test loss: 0.0175\n",
      "Epoch 16 batch 600 train loss: 0.0071 test loss: 0.0168\n",
      "Epoch 16 batch 700 train loss: 0.0085 test loss: 0.0178\n",
      "Epoch 16 batch 800 train loss: 0.0077 test loss: 0.0166\n",
      "Epoch 16 batch 900 train loss: 0.0074 test loss: 0.0162\n",
      "Epoch 16 batch 1000 train loss: 0.0058 test loss: 0.0200\n",
      "Epoch 16 batch 1100 train loss: 0.0091 test loss: 0.0188\n",
      "Epoch 16 batch 1200 train loss: 0.0075 test loss: 0.0150\n",
      "Epoch 16 batch 1300 train loss: 0.0083 test loss: 0.0177\n",
      "Epoch 16 batch 1400 train loss: 0.0090 test loss: 0.0168\n",
      "Epoch 16 batch 1500 train loss: 0.0067 test loss: 0.0168\n",
      "Epoch 16 batch 1600 train loss: 0.0074 test loss: 0.0182\n",
      "Epoch 16 batch 1700 train loss: 0.0063 test loss: 0.0162\n",
      "Epoch 16 batch 1800 train loss: 0.0059 test loss: 0.0193\n",
      "Epoch 16 batch 1900 train loss: 0.0076 test loss: 0.0160\n",
      "Epoch 16 batch 2000 train loss: 0.0088 test loss: 0.0174\n",
      "Epoch 16 batch 2100 train loss: 0.0100 test loss: 0.0177\n",
      "Epoch 16 batch 2200 train loss: 0.0059 test loss: 0.0177\n",
      "Epoch 16 batch 2300 train loss: 0.0073 test loss: 0.0183\n",
      "Epoch 16 batch 2400 train loss: 0.0066 test loss: 0.0185\n",
      "Epoch 16 batch 2500 train loss: 0.0103 test loss: 0.0155\n",
      "Epoch 16 batch 2600 train loss: 0.0083 test loss: 0.0159\n",
      "Epoch 16 batch 2700 train loss: 0.0070 test loss: 0.0184\n",
      "Epoch 16 batch 2800 train loss: 0.0062 test loss: 0.0163\n",
      "Epoch 16 batch 2900 train loss: 0.0086 test loss: 0.0161\n",
      "Epoch 17 batch 0 train loss: 0.0072 test loss: 0.0168\n",
      "Epoch 17 batch 100 train loss: 0.0069 test loss: 0.0165\n",
      "Epoch 17 batch 200 train loss: 0.0066 test loss: 0.0175\n",
      "Epoch 17 batch 300 train loss: 0.0077 test loss: 0.0153\n",
      "early stop.\n",
      "Checkpoint 14 restored!!\n",
      "Training for loss rate 0.90 start.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['bi_lstm_8/bi_lstm_block_16/bidirectional_16/forward_lstm_16/lstm_cell_49/kernel:0', 'bi_lstm_8/bi_lstm_block_16/bidirectional_16/forward_lstm_16/lstm_cell_49/recurrent_kernel:0', 'bi_lstm_8/bi_lstm_block_16/bidirectional_16/forward_lstm_16/lstm_cell_49/bias:0', 'bi_lstm_8/bi_lstm_block_16/bidirectional_16/backward_lstm_16/lstm_cell_50/kernel:0', 'bi_lstm_8/bi_lstm_block_16/bidirectional_16/backward_lstm_16/lstm_cell_50/recurrent_kernel:0', 'bi_lstm_8/bi_lstm_block_16/bidirectional_16/backward_lstm_16/lstm_cell_50/bias:0', 'bi_lstm_8/bi_lstm_block_16/layer_normalization_16/gamma:0', 'bi_lstm_8/bi_lstm_block_16/layer_normalization_16/beta:0', 'bi_lstm_8/bi_lstm_block_17/bidirectional_17/forward_lstm_17/lstm_cell_52/kernel:0', 'bi_lstm_8/bi_lstm_block_17/bidirectional_17/forward_lstm_17/lstm_cell_52/recurrent_kernel:0', 'bi_lstm_8/bi_lstm_block_17/bidirectional_17/forward_lstm_17/lstm_cell_52/bias:0', 'bi_lstm_8/bi_lstm_block_17/bidirectional_17/backward_lstm_17/lstm_cell_53/kernel:0', 'bi_lstm_8/bi_lstm_block_17/bidirectional_17/backward_lstm_17/lstm_cell_53/recurrent_kernel:0', 'bi_lstm_8/bi_lstm_block_17/bidirectional_17/backward_lstm_17/lstm_cell_53/bias:0', 'bi_lstm_8/bi_lstm_block_17/layer_normalization_17/gamma:0', 'bi_lstm_8/bi_lstm_block_17/layer_normalization_17/beta:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['bi_lstm_8/bi_lstm_block_16/bidirectional_16/forward_lstm_16/lstm_cell_49/kernel:0', 'bi_lstm_8/bi_lstm_block_16/bidirectional_16/forward_lstm_16/lstm_cell_49/recurrent_kernel:0', 'bi_lstm_8/bi_lstm_block_16/bidirectional_16/forward_lstm_16/lstm_cell_49/bias:0', 'bi_lstm_8/bi_lstm_block_16/bidirectional_16/backward_lstm_16/lstm_cell_50/kernel:0', 'bi_lstm_8/bi_lstm_block_16/bidirectional_16/backward_lstm_16/lstm_cell_50/recurrent_kernel:0', 'bi_lstm_8/bi_lstm_block_16/bidirectional_16/backward_lstm_16/lstm_cell_50/bias:0', 'bi_lstm_8/bi_lstm_block_16/layer_normalization_16/gamma:0', 'bi_lstm_8/bi_lstm_block_16/layer_normalization_16/beta:0', 'bi_lstm_8/bi_lstm_block_17/bidirectional_17/forward_lstm_17/lstm_cell_52/kernel:0', 'bi_lstm_8/bi_lstm_block_17/bidirectional_17/forward_lstm_17/lstm_cell_52/recurrent_kernel:0', 'bi_lstm_8/bi_lstm_block_17/bidirectional_17/forward_lstm_17/lstm_cell_52/bias:0', 'bi_lstm_8/bi_lstm_block_17/bidirectional_17/backward_lstm_17/lstm_cell_53/kernel:0', 'bi_lstm_8/bi_lstm_block_17/bidirectional_17/backward_lstm_17/lstm_cell_53/recurrent_kernel:0', 'bi_lstm_8/bi_lstm_block_17/bidirectional_17/backward_lstm_17/lstm_cell_53/bias:0', 'bi_lstm_8/bi_lstm_block_17/layer_normalization_17/gamma:0', 'bi_lstm_8/bi_lstm_block_17/layer_normalization_17/beta:0'] when minimizing the loss.\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.90p/ckpt-1\n",
      "Epoch 0 batch 0 train loss: 0.5543 test loss: 1.3204\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.90p/ckpt-2\n",
      "Epoch 0 batch 100 train loss: 0.2121 test loss: 0.1817\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.90p/ckpt-3\n",
      "Epoch 0 batch 200 train loss: 0.0987 test loss: 0.0780\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.90p/ckpt-4\n",
      "Epoch 0 batch 300 train loss: 0.0612 test loss: 0.0495\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.90p/ckpt-5\n",
      "Epoch 0 batch 400 train loss: 0.0446 test loss: 0.0373\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.90p/ckpt-6\n",
      "Epoch 0 batch 500 train loss: 0.0284 test loss: 0.0276\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.90p/ckpt-7\n",
      "Epoch 0 batch 600 train loss: 0.0232 test loss: 0.0263\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.90p/ckpt-8\n",
      "Epoch 0 batch 700 train loss: 0.0160 test loss: 0.0213\n",
      "Epoch 0 batch 800 train loss: 0.0126 test loss: 0.0216\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.90p/ckpt-9\n",
      "Epoch 0 batch 900 train loss: 0.0104 test loss: 0.0186\n",
      "Epoch 0 batch 1000 train loss: 0.0108 test loss: 0.0204\n",
      "Epoch 0 batch 1100 train loss: 0.0089 test loss: 0.0195\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.90p/ckpt-10\n",
      "Epoch 0 batch 1200 train loss: 0.0084 test loss: 0.0181\n",
      "Epoch 0 batch 1300 train loss: 0.0099 test loss: 0.0188\n",
      "Epoch 0 batch 1400 train loss: 0.0100 test loss: 0.0200\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.90p/ckpt-11\n",
      "Epoch 0 batch 1500 train loss: 0.0083 test loss: 0.0173\n",
      "Epoch 0 batch 1600 train loss: 0.0091 test loss: 0.0174\n",
      "Epoch 0 batch 1700 train loss: 0.0067 test loss: 0.0194\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.90p/ckpt-12\n",
      "Epoch 0 batch 1800 train loss: 0.0120 test loss: 0.0169\n",
      "Epoch 0 batch 1900 train loss: 0.0083 test loss: 0.0196\n",
      "Epoch 0 batch 2000 train loss: 0.0068 test loss: 0.0202\n",
      "Epoch 0 batch 2100 train loss: 0.0065 test loss: 0.0187\n",
      "Epoch 0 batch 2200 train loss: 0.0083 test loss: 0.0223\n",
      "Epoch 0 batch 2300 train loss: 0.0085 test loss: 0.0186\n",
      "Epoch 0 batch 2400 train loss: 0.0069 test loss: 0.0214\n",
      "Epoch 0 batch 2500 train loss: 0.0093 test loss: 0.0192\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.90p/ckpt-13\n",
      "Epoch 0 batch 2600 train loss: 0.0071 test loss: 0.0163\n",
      "Epoch 0 batch 2700 train loss: 0.0094 test loss: 0.0211\n",
      "Epoch 0 batch 2800 train loss: 0.0084 test loss: 0.0195\n",
      "Epoch 0 batch 2900 train loss: 0.0085 test loss: 0.0173\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['bi_lstm_8/bi_lstm_block_16/bidirectional_16/forward_lstm_16/lstm_cell_49/kernel:0', 'bi_lstm_8/bi_lstm_block_16/bidirectional_16/forward_lstm_16/lstm_cell_49/recurrent_kernel:0', 'bi_lstm_8/bi_lstm_block_16/bidirectional_16/forward_lstm_16/lstm_cell_49/bias:0', 'bi_lstm_8/bi_lstm_block_16/bidirectional_16/backward_lstm_16/lstm_cell_50/kernel:0', 'bi_lstm_8/bi_lstm_block_16/bidirectional_16/backward_lstm_16/lstm_cell_50/recurrent_kernel:0', 'bi_lstm_8/bi_lstm_block_16/bidirectional_16/backward_lstm_16/lstm_cell_50/bias:0', 'bi_lstm_8/bi_lstm_block_16/layer_normalization_16/gamma:0', 'bi_lstm_8/bi_lstm_block_16/layer_normalization_16/beta:0', 'bi_lstm_8/bi_lstm_block_17/bidirectional_17/forward_lstm_17/lstm_cell_52/kernel:0', 'bi_lstm_8/bi_lstm_block_17/bidirectional_17/forward_lstm_17/lstm_cell_52/recurrent_kernel:0', 'bi_lstm_8/bi_lstm_block_17/bidirectional_17/forward_lstm_17/lstm_cell_52/bias:0', 'bi_lstm_8/bi_lstm_block_17/bidirectional_17/backward_lstm_17/lstm_cell_53/kernel:0', 'bi_lstm_8/bi_lstm_block_17/bidirectional_17/backward_lstm_17/lstm_cell_53/recurrent_kernel:0', 'bi_lstm_8/bi_lstm_block_17/bidirectional_17/backward_lstm_17/lstm_cell_53/bias:0', 'bi_lstm_8/bi_lstm_block_17/layer_normalization_17/gamma:0', 'bi_lstm_8/bi_lstm_block_17/layer_normalization_17/beta:0'] when minimizing the loss.\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.90p/ckpt-14\n",
      "Epoch 1 batch 0 train loss: 0.0081 test loss: 0.0162\n",
      "Epoch 1 batch 100 train loss: 0.0084 test loss: 0.0170\n",
      "Epoch 1 batch 200 train loss: 0.0103 test loss: 0.0207\n",
      "Epoch 1 batch 300 train loss: 0.0095 test loss: 0.0180\n",
      "Epoch 1 batch 400 train loss: 0.0078 test loss: 0.0221\n",
      "Epoch 1 batch 500 train loss: 0.0067 test loss: 0.0213\n",
      "Epoch 1 batch 600 train loss: 0.0098 test loss: 0.0190\n",
      "Epoch 1 batch 700 train loss: 0.0086 test loss: 0.0191\n",
      "Epoch 1 batch 800 train loss: 0.0096 test loss: 0.0191\n",
      "Epoch 1 batch 900 train loss: 0.0084 test loss: 0.0206\n",
      "Epoch 1 batch 1000 train loss: 0.0082 test loss: 0.0188\n",
      "Epoch 1 batch 1100 train loss: 0.0093 test loss: 0.0189\n",
      "Epoch 1 batch 1200 train loss: 0.0097 test loss: 0.0177\n",
      "Epoch 1 batch 1300 train loss: 0.0083 test loss: 0.0233\n",
      "Epoch 1 batch 1400 train loss: 0.0091 test loss: 0.0182\n",
      "Epoch 1 batch 1500 train loss: 0.0068 test loss: 0.0184\n",
      "Epoch 1 batch 1600 train loss: 0.0082 test loss: 0.0185\n",
      "Epoch 1 batch 1700 train loss: 0.0068 test loss: 0.0181\n",
      "Epoch 1 batch 1800 train loss: 0.0097 test loss: 0.0162\n",
      "Epoch 1 batch 1900 train loss: 0.0080 test loss: 0.0184\n",
      "Epoch 1 batch 2000 train loss: 0.0073 test loss: 0.0207\n",
      "Epoch 1 batch 2100 train loss: 0.0086 test loss: 0.0182\n",
      "Epoch 1 batch 2200 train loss: 0.0086 test loss: 0.0207\n",
      "Epoch 1 batch 2300 train loss: 0.0075 test loss: 0.0185\n",
      "Epoch 1 batch 2400 train loss: 0.0067 test loss: 0.0190\n",
      "Epoch 1 batch 2500 train loss: 0.0083 test loss: 0.0194\n",
      "Epoch 1 batch 2600 train loss: 0.0085 test loss: 0.0179\n",
      "Epoch 1 batch 2700 train loss: 0.0069 test loss: 0.0175\n",
      "Epoch 1 batch 2800 train loss: 0.0067 test loss: 0.0183\n",
      "Epoch 1 batch 2900 train loss: 0.0052 test loss: 0.0183\n",
      "Epoch 2 batch 0 train loss: 0.0063 test loss: 0.0187\n",
      "Epoch 2 batch 100 train loss: 0.0102 test loss: 0.0176\n",
      "Epoch 2 batch 200 train loss: 0.0094 test loss: 0.0173\n",
      "Epoch 2 batch 300 train loss: 0.0062 test loss: 0.0179\n",
      "Epoch 2 batch 400 train loss: 0.0072 test loss: 0.0171\n",
      "Epoch 2 batch 500 train loss: 0.0093 test loss: 0.0172\n",
      "Epoch 2 batch 600 train loss: 0.0094 test loss: 0.0202\n",
      "Epoch 2 batch 700 train loss: 0.0067 test loss: 0.0194\n",
      "Epoch 2 batch 800 train loss: 0.0106 test loss: 0.0196\n",
      "Epoch 2 batch 900 train loss: 0.0069 test loss: 0.0192\n",
      "Epoch 2 batch 1000 train loss: 0.0069 test loss: 0.0169\n",
      "Epoch 2 batch 1100 train loss: 0.0057 test loss: 0.0228\n",
      "Epoch 2 batch 1200 train loss: 0.0078 test loss: 0.0212\n",
      "Epoch 2 batch 1300 train loss: 0.0100 test loss: 0.0198\n",
      "Epoch 2 batch 1400 train loss: 0.0079 test loss: 0.0197\n",
      "Epoch 2 batch 1500 train loss: 0.0067 test loss: 0.0181\n",
      "Epoch 2 batch 1600 train loss: 0.0083 test loss: 0.0205\n",
      "Epoch 2 batch 1700 train loss: 0.0066 test loss: 0.0189\n",
      "Epoch 2 batch 1800 train loss: 0.0081 test loss: 0.0191\n",
      "Epoch 2 batch 1900 train loss: 0.0071 test loss: 0.0173\n",
      "Epoch 2 batch 2000 train loss: 0.0117 test loss: 0.0206\n",
      "Epoch 2 batch 2100 train loss: 0.0085 test loss: 0.0209\n",
      "Epoch 2 batch 2200 train loss: 0.0056 test loss: 0.0183\n",
      "Epoch 2 batch 2300 train loss: 0.0103 test loss: 0.0174\n",
      "Epoch 2 batch 2400 train loss: 0.0073 test loss: 0.0171\n",
      "Epoch 2 batch 2500 train loss: 0.0081 test loss: 0.0199\n",
      "Epoch 2 batch 2600 train loss: 0.0077 test loss: 0.0182\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.90p/ckpt-15\n",
      "Epoch 2 batch 2700 train loss: 0.0082 test loss: 0.0157\n",
      "Epoch 2 batch 2800 train loss: 0.0083 test loss: 0.0174\n",
      "Epoch 2 batch 2900 train loss: 0.0069 test loss: 0.0181\n",
      "Epoch 3 batch 0 train loss: 0.0095 test loss: 0.0164\n",
      "Epoch 3 batch 100 train loss: 0.0099 test loss: 0.0169\n",
      "Epoch 3 batch 200 train loss: 0.0069 test loss: 0.0162\n",
      "Epoch 3 batch 300 train loss: 0.0078 test loss: 0.0165\n",
      "Epoch 3 batch 400 train loss: 0.0060 test loss: 0.0193\n",
      "Epoch 3 batch 500 train loss: 0.0058 test loss: 0.0187\n",
      "Epoch 3 batch 600 train loss: 0.0094 test loss: 0.0167\n",
      "Epoch 3 batch 700 train loss: 0.0081 test loss: 0.0179\n",
      "Epoch 3 batch 800 train loss: 0.0063 test loss: 0.0195\n",
      "Epoch 3 batch 900 train loss: 0.0080 test loss: 0.0178\n",
      "Epoch 3 batch 1000 train loss: 0.0105 test loss: 0.0195\n",
      "Epoch 3 batch 1100 train loss: 0.0081 test loss: 0.0196\n",
      "Epoch 3 batch 1200 train loss: 0.0078 test loss: 0.0181\n",
      "Epoch 3 batch 1300 train loss: 0.0070 test loss: 0.0205\n",
      "Epoch 3 batch 1400 train loss: 0.0081 test loss: 0.0174\n",
      "Epoch 3 batch 1500 train loss: 0.0078 test loss: 0.0176\n",
      "Epoch 3 batch 1600 train loss: 0.0073 test loss: 0.0182\n",
      "Epoch 3 batch 1700 train loss: 0.0087 test loss: 0.0195\n",
      "Epoch 3 batch 1800 train loss: 0.0060 test loss: 0.0195\n",
      "Epoch 3 batch 1900 train loss: 0.0077 test loss: 0.0208\n",
      "Epoch 3 batch 2000 train loss: 0.0083 test loss: 0.0197\n",
      "Epoch 3 batch 2100 train loss: 0.0076 test loss: 0.0185\n",
      "Epoch 3 batch 2200 train loss: 0.0063 test loss: 0.0184\n",
      "Epoch 3 batch 2300 train loss: 0.0076 test loss: 0.0189\n",
      "Epoch 3 batch 2400 train loss: 0.0086 test loss: 0.0212\n",
      "Epoch 3 batch 2500 train loss: 0.0076 test loss: 0.0209\n",
      "Epoch 3 batch 2600 train loss: 0.0061 test loss: 0.0183\n",
      "Epoch 3 batch 2700 train loss: 0.0087 test loss: 0.0186\n",
      "Epoch 3 batch 2800 train loss: 0.0071 test loss: 0.0188\n",
      "Epoch 3 batch 2900 train loss: 0.0070 test loss: 0.0168\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.90p/ckpt-16\n",
      "Epoch 4 batch 0 train loss: 0.0060 test loss: 0.0154\n",
      "Epoch 4 batch 100 train loss: 0.0078 test loss: 0.0208\n",
      "Epoch 4 batch 200 train loss: 0.0073 test loss: 0.0169\n",
      "Epoch 4 batch 300 train loss: 0.0088 test loss: 0.0191\n",
      "Epoch 4 batch 400 train loss: 0.0078 test loss: 0.0187\n",
      "Epoch 4 batch 500 train loss: 0.0060 test loss: 0.0212\n",
      "Epoch 4 batch 600 train loss: 0.0104 test loss: 0.0191\n",
      "Epoch 4 batch 700 train loss: 0.0067 test loss: 0.0208\n",
      "Epoch 4 batch 800 train loss: 0.0088 test loss: 0.0191\n",
      "Epoch 4 batch 900 train loss: 0.0053 test loss: 0.0181\n",
      "Epoch 4 batch 1000 train loss: 0.0078 test loss: 0.0176\n",
      "Epoch 4 batch 1100 train loss: 0.0072 test loss: 0.0180\n",
      "Epoch 4 batch 1200 train loss: 0.0068 test loss: 0.0160\n",
      "Epoch 4 batch 1300 train loss: 0.0077 test loss: 0.0170\n",
      "Epoch 4 batch 1400 train loss: 0.0069 test loss: 0.0169\n",
      "Epoch 4 batch 1500 train loss: 0.0094 test loss: 0.0206\n",
      "Epoch 4 batch 1600 train loss: 0.0074 test loss: 0.0193\n",
      "Epoch 4 batch 1700 train loss: 0.0102 test loss: 0.0174\n",
      "Epoch 4 batch 1800 train loss: 0.0068 test loss: 0.0185\n",
      "Epoch 4 batch 1900 train loss: 0.0097 test loss: 0.0178\n",
      "Epoch 4 batch 2000 train loss: 0.0070 test loss: 0.0175\n",
      "Epoch 4 batch 2100 train loss: 0.0075 test loss: 0.0175\n",
      "Epoch 4 batch 2200 train loss: 0.0090 test loss: 0.0187\n",
      "Epoch 4 batch 2300 train loss: 0.0064 test loss: 0.0192\n",
      "Epoch 4 batch 2400 train loss: 0.0062 test loss: 0.0189\n",
      "Epoch 4 batch 2500 train loss: 0.0056 test loss: 0.0186\n",
      "Epoch 4 batch 2600 train loss: 0.0093 test loss: 0.0176\n",
      "Epoch 4 batch 2700 train loss: 0.0066 test loss: 0.0173\n",
      "Epoch 4 batch 2800 train loss: 0.0063 test loss: 0.0166\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.90p/ckpt-17\n",
      "Epoch 4 batch 2900 train loss: 0.0065 test loss: 0.0149\n",
      "Epoch 5 batch 0 train loss: 0.0066 test loss: 0.0168\n",
      "Epoch 5 batch 100 train loss: 0.0078 test loss: 0.0183\n",
      "Epoch 5 batch 200 train loss: 0.0075 test loss: 0.0186\n",
      "Epoch 5 batch 300 train loss: 0.0073 test loss: 0.0173\n",
      "Epoch 5 batch 400 train loss: 0.0074 test loss: 0.0163\n",
      "Epoch 5 batch 500 train loss: 0.0068 test loss: 0.0167\n",
      "Epoch 5 batch 600 train loss: 0.0087 test loss: 0.0192\n",
      "Epoch 5 batch 700 train loss: 0.0072 test loss: 0.0174\n",
      "Epoch 5 batch 800 train loss: 0.0064 test loss: 0.0172\n",
      "Epoch 5 batch 900 train loss: 0.0094 test loss: 0.0201\n",
      "Epoch 5 batch 1000 train loss: 0.0068 test loss: 0.0167\n",
      "Epoch 5 batch 1100 train loss: 0.0093 test loss: 0.0181\n",
      "Epoch 5 batch 1200 train loss: 0.0075 test loss: 0.0181\n",
      "Epoch 5 batch 1300 train loss: 0.0068 test loss: 0.0212\n",
      "Epoch 5 batch 1400 train loss: 0.0092 test loss: 0.0167\n",
      "Epoch 5 batch 1500 train loss: 0.0081 test loss: 0.0191\n",
      "Epoch 5 batch 1600 train loss: 0.0079 test loss: 0.0188\n",
      "Epoch 5 batch 1700 train loss: 0.0081 test loss: 0.0207\n",
      "Epoch 5 batch 1800 train loss: 0.0064 test loss: 0.0190\n",
      "Epoch 5 batch 1900 train loss: 0.0079 test loss: 0.0175\n",
      "Epoch 5 batch 2000 train loss: 0.0072 test loss: 0.0216\n",
      "Epoch 5 batch 2100 train loss: 0.0071 test loss: 0.0202\n",
      "Epoch 5 batch 2200 train loss: 0.0074 test loss: 0.0186\n",
      "Epoch 5 batch 2300 train loss: 0.0103 test loss: 0.0189\n",
      "Epoch 5 batch 2400 train loss: 0.0065 test loss: 0.0191\n",
      "Epoch 5 batch 2500 train loss: 0.0088 test loss: 0.0188\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.90p/ckpt-18\n",
      "Epoch 5 batch 2600 train loss: 0.0082 test loss: 0.0141\n",
      "Epoch 5 batch 2700 train loss: 0.0090 test loss: 0.0174\n",
      "Epoch 5 batch 2800 train loss: 0.0064 test loss: 0.0185\n",
      "Epoch 5 batch 2900 train loss: 0.0061 test loss: 0.0172\n",
      "Epoch 6 batch 0 train loss: 0.0047 test loss: 0.0177\n",
      "Epoch 6 batch 100 train loss: 0.0079 test loss: 0.0187\n",
      "Epoch 6 batch 200 train loss: 0.0086 test loss: 0.0168\n",
      "Epoch 6 batch 300 train loss: 0.0079 test loss: 0.0184\n",
      "Epoch 6 batch 400 train loss: 0.0062 test loss: 0.0188\n",
      "Epoch 6 batch 500 train loss: 0.0060 test loss: 0.0210\n",
      "Epoch 6 batch 600 train loss: 0.0108 test loss: 0.0185\n",
      "Epoch 6 batch 700 train loss: 0.0077 test loss: 0.0179\n",
      "Epoch 6 batch 800 train loss: 0.0086 test loss: 0.0177\n",
      "Epoch 6 batch 900 train loss: 0.0083 test loss: 0.0173\n",
      "Epoch 6 batch 1000 train loss: 0.0065 test loss: 0.0192\n",
      "Epoch 6 batch 1100 train loss: 0.0072 test loss: 0.0168\n",
      "Epoch 6 batch 1200 train loss: 0.0087 test loss: 0.0153\n",
      "Epoch 6 batch 1300 train loss: 0.0073 test loss: 0.0171\n",
      "Epoch 6 batch 1400 train loss: 0.0102 test loss: 0.0192\n",
      "Epoch 6 batch 1500 train loss: 0.0087 test loss: 0.0209\n",
      "Epoch 6 batch 1600 train loss: 0.0084 test loss: 0.0183\n",
      "Epoch 6 batch 1700 train loss: 0.0072 test loss: 0.0170\n",
      "Epoch 6 batch 1800 train loss: 0.0070 test loss: 0.0180\n",
      "Epoch 6 batch 1900 train loss: 0.0081 test loss: 0.0197\n",
      "Epoch 6 batch 2000 train loss: 0.0071 test loss: 0.0197\n",
      "Epoch 6 batch 2100 train loss: 0.0065 test loss: 0.0202\n",
      "Epoch 6 batch 2200 train loss: 0.0094 test loss: 0.0196\n",
      "Epoch 6 batch 2300 train loss: 0.0095 test loss: 0.0171\n",
      "Epoch 6 batch 2400 train loss: 0.0072 test loss: 0.0171\n",
      "Epoch 6 batch 2500 train loss: 0.0072 test loss: 0.0182\n",
      "Epoch 6 batch 2600 train loss: 0.0072 test loss: 0.0155\n",
      "Epoch 6 batch 2700 train loss: 0.0062 test loss: 0.0189\n",
      "Epoch 6 batch 2800 train loss: 0.0100 test loss: 0.0166\n",
      "Epoch 6 batch 2900 train loss: 0.0101 test loss: 0.0184\n",
      "Epoch 7 batch 0 train loss: 0.0067 test loss: 0.0162\n",
      "Epoch 7 batch 100 train loss: 0.0061 test loss: 0.0208\n",
      "Epoch 7 batch 200 train loss: 0.0075 test loss: 0.0163\n",
      "Epoch 7 batch 300 train loss: 0.0070 test loss: 0.0164\n",
      "Epoch 7 batch 400 train loss: 0.0068 test loss: 0.0199\n",
      "Epoch 7 batch 500 train loss: 0.0096 test loss: 0.0222\n",
      "Epoch 7 batch 600 train loss: 0.0070 test loss: 0.0188\n",
      "Epoch 7 batch 700 train loss: 0.0067 test loss: 0.0157\n",
      "Epoch 7 batch 800 train loss: 0.0069 test loss: 0.0189\n",
      "Epoch 7 batch 900 train loss: 0.0079 test loss: 0.0160\n",
      "Epoch 7 batch 1000 train loss: 0.0081 test loss: 0.0188\n",
      "Epoch 7 batch 1100 train loss: 0.0057 test loss: 0.0216\n",
      "Epoch 7 batch 1200 train loss: 0.0068 test loss: 0.0158\n",
      "Epoch 7 batch 1300 train loss: 0.0065 test loss: 0.0186\n",
      "Epoch 7 batch 1400 train loss: 0.0063 test loss: 0.0190\n",
      "Epoch 7 batch 1500 train loss: 0.0076 test loss: 0.0187\n",
      "Epoch 7 batch 1600 train loss: 0.0074 test loss: 0.0185\n",
      "Epoch 7 batch 1700 train loss: 0.0072 test loss: 0.0184\n",
      "Epoch 7 batch 1800 train loss: 0.0079 test loss: 0.0172\n",
      "Epoch 7 batch 1900 train loss: 0.0061 test loss: 0.0177\n",
      "Epoch 7 batch 2000 train loss: 0.0070 test loss: 0.0198\n",
      "Epoch 7 batch 2100 train loss: 0.0065 test loss: 0.0188\n",
      "Epoch 7 batch 2200 train loss: 0.0083 test loss: 0.0172\n",
      "Epoch 7 batch 2300 train loss: 0.0064 test loss: 0.0203\n",
      "Epoch 7 batch 2400 train loss: 0.0075 test loss: 0.0198\n",
      "Epoch 7 batch 2500 train loss: 0.0073 test loss: 0.0191\n",
      "Epoch 7 batch 2600 train loss: 0.0065 test loss: 0.0170\n",
      "Epoch 7 batch 2700 train loss: 0.0077 test loss: 0.0194\n",
      "Epoch 7 batch 2800 train loss: 0.0071 test loss: 0.0172\n",
      "Epoch 7 batch 2900 train loss: 0.0068 test loss: 0.0152\n",
      "Epoch 8 batch 0 train loss: 0.0064 test loss: 0.0175\n",
      "Epoch 8 batch 100 train loss: 0.0092 test loss: 0.0168\n",
      "Epoch 8 batch 200 train loss: 0.0065 test loss: 0.0161\n",
      "Epoch 8 batch 300 train loss: 0.0063 test loss: 0.0162\n",
      "Epoch 8 batch 400 train loss: 0.0065 test loss: 0.0187\n",
      "Epoch 8 batch 500 train loss: 0.0087 test loss: 0.0198\n",
      "Epoch 8 batch 600 train loss: 0.0063 test loss: 0.0211\n",
      "Epoch 8 batch 700 train loss: 0.0043 test loss: 0.0184\n",
      "Epoch 8 batch 800 train loss: 0.0082 test loss: 0.0198\n",
      "Epoch 8 batch 900 train loss: 0.0070 test loss: 0.0187\n",
      "Epoch 8 batch 1000 train loss: 0.0068 test loss: 0.0198\n",
      "Epoch 8 batch 1100 train loss: 0.0073 test loss: 0.0203\n",
      "Epoch 8 batch 1200 train loss: 0.0069 test loss: 0.0182\n",
      "Epoch 8 batch 1300 train loss: 0.0079 test loss: 0.0174\n",
      "Epoch 8 batch 1400 train loss: 0.0080 test loss: 0.0181\n",
      "Epoch 8 batch 1500 train loss: 0.0062 test loss: 0.0193\n",
      "Epoch 8 batch 1600 train loss: 0.0084 test loss: 0.0168\n",
      "Epoch 8 batch 1700 train loss: 0.0068 test loss: 0.0190\n",
      "Epoch 8 batch 1800 train loss: 0.0073 test loss: 0.0161\n",
      "Epoch 8 batch 1900 train loss: 0.0063 test loss: 0.0195\n",
      "Epoch 8 batch 2000 train loss: 0.0085 test loss: 0.0204\n",
      "Epoch 8 batch 2100 train loss: 0.0078 test loss: 0.0174\n",
      "Epoch 8 batch 2200 train loss: 0.0061 test loss: 0.0169\n",
      "Epoch 8 batch 2300 train loss: 0.0071 test loss: 0.0191\n",
      "Epoch 8 batch 2400 train loss: 0.0059 test loss: 0.0189\n",
      "Epoch 8 batch 2500 train loss: 0.0096 test loss: 0.0202\n",
      "Epoch 8 batch 2600 train loss: 0.0051 test loss: 0.0151\n",
      "Epoch 8 batch 2700 train loss: 0.0074 test loss: 0.0175\n",
      "Epoch 8 batch 2800 train loss: 0.0079 test loss: 0.0200\n",
      "Epoch 8 batch 2900 train loss: 0.0064 test loss: 0.0174\n",
      "Epoch 9 batch 0 train loss: 0.0067 test loss: 0.0168\n",
      "Epoch 9 batch 100 train loss: 0.0068 test loss: 0.0197\n",
      "Epoch 9 batch 200 train loss: 0.0077 test loss: 0.0160\n",
      "Epoch 9 batch 300 train loss: 0.0060 test loss: 0.0160\n",
      "Epoch 9 batch 400 train loss: 0.0073 test loss: 0.0187\n",
      "Epoch 9 batch 500 train loss: 0.0089 test loss: 0.0200\n",
      "Epoch 9 batch 600 train loss: 0.0071 test loss: 0.0180\n",
      "Epoch 9 batch 700 train loss: 0.0074 test loss: 0.0170\n",
      "Epoch 9 batch 800 train loss: 0.0092 test loss: 0.0176\n",
      "Epoch 9 batch 900 train loss: 0.0081 test loss: 0.0171\n",
      "Epoch 9 batch 1000 train loss: 0.0071 test loss: 0.0175\n",
      "Epoch 9 batch 1100 train loss: 0.0079 test loss: 0.0177\n",
      "Epoch 9 batch 1200 train loss: 0.0096 test loss: 0.0199\n",
      "Epoch 9 batch 1300 train loss: 0.0084 test loss: 0.0182\n",
      "Epoch 9 batch 1400 train loss: 0.0074 test loss: 0.0160\n",
      "Epoch 9 batch 1500 train loss: 0.0064 test loss: 0.0189\n",
      "Epoch 9 batch 1600 train loss: 0.0071 test loss: 0.0187\n",
      "Epoch 9 batch 1700 train loss: 0.0111 test loss: 0.0193\n",
      "Epoch 9 batch 1800 train loss: 0.0060 test loss: 0.0182\n",
      "Epoch 9 batch 1900 train loss: 0.0074 test loss: 0.0189\n",
      "Epoch 9 batch 2000 train loss: 0.0062 test loss: 0.0209\n",
      "Epoch 9 batch 2100 train loss: 0.0078 test loss: 0.0186\n",
      "Epoch 9 batch 2200 train loss: 0.0055 test loss: 0.0187\n",
      "Epoch 9 batch 2300 train loss: 0.0101 test loss: 0.0179\n",
      "Epoch 9 batch 2400 train loss: 0.0069 test loss: 0.0248\n",
      "Epoch 9 batch 2500 train loss: 0.0074 test loss: 0.0181\n",
      "Epoch 9 batch 2600 train loss: 0.0073 test loss: 0.0175\n",
      "Epoch 9 batch 2700 train loss: 0.0075 test loss: 0.0204\n",
      "Epoch 9 batch 2800 train loss: 0.0065 test loss: 0.0168\n",
      "Epoch 9 batch 2900 train loss: 0.0070 test loss: 0.0182\n",
      "Epoch 10 batch 0 train loss: 0.0070 test loss: 0.0197\n",
      "Epoch 10 batch 100 train loss: 0.0069 test loss: 0.0182\n",
      "Epoch 10 batch 200 train loss: 0.0057 test loss: 0.0161\n",
      "Epoch 10 batch 300 train loss: 0.0069 test loss: 0.0190\n",
      "Epoch 10 batch 400 train loss: 0.0076 test loss: 0.0174\n",
      "Epoch 10 batch 500 train loss: 0.0056 test loss: 0.0172\n",
      "Epoch 10 batch 600 train loss: 0.0061 test loss: 0.0177\n",
      "Epoch 10 batch 700 train loss: 0.0078 test loss: 0.0189\n",
      "Epoch 10 batch 800 train loss: 0.0072 test loss: 0.0178\n",
      "Epoch 10 batch 900 train loss: 0.0100 test loss: 0.0172\n",
      "Epoch 10 batch 1000 train loss: 0.0057 test loss: 0.0193\n",
      "Epoch 10 batch 1100 train loss: 0.0055 test loss: 0.0196\n",
      "Epoch 10 batch 1200 train loss: 0.0102 test loss: 0.0175\n",
      "Epoch 10 batch 1300 train loss: 0.0079 test loss: 0.0188\n",
      "Epoch 10 batch 1400 train loss: 0.0089 test loss: 0.0162\n",
      "Epoch 10 batch 1500 train loss: 0.0085 test loss: 0.0176\n",
      "Epoch 10 batch 1600 train loss: 0.0073 test loss: 0.0212\n",
      "Epoch 10 batch 1700 train loss: 0.0071 test loss: 0.0211\n",
      "Epoch 10 batch 1800 train loss: 0.0084 test loss: 0.0161\n",
      "Epoch 10 batch 1900 train loss: 0.0074 test loss: 0.0184\n",
      "Epoch 10 batch 2000 train loss: 0.0064 test loss: 0.0226\n",
      "Epoch 10 batch 2100 train loss: 0.0099 test loss: 0.0169\n",
      "Epoch 10 batch 2200 train loss: 0.0078 test loss: 0.0196\n",
      "Epoch 10 batch 2300 train loss: 0.0100 test loss: 0.0162\n",
      "Epoch 10 batch 2400 train loss: 0.0072 test loss: 0.0200\n",
      "Epoch 10 batch 2500 train loss: 0.0045 test loss: 0.0187\n",
      "Epoch 10 batch 2600 train loss: 0.0068 test loss: 0.0191\n",
      "Epoch 10 batch 2700 train loss: 0.0066 test loss: 0.0172\n",
      "Epoch 10 batch 2800 train loss: 0.0076 test loss: 0.0190\n",
      "Epoch 10 batch 2900 train loss: 0.0064 test loss: 0.0171\n",
      "Epoch 11 batch 0 train loss: 0.0050 test loss: 0.0165\n",
      "Epoch 11 batch 100 train loss: 0.0073 test loss: 0.0196\n",
      "Epoch 11 batch 200 train loss: 0.0063 test loss: 0.0181\n",
      "Epoch 11 batch 300 train loss: 0.0077 test loss: 0.0143\n",
      "Epoch 11 batch 400 train loss: 0.0063 test loss: 0.0189\n",
      "Epoch 11 batch 500 train loss: 0.0083 test loss: 0.0187\n",
      "Epoch 11 batch 600 train loss: 0.0065 test loss: 0.0179\n",
      "Epoch 11 batch 700 train loss: 0.0057 test loss: 0.0171\n",
      "Epoch 11 batch 800 train loss: 0.0057 test loss: 0.0175\n",
      "Epoch 11 batch 900 train loss: 0.0065 test loss: 0.0166\n",
      "Epoch 11 batch 1000 train loss: 0.0080 test loss: 0.0173\n",
      "Epoch 11 batch 1100 train loss: 0.0099 test loss: 0.0192\n",
      "Epoch 11 batch 1200 train loss: 0.0063 test loss: 0.0152\n",
      "Epoch 11 batch 1300 train loss: 0.0089 test loss: 0.0175\n",
      "Epoch 11 batch 1400 train loss: 0.0086 test loss: 0.0182\n",
      "Epoch 11 batch 1500 train loss: 0.0070 test loss: 0.0207\n",
      "Epoch 11 batch 1600 train loss: 0.0067 test loss: 0.0179\n",
      "Epoch 11 batch 1700 train loss: 0.0075 test loss: 0.0187\n",
      "Epoch 11 batch 1800 train loss: 0.0088 test loss: 0.0188\n",
      "Epoch 11 batch 1900 train loss: 0.0056 test loss: 0.0187\n",
      "Epoch 11 batch 2000 train loss: 0.0096 test loss: 0.0176\n",
      "Epoch 11 batch 2100 train loss: 0.0066 test loss: 0.0170\n",
      "Epoch 11 batch 2200 train loss: 0.0073 test loss: 0.0179\n",
      "Epoch 11 batch 2300 train loss: 0.0094 test loss: 0.0182\n",
      "Epoch 11 batch 2400 train loss: 0.0066 test loss: 0.0162\n",
      "Epoch 11 batch 2500 train loss: 0.0081 test loss: 0.0174\n",
      "Epoch 11 batch 2600 train loss: 0.0061 test loss: 0.0165\n",
      "Epoch 11 batch 2700 train loss: 0.0072 test loss: 0.0179\n",
      "Epoch 11 batch 2800 train loss: 0.0067 test loss: 0.0189\n",
      "Epoch 11 batch 2900 train loss: 0.0093 test loss: 0.0172\n",
      "Epoch 12 batch 0 train loss: 0.0067 test loss: 0.0178\n",
      "Epoch 12 batch 100 train loss: 0.0082 test loss: 0.0195\n",
      "Epoch 12 batch 200 train loss: 0.0048 test loss: 0.0173\n",
      "Epoch 12 batch 300 train loss: 0.0075 test loss: 0.0171\n",
      "Epoch 12 batch 400 train loss: 0.0057 test loss: 0.0175\n",
      "Epoch 12 batch 500 train loss: 0.0068 test loss: 0.0177\n",
      "Epoch 12 batch 600 train loss: 0.0090 test loss: 0.0175\n",
      "Epoch 12 batch 700 train loss: 0.0074 test loss: 0.0163\n",
      "Epoch 12 batch 800 train loss: 0.0060 test loss: 0.0176\n",
      "Epoch 12 batch 900 train loss: 0.0071 test loss: 0.0162\n",
      "Epoch 12 batch 1000 train loss: 0.0065 test loss: 0.0183\n",
      "Epoch 12 batch 1100 train loss: 0.0076 test loss: 0.0179\n",
      "Epoch 12 batch 1200 train loss: 0.0088 test loss: 0.0207\n",
      "Epoch 12 batch 1300 train loss: 0.0062 test loss: 0.0194\n",
      "Epoch 12 batch 1400 train loss: 0.0065 test loss: 0.0176\n",
      "Epoch 12 batch 1500 train loss: 0.0065 test loss: 0.0202\n",
      "Epoch 12 batch 1600 train loss: 0.0070 test loss: 0.0177\n",
      "Epoch 12 batch 1700 train loss: 0.0068 test loss: 0.0166\n",
      "Epoch 12 batch 1800 train loss: 0.0092 test loss: 0.0170\n",
      "Epoch 12 batch 1900 train loss: 0.0065 test loss: 0.0188\n",
      "Epoch 12 batch 2000 train loss: 0.0072 test loss: 0.0165\n",
      "Epoch 12 batch 2100 train loss: 0.0073 test loss: 0.0195\n",
      "Epoch 12 batch 2200 train loss: 0.0087 test loss: 0.0176\n",
      "Epoch 12 batch 2300 train loss: 0.0082 test loss: 0.0204\n",
      "Epoch 12 batch 2400 train loss: 0.0074 test loss: 0.0195\n",
      "Epoch 12 batch 2500 train loss: 0.0068 test loss: 0.0183\n",
      "Epoch 12 batch 2600 train loss: 0.0084 test loss: 0.0142\n",
      "Epoch 12 batch 2700 train loss: 0.0065 test loss: 0.0180\n",
      "Epoch 12 batch 2800 train loss: 0.0097 test loss: 0.0166\n",
      "Epoch 12 batch 2900 train loss: 0.0066 test loss: 0.0193\n",
      "Epoch 13 batch 0 train loss: 0.0060 test loss: 0.0191\n",
      "Epoch 13 batch 100 train loss: 0.0069 test loss: 0.0170\n",
      "Epoch 13 batch 200 train loss: 0.0077 test loss: 0.0157\n",
      "Epoch 13 batch 300 train loss: 0.0058 test loss: 0.0172\n",
      "Epoch 13 batch 400 train loss: 0.0079 test loss: 0.0165\n",
      "Epoch 13 batch 500 train loss: 0.0072 test loss: 0.0188\n",
      "Epoch 13 batch 600 train loss: 0.0064 test loss: 0.0199\n",
      "Epoch 13 batch 700 train loss: 0.0066 test loss: 0.0170\n",
      "Epoch 13 batch 800 train loss: 0.0064 test loss: 0.0189\n",
      "Epoch 13 batch 900 train loss: 0.0070 test loss: 0.0167\n",
      "Epoch 13 batch 1000 train loss: 0.0073 test loss: 0.0211\n",
      "Epoch 13 batch 1100 train loss: 0.0078 test loss: 0.0226\n",
      "Epoch 13 batch 1200 train loss: 0.0084 test loss: 0.0173\n",
      "Epoch 13 batch 1300 train loss: 0.0062 test loss: 0.0169\n",
      "Epoch 13 batch 1400 train loss: 0.0072 test loss: 0.0167\n",
      "Epoch 13 batch 1500 train loss: 0.0074 test loss: 0.0183\n",
      "Epoch 13 batch 1600 train loss: 0.0065 test loss: 0.0192\n",
      "Epoch 13 batch 1700 train loss: 0.0063 test loss: 0.0192\n",
      "Epoch 13 batch 1800 train loss: 0.0072 test loss: 0.0189\n",
      "Epoch 13 batch 1900 train loss: 0.0066 test loss: 0.0187\n",
      "Epoch 13 batch 2000 train loss: 0.0083 test loss: 0.0182\n",
      "Epoch 13 batch 2100 train loss: 0.0073 test loss: 0.0177\n",
      "Epoch 13 batch 2200 train loss: 0.0068 test loss: 0.0195\n",
      "Epoch 13 batch 2300 train loss: 0.0088 test loss: 0.0189\n",
      "Epoch 13 batch 2400 train loss: 0.0092 test loss: 0.0208\n",
      "Epoch 13 batch 2500 train loss: 0.0072 test loss: 0.0149\n",
      "Epoch 13 batch 2600 train loss: 0.0088 test loss: 0.0176\n",
      "Epoch 13 batch 2700 train loss: 0.0082 test loss: 0.0190\n",
      "Epoch 13 batch 2800 train loss: 0.0079 test loss: 0.0162\n",
      "Epoch 13 batch 2900 train loss: 0.0081 test loss: 0.0188\n",
      "Epoch 14 batch 0 train loss: 0.0074 test loss: 0.0166\n",
      "Epoch 14 batch 100 train loss: 0.0059 test loss: 0.0190\n",
      "Epoch 14 batch 200 train loss: 0.0070 test loss: 0.0173\n",
      "Epoch 14 batch 300 train loss: 0.0080 test loss: 0.0171\n",
      "Epoch 14 batch 400 train loss: 0.0073 test loss: 0.0206\n",
      "Epoch 14 batch 500 train loss: 0.0075 test loss: 0.0190\n",
      "Epoch 14 batch 600 train loss: 0.0080 test loss: 0.0187\n",
      "Epoch 14 batch 700 train loss: 0.0057 test loss: 0.0214\n",
      "Epoch 14 batch 800 train loss: 0.0070 test loss: 0.0161\n",
      "Epoch 14 batch 900 train loss: 0.0069 test loss: 0.0160\n",
      "Epoch 14 batch 1000 train loss: 0.0082 test loss: 0.0163\n",
      "Epoch 14 batch 1100 train loss: 0.0065 test loss: 0.0175\n",
      "Epoch 14 batch 1200 train loss: 0.0076 test loss: 0.0147\n",
      "Epoch 14 batch 1300 train loss: 0.0077 test loss: 0.0166\n",
      "Epoch 14 batch 1400 train loss: 0.0070 test loss: 0.0177\n",
      "Epoch 14 batch 1500 train loss: 0.0077 test loss: 0.0165\n",
      "Epoch 14 batch 1600 train loss: 0.0056 test loss: 0.0186\n",
      "Epoch 14 batch 1700 train loss: 0.0069 test loss: 0.0196\n",
      "Epoch 14 batch 1800 train loss: 0.0075 test loss: 0.0171\n",
      "Epoch 14 batch 1900 train loss: 0.0067 test loss: 0.0192\n",
      "Epoch 14 batch 2000 train loss: 0.0066 test loss: 0.0186\n",
      "Epoch 14 batch 2100 train loss: 0.0067 test loss: 0.0185\n",
      "Epoch 14 batch 2200 train loss: 0.0062 test loss: 0.0157\n",
      "Epoch 14 batch 2300 train loss: 0.0077 test loss: 0.0207\n",
      "Epoch 14 batch 2400 train loss: 0.0074 test loss: 0.0198\n",
      "Epoch 14 batch 2500 train loss: 0.0081 test loss: 0.0209\n",
      "Epoch 14 batch 2600 train loss: 0.0072 test loss: 0.0193\n",
      "Epoch 14 batch 2700 train loss: 0.0081 test loss: 0.0180\n",
      "Epoch 14 batch 2800 train loss: 0.0075 test loss: 0.0174\n",
      "Epoch 14 batch 2900 train loss: 0.0075 test loss: 0.0157\n",
      "Epoch 15 batch 0 train loss: 0.0082 test loss: 0.0164\n",
      "Epoch 15 batch 100 train loss: 0.0082 test loss: 0.0202\n",
      "Epoch 15 batch 200 train loss: 0.0105 test loss: 0.0152\n",
      "Epoch 15 batch 300 train loss: 0.0069 test loss: 0.0177\n",
      "Epoch 15 batch 400 train loss: 0.0059 test loss: 0.0188\n",
      "Epoch 15 batch 500 train loss: 0.0077 test loss: 0.0197\n",
      "Epoch 15 batch 600 train loss: 0.0084 test loss: 0.0176\n",
      "Epoch 15 batch 700 train loss: 0.0051 test loss: 0.0185\n",
      "Epoch 15 batch 800 train loss: 0.0076 test loss: 0.0189\n",
      "Epoch 15 batch 900 train loss: 0.0075 test loss: 0.0165\n",
      "Epoch 15 batch 1000 train loss: 0.0069 test loss: 0.0178\n",
      "Epoch 15 batch 1100 train loss: 0.0072 test loss: 0.0178\n",
      "Epoch 15 batch 1200 train loss: 0.0062 test loss: 0.0189\n",
      "Epoch 15 batch 1300 train loss: 0.0084 test loss: 0.0170\n",
      "Epoch 15 batch 1400 train loss: 0.0069 test loss: 0.0176\n",
      "Epoch 15 batch 1500 train loss: 0.0091 test loss: 0.0173\n",
      "Epoch 15 batch 1600 train loss: 0.0093 test loss: 0.0198\n",
      "Epoch 15 batch 1700 train loss: 0.0068 test loss: 0.0159\n",
      "Epoch 15 batch 1800 train loss: 0.0082 test loss: 0.0153\n",
      "Epoch 15 batch 1900 train loss: 0.0085 test loss: 0.0176\n",
      "Epoch 15 batch 2000 train loss: 0.0079 test loss: 0.0168\n",
      "Epoch 15 batch 2100 train loss: 0.0073 test loss: 0.0167\n",
      "Epoch 15 batch 2200 train loss: 0.0052 test loss: 0.0196\n",
      "Epoch 15 batch 2300 train loss: 0.0059 test loss: 0.0182\n",
      "Epoch 15 batch 2400 train loss: 0.0055 test loss: 0.0197\n",
      "Epoch 15 batch 2500 train loss: 0.0127 test loss: 0.0185\n",
      "Epoch 15 batch 2600 train loss: 0.0073 test loss: 0.0177\n",
      "Epoch 15 batch 2700 train loss: 0.0081 test loss: 0.0186\n",
      "Epoch 15 batch 2800 train loss: 0.0083 test loss: 0.0175\n",
      "Epoch 15 batch 2900 train loss: 0.0114 test loss: 0.0170\n",
      "Epoch 16 batch 0 train loss: 0.0066 test loss: 0.0143\n",
      "Epoch 16 batch 100 train loss: 0.0075 test loss: 0.0185\n",
      "Epoch 16 batch 200 train loss: 0.0105 test loss: 0.0164\n",
      "Epoch 16 batch 300 train loss: 0.0089 test loss: 0.0160\n",
      "Epoch 16 batch 400 train loss: 0.0065 test loss: 0.0180\n",
      "Epoch 16 batch 500 train loss: 0.0066 test loss: 0.0166\n",
      "Epoch 16 batch 600 train loss: 0.0056 test loss: 0.0179\n",
      "Epoch 16 batch 700 train loss: 0.0104 test loss: 0.0181\n",
      "Epoch 16 batch 800 train loss: 0.0065 test loss: 0.0181\n",
      "Epoch 16 batch 900 train loss: 0.0069 test loss: 0.0182\n",
      "Epoch 16 batch 1000 train loss: 0.0088 test loss: 0.0189\n",
      "Epoch 16 batch 1100 train loss: 0.0071 test loss: 0.0189\n",
      "Epoch 16 batch 1200 train loss: 0.0111 test loss: 0.0185\n",
      "Epoch 16 batch 1300 train loss: 0.0076 test loss: 0.0194\n",
      "Epoch 16 batch 1400 train loss: 0.0057 test loss: 0.0169\n",
      "Epoch 16 batch 1500 train loss: 0.0067 test loss: 0.0201\n",
      "Epoch 16 batch 1600 train loss: 0.0085 test loss: 0.0166\n",
      "Epoch 16 batch 1700 train loss: 0.0053 test loss: 0.0199\n",
      "Epoch 16 batch 1800 train loss: 0.0054 test loss: 0.0179\n",
      "Epoch 16 batch 1900 train loss: 0.0084 test loss: 0.0196\n",
      "Epoch 16 batch 2000 train loss: 0.0064 test loss: 0.0203\n",
      "Epoch 16 batch 2100 train loss: 0.0073 test loss: 0.0207\n",
      "Epoch 16 batch 2200 train loss: 0.0066 test loss: 0.0202\n",
      "Epoch 16 batch 2300 train loss: 0.0081 test loss: 0.0182\n",
      "Epoch 16 batch 2400 train loss: 0.0094 test loss: 0.0187\n",
      "Epoch 16 batch 2500 train loss: 0.0057 test loss: 0.0174\n",
      "Epoch 16 batch 2600 train loss: 0.0078 test loss: 0.0170\n",
      "Epoch 16 batch 2700 train loss: 0.0071 test loss: 0.0163\n",
      "Epoch 16 batch 2800 train loss: 0.0068 test loss: 0.0198\n",
      "Epoch 16 batch 2900 train loss: 0.0093 test loss: 0.0189\n",
      "Epoch 17 batch 0 train loss: 0.0079 test loss: 0.0172\n",
      "Epoch 17 batch 100 train loss: 0.0056 test loss: 0.0175\n",
      "Epoch 17 batch 200 train loss: 0.0100 test loss: 0.0190\n",
      "Epoch 17 batch 300 train loss: 0.0060 test loss: 0.0157\n",
      "Epoch 17 batch 400 train loss: 0.0066 test loss: 0.0182\n",
      "Epoch 17 batch 500 train loss: 0.0059 test loss: 0.0170\n",
      "Epoch 17 batch 600 train loss: 0.0094 test loss: 0.0181\n",
      "Epoch 17 batch 700 train loss: 0.0065 test loss: 0.0193\n",
      "early stop.\n",
      "Checkpoint 18 restored!!\n",
      "Training for loss rate 0.95 start.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['bi_lstm_9/bi_lstm_block_18/bidirectional_18/forward_lstm_18/lstm_cell_55/kernel:0', 'bi_lstm_9/bi_lstm_block_18/bidirectional_18/forward_lstm_18/lstm_cell_55/recurrent_kernel:0', 'bi_lstm_9/bi_lstm_block_18/bidirectional_18/forward_lstm_18/lstm_cell_55/bias:0', 'bi_lstm_9/bi_lstm_block_18/bidirectional_18/backward_lstm_18/lstm_cell_56/kernel:0', 'bi_lstm_9/bi_lstm_block_18/bidirectional_18/backward_lstm_18/lstm_cell_56/recurrent_kernel:0', 'bi_lstm_9/bi_lstm_block_18/bidirectional_18/backward_lstm_18/lstm_cell_56/bias:0', 'bi_lstm_9/bi_lstm_block_18/layer_normalization_18/gamma:0', 'bi_lstm_9/bi_lstm_block_18/layer_normalization_18/beta:0', 'bi_lstm_9/bi_lstm_block_19/bidirectional_19/forward_lstm_19/lstm_cell_58/kernel:0', 'bi_lstm_9/bi_lstm_block_19/bidirectional_19/forward_lstm_19/lstm_cell_58/recurrent_kernel:0', 'bi_lstm_9/bi_lstm_block_19/bidirectional_19/forward_lstm_19/lstm_cell_58/bias:0', 'bi_lstm_9/bi_lstm_block_19/bidirectional_19/backward_lstm_19/lstm_cell_59/kernel:0', 'bi_lstm_9/bi_lstm_block_19/bidirectional_19/backward_lstm_19/lstm_cell_59/recurrent_kernel:0', 'bi_lstm_9/bi_lstm_block_19/bidirectional_19/backward_lstm_19/lstm_cell_59/bias:0', 'bi_lstm_9/bi_lstm_block_19/layer_normalization_19/gamma:0', 'bi_lstm_9/bi_lstm_block_19/layer_normalization_19/beta:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['bi_lstm_9/bi_lstm_block_18/bidirectional_18/forward_lstm_18/lstm_cell_55/kernel:0', 'bi_lstm_9/bi_lstm_block_18/bidirectional_18/forward_lstm_18/lstm_cell_55/recurrent_kernel:0', 'bi_lstm_9/bi_lstm_block_18/bidirectional_18/forward_lstm_18/lstm_cell_55/bias:0', 'bi_lstm_9/bi_lstm_block_18/bidirectional_18/backward_lstm_18/lstm_cell_56/kernel:0', 'bi_lstm_9/bi_lstm_block_18/bidirectional_18/backward_lstm_18/lstm_cell_56/recurrent_kernel:0', 'bi_lstm_9/bi_lstm_block_18/bidirectional_18/backward_lstm_18/lstm_cell_56/bias:0', 'bi_lstm_9/bi_lstm_block_18/layer_normalization_18/gamma:0', 'bi_lstm_9/bi_lstm_block_18/layer_normalization_18/beta:0', 'bi_lstm_9/bi_lstm_block_19/bidirectional_19/forward_lstm_19/lstm_cell_58/kernel:0', 'bi_lstm_9/bi_lstm_block_19/bidirectional_19/forward_lstm_19/lstm_cell_58/recurrent_kernel:0', 'bi_lstm_9/bi_lstm_block_19/bidirectional_19/forward_lstm_19/lstm_cell_58/bias:0', 'bi_lstm_9/bi_lstm_block_19/bidirectional_19/backward_lstm_19/lstm_cell_59/kernel:0', 'bi_lstm_9/bi_lstm_block_19/bidirectional_19/backward_lstm_19/lstm_cell_59/recurrent_kernel:0', 'bi_lstm_9/bi_lstm_block_19/bidirectional_19/backward_lstm_19/lstm_cell_59/bias:0', 'bi_lstm_9/bi_lstm_block_19/layer_normalization_19/gamma:0', 'bi_lstm_9/bi_lstm_block_19/layer_normalization_19/beta:0'] when minimizing the loss.\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.95p/ckpt-1\n",
      "Epoch 0 batch 0 train loss: 0.6316 test loss: 1.5323\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.95p/ckpt-2\n",
      "Epoch 0 batch 100 train loss: 0.2599 test loss: 0.3772\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.95p/ckpt-3\n",
      "Epoch 0 batch 200 train loss: 0.1348 test loss: 0.1312\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.95p/ckpt-4\n",
      "Epoch 0 batch 300 train loss: 0.0674 test loss: 0.0592\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.95p/ckpt-5\n",
      "Epoch 0 batch 400 train loss: 0.0435 test loss: 0.0408\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.95p/ckpt-6\n",
      "Epoch 0 batch 500 train loss: 0.0299 test loss: 0.0328\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.95p/ckpt-7\n",
      "Epoch 0 batch 600 train loss: 0.0243 test loss: 0.0287\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.95p/ckpt-8\n",
      "Epoch 0 batch 700 train loss: 0.0201 test loss: 0.0282\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.95p/ckpt-9\n",
      "Epoch 0 batch 800 train loss: 0.0171 test loss: 0.0251\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.95p/ckpt-10\n",
      "Epoch 0 batch 900 train loss: 0.0144 test loss: 0.0238\n",
      "Epoch 0 batch 1000 train loss: 0.0139 test loss: 0.0239\n",
      "Epoch 0 batch 1100 train loss: 0.0141 test loss: 0.0246\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.95p/ckpt-11\n",
      "Epoch 0 batch 1200 train loss: 0.0117 test loss: 0.0221\n",
      "Epoch 0 batch 1300 train loss: 0.0116 test loss: 0.0232\n",
      "Epoch 0 batch 1400 train loss: 0.0095 test loss: 0.0222\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.95p/ckpt-12\n",
      "Epoch 0 batch 1500 train loss: 0.0089 test loss: 0.0218\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.95p/ckpt-13\n",
      "Epoch 0 batch 1600 train loss: 0.0095 test loss: 0.0208\n",
      "Epoch 0 batch 1700 train loss: 0.0079 test loss: 0.0213\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.95p/ckpt-14\n",
      "Epoch 0 batch 1800 train loss: 0.0094 test loss: 0.0205\n",
      "Epoch 0 batch 1900 train loss: 0.0080 test loss: 0.0208\n",
      "Epoch 0 batch 2000 train loss: 0.0097 test loss: 0.0219\n",
      "Epoch 0 batch 2100 train loss: 0.0096 test loss: 0.0214\n",
      "Epoch 0 batch 2200 train loss: 0.0079 test loss: 0.0219\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.95p/ckpt-15\n",
      "Epoch 0 batch 2300 train loss: 0.0095 test loss: 0.0196\n",
      "Epoch 0 batch 2400 train loss: 0.0078 test loss: 0.0226\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.95p/ckpt-16\n",
      "Epoch 0 batch 2500 train loss: 0.0077 test loss: 0.0191\n",
      "Epoch 0 batch 2600 train loss: 0.0089 test loss: 0.0202\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.95p/ckpt-17\n",
      "Epoch 0 batch 2700 train loss: 0.0070 test loss: 0.0187\n",
      "Epoch 0 batch 2800 train loss: 0.0071 test loss: 0.0193\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.95p/ckpt-18\n",
      "Epoch 0 batch 2900 train loss: 0.0095 test loss: 0.0179\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['bi_lstm_9/bi_lstm_block_18/bidirectional_18/forward_lstm_18/lstm_cell_55/kernel:0', 'bi_lstm_9/bi_lstm_block_18/bidirectional_18/forward_lstm_18/lstm_cell_55/recurrent_kernel:0', 'bi_lstm_9/bi_lstm_block_18/bidirectional_18/forward_lstm_18/lstm_cell_55/bias:0', 'bi_lstm_9/bi_lstm_block_18/bidirectional_18/backward_lstm_18/lstm_cell_56/kernel:0', 'bi_lstm_9/bi_lstm_block_18/bidirectional_18/backward_lstm_18/lstm_cell_56/recurrent_kernel:0', 'bi_lstm_9/bi_lstm_block_18/bidirectional_18/backward_lstm_18/lstm_cell_56/bias:0', 'bi_lstm_9/bi_lstm_block_18/layer_normalization_18/gamma:0', 'bi_lstm_9/bi_lstm_block_18/layer_normalization_18/beta:0', 'bi_lstm_9/bi_lstm_block_19/bidirectional_19/forward_lstm_19/lstm_cell_58/kernel:0', 'bi_lstm_9/bi_lstm_block_19/bidirectional_19/forward_lstm_19/lstm_cell_58/recurrent_kernel:0', 'bi_lstm_9/bi_lstm_block_19/bidirectional_19/forward_lstm_19/lstm_cell_58/bias:0', 'bi_lstm_9/bi_lstm_block_19/bidirectional_19/backward_lstm_19/lstm_cell_59/kernel:0', 'bi_lstm_9/bi_lstm_block_19/bidirectional_19/backward_lstm_19/lstm_cell_59/recurrent_kernel:0', 'bi_lstm_9/bi_lstm_block_19/bidirectional_19/backward_lstm_19/lstm_cell_59/bias:0', 'bi_lstm_9/bi_lstm_block_19/layer_normalization_19/gamma:0', 'bi_lstm_9/bi_lstm_block_19/layer_normalization_19/beta:0'] when minimizing the loss.\n",
      "Epoch 1 batch 0 train loss: 0.0101 test loss: 0.0190\n",
      "Epoch 1 batch 100 train loss: 0.0065 test loss: 0.0194\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.95p/ckpt-19\n",
      "Epoch 1 batch 200 train loss: 0.0063 test loss: 0.0178\n",
      "Epoch 1 batch 300 train loss: 0.0091 test loss: 0.0203\n",
      "Epoch 1 batch 400 train loss: 0.0092 test loss: 0.0206\n",
      "Epoch 1 batch 500 train loss: 0.0077 test loss: 0.0193\n",
      "Epoch 1 batch 600 train loss: 0.0109 test loss: 0.0199\n",
      "Epoch 1 batch 700 train loss: 0.0085 test loss: 0.0212\n",
      "Epoch 1 batch 800 train loss: 0.0082 test loss: 0.0198\n",
      "Epoch 1 batch 900 train loss: 0.0078 test loss: 0.0196\n",
      "Epoch 1 batch 1000 train loss: 0.0076 test loss: 0.0194\n",
      "Epoch 1 batch 1100 train loss: 0.0066 test loss: 0.0210\n",
      "Epoch 1 batch 1200 train loss: 0.0083 test loss: 0.0196\n",
      "Epoch 1 batch 1300 train loss: 0.0078 test loss: 0.0213\n",
      "Epoch 1 batch 1400 train loss: 0.0076 test loss: 0.0192\n",
      "Epoch 1 batch 1500 train loss: 0.0070 test loss: 0.0179\n",
      "Epoch 1 batch 1600 train loss: 0.0103 test loss: 0.0208\n",
      "Epoch 1 batch 1700 train loss: 0.0062 test loss: 0.0192\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.95p/ckpt-20\n",
      "Epoch 1 batch 1800 train loss: 0.0087 test loss: 0.0172\n",
      "Epoch 1 batch 1900 train loss: 0.0079 test loss: 0.0204\n",
      "Epoch 1 batch 2000 train loss: 0.0092 test loss: 0.0214\n",
      "Epoch 1 batch 2100 train loss: 0.0072 test loss: 0.0224\n",
      "Epoch 1 batch 2200 train loss: 0.0069 test loss: 0.0179\n",
      "Epoch 1 batch 2300 train loss: 0.0085 test loss: 0.0248\n",
      "Epoch 1 batch 2400 train loss: 0.0065 test loss: 0.0203\n",
      "Epoch 1 batch 2500 train loss: 0.0074 test loss: 0.0223\n",
      "Epoch 1 batch 2600 train loss: 0.0068 test loss: 0.0177\n",
      "Epoch 1 batch 2700 train loss: 0.0073 test loss: 0.0200\n",
      "Epoch 1 batch 2800 train loss: 0.0100 test loss: 0.0182\n",
      "Epoch 1 batch 2900 train loss: 0.0090 test loss: 0.0195\n",
      "Epoch 2 batch 0 train loss: 0.0079 test loss: 0.0202\n",
      "Epoch 2 batch 100 train loss: 0.0065 test loss: 0.0217\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.95p/ckpt-21\n",
      "Epoch 2 batch 200 train loss: 0.0079 test loss: 0.0168\n",
      "Epoch 2 batch 300 train loss: 0.0066 test loss: 0.0203\n",
      "Epoch 2 batch 400 train loss: 0.0076 test loss: 0.0187\n",
      "Epoch 2 batch 500 train loss: 0.0061 test loss: 0.0198\n",
      "Epoch 2 batch 600 train loss: 0.0070 test loss: 0.0194\n",
      "Epoch 2 batch 700 train loss: 0.0075 test loss: 0.0188\n",
      "Epoch 2 batch 800 train loss: 0.0088 test loss: 0.0170\n",
      "Epoch 2 batch 900 train loss: 0.0069 test loss: 0.0208\n",
      "Epoch 2 batch 1000 train loss: 0.0084 test loss: 0.0183\n",
      "Epoch 2 batch 1100 train loss: 0.0091 test loss: 0.0212\n",
      "Epoch 2 batch 1200 train loss: 0.0106 test loss: 0.0196\n",
      "Epoch 2 batch 1300 train loss: 0.0058 test loss: 0.0181\n",
      "Epoch 2 batch 1400 train loss: 0.0105 test loss: 0.0203\n",
      "Epoch 2 batch 1500 train loss: 0.0058 test loss: 0.0183\n",
      "Epoch 2 batch 1600 train loss: 0.0090 test loss: 0.0207\n",
      "Epoch 2 batch 1700 train loss: 0.0070 test loss: 0.0206\n",
      "Epoch 2 batch 1800 train loss: 0.0066 test loss: 0.0214\n",
      "Epoch 2 batch 1900 train loss: 0.0076 test loss: 0.0201\n",
      "Epoch 2 batch 2000 train loss: 0.0045 test loss: 0.0206\n",
      "Epoch 2 batch 2100 train loss: 0.0078 test loss: 0.0209\n",
      "Epoch 2 batch 2200 train loss: 0.0083 test loss: 0.0214\n",
      "Epoch 2 batch 2300 train loss: 0.0065 test loss: 0.0204\n",
      "Epoch 2 batch 2400 train loss: 0.0074 test loss: 0.0209\n",
      "Epoch 2 batch 2500 train loss: 0.0110 test loss: 0.0219\n",
      "Epoch 2 batch 2600 train loss: 0.0063 test loss: 0.0201\n",
      "Epoch 2 batch 2700 train loss: 0.0064 test loss: 0.0244\n",
      "Epoch 2 batch 2800 train loss: 0.0060 test loss: 0.0179\n",
      "Epoch 2 batch 2900 train loss: 0.0063 test loss: 0.0170\n",
      "Epoch 3 batch 0 train loss: 0.0074 test loss: 0.0183\n",
      "Epoch 3 batch 100 train loss: 0.0069 test loss: 0.0202\n",
      "Epoch 3 batch 200 train loss: 0.0077 test loss: 0.0192\n",
      "Epoch 3 batch 300 train loss: 0.0065 test loss: 0.0178\n",
      "Epoch 3 batch 400 train loss: 0.0060 test loss: 0.0182\n",
      "Epoch 3 batch 500 train loss: 0.0092 test loss: 0.0215\n",
      "Epoch 3 batch 600 train loss: 0.0106 test loss: 0.0211\n",
      "Epoch 3 batch 700 train loss: 0.0073 test loss: 0.0189\n",
      "Epoch 3 batch 800 train loss: 0.0076 test loss: 0.0214\n",
      "Epoch 3 batch 900 train loss: 0.0077 test loss: 0.0181\n",
      "Epoch 3 batch 1000 train loss: 0.0069 test loss: 0.0220\n",
      "Epoch 3 batch 1100 train loss: 0.0089 test loss: 0.0197\n",
      "Epoch 3 batch 1200 train loss: 0.0068 test loss: 0.0203\n",
      "Epoch 3 batch 1300 train loss: 0.0089 test loss: 0.0212\n",
      "Epoch 3 batch 1400 train loss: 0.0065 test loss: 0.0202\n",
      "Epoch 3 batch 1500 train loss: 0.0077 test loss: 0.0175\n",
      "Epoch 3 batch 1600 train loss: 0.0062 test loss: 0.0202\n",
      "Epoch 3 batch 1700 train loss: 0.0088 test loss: 0.0205\n",
      "Epoch 3 batch 1800 train loss: 0.0088 test loss: 0.0197\n",
      "Epoch 3 batch 1900 train loss: 0.0077 test loss: 0.0171\n",
      "Epoch 3 batch 2000 train loss: 0.0063 test loss: 0.0211\n",
      "Epoch 3 batch 2100 train loss: 0.0064 test loss: 0.0201\n",
      "Epoch 3 batch 2200 train loss: 0.0067 test loss: 0.0211\n",
      "Epoch 3 batch 2300 train loss: 0.0093 test loss: 0.0194\n",
      "Epoch 3 batch 2400 train loss: 0.0058 test loss: 0.0229\n",
      "Epoch 3 batch 2500 train loss: 0.0075 test loss: 0.0183\n",
      "Epoch 3 batch 2600 train loss: 0.0069 test loss: 0.0195\n",
      "Epoch 3 batch 2700 train loss: 0.0067 test loss: 0.0200\n",
      "Epoch 3 batch 2800 train loss: 0.0061 test loss: 0.0196\n",
      "Epoch 3 batch 2900 train loss: 0.0076 test loss: 0.0210\n",
      "Epoch 4 batch 0 train loss: 0.0061 test loss: 0.0185\n",
      "Epoch 4 batch 100 train loss: 0.0083 test loss: 0.0177\n",
      "Epoch 4 batch 200 train loss: 0.0064 test loss: 0.0189\n",
      "Epoch 4 batch 300 train loss: 0.0072 test loss: 0.0215\n",
      "Epoch 4 batch 400 train loss: 0.0076 test loss: 0.0192\n",
      "Epoch 4 batch 500 train loss: 0.0057 test loss: 0.0208\n",
      "Epoch 4 batch 600 train loss: 0.0097 test loss: 0.0181\n",
      "Epoch 4 batch 700 train loss: 0.0093 test loss: 0.0177\n",
      "Epoch 4 batch 800 train loss: 0.0074 test loss: 0.0197\n",
      "Epoch 4 batch 900 train loss: 0.0077 test loss: 0.0179\n",
      "Epoch 4 batch 1000 train loss: 0.0058 test loss: 0.0176\n",
      "Epoch 4 batch 1100 train loss: 0.0095 test loss: 0.0191\n",
      "Epoch 4 batch 1200 train loss: 0.0092 test loss: 0.0198\n",
      "Epoch 4 batch 1300 train loss: 0.0069 test loss: 0.0200\n",
      "Epoch 4 batch 1400 train loss: 0.0083 test loss: 0.0231\n",
      "Epoch 4 batch 1500 train loss: 0.0082 test loss: 0.0177\n",
      "Epoch 4 batch 1600 train loss: 0.0105 test loss: 0.0189\n",
      "Epoch 4 batch 1700 train loss: 0.0093 test loss: 0.0203\n",
      "Epoch 4 batch 1800 train loss: 0.0075 test loss: 0.0194\n",
      "Epoch 4 batch 1900 train loss: 0.0060 test loss: 0.0201\n",
      "Epoch 4 batch 2000 train loss: 0.0060 test loss: 0.0207\n",
      "Epoch 4 batch 2100 train loss: 0.0090 test loss: 0.0199\n",
      "Epoch 4 batch 2200 train loss: 0.0060 test loss: 0.0216\n",
      "Epoch 4 batch 2300 train loss: 0.0072 test loss: 0.0214\n",
      "Epoch 4 batch 2400 train loss: 0.0079 test loss: 0.0196\n",
      "Epoch 4 batch 2500 train loss: 0.0052 test loss: 0.0196\n",
      "Epoch 4 batch 2600 train loss: 0.0082 test loss: 0.0196\n",
      "Epoch 4 batch 2700 train loss: 0.0079 test loss: 0.0210\n",
      "Epoch 4 batch 2800 train loss: 0.0067 test loss: 0.0238\n",
      "Epoch 4 batch 2900 train loss: 0.0092 test loss: 0.0201\n",
      "Epoch 5 batch 0 train loss: 0.0069 test loss: 0.0186\n",
      "Epoch 5 batch 100 train loss: 0.0084 test loss: 0.0230\n",
      "Epoch 5 batch 200 train loss: 0.0083 test loss: 0.0177\n",
      "Epoch 5 batch 300 train loss: 0.0081 test loss: 0.0197\n",
      "Epoch 5 batch 400 train loss: 0.0074 test loss: 0.0232\n",
      "Epoch 5 batch 500 train loss: 0.0080 test loss: 0.0181\n",
      "Epoch 5 batch 600 train loss: 0.0072 test loss: 0.0183\n",
      "Epoch 5 batch 700 train loss: 0.0086 test loss: 0.0197\n",
      "Epoch 5 batch 800 train loss: 0.0051 test loss: 0.0201\n",
      "Epoch 5 batch 900 train loss: 0.0061 test loss: 0.0193\n",
      "Epoch 5 batch 1000 train loss: 0.0079 test loss: 0.0225\n",
      "Epoch 5 batch 1100 train loss: 0.0079 test loss: 0.0193\n",
      "Epoch 5 batch 1200 train loss: 0.0065 test loss: 0.0172\n",
      "Epoch 5 batch 1300 train loss: 0.0074 test loss: 0.0217\n",
      "Epoch 5 batch 1400 train loss: 0.0091 test loss: 0.0189\n",
      "Epoch 5 batch 1500 train loss: 0.0071 test loss: 0.0185\n",
      "Epoch 5 batch 1600 train loss: 0.0073 test loss: 0.0186\n",
      "Epoch 5 batch 1700 train loss: 0.0055 test loss: 0.0215\n",
      "Epoch 5 batch 1800 train loss: 0.0076 test loss: 0.0168\n",
      "Epoch 5 batch 1900 train loss: 0.0062 test loss: 0.0216\n",
      "Epoch 5 batch 2000 train loss: 0.0067 test loss: 0.0230\n",
      "Epoch 5 batch 2100 train loss: 0.0076 test loss: 0.0205\n",
      "Epoch 5 batch 2200 train loss: 0.0074 test loss: 0.0238\n",
      "Epoch 5 batch 2300 train loss: 0.0086 test loss: 0.0189\n",
      "Epoch 5 batch 2400 train loss: 0.0066 test loss: 0.0205\n",
      "Epoch 5 batch 2500 train loss: 0.0110 test loss: 0.0195\n",
      "Epoch 5 batch 2600 train loss: 0.0076 test loss: 0.0211\n",
      "Epoch 5 batch 2700 train loss: 0.0062 test loss: 0.0181\n",
      "Epoch 5 batch 2800 train loss: 0.0068 test loss: 0.0188\n",
      "Epoch 5 batch 2900 train loss: 0.0073 test loss: 0.0210\n",
      "Epoch 6 batch 0 train loss: 0.0061 test loss: 0.0172\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.95p/ckpt-22\n",
      "Epoch 6 batch 100 train loss: 0.0063 test loss: 0.0168\n",
      "Epoch 6 batch 200 train loss: 0.0066 test loss: 0.0181\n",
      "Epoch 6 batch 300 train loss: 0.0088 test loss: 0.0189\n",
      "Epoch 6 batch 400 train loss: 0.0088 test loss: 0.0196\n",
      "Epoch 6 batch 500 train loss: 0.0082 test loss: 0.0184\n",
      "Epoch 6 batch 600 train loss: 0.0084 test loss: 0.0215\n",
      "Epoch 6 batch 700 train loss: 0.0096 test loss: 0.0177\n",
      "Epoch 6 batch 800 train loss: 0.0068 test loss: 0.0196\n",
      "Epoch 6 batch 900 train loss: 0.0076 test loss: 0.0194\n",
      "Epoch 6 batch 1000 train loss: 0.0089 test loss: 0.0205\n",
      "Epoch 6 batch 1100 train loss: 0.0066 test loss: 0.0184\n",
      "Epoch 6 batch 1200 train loss: 0.0080 test loss: 0.0189\n",
      "Epoch 6 batch 1300 train loss: 0.0083 test loss: 0.0198\n",
      "Epoch 6 batch 1400 train loss: 0.0078 test loss: 0.0209\n",
      "Epoch 6 batch 1500 train loss: 0.0065 test loss: 0.0181\n",
      "Epoch 6 batch 1600 train loss: 0.0068 test loss: 0.0200\n",
      "Epoch 6 batch 1700 train loss: 0.0075 test loss: 0.0191\n",
      "Epoch 6 batch 1800 train loss: 0.0080 test loss: 0.0189\n",
      "Epoch 6 batch 1900 train loss: 0.0062 test loss: 0.0191\n",
      "Epoch 6 batch 2000 train loss: 0.0071 test loss: 0.0227\n",
      "Epoch 6 batch 2100 train loss: 0.0083 test loss: 0.0198\n",
      "Epoch 6 batch 2200 train loss: 0.0081 test loss: 0.0188\n",
      "Epoch 6 batch 2300 train loss: 0.0091 test loss: 0.0211\n",
      "Epoch 6 batch 2400 train loss: 0.0086 test loss: 0.0239\n",
      "Epoch 6 batch 2500 train loss: 0.0078 test loss: 0.0190\n",
      "Epoch 6 batch 2600 train loss: 0.0060 test loss: 0.0194\n",
      "Epoch 6 batch 2700 train loss: 0.0087 test loss: 0.0192\n",
      "Epoch 6 batch 2800 train loss: 0.0081 test loss: 0.0190\n",
      "Saving checkpoint at ./checkpoints/RNN_best_mtr_loss_0.95p/ckpt-23\n",
      "Epoch 6 batch 2900 train loss: 0.0054 test loss: 0.0151\n",
      "Epoch 7 batch 0 train loss: 0.0089 test loss: 0.0191\n",
      "Epoch 7 batch 100 train loss: 0.0094 test loss: 0.0202\n",
      "Epoch 7 batch 200 train loss: 0.0076 test loss: 0.0191\n",
      "Epoch 7 batch 300 train loss: 0.0065 test loss: 0.0204\n",
      "Epoch 7 batch 400 train loss: 0.0075 test loss: 0.0185\n",
      "Epoch 7 batch 500 train loss: 0.0098 test loss: 0.0195\n",
      "Epoch 7 batch 600 train loss: 0.0081 test loss: 0.0191\n",
      "Epoch 7 batch 700 train loss: 0.0084 test loss: 0.0202\n",
      "Epoch 7 batch 800 train loss: 0.0069 test loss: 0.0188\n",
      "Epoch 7 batch 900 train loss: 0.0071 test loss: 0.0197\n",
      "Epoch 7 batch 1000 train loss: 0.0081 test loss: 0.0190\n",
      "Epoch 7 batch 1100 train loss: 0.0062 test loss: 0.0175\n",
      "Epoch 7 batch 1200 train loss: 0.0074 test loss: 0.0175\n",
      "Epoch 7 batch 1300 train loss: 0.0068 test loss: 0.0236\n",
      "Epoch 7 batch 1400 train loss: 0.0075 test loss: 0.0179\n",
      "Epoch 7 batch 1500 train loss: 0.0068 test loss: 0.0209\n",
      "Epoch 7 batch 1600 train loss: 0.0103 test loss: 0.0181\n",
      "Epoch 7 batch 1700 train loss: 0.0077 test loss: 0.0220\n",
      "Epoch 7 batch 1800 train loss: 0.0104 test loss: 0.0183\n",
      "Epoch 7 batch 1900 train loss: 0.0085 test loss: 0.0180\n",
      "Epoch 7 batch 2000 train loss: 0.0061 test loss: 0.0196\n",
      "Epoch 7 batch 2100 train loss: 0.0060 test loss: 0.0186\n",
      "Epoch 7 batch 2200 train loss: 0.0069 test loss: 0.0216\n",
      "Epoch 7 batch 2300 train loss: 0.0082 test loss: 0.0209\n",
      "Epoch 7 batch 2400 train loss: 0.0062 test loss: 0.0202\n",
      "Epoch 7 batch 2500 train loss: 0.0082 test loss: 0.0203\n",
      "Epoch 7 batch 2600 train loss: 0.0074 test loss: 0.0197\n",
      "Epoch 7 batch 2700 train loss: 0.0108 test loss: 0.0202\n",
      "Epoch 7 batch 2800 train loss: 0.0059 test loss: 0.0191\n",
      "Epoch 7 batch 2900 train loss: 0.0071 test loss: 0.0189\n",
      "Epoch 8 batch 0 train loss: 0.0066 test loss: 0.0161\n",
      "Epoch 8 batch 100 train loss: 0.0060 test loss: 0.0184\n",
      "Epoch 8 batch 200 train loss: 0.0066 test loss: 0.0178\n",
      "Epoch 8 batch 300 train loss: 0.0084 test loss: 0.0191\n",
      "Epoch 8 batch 400 train loss: 0.0073 test loss: 0.0231\n",
      "Epoch 8 batch 500 train loss: 0.0089 test loss: 0.0195\n",
      "Epoch 8 batch 600 train loss: 0.0071 test loss: 0.0198\n",
      "Epoch 8 batch 700 train loss: 0.0072 test loss: 0.0202\n",
      "Epoch 8 batch 800 train loss: 0.0092 test loss: 0.0174\n",
      "Epoch 8 batch 900 train loss: 0.0073 test loss: 0.0184\n",
      "Epoch 8 batch 1000 train loss: 0.0070 test loss: 0.0206\n",
      "Epoch 8 batch 1100 train loss: 0.0066 test loss: 0.0224\n",
      "Epoch 8 batch 1200 train loss: 0.0079 test loss: 0.0177\n",
      "Epoch 8 batch 1300 train loss: 0.0062 test loss: 0.0235\n",
      "Epoch 8 batch 1400 train loss: 0.0094 test loss: 0.0201\n",
      "Epoch 8 batch 1500 train loss: 0.0076 test loss: 0.0202\n",
      "Epoch 8 batch 1600 train loss: 0.0089 test loss: 0.0201\n",
      "Epoch 8 batch 1700 train loss: 0.0078 test loss: 0.0211\n",
      "Epoch 8 batch 1800 train loss: 0.0081 test loss: 0.0188\n",
      "Epoch 8 batch 1900 train loss: 0.0082 test loss: 0.0189\n",
      "Epoch 8 batch 2000 train loss: 0.0081 test loss: 0.0201\n",
      "Epoch 8 batch 2100 train loss: 0.0091 test loss: 0.0192\n",
      "Epoch 8 batch 2200 train loss: 0.0090 test loss: 0.0195\n",
      "Epoch 8 batch 2300 train loss: 0.0071 test loss: 0.0223\n",
      "Epoch 8 batch 2400 train loss: 0.0072 test loss: 0.0226\n",
      "Epoch 8 batch 2500 train loss: 0.0064 test loss: 0.0192\n",
      "Epoch 8 batch 2600 train loss: 0.0069 test loss: 0.0181\n",
      "Epoch 8 batch 2700 train loss: 0.0074 test loss: 0.0212\n",
      "Epoch 8 batch 2800 train loss: 0.0090 test loss: 0.0207\n",
      "Epoch 8 batch 2900 train loss: 0.0065 test loss: 0.0182\n",
      "Epoch 9 batch 0 train loss: 0.0095 test loss: 0.0161\n",
      "Epoch 9 batch 100 train loss: 0.0073 test loss: 0.0187\n",
      "Epoch 9 batch 200 train loss: 0.0080 test loss: 0.0197\n",
      "Epoch 9 batch 300 train loss: 0.0075 test loss: 0.0182\n",
      "Epoch 9 batch 400 train loss: 0.0076 test loss: 0.0187\n",
      "Epoch 9 batch 500 train loss: 0.0060 test loss: 0.0194\n",
      "Epoch 9 batch 600 train loss: 0.0073 test loss: 0.0227\n",
      "Epoch 9 batch 700 train loss: 0.0058 test loss: 0.0192\n",
      "Epoch 9 batch 800 train loss: 0.0076 test loss: 0.0205\n",
      "Epoch 9 batch 900 train loss: 0.0081 test loss: 0.0190\n",
      "Epoch 9 batch 1000 train loss: 0.0103 test loss: 0.0182\n",
      "Epoch 9 batch 1100 train loss: 0.0077 test loss: 0.0216\n",
      "Epoch 9 batch 1200 train loss: 0.0077 test loss: 0.0201\n",
      "Epoch 9 batch 1300 train loss: 0.0087 test loss: 0.0208\n",
      "Epoch 9 batch 1400 train loss: 0.0072 test loss: 0.0199\n",
      "Epoch 9 batch 1500 train loss: 0.0069 test loss: 0.0212\n",
      "Epoch 9 batch 1600 train loss: 0.0068 test loss: 0.0227\n",
      "Epoch 9 batch 1700 train loss: 0.0067 test loss: 0.0208\n",
      "Epoch 9 batch 1800 train loss: 0.0078 test loss: 0.0200\n",
      "Epoch 9 batch 1900 train loss: 0.0088 test loss: 0.0182\n",
      "Epoch 9 batch 2000 train loss: 0.0074 test loss: 0.0177\n",
      "Epoch 9 batch 2100 train loss: 0.0066 test loss: 0.0208\n",
      "Epoch 9 batch 2200 train loss: 0.0080 test loss: 0.0187\n",
      "Epoch 9 batch 2300 train loss: 0.0067 test loss: 0.0195\n",
      "Epoch 9 batch 2400 train loss: 0.0053 test loss: 0.0199\n",
      "Epoch 9 batch 2500 train loss: 0.0064 test loss: 0.0217\n",
      "Epoch 9 batch 2600 train loss: 0.0076 test loss: 0.0173\n",
      "Epoch 9 batch 2700 train loss: 0.0071 test loss: 0.0196\n",
      "Epoch 9 batch 2800 train loss: 0.0098 test loss: 0.0173\n",
      "Epoch 9 batch 2900 train loss: 0.0093 test loss: 0.0175\n",
      "Epoch 10 batch 0 train loss: 0.0085 test loss: 0.0183\n",
      "Epoch 10 batch 100 train loss: 0.0077 test loss: 0.0211\n",
      "Epoch 10 batch 200 train loss: 0.0089 test loss: 0.0189\n",
      "Epoch 10 batch 300 train loss: 0.0066 test loss: 0.0182\n",
      "Epoch 10 batch 400 train loss: 0.0081 test loss: 0.0224\n",
      "Epoch 10 batch 500 train loss: 0.0070 test loss: 0.0175\n",
      "Epoch 10 batch 600 train loss: 0.0054 test loss: 0.0213\n",
      "Epoch 10 batch 700 train loss: 0.0055 test loss: 0.0215\n",
      "Epoch 10 batch 800 train loss: 0.0078 test loss: 0.0222\n",
      "Epoch 10 batch 900 train loss: 0.0060 test loss: 0.0190\n",
      "Epoch 10 batch 1000 train loss: 0.0060 test loss: 0.0181\n",
      "Epoch 10 batch 1100 train loss: 0.0092 test loss: 0.0193\n",
      "Epoch 10 batch 1200 train loss: 0.0065 test loss: 0.0187\n",
      "Epoch 10 batch 1300 train loss: 0.0062 test loss: 0.0212\n",
      "Epoch 10 batch 1400 train loss: 0.0071 test loss: 0.0216\n",
      "Epoch 10 batch 1500 train loss: 0.0074 test loss: 0.0195\n",
      "Epoch 10 batch 1600 train loss: 0.0067 test loss: 0.0178\n",
      "Epoch 10 batch 1700 train loss: 0.0071 test loss: 0.0227\n",
      "Epoch 10 batch 1800 train loss: 0.0070 test loss: 0.0198\n",
      "Epoch 10 batch 1900 train loss: 0.0057 test loss: 0.0232\n",
      "Epoch 10 batch 2000 train loss: 0.0074 test loss: 0.0218\n",
      "Epoch 10 batch 2100 train loss: 0.0090 test loss: 0.0177\n",
      "Epoch 10 batch 2200 train loss: 0.0079 test loss: 0.0189\n",
      "Epoch 10 batch 2300 train loss: 0.0079 test loss: 0.0195\n",
      "Epoch 10 batch 2400 train loss: 0.0074 test loss: 0.0191\n",
      "Epoch 10 batch 2500 train loss: 0.0068 test loss: 0.0207\n",
      "Epoch 10 batch 2600 train loss: 0.0087 test loss: 0.0215\n",
      "Epoch 10 batch 2700 train loss: 0.0083 test loss: 0.0195\n",
      "Epoch 10 batch 2800 train loss: 0.0075 test loss: 0.0187\n",
      "Epoch 10 batch 2900 train loss: 0.0069 test loss: 0.0184\n",
      "Epoch 11 batch 0 train loss: 0.0089 test loss: 0.0192\n",
      "Epoch 11 batch 100 train loss: 0.0076 test loss: 0.0231\n",
      "Epoch 11 batch 200 train loss: 0.0060 test loss: 0.0172\n",
      "Epoch 11 batch 300 train loss: 0.0068 test loss: 0.0183\n",
      "Epoch 11 batch 400 train loss: 0.0071 test loss: 0.0193\n",
      "Epoch 11 batch 500 train loss: 0.0077 test loss: 0.0217\n",
      "Epoch 11 batch 600 train loss: 0.0073 test loss: 0.0183\n",
      "Epoch 11 batch 700 train loss: 0.0072 test loss: 0.0209\n",
      "Epoch 11 batch 800 train loss: 0.0077 test loss: 0.0194\n",
      "Epoch 11 batch 900 train loss: 0.0063 test loss: 0.0198\n",
      "Epoch 11 batch 1000 train loss: 0.0090 test loss: 0.0219\n",
      "Epoch 11 batch 1100 train loss: 0.0063 test loss: 0.0182\n",
      "Epoch 11 batch 1200 train loss: 0.0082 test loss: 0.0169\n",
      "Epoch 11 batch 1300 train loss: 0.0081 test loss: 0.0219\n",
      "Epoch 11 batch 1400 train loss: 0.0059 test loss: 0.0213\n",
      "Epoch 11 batch 1500 train loss: 0.0066 test loss: 0.0179\n",
      "Epoch 11 batch 1600 train loss: 0.0089 test loss: 0.0195\n",
      "Epoch 11 batch 1700 train loss: 0.0081 test loss: 0.0160\n",
      "Epoch 11 batch 1800 train loss: 0.0057 test loss: 0.0204\n",
      "Epoch 11 batch 1900 train loss: 0.0078 test loss: 0.0208\n",
      "Epoch 11 batch 2000 train loss: 0.0091 test loss: 0.0205\n",
      "Epoch 11 batch 2100 train loss: 0.0062 test loss: 0.0204\n",
      "Epoch 11 batch 2200 train loss: 0.0069 test loss: 0.0201\n",
      "Epoch 11 batch 2300 train loss: 0.0072 test loss: 0.0184\n",
      "Epoch 11 batch 2400 train loss: 0.0076 test loss: 0.0166\n",
      "Epoch 11 batch 2500 train loss: 0.0071 test loss: 0.0217\n",
      "Epoch 11 batch 2600 train loss: 0.0076 test loss: 0.0197\n",
      "Epoch 11 batch 2700 train loss: 0.0072 test loss: 0.0175\n",
      "Epoch 11 batch 2800 train loss: 0.0078 test loss: 0.0211\n",
      "Epoch 11 batch 2900 train loss: 0.0081 test loss: 0.0197\n",
      "Epoch 12 batch 0 train loss: 0.0078 test loss: 0.0191\n",
      "Epoch 12 batch 100 train loss: 0.0055 test loss: 0.0191\n",
      "Epoch 12 batch 200 train loss: 0.0080 test loss: 0.0187\n",
      "Epoch 12 batch 300 train loss: 0.0082 test loss: 0.0181\n",
      "Epoch 12 batch 400 train loss: 0.0096 test loss: 0.0201\n",
      "Epoch 12 batch 500 train loss: 0.0058 test loss: 0.0181\n",
      "Epoch 12 batch 600 train loss: 0.0071 test loss: 0.0209\n",
      "Epoch 12 batch 700 train loss: 0.0065 test loss: 0.0202\n",
      "Epoch 12 batch 800 train loss: 0.0059 test loss: 0.0203\n",
      "Epoch 12 batch 900 train loss: 0.0069 test loss: 0.0209\n",
      "Epoch 12 batch 1000 train loss: 0.0084 test loss: 0.0215\n",
      "Epoch 12 batch 1100 train loss: 0.0070 test loss: 0.0228\n",
      "Epoch 12 batch 1200 train loss: 0.0074 test loss: 0.0174\n",
      "Epoch 12 batch 1300 train loss: 0.0063 test loss: 0.0213\n",
      "Epoch 12 batch 1400 train loss: 0.0066 test loss: 0.0183\n",
      "Epoch 12 batch 1500 train loss: 0.0100 test loss: 0.0205\n",
      "Epoch 12 batch 1600 train loss: 0.0077 test loss: 0.0209\n",
      "Epoch 12 batch 1700 train loss: 0.0065 test loss: 0.0204\n",
      "Epoch 12 batch 1800 train loss: 0.0071 test loss: 0.0194\n",
      "Epoch 12 batch 1900 train loss: 0.0086 test loss: 0.0194\n",
      "Epoch 12 batch 2000 train loss: 0.0058 test loss: 0.0181\n",
      "Epoch 12 batch 2100 train loss: 0.0083 test loss: 0.0220\n",
      "Epoch 12 batch 2200 train loss: 0.0063 test loss: 0.0232\n",
      "Epoch 12 batch 2300 train loss: 0.0064 test loss: 0.0230\n",
      "Epoch 12 batch 2400 train loss: 0.0075 test loss: 0.0211\n",
      "Epoch 12 batch 2500 train loss: 0.0067 test loss: 0.0230\n",
      "Epoch 12 batch 2600 train loss: 0.0088 test loss: 0.0189\n",
      "Epoch 12 batch 2700 train loss: 0.0082 test loss: 0.0202\n",
      "Epoch 12 batch 2800 train loss: 0.0065 test loss: 0.0203\n",
      "Epoch 12 batch 2900 train loss: 0.0079 test loss: 0.0182\n",
      "Epoch 13 batch 0 train loss: 0.0080 test loss: 0.0190\n",
      "Epoch 13 batch 100 train loss: 0.0062 test loss: 0.0202\n",
      "Epoch 13 batch 200 train loss: 0.0062 test loss: 0.0171\n",
      "Epoch 13 batch 300 train loss: 0.0056 test loss: 0.0191\n",
      "Epoch 13 batch 400 train loss: 0.0072 test loss: 0.0220\n",
      "Epoch 13 batch 500 train loss: 0.0071 test loss: 0.0223\n",
      "Epoch 13 batch 600 train loss: 0.0075 test loss: 0.0169\n",
      "Epoch 13 batch 700 train loss: 0.0067 test loss: 0.0195\n",
      "Epoch 13 batch 800 train loss: 0.0107 test loss: 0.0192\n",
      "Epoch 13 batch 900 train loss: 0.0061 test loss: 0.0166\n",
      "Epoch 13 batch 1000 train loss: 0.0064 test loss: 0.0199\n",
      "Epoch 13 batch 1100 train loss: 0.0094 test loss: 0.0199\n",
      "Epoch 13 batch 1200 train loss: 0.0069 test loss: 0.0185\n",
      "Epoch 13 batch 1300 train loss: 0.0084 test loss: 0.0209\n",
      "Epoch 13 batch 1400 train loss: 0.0055 test loss: 0.0210\n",
      "Epoch 13 batch 1500 train loss: 0.0093 test loss: 0.0208\n",
      "Epoch 13 batch 1600 train loss: 0.0050 test loss: 0.0244\n",
      "Epoch 13 batch 1700 train loss: 0.0084 test loss: 0.0188\n",
      "Epoch 13 batch 1800 train loss: 0.0076 test loss: 0.0225\n",
      "Epoch 13 batch 1900 train loss: 0.0066 test loss: 0.0208\n",
      "Epoch 13 batch 2000 train loss: 0.0077 test loss: 0.0213\n",
      "Epoch 13 batch 2100 train loss: 0.0076 test loss: 0.0194\n",
      "Epoch 13 batch 2200 train loss: 0.0063 test loss: 0.0184\n",
      "Epoch 13 batch 2300 train loss: 0.0066 test loss: 0.0189\n",
      "Epoch 13 batch 2400 train loss: 0.0068 test loss: 0.0218\n",
      "Epoch 13 batch 2500 train loss: 0.0079 test loss: 0.0179\n",
      "Epoch 13 batch 2600 train loss: 0.0078 test loss: 0.0207\n",
      "Epoch 13 batch 2700 train loss: 0.0078 test loss: 0.0193\n",
      "Epoch 13 batch 2800 train loss: 0.0075 test loss: 0.0163\n",
      "Epoch 13 batch 2900 train loss: 0.0106 test loss: 0.0172\n",
      "Epoch 14 batch 0 train loss: 0.0074 test loss: 0.0162\n",
      "Epoch 14 batch 100 train loss: 0.0058 test loss: 0.0216\n",
      "Epoch 14 batch 200 train loss: 0.0079 test loss: 0.0178\n",
      "Epoch 14 batch 300 train loss: 0.0081 test loss: 0.0182\n",
      "Epoch 14 batch 400 train loss: 0.0063 test loss: 0.0195\n",
      "Epoch 14 batch 500 train loss: 0.0087 test loss: 0.0206\n",
      "Epoch 14 batch 600 train loss: 0.0065 test loss: 0.0176\n",
      "Epoch 14 batch 700 train loss: 0.0060 test loss: 0.0215\n",
      "Epoch 14 batch 800 train loss: 0.0066 test loss: 0.0181\n",
      "Epoch 14 batch 900 train loss: 0.0076 test loss: 0.0180\n",
      "Epoch 14 batch 1000 train loss: 0.0060 test loss: 0.0222\n",
      "Epoch 14 batch 1100 train loss: 0.0085 test loss: 0.0188\n",
      "Epoch 14 batch 1200 train loss: 0.0073 test loss: 0.0201\n",
      "Epoch 14 batch 1300 train loss: 0.0059 test loss: 0.0215\n",
      "Epoch 14 batch 1400 train loss: 0.0081 test loss: 0.0193\n",
      "Epoch 14 batch 1500 train loss: 0.0077 test loss: 0.0210\n",
      "Epoch 14 batch 1600 train loss: 0.0096 test loss: 0.0195\n",
      "Epoch 14 batch 1700 train loss: 0.0072 test loss: 0.0202\n",
      "Epoch 14 batch 1800 train loss: 0.0086 test loss: 0.0207\n",
      "Epoch 14 batch 1900 train loss: 0.0057 test loss: 0.0187\n",
      "Epoch 14 batch 2000 train loss: 0.0073 test loss: 0.0209\n",
      "Epoch 14 batch 2100 train loss: 0.0070 test loss: 0.0199\n",
      "Epoch 14 batch 2200 train loss: 0.0069 test loss: 0.0222\n",
      "Epoch 14 batch 2300 train loss: 0.0072 test loss: 0.0197\n",
      "Epoch 14 batch 2400 train loss: 0.0066 test loss: 0.0243\n",
      "Epoch 14 batch 2500 train loss: 0.0093 test loss: 0.0197\n",
      "Epoch 14 batch 2600 train loss: 0.0092 test loss: 0.0181\n",
      "Epoch 14 batch 2700 train loss: 0.0089 test loss: 0.0189\n",
      "Epoch 14 batch 2800 train loss: 0.0078 test loss: 0.0189\n",
      "Epoch 14 batch 2900 train loss: 0.0066 test loss: 0.0186\n",
      "Epoch 15 batch 0 train loss: 0.0079 test loss: 0.0172\n",
      "Epoch 15 batch 100 train loss: 0.0051 test loss: 0.0179\n",
      "Epoch 15 batch 200 train loss: 0.0061 test loss: 0.0218\n",
      "Epoch 15 batch 300 train loss: 0.0068 test loss: 0.0169\n",
      "Epoch 15 batch 400 train loss: 0.0112 test loss: 0.0201\n",
      "Epoch 15 batch 500 train loss: 0.0077 test loss: 0.0181\n",
      "Epoch 15 batch 600 train loss: 0.0095 test loss: 0.0203\n",
      "Epoch 15 batch 700 train loss: 0.0077 test loss: 0.0198\n",
      "Epoch 15 batch 800 train loss: 0.0082 test loss: 0.0197\n",
      "Epoch 15 batch 900 train loss: 0.0063 test loss: 0.0186\n",
      "Epoch 15 batch 1000 train loss: 0.0079 test loss: 0.0203\n",
      "Epoch 15 batch 1100 train loss: 0.0061 test loss: 0.0184\n",
      "Epoch 15 batch 1200 train loss: 0.0090 test loss: 0.0156\n",
      "Epoch 15 batch 1300 train loss: 0.0082 test loss: 0.0235\n",
      "Epoch 15 batch 1400 train loss: 0.0091 test loss: 0.0183\n",
      "Epoch 15 batch 1500 train loss: 0.0071 test loss: 0.0199\n",
      "Epoch 15 batch 1600 train loss: 0.0085 test loss: 0.0230\n",
      "Epoch 15 batch 1700 train loss: 0.0076 test loss: 0.0179\n",
      "Epoch 15 batch 1800 train loss: 0.0074 test loss: 0.0190\n",
      "Epoch 15 batch 1900 train loss: 0.0049 test loss: 0.0208\n",
      "Epoch 15 batch 2000 train loss: 0.0071 test loss: 0.0220\n",
      "Epoch 15 batch 2100 train loss: 0.0093 test loss: 0.0194\n",
      "Epoch 15 batch 2200 train loss: 0.0068 test loss: 0.0212\n",
      "Epoch 15 batch 2300 train loss: 0.0078 test loss: 0.0209\n",
      "Epoch 15 batch 2400 train loss: 0.0090 test loss: 0.0209\n",
      "Epoch 15 batch 2500 train loss: 0.0079 test loss: 0.0191\n",
      "Epoch 15 batch 2600 train loss: 0.0075 test loss: 0.0190\n",
      "Epoch 15 batch 2700 train loss: 0.0079 test loss: 0.0190\n",
      "Epoch 15 batch 2800 train loss: 0.0084 test loss: 0.0193\n",
      "Epoch 15 batch 2900 train loss: 0.0060 test loss: 0.0169\n",
      "Epoch 16 batch 0 train loss: 0.0058 test loss: 0.0176\n",
      "Epoch 16 batch 100 train loss: 0.0065 test loss: 0.0191\n",
      "Epoch 16 batch 200 train loss: 0.0087 test loss: 0.0205\n",
      "Epoch 16 batch 300 train loss: 0.0079 test loss: 0.0197\n",
      "Epoch 16 batch 400 train loss: 0.0061 test loss: 0.0197\n",
      "Epoch 16 batch 500 train loss: 0.0078 test loss: 0.0183\n",
      "Epoch 16 batch 600 train loss: 0.0082 test loss: 0.0203\n",
      "Epoch 16 batch 700 train loss: 0.0081 test loss: 0.0184\n",
      "Epoch 16 batch 800 train loss: 0.0081 test loss: 0.0199\n",
      "Epoch 16 batch 900 train loss: 0.0063 test loss: 0.0184\n",
      "Epoch 16 batch 1000 train loss: 0.0071 test loss: 0.0184\n",
      "Epoch 16 batch 1100 train loss: 0.0074 test loss: 0.0208\n",
      "Epoch 16 batch 1200 train loss: 0.0051 test loss: 0.0196\n",
      "Epoch 16 batch 1300 train loss: 0.0056 test loss: 0.0247\n",
      "Epoch 16 batch 1400 train loss: 0.0096 test loss: 0.0177\n",
      "Epoch 16 batch 1500 train loss: 0.0119 test loss: 0.0220\n",
      "Epoch 16 batch 1600 train loss: 0.0055 test loss: 0.0194\n",
      "Epoch 16 batch 1700 train loss: 0.0102 test loss: 0.0221\n",
      "Epoch 16 batch 1800 train loss: 0.0066 test loss: 0.0192\n",
      "Epoch 16 batch 1900 train loss: 0.0087 test loss: 0.0222\n",
      "Epoch 16 batch 2000 train loss: 0.0072 test loss: 0.0214\n",
      "Epoch 16 batch 2100 train loss: 0.0091 test loss: 0.0181\n",
      "Epoch 16 batch 2200 train loss: 0.0055 test loss: 0.0197\n",
      "Epoch 16 batch 2300 train loss: 0.0088 test loss: 0.0202\n",
      "Epoch 16 batch 2400 train loss: 0.0057 test loss: 0.0195\n",
      "Epoch 16 batch 2500 train loss: 0.0090 test loss: 0.0191\n",
      "Epoch 16 batch 2600 train loss: 0.0125 test loss: 0.0196\n",
      "Epoch 16 batch 2700 train loss: 0.0097 test loss: 0.0184\n",
      "Epoch 16 batch 2800 train loss: 0.0065 test loss: 0.0206\n",
      "Epoch 16 batch 2900 train loss: 0.0098 test loss: 0.0205\n",
      "Epoch 17 batch 0 train loss: 0.0063 test loss: 0.0168\n",
      "Epoch 17 batch 100 train loss: 0.0088 test loss: 0.0196\n",
      "Epoch 17 batch 200 train loss: 0.0064 test loss: 0.0192\n",
      "Epoch 17 batch 300 train loss: 0.0076 test loss: 0.0189\n",
      "Epoch 17 batch 400 train loss: 0.0084 test loss: 0.0190\n",
      "Epoch 17 batch 500 train loss: 0.0081 test loss: 0.0191\n",
      "Epoch 17 batch 600 train loss: 0.0062 test loss: 0.0172\n",
      "Epoch 17 batch 700 train loss: 0.0069 test loss: 0.0214\n",
      "Epoch 17 batch 800 train loss: 0.0063 test loss: 0.0210\n",
      "Epoch 17 batch 900 train loss: 0.0071 test loss: 0.0171\n",
      "Epoch 17 batch 1000 train loss: 0.0073 test loss: 0.0171\n",
      "Epoch 17 batch 1100 train loss: 0.0083 test loss: 0.0193\n",
      "Epoch 17 batch 1200 train loss: 0.0062 test loss: 0.0191\n",
      "early stop.\n",
      "Checkpoint 23 restored!!\n"
     ]
    }
   ],
   "source": [
    "for LOSS_RATE in LOSS_RATES:    \n",
    "    l = np.load('./data/tot_dataset_mtr_loss_%.2f.npz' % LOSS_RATE)\n",
    "    raw_input = l['raw_input']\n",
    "    raw_label = l['raw_label']\n",
    "    test_input = l['test_input']\n",
    "    test_label = l['test_label']\n",
    "    MAXS = l['MAXS']\n",
    "    MINS = l['MINS']\n",
    "    MAXS = MAXS[:5]\n",
    "    MINS = MINS[:5]\n",
    "    SCREEN_SIZE = l['SCREEN_SIZE']\n",
    "\n",
    "    raw_input = np.concatenate([raw_input[..., :5, 0], raw_input[..., :5, 1], raw_input[..., :5, 2], raw_input[..., :5, 3]], axis=-1)\n",
    "    raw_label = raw_label[..., :5, 0]\n",
    "    test_input = np.concatenate([test_input[..., :5, 0], test_input[..., :5, 1], test_input[..., :5, 2], test_input[..., :5, 3]], axis=-1)\n",
    "    test_label = test_label[..., :5, 0]\n",
    "\n",
    "    raw_input = raw_input.astype(np.float32)\n",
    "    raw_label = raw_label.astype(np.float32)\n",
    "    test_input = test_input.astype(np.float32)\n",
    "    test_label = test_label.astype(np.float32)\n",
    "\n",
    "    num_train = int(raw_input.shape[0]*.7)\n",
    "    raw_input, raw_label = shuffle(raw_input, raw_label, random_state=4574)\n",
    "    train_input, train_label = raw_input[:num_train, ...], raw_label[:num_train, ...]\n",
    "    val_input, val_label = raw_input[num_train:, ...], raw_label[num_train:, ...]\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_input, train_label))\n",
    "    train_dataset = train_dataset.cache().shuffle(BATCH_SIZE*50).batch(BATCH_SIZE)\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((val_input, val_label))\n",
    "    val_dataset = val_dataset.cache().shuffle(BATCH_SIZE*50).batch(BATCH_SIZE)\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((test_input, test_label))\n",
    "    test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "    bilstm_model = BiLSTM(test_label.shape[-1])\n",
    "    opt = tf.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    \n",
    "    print('Training for loss rate %.2f start.' % LOSS_RATE)\n",
    "    BEST_PATH = './checkpoints/RNN_best_mtr_loss_%.2fp' % LOSS_RATE\n",
    "    \n",
    "    @tf.function\n",
    "    def train(loss_function, model, opt, inp, tar):\n",
    "        with tf.GradientTape() as tape:\n",
    "            gradients = tape.gradient(loss_function(model, inp, tar), model.trainable_variables)\n",
    "            gradient_variables = zip(gradients, model.trainable_variables)\n",
    "            opt.apply_gradients(gradient_variables)\n",
    "\n",
    "    checkpoint_path = BEST_PATH\n",
    "\n",
    "    ckpt = tf.train.Checkpoint(bilstm_model=bilstm_model, opt=opt)\n",
    "    ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=10)\n",
    "    writer = tf.summary.create_file_writer('tmp')\n",
    "\n",
    "    prev_test_loss = 100.0\n",
    "    early_stop_buffer = 500\n",
    "    with writer.as_default():\n",
    "        with tf.summary.record_if(True):\n",
    "            for epoch in range(TRAINING_EPOCHS):\n",
    "                for step, (inp, tar) in enumerate(train_dataset):\n",
    "                    train(loss_function, bilstm_model, opt, inp, tar)\n",
    "                    loss_values = loss_function(bilstm_model, inp, tar)\n",
    "                    tf.summary.scalar('loss', loss_values, step=step)\n",
    "\n",
    "                    if step % DISP_STEPS == 0:\n",
    "                        test_loss = 0\n",
    "                        for step_, (inp_, tar_) in enumerate(test_dataset):\n",
    "                            test_loss += loss_function(bilstm_model, inp_, tar_)\n",
    "\n",
    "                            if step_ > DISP_STEPS:\n",
    "                                test_loss /= DISP_STEPS\n",
    "                                break\n",
    "                        if test_loss.numpy() < prev_test_loss:\n",
    "                            ckpt_save_path = ckpt_manager.save()\n",
    "                            prev_test_loss = test_loss.numpy()\n",
    "                            print('Saving checkpoint at {}'.format(ckpt_save_path))\n",
    "                        else:\n",
    "                            early_stop_buffer -= 1\n",
    "\n",
    "                        print('Epoch {} batch {} train loss: {:.4f} test loss: {:.4f}'\n",
    "                              .format(epoch, step, loss_values.numpy(), test_loss.numpy()))\n",
    "                    if early_stop_buffer <= 0:\n",
    "                        print('early stop.')\n",
    "                        break\n",
    "                if early_stop_buffer <= 0:\n",
    "                    break\n",
    "\n",
    "    i = -1\n",
    "    if ckpt_manager.checkpoints:\n",
    "        ckpt.restore(ckpt_manager.checkpoints[i])\n",
    "        print ('Checkpoint ' + ckpt_manager.checkpoints[i][-2:] +' restored!!')\n",
    "\n",
    "    bilstm_model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "                       loss = tf.keras.losses.MeanSquaredError())\n",
    "    pred_result = bilstm_model.predict(test_dataset)\n",
    "    \n",
    "    masking = test_input[..., 5:10]\n",
    "    masked_pred = np.ma.array(pred_result, mask=masking)\n",
    "    masked_label = np.ma.array(test_label, mask=masking)\n",
    "\n",
    "    plot_label = ((MAXS[:5]-MINS[:5])*masked_label[..., :5] + MINS[:5])\n",
    "    plot_label.fill_value = np.nan\n",
    "    plot_pred = ((MAXS[:5]-MINS[:5])*masked_pred[..., :5] + MINS[:5])\n",
    "    plot_pred.fill_value = np.nan\n",
    "\n",
    "    f = open('./results/RNN_mtr_%.2fp.npz' % LOSS_RATE, 'wb')\n",
    "    np.savez(f,\n",
    "             test_label = plot_label.filled(),\n",
    "             test_pred = plot_pred.filled()\n",
    "            )\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p36)",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

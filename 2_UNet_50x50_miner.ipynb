{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import *\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 2 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import Metric\n",
    "class RSquare(Metric):\n",
    "    \"\"\"Compute R^2 score.\n",
    "     This is also called as coefficient of determination.\n",
    "     It tells how close are data to the fitted regression line.\n",
    "     - Highest score can be 1.0 and it indicates that the predictors\n",
    "       perfectly accounts for variation in the target.\n",
    "     - Score 0.0 indicates that the predictors do not\n",
    "       account for variation in the target.\n",
    "     - It can also be negative if the model is worse.\n",
    "     Usage:\n",
    "     ```python\n",
    "     actuals = tf.constant([1, 4, 3], dtype=tf.float32)\n",
    "     preds = tf.constant([2, 4, 4], dtype=tf.float32)\n",
    "     result = tf.keras.metrics.RSquare()\n",
    "     result.update_state(actuals, preds)\n",
    "     print('R^2 score is: ', r1.result().numpy()) # 0.57142866\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name='r_square', dtype=tf.float32):\n",
    "        super(RSquare, self).__init__(name=name, dtype=dtype)\n",
    "        self.squared_sum = self.add_weight(\"squared_sum\", initializer=\"zeros\")\n",
    "        self.sum = self.add_weight(\"sum\", initializer=\"zeros\")\n",
    "        self.res = self.add_weight(\"residual\", initializer=\"zeros\")\n",
    "        self.count = self.add_weight(\"count\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred):\n",
    "        y_true = tf.convert_to_tensor(y_true, tf.float32)\n",
    "        y_pred = tf.convert_to_tensor(y_pred, tf.float32)\n",
    "        self.squared_sum.assign_add(tf.reduce_sum(y_true**2))\n",
    "        self.sum.assign_add(tf.reduce_sum(y_true))\n",
    "        self.res.assign_add(\n",
    "            tf.reduce_sum(tf.square(tf.subtract(y_true, y_pred))))\n",
    "        self.count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        mean = self.sum / self.count\n",
    "        total = self.squared_sum - 2 * self.sum * mean + self.count * mean**2\n",
    "        return 1 - (self.res / total)\n",
    "\n",
    "    def reset_states(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.squared_sum.assign(0.0)\n",
    "        self.sum.assign(0.0)\n",
    "        self.res.assign(0.0)\n",
    "        self.count.assign(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = ((8/2.54), (6/2.54))\n",
    "plt.rcParams[\"font.family\"] = \"Arial\"\n",
    "plt.rcParams[\"mathtext.default\"] = \"rm\"\n",
    "plt.rcParams.update({'font.size': 11})\n",
    "MARKER_SIZE = 15\n",
    "cmap_m = [\"#f4a6ad\", \"#f6957e\", \"#fccfa2\", \"#8de7be\", \"#86d6f2\", \"#24a9e4\", \"#b586e0\", \"#d7f293\"]\n",
    "cmap = [\"#e94d5b\", \"#ef4d28\", \"#f9a54f\", \"#25b575\", \"#1bb1e7\", \"#1477a2\", \"#a662e5\", \"#c2f442\"]\n",
    "\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "# plt.rcParams['axes.edgecolor'] = \n",
    "plt.rcParams['axes.linewidth'] = 1\n",
    "plt.rcParams['lines.linewidth'] = 1.5\n",
    "plt.rcParams['xtick.major.width'] = 1\n",
    "plt.rcParams['xtick.minor.width'] = 1\n",
    "plt.rcParams['ytick.major.width'] = 1\n",
    "plt.rcParams['ytick.minor.width'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 50\n",
    "LOSS_RATES = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]\n",
    "DISP_STEPS = 100\n",
    "TRAINING_EPOCHS = 500\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, dropout_rate):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.conv1 = layers.Conv2D(self.filters, self.kernel_size,\n",
    "                                   activation='relu', kernel_initializer='he_normal', padding='same')\n",
    "        self.batch1 = layers.BatchNormalization()\n",
    "        self.drop = layers.Dropout(self.dropout_rate)\n",
    "        self.conv2 = layers.Conv2D(self.filters, self.kernel_size,\n",
    "                                   activation='relu', kernel_initializer='he_normal', padding='same')\n",
    "        self.batch2 = layers.BatchNormalization()\n",
    "        \n",
    "    def call(self, inp):\n",
    "        \n",
    "        inp = self.batch1(self.conv1(inp))\n",
    "        inp = self.drop(inp)\n",
    "        inp = self.batch2(self.conv2(inp))\n",
    "        \n",
    "        return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeconvBlock(layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, strides):\n",
    "        super(DeconvBlock, self).__init__()\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.strides = strides\n",
    "        \n",
    "        self.deconv1 = layers.Conv2DTranspose(self.filters, self.kernel_size, strides=self.strides, padding='same')\n",
    "        \n",
    "    def call(self, inp):\n",
    "        \n",
    "        inp = self.deconv1(inp)\n",
    "        \n",
    "        return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(Model):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        self.conv_block1 = ConvBlock(64, (2, 2), 0.1)\n",
    "        self.pool1 = layers.MaxPooling2D()\n",
    "        \n",
    "        self.conv_block2 = ConvBlock(128, (2, 2), 0.2)\n",
    "        self.pool2 = layers.MaxPooling2D()\n",
    "        \n",
    "        self.conv_block3 = ConvBlock(256, (2, 2), 0.2)\n",
    "        \n",
    "        self.deconv_block1 = DeconvBlock(128, (2, 2), (2, 2))\n",
    "        self.conv_block4 = ConvBlock(128, (2, 2), 0.2)\n",
    "        \n",
    "        self.deconv_block2 = DeconvBlock(32, (2, 2), (2, 2))\n",
    "        self.padding = layers.ZeroPadding2D(((1, 0), (0, 1)))\n",
    "        self.conv_block5 = ConvBlock(64, (2, 2), 0.1)\n",
    "        \n",
    "        self.output_conv = layers.Conv2D(1, (1, 1), activation='sigmoid')\n",
    "        \n",
    "    def call(self, inp):\n",
    "        \n",
    "        conv1 = self.conv_block1(inp)\n",
    "        pooled1 = self.pool1(conv1)\n",
    "        conv2 = self.conv_block2(pooled1)\n",
    "        pooled2 = self.pool2(conv2)\n",
    "        \n",
    "        bottom = self.conv_block3(pooled2)\n",
    "        \n",
    "        deconv1 = self.padding(self.deconv_block1(bottom))\n",
    "        deconv1 = layers.concatenate([deconv1, conv2])\n",
    "        deconv1 = self.conv_block4(deconv1)\n",
    "        deconv2 = self.deconv_block2(deconv1)\n",
    "        deconv2 = layers.concatenate([deconv2, conv1])\n",
    "        deconv2 = self.conv_block5(deconv2)\n",
    "        \n",
    "        return self.output_conv(deconv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #loss inputs should be masked.\n",
    "    loss_object = tf.keras.losses.MeanSquaredError()\n",
    "    def loss_function(model, inp, tar):\n",
    "\n",
    "        masked_real = tar * (1 - inp[..., 1:2])\n",
    "        masked_pred = model(inp) * (1 - inp[..., 1:2])\n",
    "\n",
    "        return loss_object(masked_real, masked_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for loss rate 0.10 start.\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.10p/ckpt-1\n",
      "Epoch 0 batch 0 train loss: 0.0433 test loss: 0.0679\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.10p/ckpt-2\n",
      "Epoch 0 batch 100 train loss: 0.0036 test loss: 0.0045\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.10p/ckpt-3\n",
      "Epoch 0 batch 200 train loss: 0.0023 test loss: 0.0033\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.10p/ckpt-4\n",
      "Epoch 0 batch 300 train loss: 0.0020 test loss: 0.0020\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.10p/ckpt-5\n",
      "Epoch 0 batch 400 train loss: 0.0015 test loss: 0.0013\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.10p/ckpt-6\n",
      "Epoch 0 batch 500 train loss: 0.0015 test loss: 0.0013\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.10p/ckpt-7\n",
      "Epoch 0 batch 600 train loss: 0.0020 test loss: 0.0013\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.10p/ckpt-8\n",
      "Epoch 0 batch 700 train loss: 0.0016 test loss: 0.0010\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.10p/ckpt-9\n",
      "Epoch 0 batch 800 train loss: 0.0013 test loss: 0.0009\n",
      "Epoch 0 batch 900 train loss: 0.0011 test loss: 0.0013\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.10p/ckpt-10\n",
      "Epoch 0 batch 1000 train loss: 0.0025 test loss: 0.0008\n",
      "Epoch 0 batch 1100 train loss: 0.0008 test loss: 0.0008\n",
      "Epoch 0 batch 1200 train loss: 0.0007 test loss: 0.0009\n",
      "Epoch 0 batch 1300 train loss: 0.0009 test loss: 0.0009\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.10p/ckpt-11\n",
      "Epoch 0 batch 1400 train loss: 0.0008 test loss: 0.0008\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.10p/ckpt-12\n",
      "Epoch 0 batch 1500 train loss: 0.0009 test loss: 0.0008\n",
      "Epoch 0 batch 1600 train loss: 0.0008 test loss: 0.0008\n",
      "Epoch 0 batch 1700 train loss: 0.0006 test loss: 0.0008\n",
      "Epoch 0 batch 1800 train loss: 0.0006 test loss: 0.0008\n",
      "Epoch 0 batch 1900 train loss: 0.0008 test loss: 0.0012\n",
      "Epoch 0 batch 2000 train loss: 0.0008 test loss: 0.0010\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.10p/ckpt-13\n",
      "Epoch 0 batch 2100 train loss: 0.0005 test loss: 0.0008\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.10p/ckpt-14\n",
      "Epoch 0 batch 2200 train loss: 0.0006 test loss: 0.0007\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.10p/ckpt-15\n",
      "Epoch 0 batch 2300 train loss: 0.0004 test loss: 0.0006\n",
      "Epoch 0 batch 2400 train loss: 0.0009 test loss: 0.0008\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.10p/ckpt-16\n",
      "Epoch 0 batch 2500 train loss: 0.0020 test loss: 0.0006\n",
      "Epoch 0 batch 2600 train loss: 0.0006 test loss: 0.0009\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.10p/ckpt-17\n",
      "Epoch 0 batch 2700 train loss: 0.0004 test loss: 0.0006\n",
      "Epoch 0 batch 2800 train loss: 0.0006 test loss: 0.0006\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.10p/ckpt-18\n",
      "Epoch 0 batch 2900 train loss: 0.0022 test loss: 0.0006\n",
      "Epoch 1 batch 0 train loss: 0.0005 test loss: 0.0007\n",
      "Epoch 1 batch 100 train loss: 0.0007 test loss: 0.0006\n",
      "Epoch 1 batch 200 train loss: 0.0005 test loss: 0.0006\n",
      "Epoch 1 batch 300 train loss: 0.0005 test loss: 0.0006\n",
      "Epoch 1 batch 400 train loss: 0.0006 test loss: 0.0007\n",
      "Epoch 1 batch 500 train loss: 0.0004 test loss: 0.0006\n",
      "Epoch 1 batch 600 train loss: 0.0007 test loss: 0.0006\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.10p/ckpt-19\n",
      "Epoch 1 batch 700 train loss: 0.0004 test loss: 0.0006\n",
      "Epoch 1 batch 800 train loss: 0.0004 test loss: 0.0007\n",
      "Epoch 1 batch 900 train loss: 0.0004 test loss: 0.0006\n",
      "Epoch 1 batch 1000 train loss: 0.0004 test loss: 0.0006\n",
      "Epoch 1 batch 1100 train loss: 0.0007 test loss: 0.0006\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.10p/ckpt-20\n",
      "Epoch 1 batch 1200 train loss: 0.0011 test loss: 0.0006\n",
      "Epoch 1 batch 1300 train loss: 0.0004 test loss: 0.0006\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.10p/ckpt-21\n",
      "Epoch 1 batch 1400 train loss: 0.0004 test loss: 0.0005\n",
      "Epoch 1 batch 1500 train loss: 0.0006 test loss: 0.0007\n",
      "Epoch 1 batch 1600 train loss: 0.0004 test loss: 0.0006\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.10p/ckpt-22\n",
      "Epoch 1 batch 1700 train loss: 0.0005 test loss: 0.0005\n",
      "Epoch 1 batch 1800 train loss: 0.0005 test loss: 0.0006\n",
      "Epoch 1 batch 1900 train loss: 0.0007 test loss: 0.0006\n",
      "Epoch 1 batch 2000 train loss: 0.0005 test loss: 0.0006\n",
      "Epoch 1 batch 2100 train loss: 0.0004 test loss: 0.0006\n",
      "Epoch 1 batch 2200 train loss: 0.0005 test loss: 0.0006\n",
      "Epoch 1 batch 2300 train loss: 0.0003 test loss: 0.0006\n",
      "Epoch 1 batch 2400 train loss: 0.0004 test loss: 0.0006\n",
      "Epoch 1 batch 2500 train loss: 0.0005 test loss: 0.0006\n",
      "Epoch 1 batch 2600 train loss: 0.0003 test loss: 0.0006\n",
      "Epoch 1 batch 2700 train loss: 0.0005 test loss: 0.0005\n",
      "Epoch 1 batch 2800 train loss: 0.0002 test loss: 0.0006\n",
      "Epoch 1 batch 2900 train loss: 0.0004 test loss: 0.0006\n",
      "Epoch 2 batch 0 train loss: 0.0003 test loss: 0.0006\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.10p/ckpt-23\n",
      "Epoch 2 batch 100 train loss: 0.0018 test loss: 0.0005\n",
      "Epoch 2 batch 200 train loss: 0.0003 test loss: 0.0006\n",
      "Epoch 2 batch 300 train loss: 0.0011 test loss: 0.0006\n",
      "Epoch 2 batch 400 train loss: 0.0004 test loss: 0.0005\n",
      "Epoch 2 batch 500 train loss: 0.0003 test loss: 0.0006\n",
      "Epoch 2 batch 600 train loss: 0.0003 test loss: 0.0005\n",
      "Epoch 2 batch 700 train loss: 0.0003 test loss: 0.0005\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.10p/ckpt-24\n",
      "Epoch 2 batch 800 train loss: 0.0006 test loss: 0.0005\n",
      "Epoch 2 batch 900 train loss: 0.0004 test loss: 0.0006\n",
      "Epoch 2 batch 1000 train loss: 0.0003 test loss: 0.0006\n",
      "Epoch 2 batch 1100 train loss: 0.0004 test loss: 0.0006\n",
      "Epoch 2 batch 1200 train loss: 0.0003 test loss: 0.0005\n",
      "Epoch 2 batch 1300 train loss: 0.0004 test loss: 0.0006\n",
      "Epoch 2 batch 1400 train loss: 0.0004 test loss: 0.0006\n",
      "Epoch 2 batch 1500 train loss: 0.0003 test loss: 0.0006\n",
      "Epoch 2 batch 1600 train loss: 0.0005 test loss: 0.0007\n",
      "Epoch 2 batch 1700 train loss: 0.0003 test loss: 0.0007\n",
      "Epoch 2 batch 1800 train loss: 0.0008 test loss: 0.0006\n",
      "Epoch 2 batch 1900 train loss: 0.0004 test loss: 0.0005\n",
      "Epoch 2 batch 2000 train loss: 0.0004 test loss: 0.0006\n",
      "Epoch 2 batch 2100 train loss: 0.0004 test loss: 0.0006\n",
      "Epoch 2 batch 2200 train loss: 0.0010 test loss: 0.0005\n",
      "Epoch 2 batch 2300 train loss: 0.0003 test loss: 0.0006\n",
      "Epoch 2 batch 2400 train loss: 0.0003 test loss: 0.0006\n",
      "Epoch 2 batch 2500 train loss: 0.0006 test loss: 0.0005\n",
      "Epoch 2 batch 2600 train loss: 0.0003 test loss: 0.0005\n",
      "Epoch 2 batch 2700 train loss: 0.0003 test loss: 0.0005\n",
      "Epoch 2 batch 2800 train loss: 0.0003 test loss: 0.0005\n",
      "Epoch 2 batch 2900 train loss: 0.0003 test loss: 0.0005\n",
      "Epoch 3 batch 0 train loss: 0.0003 test loss: 0.0005\n",
      "Epoch 3 batch 100 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 3 batch 200 train loss: 0.0002 test loss: 0.0005\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.10p/ckpt-25\n",
      "Epoch 3 batch 300 train loss: 0.0003 test loss: 0.0005\n",
      "Epoch 3 batch 400 train loss: 0.0009 test loss: 0.0005\n",
      "Epoch 3 batch 500 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 3 batch 600 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 3 batch 700 train loss: 0.0003 test loss: 0.0005\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.10p/ckpt-26\n",
      "Epoch 3 batch 800 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 3 batch 900 train loss: 0.0003 test loss: 0.0006\n",
      "Epoch 3 batch 1000 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 3 batch 1100 train loss: 0.0003 test loss: 0.0005\n",
      "Epoch 3 batch 1200 train loss: 0.0003 test loss: 0.0005\n",
      "Epoch 3 batch 1300 train loss: 0.0002 test loss: 0.0005\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.10p/ckpt-27\n",
      "Epoch 3 batch 1400 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 3 batch 1500 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 3 batch 1600 train loss: 0.0003 test loss: 0.0005\n",
      "Epoch 3 batch 1700 train loss: 0.0003 test loss: 0.0005\n",
      "Epoch 3 batch 1800 train loss: 0.0003 test loss: 0.0005\n",
      "Epoch 3 batch 1900 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 3 batch 2000 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 3 batch 2100 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 3 batch 2200 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 3 batch 2300 train loss: 0.0003 test loss: 0.0005\n",
      "Epoch 3 batch 2400 train loss: 0.0003 test loss: 0.0005\n",
      "Epoch 3 batch 2500 train loss: 0.0002 test loss: 0.0005\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.10p/ckpt-28\n",
      "Epoch 3 batch 2600 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 3 batch 2700 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 3 batch 2800 train loss: 0.0003 test loss: 0.0005\n",
      "Epoch 3 batch 2900 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 4 batch 0 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 4 batch 100 train loss: 0.0003 test loss: 0.0005\n",
      "Epoch 4 batch 200 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 4 batch 300 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 4 batch 400 train loss: 0.0002 test loss: 0.0005\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.10p/ckpt-29\n",
      "Epoch 4 batch 500 train loss: 0.0018 test loss: 0.0004\n",
      "Epoch 4 batch 600 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 4 batch 700 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 4 batch 800 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 4 batch 900 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 4 batch 1000 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 4 batch 1100 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 4 batch 1200 train loss: 0.0004 test loss: 0.0005\n",
      "Epoch 4 batch 1300 train loss: 0.0003 test loss: 0.0005\n",
      "Epoch 4 batch 1400 train loss: 0.0003 test loss: 0.0005\n",
      "Epoch 4 batch 1500 train loss: 0.0002 test loss: 0.0004\n",
      "Epoch 4 batch 1600 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 4 batch 1700 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 4 batch 1800 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 4 batch 1900 train loss: 0.0002 test loss: 0.0005\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.10p/ckpt-30\n",
      "Epoch 4 batch 2000 train loss: 0.0011 test loss: 0.0004\n",
      "Epoch 4 batch 2100 train loss: 0.0002 test loss: 0.0004\n",
      "Epoch 4 batch 2200 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 4 batch 2300 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 4 batch 2400 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 4 batch 2500 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 4 batch 2600 train loss: 0.0001 test loss: 0.0005\n",
      "Epoch 4 batch 2700 train loss: 0.0002 test loss: 0.0004\n",
      "Epoch 4 batch 2800 train loss: 0.0005 test loss: 0.0005\n",
      "Epoch 4 batch 2900 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 5 batch 0 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 5 batch 100 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 5 batch 200 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 5 batch 300 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 5 batch 400 train loss: 0.0003 test loss: 0.0005\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.10p/ckpt-31\n",
      "Epoch 5 batch 500 train loss: 0.0002 test loss: 0.0004\n",
      "Epoch 5 batch 600 train loss: 0.0022 test loss: 0.0005\n",
      "Epoch 5 batch 700 train loss: 0.0002 test loss: 0.0005\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.10p/ckpt-32\n",
      "Epoch 5 batch 800 train loss: 0.0002 test loss: 0.0004\n",
      "Epoch 5 batch 900 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 5 batch 1000 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 5 batch 1100 train loss: 0.0002 test loss: 0.0004\n",
      "Epoch 5 batch 1200 train loss: 0.0001 test loss: 0.0005\n",
      "Epoch 5 batch 1300 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 5 batch 1400 train loss: 0.0002 test loss: 0.0005\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.10p/ckpt-33\n",
      "Epoch 5 batch 1500 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 5 batch 1600 train loss: 0.0002 test loss: 0.0004\n",
      "Epoch 5 batch 1700 train loss: 0.0012 test loss: 0.0005\n",
      "Epoch 5 batch 1800 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 5 batch 1900 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 5 batch 2000 train loss: 0.0002 test loss: 0.0004\n",
      "Epoch 5 batch 2100 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 5 batch 2200 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 5 batch 2300 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 5 batch 2400 train loss: 0.0003 test loss: 0.0005\n",
      "Epoch 5 batch 2500 train loss: 0.0003 test loss: 0.0005\n",
      "Epoch 5 batch 2600 train loss: 0.0002 test loss: 0.0004\n",
      "Epoch 5 batch 2700 train loss: 0.0002 test loss: 0.0004\n",
      "Epoch 5 batch 2800 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 5 batch 2900 train loss: 0.0003 test loss: 0.0004\n",
      "Epoch 6 batch 0 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 6 batch 100 train loss: 0.0002 test loss: 0.0004\n",
      "Epoch 6 batch 200 train loss: 0.0002 test loss: 0.0004\n",
      "Epoch 6 batch 300 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 6 batch 400 train loss: 0.0001 test loss: 0.0005\n",
      "Epoch 6 batch 500 train loss: 0.0013 test loss: 0.0005\n",
      "Epoch 6 batch 600 train loss: 0.0002 test loss: 0.0004\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.10p/ckpt-34\n",
      "Epoch 6 batch 700 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 6 batch 800 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 6 batch 900 train loss: 0.0002 test loss: 0.0004\n",
      "Epoch 6 batch 1000 train loss: 0.0021 test loss: 0.0005\n",
      "Epoch 6 batch 1100 train loss: 0.0002 test loss: 0.0004\n",
      "Epoch 6 batch 1200 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 6 batch 1300 train loss: 0.0001 test loss: 0.0005\n",
      "Epoch 6 batch 1400 train loss: 0.0002 test loss: 0.0004\n",
      "Epoch 6 batch 1500 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 6 batch 1600 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 6 batch 1700 train loss: 0.0002 test loss: 0.0004\n",
      "Epoch 6 batch 1800 train loss: 0.0008 test loss: 0.0005\n",
      "Epoch 6 batch 1900 train loss: 0.0021 test loss: 0.0005\n",
      "Epoch 6 batch 2000 train loss: 0.0013 test loss: 0.0005\n",
      "Epoch 6 batch 2100 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 6 batch 2200 train loss: 0.0009 test loss: 0.0005\n",
      "Epoch 6 batch 2300 train loss: 0.0001 test loss: 0.0005\n",
      "Epoch 6 batch 2400 train loss: 0.0003 test loss: 0.0005\n",
      "Epoch 6 batch 2500 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 6 batch 2600 train loss: 0.0021 test loss: 0.0005\n",
      "Epoch 6 batch 2700 train loss: 0.0012 test loss: 0.0004\n",
      "Epoch 6 batch 2800 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 6 batch 2900 train loss: 0.0002 test loss: 0.0004\n",
      "Epoch 7 batch 0 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 7 batch 100 train loss: 0.0012 test loss: 0.0004\n",
      "Epoch 7 batch 200 train loss: 0.0002 test loss: 0.0004\n",
      "Epoch 7 batch 300 train loss: 0.0001 test loss: 0.0005\n",
      "Epoch 7 batch 400 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 7 batch 500 train loss: 0.0003 test loss: 0.0004\n",
      "Epoch 7 batch 600 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 7 batch 700 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 7 batch 800 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 7 batch 900 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 7 batch 1000 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 7 batch 1100 train loss: 0.0002 test loss: 0.0004\n",
      "Epoch 7 batch 1200 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 7 batch 1300 train loss: 0.0001 test loss: 0.0005\n",
      "Epoch 7 batch 1400 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 7 batch 1500 train loss: 0.0002 test loss: 0.0004\n",
      "Epoch 7 batch 1600 train loss: 0.0002 test loss: 0.0004\n",
      "Epoch 7 batch 1700 train loss: 0.0002 test loss: 0.0004\n",
      "Epoch 7 batch 1800 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 7 batch 1900 train loss: 0.0001 test loss: 0.0005\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.10p/ckpt-35\n",
      "Epoch 7 batch 2000 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 7 batch 2100 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 7 batch 2200 train loss: 0.0025 test loss: 0.0005\n",
      "Epoch 7 batch 2300 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 7 batch 2400 train loss: 0.0001 test loss: 0.0005\n",
      "Epoch 7 batch 2500 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 7 batch 2600 train loss: 0.0009 test loss: 0.0004\n",
      "Epoch 7 batch 2700 train loss: 0.0002 test loss: 0.0004\n",
      "Epoch 7 batch 2800 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 7 batch 2900 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 8 batch 0 train loss: 0.0002 test loss: 0.0004\n",
      "Epoch 8 batch 100 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 8 batch 200 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 8 batch 300 train loss: 0.0013 test loss: 0.0004\n",
      "Epoch 8 batch 400 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 8 batch 500 train loss: 0.0002 test loss: 0.0004\n",
      "Epoch 8 batch 600 train loss: 0.0002 test loss: 0.0004\n",
      "Epoch 8 batch 700 train loss: 0.0003 test loss: 0.0004\n",
      "Epoch 8 batch 800 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 8 batch 900 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 8 batch 1000 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 8 batch 1100 train loss: 0.0002 test loss: 0.0004\n",
      "Epoch 8 batch 1200 train loss: 0.0002 test loss: 0.0004\n",
      "Epoch 8 batch 1300 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 8 batch 1400 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 8 batch 1500 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 8 batch 1600 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 8 batch 1700 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 8 batch 1800 train loss: 0.0011 test loss: 0.0005\n",
      "Epoch 8 batch 1900 train loss: 0.0002 test loss: 0.0004\n",
      "Epoch 8 batch 2000 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 8 batch 2100 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 8 batch 2200 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 8 batch 2300 train loss: 0.0001 test loss: 0.0005\n",
      "Epoch 8 batch 2400 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 8 batch 2500 train loss: 0.0002 test loss: 0.0004\n",
      "Epoch 8 batch 2600 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 8 batch 2700 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 8 batch 2800 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 8 batch 2900 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 9 batch 0 train loss: 0.0001 test loss: 0.0005\n",
      "Epoch 9 batch 100 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 9 batch 200 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 9 batch 300 train loss: 0.0019 test loss: 0.0004\n",
      "Epoch 9 batch 400 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 9 batch 500 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 9 batch 600 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 9 batch 700 train loss: 0.0001 test loss: 0.0005\n",
      "Epoch 9 batch 800 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 9 batch 900 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 9 batch 1000 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 9 batch 1100 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 9 batch 1200 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 9 batch 1300 train loss: 0.0018 test loss: 0.0005\n",
      "Epoch 9 batch 1400 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 9 batch 1500 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 9 batch 1600 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 9 batch 1700 train loss: 0.0001 test loss: 0.0004\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.10p/ckpt-36\n",
      "Epoch 9 batch 1800 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 9 batch 1900 train loss: 0.0001 test loss: 0.0005\n",
      "Epoch 9 batch 2000 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 9 batch 2100 train loss: 0.0001 test loss: 0.0005\n",
      "Epoch 9 batch 2200 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 9 batch 2300 train loss: 0.0002 test loss: 0.0004\n",
      "Epoch 9 batch 2400 train loss: 0.0002 test loss: 0.0004\n",
      "Epoch 9 batch 2500 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 9 batch 2600 train loss: 0.0005 test loss: 0.0004\n",
      "Epoch 9 batch 2700 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 9 batch 2800 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 9 batch 2900 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 10 batch 0 train loss: 0.0012 test loss: 0.0005\n",
      "Epoch 10 batch 100 train loss: 0.0001 test loss: 0.0004\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.10p/ckpt-37\n",
      "Epoch 10 batch 200 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 10 batch 300 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 10 batch 400 train loss: 0.0001 test loss: 0.0005\n",
      "Epoch 10 batch 500 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 10 batch 600 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 10 batch 700 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 10 batch 800 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 10 batch 900 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 10 batch 1000 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 10 batch 1100 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 10 batch 1200 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 10 batch 1300 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 10 batch 1400 train loss: 0.0020 test loss: 0.0004\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.10p/ckpt-38\n",
      "Epoch 10 batch 1500 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 10 batch 1600 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 10 batch 1700 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 10 batch 1800 train loss: 0.0001 test loss: 0.0005\n",
      "Epoch 10 batch 1900 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 10 batch 2000 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 10 batch 2100 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 10 batch 2200 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 10 batch 2300 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 10 batch 2400 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 10 batch 2500 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 10 batch 2600 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 10 batch 2700 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 10 batch 2800 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 10 batch 2900 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 11 batch 0 train loss: 0.0001 test loss: 0.0005\n",
      "Epoch 11 batch 100 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 11 batch 200 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 11 batch 300 train loss: 0.0005 test loss: 0.0004\n",
      "Epoch 11 batch 400 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 11 batch 500 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 11 batch 600 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 11 batch 700 train loss: 0.0013 test loss: 0.0004\n",
      "Epoch 11 batch 800 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 11 batch 900 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 11 batch 1000 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 11 batch 1100 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 11 batch 1200 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 11 batch 1300 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 11 batch 1400 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 11 batch 1500 train loss: 0.0001 test loss: 0.0004\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.10p/ckpt-39\n",
      "Epoch 11 batch 1600 train loss: 0.0006 test loss: 0.0004\n",
      "Epoch 11 batch 1700 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 11 batch 1800 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 11 batch 1900 train loss: 0.0003 test loss: 0.0004\n",
      "Epoch 11 batch 2000 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 11 batch 2100 train loss: 0.0001 test loss: 0.0004\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.10p/ckpt-40\n",
      "Epoch 11 batch 2200 train loss: 0.0010 test loss: 0.0004\n",
      "Epoch 11 batch 2300 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 11 batch 2400 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 11 batch 2500 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 11 batch 2600 train loss: 0.0001 test loss: 0.0005\n",
      "Epoch 11 batch 2700 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 11 batch 2800 train loss: 0.0006 test loss: 0.0004\n",
      "Epoch 11 batch 2900 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 12 batch 0 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 12 batch 100 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 12 batch 200 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 12 batch 300 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 12 batch 400 train loss: 0.0020 test loss: 0.0004\n",
      "Epoch 12 batch 500 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 12 batch 600 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 12 batch 700 train loss: 0.0003 test loss: 0.0004\n",
      "Epoch 12 batch 800 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 12 batch 900 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 12 batch 1000 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 12 batch 1100 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 12 batch 1200 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 12 batch 1300 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 12 batch 1400 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 12 batch 1500 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 12 batch 1600 train loss: 0.0001 test loss: 0.0005\n",
      "Epoch 12 batch 1700 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 12 batch 1800 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 12 batch 1900 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 12 batch 2000 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 12 batch 2100 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 12 batch 2200 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 12 batch 2300 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 12 batch 2400 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 12 batch 2500 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 12 batch 2600 train loss: 0.0010 test loss: 0.0004\n",
      "Epoch 12 batch 2700 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 12 batch 2800 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 12 batch 2900 train loss: 0.0015 test loss: 0.0004\n",
      "Epoch 13 batch 0 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 13 batch 100 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 13 batch 200 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 13 batch 300 train loss: 0.0013 test loss: 0.0004\n",
      "Epoch 13 batch 400 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 13 batch 500 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 13 batch 600 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 13 batch 700 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 13 batch 800 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 13 batch 900 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 13 batch 1000 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 13 batch 1100 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 13 batch 1200 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 13 batch 1300 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 13 batch 1400 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 13 batch 1500 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 13 batch 1600 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 13 batch 1700 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 13 batch 1800 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 13 batch 1900 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 13 batch 2000 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 13 batch 2100 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 13 batch 2200 train loss: 0.0005 test loss: 0.0006\n",
      "Epoch 13 batch 2300 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 13 batch 2400 train loss: 0.0012 test loss: 0.0004\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.10p/ckpt-41\n",
      "Epoch 13 batch 2500 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 13 batch 2600 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 13 batch 2700 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 13 batch 2800 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 13 batch 2900 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 14 batch 0 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 14 batch 100 train loss: 0.0011 test loss: 0.0004\n",
      "Epoch 14 batch 200 train loss: 0.0002 test loss: 0.0004\n",
      "Epoch 14 batch 300 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 14 batch 400 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 14 batch 500 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 14 batch 600 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 14 batch 700 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 14 batch 800 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 14 batch 900 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 14 batch 1000 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 14 batch 1100 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 14 batch 1200 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 14 batch 1300 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 14 batch 1400 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 14 batch 1500 train loss: 0.0024 test loss: 0.0004\n",
      "Epoch 14 batch 1600 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 14 batch 1700 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 14 batch 1800 train loss: 0.0013 test loss: 0.0004\n",
      "Epoch 14 batch 1900 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 14 batch 2000 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 14 batch 2100 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 14 batch 2200 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 14 batch 2300 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 14 batch 2400 train loss: 0.0006 test loss: 0.0004\n",
      "Epoch 14 batch 2500 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 14 batch 2600 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 14 batch 2700 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 14 batch 2800 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 14 batch 2900 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 15 batch 0 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 15 batch 100 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 15 batch 200 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 15 batch 300 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 15 batch 400 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 15 batch 500 train loss: 0.0001 test loss: 0.0005\n",
      "Epoch 15 batch 600 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 15 batch 700 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 15 batch 800 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 15 batch 900 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 15 batch 1000 train loss: 0.0008 test loss: 0.0004\n",
      "Epoch 15 batch 1100 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 15 batch 1200 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 15 batch 1300 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 15 batch 1400 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 15 batch 1500 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 15 batch 1600 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 15 batch 1700 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 15 batch 1800 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 15 batch 1900 train loss: 0.0002 test loss: 0.0005\n",
      "Epoch 15 batch 2000 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 15 batch 2100 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 15 batch 2200 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 15 batch 2300 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 15 batch 2400 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 15 batch 2500 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 15 batch 2600 train loss: 0.0016 test loss: 0.0004\n",
      "Epoch 15 batch 2700 train loss: 0.0015 test loss: 0.0004\n",
      "Epoch 15 batch 2800 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 15 batch 2900 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 16 batch 0 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 16 batch 100 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 16 batch 200 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 16 batch 300 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 16 batch 400 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 16 batch 500 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 16 batch 600 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 16 batch 700 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 16 batch 800 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 16 batch 900 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 16 batch 1000 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 16 batch 1100 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 16 batch 1200 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 16 batch 1300 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 16 batch 1400 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 16 batch 1500 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 16 batch 1600 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 16 batch 1700 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 16 batch 1800 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 16 batch 1900 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 16 batch 2000 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 16 batch 2100 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 16 batch 2200 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 16 batch 2300 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 16 batch 2400 train loss: 0.0001 test loss: 0.0005\n",
      "Epoch 16 batch 2500 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 16 batch 2600 train loss: 0.0001 test loss: 0.0005\n",
      "Epoch 16 batch 2700 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 16 batch 2800 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 16 batch 2900 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 17 batch 0 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 17 batch 100 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 17 batch 200 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 17 batch 300 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 17 batch 400 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 17 batch 500 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 17 batch 600 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 17 batch 700 train loss: 0.0019 test loss: 0.0004\n",
      "Epoch 17 batch 800 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 17 batch 900 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 17 batch 1000 train loss: 0.0002 test loss: 0.0004\n",
      "Epoch 17 batch 1100 train loss: 0.0011 test loss: 0.0004\n",
      "Epoch 17 batch 1200 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 17 batch 1300 train loss: 0.0017 test loss: 0.0004\n",
      "Epoch 17 batch 1400 train loss: 0.0002 test loss: 0.0004\n",
      "Epoch 17 batch 1500 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 17 batch 1600 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 17 batch 1700 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 17 batch 1800 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 17 batch 1900 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 17 batch 2000 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 17 batch 2100 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 17 batch 2200 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 17 batch 2300 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 17 batch 2400 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 17 batch 2500 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 17 batch 2600 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 17 batch 2700 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 17 batch 2800 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 17 batch 2900 train loss: 0.0001 test loss: 0.0004\n",
      "Epoch 18 batch 0 train loss: 0.0001 test loss: 0.0004\n",
      "early stop.\n",
      "Checkpoint 41 restored!!\n",
      "2446/2446 [==============================] - 16s 7ms/step - loss: 0.0013\n",
      "Training for loss rate 0.20 start.\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.20p/ckpt-1\n",
      "Epoch 0 batch 0 train loss: 0.0520 test loss: 0.0643\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.20p/ckpt-2\n",
      "Epoch 0 batch 100 train loss: 0.0070 test loss: 0.0073\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.20p/ckpt-3\n",
      "Epoch 0 batch 200 train loss: 0.0056 test loss: 0.0058\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.20p/ckpt-4\n",
      "Epoch 0 batch 300 train loss: 0.0035 test loss: 0.0037\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.20p/ckpt-5\n",
      "Epoch 0 batch 400 train loss: 0.0024 test loss: 0.0035\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.20p/ckpt-6\n",
      "Epoch 0 batch 500 train loss: 0.0047 test loss: 0.0029\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.20p/ckpt-7\n",
      "Epoch 0 batch 600 train loss: 0.0043 test loss: 0.0027\n",
      "Epoch 0 batch 700 train loss: 0.0013 test loss: 0.0038\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.20p/ckpt-8\n",
      "Epoch 0 batch 800 train loss: 0.0025 test loss: 0.0025\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.20p/ckpt-9\n",
      "Epoch 0 batch 900 train loss: 0.0012 test loss: 0.0024\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.20p/ckpt-10\n",
      "Epoch 0 batch 1000 train loss: 0.0018 test loss: 0.0022\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.20p/ckpt-11\n",
      "Epoch 0 batch 1100 train loss: 0.0025 test loss: 0.0022\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.20p/ckpt-12\n",
      "Epoch 0 batch 1200 train loss: 0.0023 test loss: 0.0021\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.20p/ckpt-13\n",
      "Epoch 0 batch 1300 train loss: 0.0014 test loss: 0.0021\n",
      "Epoch 0 batch 1400 train loss: 0.0012 test loss: 0.0022\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.20p/ckpt-14\n",
      "Epoch 0 batch 1500 train loss: 0.0019 test loss: 0.0020\n",
      "Epoch 0 batch 1600 train loss: 0.0014 test loss: 0.0021\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.20p/ckpt-15\n",
      "Epoch 0 batch 1700 train loss: 0.0016 test loss: 0.0019\n",
      "Epoch 0 batch 1800 train loss: 0.0034 test loss: 0.0021\n",
      "Epoch 0 batch 1900 train loss: 0.0013 test loss: 0.0020\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.20p/ckpt-16\n",
      "Epoch 0 batch 2000 train loss: 0.0014 test loss: 0.0019\n",
      "Epoch 0 batch 2100 train loss: 0.0032 test loss: 0.0019\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.20p/ckpt-17\n",
      "Epoch 0 batch 2200 train loss: 0.0015 test loss: 0.0018\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.20p/ckpt-18\n",
      "Epoch 0 batch 2300 train loss: 0.0011 test loss: 0.0018\n",
      "Epoch 0 batch 2400 train loss: 0.0019 test loss: 0.0023\n",
      "Epoch 0 batch 2500 train loss: 0.0017 test loss: 0.0018\n",
      "Epoch 0 batch 2600 train loss: 0.0011 test loss: 0.0019\n",
      "Epoch 0 batch 2700 train loss: 0.0011 test loss: 0.0019\n",
      "Epoch 0 batch 2800 train loss: 0.0008 test loss: 0.0021\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.20p/ckpt-19\n",
      "Epoch 0 batch 2900 train loss: 0.0007 test loss: 0.0017\n",
      "Epoch 1 batch 0 train loss: 0.0011 test loss: 0.0017\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.20p/ckpt-20\n",
      "Epoch 1 batch 100 train loss: 0.0027 test loss: 0.0017\n",
      "Epoch 1 batch 200 train loss: 0.0014 test loss: 0.0019\n",
      "Epoch 1 batch 300 train loss: 0.0009 test loss: 0.0018\n",
      "Epoch 1 batch 400 train loss: 0.0019 test loss: 0.0017\n",
      "Epoch 1 batch 500 train loss: 0.0009 test loss: 0.0018\n",
      "Epoch 1 batch 600 train loss: 0.0024 test loss: 0.0018\n",
      "Epoch 1 batch 700 train loss: 0.0008 test loss: 0.0018\n",
      "Epoch 1 batch 800 train loss: 0.0006 test loss: 0.0017\n",
      "Epoch 1 batch 900 train loss: 0.0007 test loss: 0.0019\n",
      "Epoch 1 batch 1000 train loss: 0.0009 test loss: 0.0018\n",
      "Epoch 1 batch 1100 train loss: 0.0006 test loss: 0.0017\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.20p/ckpt-21\n",
      "Epoch 1 batch 1200 train loss: 0.0020 test loss: 0.0017\n",
      "Epoch 1 batch 1300 train loss: 0.0008 test loss: 0.0019\n",
      "Epoch 1 batch 1400 train loss: 0.0023 test loss: 0.0017\n",
      "Epoch 1 batch 1500 train loss: 0.0007 test loss: 0.0017\n",
      "Epoch 1 batch 1600 train loss: 0.0009 test loss: 0.0017\n",
      "Epoch 1 batch 1700 train loss: 0.0010 test loss: 0.0017\n",
      "Epoch 1 batch 1800 train loss: 0.0007 test loss: 0.0018\n",
      "Epoch 1 batch 1900 train loss: 0.0010 test loss: 0.0018\n",
      "Epoch 1 batch 2000 train loss: 0.0005 test loss: 0.0017\n",
      "Epoch 1 batch 2100 train loss: 0.0019 test loss: 0.0019\n",
      "Epoch 1 batch 2200 train loss: 0.0003 test loss: 0.0017\n",
      "Epoch 1 batch 2300 train loss: 0.0007 test loss: 0.0020\n",
      "Epoch 1 batch 2400 train loss: 0.0007 test loss: 0.0018\n",
      "Epoch 1 batch 2500 train loss: 0.0016 test loss: 0.0017\n",
      "Epoch 1 batch 2600 train loss: 0.0021 test loss: 0.0017\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.20p/ckpt-22\n",
      "Epoch 1 batch 2700 train loss: 0.0006 test loss: 0.0016\n",
      "Epoch 1 batch 2800 train loss: 0.0009 test loss: 0.0016\n",
      "Epoch 1 batch 2900 train loss: 0.0008 test loss: 0.0017\n",
      "Epoch 2 batch 0 train loss: 0.0007 test loss: 0.0017\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.20p/ckpt-23\n",
      "Epoch 2 batch 100 train loss: 0.0008 test loss: 0.0016\n",
      "Epoch 2 batch 200 train loss: 0.0011 test loss: 0.0018\n",
      "Epoch 2 batch 300 train loss: 0.0011 test loss: 0.0018\n",
      "Epoch 2 batch 400 train loss: 0.0004 test loss: 0.0016\n",
      "Epoch 2 batch 500 train loss: 0.0009 test loss: 0.0018\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.20p/ckpt-24\n",
      "Epoch 2 batch 600 train loss: 0.0006 test loss: 0.0016\n",
      "Epoch 2 batch 700 train loss: 0.0014 test loss: 0.0017\n",
      "Epoch 2 batch 800 train loss: 0.0005 test loss: 0.0018\n",
      "Epoch 2 batch 900 train loss: 0.0005 test loss: 0.0017\n",
      "Epoch 2 batch 1000 train loss: 0.0019 test loss: 0.0018\n",
      "Epoch 2 batch 1100 train loss: 0.0024 test loss: 0.0016\n",
      "Epoch 2 batch 1200 train loss: 0.0013 test loss: 0.0017\n",
      "Epoch 2 batch 1300 train loss: 0.0004 test loss: 0.0017\n",
      "Epoch 2 batch 1400 train loss: 0.0006 test loss: 0.0017\n",
      "Epoch 2 batch 1500 train loss: 0.0005 test loss: 0.0016\n",
      "Epoch 2 batch 1600 train loss: 0.0005 test loss: 0.0017\n",
      "Epoch 2 batch 1700 train loss: 0.0004 test loss: 0.0016\n",
      "Epoch 2 batch 1800 train loss: 0.0005 test loss: 0.0017\n",
      "Epoch 2 batch 1900 train loss: 0.0005 test loss: 0.0017\n",
      "Epoch 2 batch 2000 train loss: 0.0015 test loss: 0.0017\n",
      "Epoch 2 batch 2100 train loss: 0.0004 test loss: 0.0016\n",
      "Epoch 2 batch 2200 train loss: 0.0004 test loss: 0.0016\n",
      "Epoch 2 batch 2300 train loss: 0.0012 test loss: 0.0017\n",
      "Epoch 2 batch 2400 train loss: 0.0007 test loss: 0.0018\n",
      "Epoch 2 batch 2500 train loss: 0.0014 test loss: 0.0017\n",
      "Epoch 2 batch 2600 train loss: 0.0006 test loss: 0.0016\n",
      "Epoch 2 batch 2700 train loss: 0.0006 test loss: 0.0017\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.20p/ckpt-25\n",
      "Epoch 2 batch 2800 train loss: 0.0005 test loss: 0.0016\n",
      "Epoch 2 batch 2900 train loss: 0.0021 test loss: 0.0016\n",
      "Epoch 3 batch 0 train loss: 0.0005 test loss: 0.0017\n",
      "Epoch 3 batch 100 train loss: 0.0004 test loss: 0.0017\n",
      "Epoch 3 batch 200 train loss: 0.0019 test loss: 0.0018\n",
      "Epoch 3 batch 300 train loss: 0.0022 test loss: 0.0016\n",
      "Epoch 3 batch 400 train loss: 0.0004 test loss: 0.0016\n",
      "Epoch 3 batch 500 train loss: 0.0006 test loss: 0.0016\n",
      "Epoch 3 batch 600 train loss: 0.0004 test loss: 0.0017\n",
      "Epoch 3 batch 700 train loss: 0.0008 test loss: 0.0016\n",
      "Epoch 3 batch 800 train loss: 0.0005 test loss: 0.0017\n",
      "Epoch 3 batch 900 train loss: 0.0006 test loss: 0.0016\n",
      "Epoch 3 batch 1000 train loss: 0.0006 test loss: 0.0016\n",
      "Epoch 3 batch 1100 train loss: 0.0006 test loss: 0.0016\n",
      "Epoch 3 batch 1200 train loss: 0.0020 test loss: 0.0016\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.20p/ckpt-26\n",
      "Epoch 3 batch 1300 train loss: 0.0004 test loss: 0.0016\n",
      "Epoch 3 batch 1400 train loss: 0.0003 test loss: 0.0018\n",
      "Epoch 3 batch 1500 train loss: 0.0005 test loss: 0.0016\n",
      "Epoch 3 batch 1600 train loss: 0.0005 test loss: 0.0017\n",
      "Epoch 3 batch 1700 train loss: 0.0005 test loss: 0.0016\n",
      "Epoch 3 batch 1800 train loss: 0.0016 test loss: 0.0016\n",
      "Epoch 3 batch 1900 train loss: 0.0003 test loss: 0.0016\n",
      "Epoch 3 batch 2000 train loss: 0.0004 test loss: 0.0017\n",
      "Epoch 3 batch 2100 train loss: 0.0009 test loss: 0.0016\n",
      "Epoch 3 batch 2200 train loss: 0.0010 test loss: 0.0016\n",
      "Epoch 3 batch 2300 train loss: 0.0003 test loss: 0.0016\n",
      "Epoch 3 batch 2400 train loss: 0.0003 test loss: 0.0016\n",
      "Epoch 3 batch 2500 train loss: 0.0005 test loss: 0.0017\n",
      "Epoch 3 batch 2600 train loss: 0.0004 test loss: 0.0016\n",
      "Epoch 3 batch 2700 train loss: 0.0014 test loss: 0.0016\n",
      "Epoch 3 batch 2800 train loss: 0.0004 test loss: 0.0016\n",
      "Epoch 3 batch 2900 train loss: 0.0005 test loss: 0.0016\n",
      "Epoch 4 batch 0 train loss: 0.0004 test loss: 0.0016\n",
      "Epoch 4 batch 100 train loss: 0.0005 test loss: 0.0017\n",
      "Epoch 4 batch 200 train loss: 0.0021 test loss: 0.0016\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.20p/ckpt-27\n",
      "Epoch 4 batch 300 train loss: 0.0012 test loss: 0.0015\n",
      "Epoch 4 batch 400 train loss: 0.0003 test loss: 0.0017\n",
      "Epoch 4 batch 500 train loss: 0.0004 test loss: 0.0016\n",
      "Epoch 4 batch 600 train loss: 0.0019 test loss: 0.0016\n",
      "Epoch 4 batch 700 train loss: 0.0012 test loss: 0.0016\n",
      "Epoch 4 batch 800 train loss: 0.0007 test loss: 0.0016\n",
      "Epoch 4 batch 900 train loss: 0.0004 test loss: 0.0016\n",
      "Epoch 4 batch 1000 train loss: 0.0023 test loss: 0.0016\n",
      "Epoch 4 batch 1100 train loss: 0.0023 test loss: 0.0016\n",
      "Epoch 4 batch 1200 train loss: 0.0004 test loss: 0.0016\n",
      "Epoch 4 batch 1300 train loss: 0.0003 test loss: 0.0015\n",
      "Epoch 4 batch 1400 train loss: 0.0003 test loss: 0.0016\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.20p/ckpt-28\n",
      "Epoch 4 batch 1500 train loss: 0.0004 test loss: 0.0015\n",
      "Epoch 4 batch 1600 train loss: 0.0003 test loss: 0.0017\n",
      "Epoch 4 batch 1700 train loss: 0.0003 test loss: 0.0016\n",
      "Epoch 4 batch 1800 train loss: 0.0003 test loss: 0.0016\n",
      "Epoch 4 batch 1900 train loss: 0.0003 test loss: 0.0016\n",
      "Epoch 4 batch 2000 train loss: 0.0013 test loss: 0.0017\n",
      "Epoch 4 batch 2100 train loss: 0.0009 test loss: 0.0016\n",
      "Epoch 4 batch 2200 train loss: 0.0003 test loss: 0.0016\n",
      "Epoch 4 batch 2300 train loss: 0.0003 test loss: 0.0015\n",
      "Epoch 4 batch 2400 train loss: 0.0006 test loss: 0.0016\n",
      "Epoch 4 batch 2500 train loss: 0.0003 test loss: 0.0016\n",
      "Epoch 4 batch 2600 train loss: 0.0004 test loss: 0.0015\n",
      "Epoch 4 batch 2700 train loss: 0.0011 test loss: 0.0015\n",
      "Epoch 4 batch 2800 train loss: 0.0004 test loss: 0.0015\n",
      "Epoch 4 batch 2900 train loss: 0.0004 test loss: 0.0016\n",
      "Epoch 5 batch 0 train loss: 0.0002 test loss: 0.0017\n",
      "Epoch 5 batch 100 train loss: 0.0003 test loss: 0.0016\n",
      "Epoch 5 batch 200 train loss: 0.0003 test loss: 0.0017\n",
      "Epoch 5 batch 300 train loss: 0.0003 test loss: 0.0016\n",
      "Epoch 5 batch 400 train loss: 0.0008 test loss: 0.0017\n",
      "Epoch 5 batch 500 train loss: 0.0003 test loss: 0.0016\n",
      "Epoch 5 batch 600 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 5 batch 700 train loss: 0.0038 test loss: 0.0015\n",
      "Epoch 5 batch 800 train loss: 0.0003 test loss: 0.0016\n",
      "Epoch 5 batch 900 train loss: 0.0011 test loss: 0.0015\n",
      "Epoch 5 batch 1000 train loss: 0.0004 test loss: 0.0016\n",
      "Epoch 5 batch 1100 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 5 batch 1200 train loss: 0.0012 test loss: 0.0017\n",
      "Epoch 5 batch 1300 train loss: 0.0010 test loss: 0.0015\n",
      "Epoch 5 batch 1400 train loss: 0.0003 test loss: 0.0016\n",
      "Epoch 5 batch 1500 train loss: 0.0003 test loss: 0.0015\n",
      "Epoch 5 batch 1600 train loss: 0.0010 test loss: 0.0016\n",
      "Epoch 5 batch 1700 train loss: 0.0006 test loss: 0.0016\n",
      "Epoch 5 batch 1800 train loss: 0.0003 test loss: 0.0015\n",
      "Epoch 5 batch 1900 train loss: 0.0022 test loss: 0.0015\n",
      "Epoch 5 batch 2000 train loss: 0.0003 test loss: 0.0015\n",
      "Epoch 5 batch 2100 train loss: 0.0003 test loss: 0.0015\n",
      "Epoch 5 batch 2200 train loss: 0.0004 test loss: 0.0016\n",
      "Epoch 5 batch 2300 train loss: 0.0003 test loss: 0.0016\n",
      "Epoch 5 batch 2400 train loss: 0.0008 test loss: 0.0015\n",
      "Epoch 5 batch 2500 train loss: 0.0003 test loss: 0.0015\n",
      "Epoch 5 batch 2600 train loss: 0.0003 test loss: 0.0015\n",
      "Epoch 5 batch 2700 train loss: 0.0003 test loss: 0.0016\n",
      "Epoch 5 batch 2800 train loss: 0.0003 test loss: 0.0015\n",
      "Epoch 5 batch 2900 train loss: 0.0010 test loss: 0.0015\n",
      "Epoch 6 batch 0 train loss: 0.0011 test loss: 0.0015\n",
      "Epoch 6 batch 100 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 6 batch 200 train loss: 0.0015 test loss: 0.0016\n",
      "Epoch 6 batch 300 train loss: 0.0005 test loss: 0.0016\n",
      "Epoch 6 batch 400 train loss: 0.0018 test loss: 0.0016\n",
      "Epoch 6 batch 500 train loss: 0.0003 test loss: 0.0016\n",
      "Epoch 6 batch 600 train loss: 0.0012 test loss: 0.0016\n",
      "Epoch 6 batch 700 train loss: 0.0022 test loss: 0.0016\n",
      "Epoch 6 batch 800 train loss: 0.0011 test loss: 0.0016\n",
      "Epoch 6 batch 900 train loss: 0.0003 test loss: 0.0015\n",
      "Epoch 6 batch 1000 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 6 batch 1100 train loss: 0.0004 test loss: 0.0017\n",
      "Epoch 6 batch 1200 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 6 batch 1300 train loss: 0.0003 test loss: 0.0015\n",
      "Epoch 6 batch 1400 train loss: 0.0032 test loss: 0.0016\n",
      "Epoch 6 batch 1500 train loss: 0.0011 test loss: 0.0015\n",
      "Epoch 6 batch 1600 train loss: 0.0003 test loss: 0.0016\n",
      "Epoch 6 batch 1700 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 6 batch 1800 train loss: 0.0003 test loss: 0.0016\n",
      "Epoch 6 batch 1900 train loss: 0.0009 test loss: 0.0015\n",
      "Epoch 6 batch 2000 train loss: 0.0008 test loss: 0.0016\n",
      "Epoch 6 batch 2100 train loss: 0.0003 test loss: 0.0016\n",
      "Epoch 6 batch 2200 train loss: 0.0023 test loss: 0.0015\n",
      "Epoch 6 batch 2300 train loss: 0.0022 test loss: 0.0016\n",
      "Epoch 6 batch 2400 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 6 batch 2500 train loss: 0.0002 test loss: 0.0016\n",
      "Epoch 6 batch 2600 train loss: 0.0014 test loss: 0.0016\n",
      "Epoch 6 batch 2700 train loss: 0.0008 test loss: 0.0015\n",
      "Epoch 6 batch 2800 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 6 batch 2900 train loss: 0.0015 test loss: 0.0015\n",
      "Epoch 7 batch 0 train loss: 0.0003 test loss: 0.0015\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.20p/ckpt-29\n",
      "Epoch 7 batch 100 train loss: 0.0013 test loss: 0.0014\n",
      "Epoch 7 batch 200 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 7 batch 300 train loss: 0.0004 test loss: 0.0016\n",
      "Epoch 7 batch 400 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 7 batch 500 train loss: 0.0003 test loss: 0.0015\n",
      "Epoch 7 batch 600 train loss: 0.0002 test loss: 0.0016\n",
      "Epoch 7 batch 700 train loss: 0.0013 test loss: 0.0016\n",
      "Epoch 7 batch 800 train loss: 0.0002 test loss: 0.0016\n",
      "Epoch 7 batch 900 train loss: 0.0002 test loss: 0.0016\n",
      "Epoch 7 batch 1000 train loss: 0.0005 test loss: 0.0015\n",
      "Epoch 7 batch 1100 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 7 batch 1200 train loss: 0.0003 test loss: 0.0017\n",
      "Epoch 7 batch 1300 train loss: 0.0010 test loss: 0.0015\n",
      "Epoch 7 batch 1400 train loss: 0.0002 test loss: 0.0016\n",
      "Epoch 7 batch 1500 train loss: 0.0030 test loss: 0.0015\n",
      "Epoch 7 batch 1600 train loss: 0.0019 test loss: 0.0016\n",
      "Epoch 7 batch 1700 train loss: 0.0003 test loss: 0.0016\n",
      "Epoch 7 batch 1800 train loss: 0.0003 test loss: 0.0015\n",
      "Epoch 7 batch 1900 train loss: 0.0009 test loss: 0.0015\n",
      "Epoch 7 batch 2000 train loss: 0.0002 test loss: 0.0016\n",
      "Epoch 7 batch 2100 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 7 batch 2200 train loss: 0.0003 test loss: 0.0015\n",
      "Epoch 7 batch 2300 train loss: 0.0019 test loss: 0.0015\n",
      "Epoch 7 batch 2400 train loss: 0.0010 test loss: 0.0016\n",
      "Epoch 7 batch 2500 train loss: 0.0002 test loss: 0.0016\n",
      "Epoch 7 batch 2600 train loss: 0.0006 test loss: 0.0016\n",
      "Epoch 7 batch 2700 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 7 batch 2800 train loss: 0.0003 test loss: 0.0015\n",
      "Epoch 7 batch 2900 train loss: 0.0003 test loss: 0.0015\n",
      "Epoch 8 batch 0 train loss: 0.0005 test loss: 0.0015\n",
      "Epoch 8 batch 100 train loss: 0.0002 test loss: 0.0016\n",
      "Epoch 8 batch 200 train loss: 0.0002 test loss: 0.0016\n",
      "Epoch 8 batch 300 train loss: 0.0002 test loss: 0.0016\n",
      "Epoch 8 batch 400 train loss: 0.0016 test loss: 0.0016\n",
      "Epoch 8 batch 500 train loss: 0.0012 test loss: 0.0016\n",
      "Epoch 8 batch 600 train loss: 0.0011 test loss: 0.0016\n",
      "Epoch 8 batch 700 train loss: 0.0006 test loss: 0.0016\n",
      "Epoch 8 batch 800 train loss: 0.0009 test loss: 0.0016\n",
      "Epoch 8 batch 900 train loss: 0.0004 test loss: 0.0016\n",
      "Epoch 8 batch 1000 train loss: 0.0005 test loss: 0.0015\n",
      "Epoch 8 batch 1100 train loss: 0.0011 test loss: 0.0016\n",
      "Epoch 8 batch 1200 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 8 batch 1300 train loss: 0.0009 test loss: 0.0016\n",
      "Epoch 8 batch 1400 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 8 batch 1500 train loss: 0.0010 test loss: 0.0015\n",
      "Epoch 8 batch 1600 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 8 batch 1700 train loss: 0.0002 test loss: 0.0016\n",
      "Epoch 8 batch 1800 train loss: 0.0018 test loss: 0.0015\n",
      "Epoch 8 batch 1900 train loss: 0.0013 test loss: 0.0015\n",
      "Epoch 8 batch 2000 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 8 batch 2100 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 8 batch 2200 train loss: 0.0023 test loss: 0.0015\n",
      "Epoch 8 batch 2300 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 8 batch 2400 train loss: 0.0014 test loss: 0.0016\n",
      "Epoch 8 batch 2500 train loss: 0.0008 test loss: 0.0016\n",
      "Epoch 8 batch 2600 train loss: 0.0002 test loss: 0.0016\n",
      "Epoch 8 batch 2700 train loss: 0.0025 test loss: 0.0015\n",
      "Epoch 8 batch 2800 train loss: 0.0017 test loss: 0.0015\n",
      "Epoch 8 batch 2900 train loss: 0.0008 test loss: 0.0015\n",
      "Epoch 9 batch 0 train loss: 0.0012 test loss: 0.0015\n",
      "Epoch 9 batch 100 train loss: 0.0015 test loss: 0.0015\n",
      "Epoch 9 batch 200 train loss: 0.0003 test loss: 0.0015\n",
      "Epoch 9 batch 300 train loss: 0.0029 test loss: 0.0016\n",
      "Epoch 9 batch 400 train loss: 0.0002 test loss: 0.0016\n",
      "Epoch 9 batch 500 train loss: 0.0005 test loss: 0.0015\n",
      "Epoch 9 batch 600 train loss: 0.0015 test loss: 0.0015\n",
      "Epoch 9 batch 700 train loss: 0.0011 test loss: 0.0015\n",
      "Epoch 9 batch 800 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 9 batch 900 train loss: 0.0002 test loss: 0.0016\n",
      "Epoch 9 batch 1000 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 9 batch 1100 train loss: 0.0002 test loss: 0.0016\n",
      "Epoch 9 batch 1200 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 9 batch 1300 train loss: 0.0003 test loss: 0.0015\n",
      "Epoch 9 batch 1400 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 9 batch 1500 train loss: 0.0003 test loss: 0.0015\n",
      "Epoch 9 batch 1600 train loss: 0.0026 test loss: 0.0015\n",
      "Epoch 9 batch 1700 train loss: 0.0003 test loss: 0.0015\n",
      "Epoch 9 batch 1800 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 9 batch 1900 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 9 batch 2000 train loss: 0.0002 test loss: 0.0016\n",
      "Epoch 9 batch 2100 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 9 batch 2200 train loss: 0.0005 test loss: 0.0015\n",
      "Epoch 9 batch 2300 train loss: 0.0003 test loss: 0.0015\n",
      "Epoch 9 batch 2400 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 9 batch 2500 train loss: 0.0013 test loss: 0.0016\n",
      "Epoch 9 batch 2600 train loss: 0.0007 test loss: 0.0016\n",
      "Epoch 9 batch 2700 train loss: 0.0007 test loss: 0.0015\n",
      "Epoch 9 batch 2800 train loss: 0.0008 test loss: 0.0015\n",
      "Epoch 9 batch 2900 train loss: 0.0003 test loss: 0.0015\n",
      "Epoch 10 batch 0 train loss: 0.0003 test loss: 0.0016\n",
      "Epoch 10 batch 100 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 10 batch 200 train loss: 0.0020 test loss: 0.0015\n",
      "Epoch 10 batch 300 train loss: 0.0019 test loss: 0.0015\n",
      "Epoch 10 batch 400 train loss: 0.0002 test loss: 0.0016\n",
      "Epoch 10 batch 500 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 10 batch 600 train loss: 0.0032 test loss: 0.0016\n",
      "Epoch 10 batch 700 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 10 batch 800 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 10 batch 900 train loss: 0.0008 test loss: 0.0015\n",
      "Epoch 10 batch 1000 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 10 batch 1100 train loss: 0.0006 test loss: 0.0015\n",
      "Epoch 10 batch 1200 train loss: 0.0011 test loss: 0.0015\n",
      "Epoch 10 batch 1300 train loss: 0.0005 test loss: 0.0015\n",
      "Epoch 10 batch 1400 train loss: 0.0002 test loss: 0.0016\n",
      "Epoch 10 batch 1500 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 10 batch 1600 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 10 batch 1700 train loss: 0.0003 test loss: 0.0016\n",
      "Epoch 10 batch 1800 train loss: 0.0011 test loss: 0.0015\n",
      "Epoch 10 batch 1900 train loss: 0.0026 test loss: 0.0015\n",
      "Epoch 10 batch 2000 train loss: 0.0007 test loss: 0.0015\n",
      "Epoch 10 batch 2100 train loss: 0.0011 test loss: 0.0015\n",
      "Epoch 10 batch 2200 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 10 batch 2300 train loss: 0.0007 test loss: 0.0015\n",
      "Epoch 10 batch 2400 train loss: 0.0003 test loss: 0.0015\n",
      "Epoch 10 batch 2500 train loss: 0.0004 test loss: 0.0015\n",
      "Epoch 10 batch 2600 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 10 batch 2700 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 10 batch 2800 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 10 batch 2900 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 11 batch 0 train loss: 0.0008 test loss: 0.0015\n",
      "Epoch 11 batch 100 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 11 batch 200 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 11 batch 300 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 11 batch 400 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 11 batch 500 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 11 batch 600 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 11 batch 700 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 11 batch 800 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 11 batch 900 train loss: 0.0009 test loss: 0.0015\n",
      "Epoch 11 batch 1000 train loss: 0.0009 test loss: 0.0015\n",
      "Epoch 11 batch 1100 train loss: 0.0007 test loss: 0.0015\n",
      "Epoch 11 batch 1200 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 11 batch 1300 train loss: 0.0011 test loss: 0.0015\n",
      "Epoch 11 batch 1400 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 11 batch 1500 train loss: 0.0003 test loss: 0.0014\n",
      "Epoch 11 batch 1600 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 11 batch 1700 train loss: 0.0016 test loss: 0.0015\n",
      "Epoch 11 batch 1800 train loss: 0.0003 test loss: 0.0015\n",
      "Epoch 11 batch 1900 train loss: 0.0003 test loss: 0.0015\n",
      "Epoch 11 batch 2000 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 11 batch 2100 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 11 batch 2200 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 11 batch 2300 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 11 batch 2400 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 11 batch 2500 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 11 batch 2600 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 11 batch 2700 train loss: 0.0002 test loss: 0.0016\n",
      "Epoch 11 batch 2800 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 11 batch 2900 train loss: 0.0029 test loss: 0.0015\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.20p/ckpt-30\n",
      "Epoch 12 batch 0 train loss: 0.0001 test loss: 0.0014\n",
      "Epoch 12 batch 100 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 12 batch 200 train loss: 0.0013 test loss: 0.0015\n",
      "Epoch 12 batch 300 train loss: 0.0004 test loss: 0.0015\n",
      "Epoch 12 batch 400 train loss: 0.0006 test loss: 0.0015\n",
      "Epoch 12 batch 500 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 12 batch 600 train loss: 0.0010 test loss: 0.0015\n",
      "Epoch 12 batch 700 train loss: 0.0019 test loss: 0.0015\n",
      "Epoch 12 batch 800 train loss: 0.0024 test loss: 0.0015\n",
      "Epoch 12 batch 900 train loss: 0.0007 test loss: 0.0015\n",
      "Epoch 12 batch 1000 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 12 batch 1100 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 12 batch 1200 train loss: 0.0018 test loss: 0.0015\n",
      "Epoch 12 batch 1300 train loss: 0.0019 test loss: 0.0015\n",
      "Epoch 12 batch 1400 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 12 batch 1500 train loss: 0.0016 test loss: 0.0014\n",
      "Epoch 12 batch 1600 train loss: 0.0014 test loss: 0.0015\n",
      "Epoch 12 batch 1700 train loss: 0.0010 test loss: 0.0016\n",
      "Epoch 12 batch 1800 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 12 batch 1900 train loss: 0.0007 test loss: 0.0015\n",
      "Epoch 12 batch 2000 train loss: 0.0012 test loss: 0.0016\n",
      "Epoch 12 batch 2100 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 12 batch 2200 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 12 batch 2300 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 12 batch 2400 train loss: 0.0008 test loss: 0.0015\n",
      "Epoch 12 batch 2500 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 12 batch 2600 train loss: 0.0001 test loss: 0.0014\n",
      "Epoch 12 batch 2700 train loss: 0.0021 test loss: 0.0015\n",
      "Epoch 12 batch 2800 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 12 batch 2900 train loss: 0.0010 test loss: 0.0014\n",
      "Epoch 13 batch 0 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 13 batch 100 train loss: 0.0026 test loss: 0.0015\n",
      "Epoch 13 batch 200 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 13 batch 300 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 13 batch 400 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 13 batch 500 train loss: 0.0019 test loss: 0.0015\n",
      "Epoch 13 batch 600 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 13 batch 700 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 13 batch 800 train loss: 0.0011 test loss: 0.0015\n",
      "Epoch 13 batch 900 train loss: 0.0002 test loss: 0.0016\n",
      "Epoch 13 batch 1000 train loss: 0.0005 test loss: 0.0015\n",
      "Epoch 13 batch 1100 train loss: 0.0003 test loss: 0.0015\n",
      "Epoch 13 batch 1200 train loss: 0.0012 test loss: 0.0014\n",
      "Epoch 13 batch 1300 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 13 batch 1400 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 13 batch 1500 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 13 batch 1600 train loss: 0.0014 test loss: 0.0015\n",
      "Epoch 13 batch 1700 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 13 batch 1800 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 13 batch 1900 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 13 batch 2000 train loss: 0.0002 test loss: 0.0015\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.20p/ckpt-31\n",
      "Epoch 13 batch 2100 train loss: 0.0001 test loss: 0.0014\n",
      "Epoch 13 batch 2200 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 13 batch 2300 train loss: 0.0016 test loss: 0.0015\n",
      "Epoch 13 batch 2400 train loss: 0.0006 test loss: 0.0015\n",
      "Epoch 13 batch 2500 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 13 batch 2600 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 13 batch 2700 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 13 batch 2800 train loss: 0.0007 test loss: 0.0015\n",
      "Epoch 13 batch 2900 train loss: 0.0012 test loss: 0.0015\n",
      "Epoch 14 batch 0 train loss: 0.0017 test loss: 0.0014\n",
      "Epoch 14 batch 100 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 14 batch 200 train loss: 0.0005 test loss: 0.0015\n",
      "Epoch 14 batch 300 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 14 batch 400 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 14 batch 500 train loss: 0.0013 test loss: 0.0015\n",
      "Epoch 14 batch 600 train loss: 0.0009 test loss: 0.0015\n",
      "Epoch 14 batch 700 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 14 batch 800 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 14 batch 900 train loss: 0.0006 test loss: 0.0016\n",
      "Epoch 14 batch 1000 train loss: 0.0006 test loss: 0.0016\n",
      "Epoch 14 batch 1100 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 14 batch 1200 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 14 batch 1300 train loss: 0.0010 test loss: 0.0015\n",
      "Epoch 14 batch 1400 train loss: 0.0017 test loss: 0.0015\n",
      "Epoch 14 batch 1500 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 14 batch 1600 train loss: 0.0001 test loss: 0.0016\n",
      "Epoch 14 batch 1700 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 14 batch 1800 train loss: 0.0010 test loss: 0.0015\n",
      "Epoch 14 batch 1900 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 14 batch 2000 train loss: 0.0006 test loss: 0.0015\n",
      "Epoch 14 batch 2100 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 14 batch 2200 train loss: 0.0005 test loss: 0.0015\n",
      "Epoch 14 batch 2300 train loss: 0.0021 test loss: 0.0015\n",
      "Epoch 14 batch 2400 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 14 batch 2500 train loss: 0.0003 test loss: 0.0016\n",
      "Epoch 14 batch 2600 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 14 batch 2700 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 14 batch 2800 train loss: 0.0020 test loss: 0.0015\n",
      "Epoch 14 batch 2900 train loss: 0.0007 test loss: 0.0016\n",
      "Epoch 15 batch 0 train loss: 0.0010 test loss: 0.0016\n",
      "Epoch 15 batch 100 train loss: 0.0013 test loss: 0.0015\n",
      "Epoch 15 batch 200 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 15 batch 300 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 15 batch 400 train loss: 0.0002 test loss: 0.0016\n",
      "Epoch 15 batch 500 train loss: 0.0005 test loss: 0.0016\n",
      "Epoch 15 batch 600 train loss: 0.0017 test loss: 0.0015\n",
      "Epoch 15 batch 700 train loss: 0.0020 test loss: 0.0016\n",
      "Epoch 15 batch 800 train loss: 0.0010 test loss: 0.0014\n",
      "Epoch 15 batch 900 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 15 batch 1000 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 15 batch 1100 train loss: 0.0009 test loss: 0.0015\n",
      "Epoch 15 batch 1200 train loss: 0.0018 test loss: 0.0015\n",
      "Epoch 15 batch 1300 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 15 batch 1400 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 15 batch 1500 train loss: 0.0011 test loss: 0.0015\n",
      "Epoch 15 batch 1600 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 15 batch 1700 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 15 batch 1800 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 15 batch 1900 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 15 batch 2000 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 15 batch 2100 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 15 batch 2200 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 15 batch 2300 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 15 batch 2400 train loss: 0.0001 test loss: 0.0016\n",
      "Epoch 15 batch 2500 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 15 batch 2600 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 15 batch 2700 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 15 batch 2800 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 15 batch 2900 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 16 batch 0 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 16 batch 100 train loss: 0.0016 test loss: 0.0015\n",
      "Epoch 16 batch 200 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 16 batch 300 train loss: 0.0002 test loss: 0.0016\n",
      "Epoch 16 batch 400 train loss: 0.0009 test loss: 0.0015\n",
      "Epoch 16 batch 500 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 16 batch 600 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 16 batch 700 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 16 batch 800 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 16 batch 900 train loss: 0.0007 test loss: 0.0015\n",
      "Epoch 16 batch 1000 train loss: 0.0009 test loss: 0.0015\n",
      "Epoch 16 batch 1100 train loss: 0.0007 test loss: 0.0015\n",
      "Epoch 16 batch 1200 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 16 batch 1300 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 16 batch 1400 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 16 batch 1500 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 16 batch 1600 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 16 batch 1700 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 16 batch 1800 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 16 batch 1900 train loss: 0.0004 test loss: 0.0015\n",
      "Epoch 16 batch 2000 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 16 batch 2100 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 16 batch 2200 train loss: 0.0006 test loss: 0.0015\n",
      "Epoch 16 batch 2300 train loss: 0.0003 test loss: 0.0015\n",
      "Epoch 16 batch 2400 train loss: 0.0002 test loss: 0.0016\n",
      "Epoch 16 batch 2500 train loss: 0.0028 test loss: 0.0015\n",
      "Epoch 16 batch 2600 train loss: 0.0008 test loss: 0.0015\n",
      "Epoch 16 batch 2700 train loss: 0.0010 test loss: 0.0016\n",
      "Epoch 16 batch 2800 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 16 batch 2900 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 17 batch 0 train loss: 0.0005 test loss: 0.0015\n",
      "Epoch 17 batch 100 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 17 batch 200 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 17 batch 300 train loss: 0.0001 test loss: 0.0016\n",
      "Epoch 17 batch 400 train loss: 0.0020 test loss: 0.0015\n",
      "Epoch 17 batch 500 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 17 batch 600 train loss: 0.0020 test loss: 0.0015\n",
      "Epoch 17 batch 700 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 17 batch 800 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 17 batch 900 train loss: 0.0014 test loss: 0.0016\n",
      "Epoch 17 batch 1000 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 17 batch 1100 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 17 batch 1200 train loss: 0.0002 test loss: 0.0016\n",
      "Epoch 17 batch 1300 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 17 batch 1400 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 17 batch 1500 train loss: 0.0002 test loss: 0.0015\n",
      "Epoch 17 batch 1600 train loss: 0.0017 test loss: 0.0015\n",
      "Epoch 17 batch 1700 train loss: 0.0016 test loss: 0.0015\n",
      "Epoch 17 batch 1800 train loss: 0.0010 test loss: 0.0015\n",
      "Epoch 17 batch 1900 train loss: 0.0001 test loss: 0.0015\n",
      "Epoch 17 batch 2000 train loss: 0.0006 test loss: 0.0015\n",
      "early stop.\n",
      "Checkpoint 31 restored!!\n",
      "2446/2446 [==============================] - 18s 7ms/step - loss: 0.0017\n",
      "Training for loss rate 0.30 start.\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.30p/ckpt-1\n",
      "Epoch 0 batch 0 train loss: 0.0566 test loss: 0.0969\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.30p/ckpt-2\n",
      "Epoch 0 batch 100 train loss: 0.0090 test loss: 0.0096\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.30p/ckpt-3\n",
      "Epoch 0 batch 200 train loss: 0.0045 test loss: 0.0076\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.30p/ckpt-4\n",
      "Epoch 0 batch 300 train loss: 0.0041 test loss: 0.0054\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.30p/ckpt-5\n",
      "Epoch 0 batch 400 train loss: 0.0020 test loss: 0.0052\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.30p/ckpt-6\n",
      "Epoch 0 batch 500 train loss: 0.0038 test loss: 0.0047\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.30p/ckpt-7\n",
      "Epoch 0 batch 600 train loss: 0.0034 test loss: 0.0043\n",
      "Epoch 0 batch 700 train loss: 0.0039 test loss: 0.0044\n",
      "Epoch 0 batch 800 train loss: 0.0035 test loss: 0.0049\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.30p/ckpt-8\n",
      "Epoch 0 batch 900 train loss: 0.0018 test loss: 0.0040\n",
      "Epoch 0 batch 1000 train loss: 0.0029 test loss: 0.0042\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.30p/ckpt-9\n",
      "Epoch 0 batch 1100 train loss: 0.0023 test loss: 0.0039\n",
      "Epoch 0 batch 1200 train loss: 0.0043 test loss: 0.0046\n",
      "Epoch 0 batch 1300 train loss: 0.0034 test loss: 0.0041\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.30p/ckpt-10\n",
      "Epoch 0 batch 1400 train loss: 0.0015 test loss: 0.0037\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.30p/ckpt-11\n",
      "Epoch 0 batch 1500 train loss: 0.0055 test loss: 0.0037\n",
      "Epoch 0 batch 1600 train loss: 0.0013 test loss: 0.0039\n",
      "Epoch 0 batch 1700 train loss: 0.0039 test loss: 0.0040\n",
      "Epoch 0 batch 1800 train loss: 0.0019 test loss: 0.0037\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.30p/ckpt-12\n",
      "Epoch 0 batch 1900 train loss: 0.0038 test loss: 0.0037\n",
      "Epoch 0 batch 2000 train loss: 0.0036 test loss: 0.0048\n",
      "Epoch 0 batch 2100 train loss: 0.0028 test loss: 0.0037\n",
      "Epoch 0 batch 2200 train loss: 0.0011 test loss: 0.0040\n",
      "Epoch 0 batch 2300 train loss: 0.0034 test loss: 0.0038\n",
      "Epoch 0 batch 2400 train loss: 0.0044 test loss: 0.0038\n",
      "Epoch 0 batch 2500 train loss: 0.0025 test loss: 0.0038\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.30p/ckpt-13\n",
      "Epoch 0 batch 2600 train loss: 0.0019 test loss: 0.0035\n",
      "Epoch 0 batch 2700 train loss: 0.0020 test loss: 0.0037\n",
      "Epoch 0 batch 2800 train loss: 0.0013 test loss: 0.0038\n",
      "Epoch 0 batch 2900 train loss: 0.0015 test loss: 0.0036\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.30p/ckpt-14\n",
      "Epoch 1 batch 0 train loss: 0.0019 test loss: 0.0033\n",
      "Epoch 1 batch 100 train loss: 0.0016 test loss: 0.0035\n",
      "Epoch 1 batch 200 train loss: 0.0029 test loss: 0.0035\n",
      "Epoch 1 batch 300 train loss: 0.0014 test loss: 0.0035\n",
      "Epoch 1 batch 400 train loss: 0.0010 test loss: 0.0035\n",
      "Epoch 1 batch 500 train loss: 0.0027 test loss: 0.0037\n",
      "Epoch 1 batch 600 train loss: 0.0027 test loss: 0.0035\n",
      "Epoch 1 batch 700 train loss: 0.0011 test loss: 0.0039\n",
      "Epoch 1 batch 800 train loss: 0.0027 test loss: 0.0034\n",
      "Epoch 1 batch 900 train loss: 0.0026 test loss: 0.0034\n",
      "Epoch 1 batch 1000 train loss: 0.0025 test loss: 0.0035\n",
      "Epoch 1 batch 1100 train loss: 0.0050 test loss: 0.0033\n",
      "Epoch 1 batch 1200 train loss: 0.0021 test loss: 0.0034\n",
      "Epoch 1 batch 1300 train loss: 0.0010 test loss: 0.0034\n",
      "Epoch 1 batch 1400 train loss: 0.0022 test loss: 0.0038\n",
      "Epoch 1 batch 1500 train loss: 0.0037 test loss: 0.0035\n",
      "Epoch 1 batch 1600 train loss: 0.0015 test loss: 0.0036\n",
      "Epoch 1 batch 1700 train loss: 0.0009 test loss: 0.0035\n",
      "Epoch 1 batch 1800 train loss: 0.0019 test loss: 0.0034\n",
      "Epoch 1 batch 1900 train loss: 0.0057 test loss: 0.0034\n",
      "Epoch 1 batch 2000 train loss: 0.0026 test loss: 0.0034\n",
      "Epoch 1 batch 2100 train loss: 0.0009 test loss: 0.0035\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.30p/ckpt-15\n",
      "Epoch 1 batch 2200 train loss: 0.0049 test loss: 0.0033\n",
      "Epoch 1 batch 2300 train loss: 0.0012 test loss: 0.0035\n",
      "Epoch 1 batch 2400 train loss: 0.0006 test loss: 0.0033\n",
      "Epoch 1 batch 2500 train loss: 0.0056 test loss: 0.0033\n",
      "Epoch 1 batch 2600 train loss: 0.0010 test loss: 0.0034\n",
      "Epoch 1 batch 2700 train loss: 0.0035 test loss: 0.0036\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.30p/ckpt-16\n",
      "Epoch 1 batch 2800 train loss: 0.0011 test loss: 0.0032\n",
      "Epoch 1 batch 2900 train loss: 0.0036 test loss: 0.0035\n",
      "Epoch 2 batch 0 train loss: 0.0006 test loss: 0.0033\n",
      "Epoch 2 batch 100 train loss: 0.0019 test loss: 0.0034\n",
      "Epoch 2 batch 200 train loss: 0.0048 test loss: 0.0033\n",
      "Epoch 2 batch 300 train loss: 0.0014 test loss: 0.0034\n",
      "Epoch 2 batch 400 train loss: 0.0023 test loss: 0.0036\n",
      "Epoch 2 batch 500 train loss: 0.0010 test loss: 0.0036\n",
      "Epoch 2 batch 600 train loss: 0.0019 test loss: 0.0035\n",
      "Epoch 2 batch 700 train loss: 0.0042 test loss: 0.0033\n",
      "Epoch 2 batch 800 train loss: 0.0016 test loss: 0.0034\n",
      "Epoch 2 batch 900 train loss: 0.0007 test loss: 0.0034\n",
      "Epoch 2 batch 1000 train loss: 0.0011 test loss: 0.0033\n",
      "Epoch 2 batch 1100 train loss: 0.0029 test loss: 0.0036\n",
      "Epoch 2 batch 1200 train loss: 0.0033 test loss: 0.0033\n",
      "Epoch 2 batch 1300 train loss: 0.0051 test loss: 0.0033\n",
      "Epoch 2 batch 1400 train loss: 0.0030 test loss: 0.0033\n",
      "Epoch 2 batch 1500 train loss: 0.0023 test loss: 0.0033\n",
      "Epoch 2 batch 1600 train loss: 0.0061 test loss: 0.0032\n",
      "Epoch 2 batch 1700 train loss: 0.0028 test loss: 0.0033\n",
      "Epoch 2 batch 1800 train loss: 0.0053 test loss: 0.0033\n",
      "Epoch 2 batch 1900 train loss: 0.0026 test loss: 0.0033\n",
      "Epoch 2 batch 2000 train loss: 0.0006 test loss: 0.0034\n",
      "Epoch 2 batch 2100 train loss: 0.0010 test loss: 0.0034\n",
      "Epoch 2 batch 2200 train loss: 0.0006 test loss: 0.0034\n",
      "Epoch 2 batch 2300 train loss: 0.0031 test loss: 0.0033\n",
      "Epoch 2 batch 2400 train loss: 0.0050 test loss: 0.0034\n",
      "Epoch 2 batch 2500 train loss: 0.0037 test loss: 0.0033\n",
      "Epoch 2 batch 2600 train loss: 0.0058 test loss: 0.0033\n",
      "Epoch 2 batch 2700 train loss: 0.0007 test loss: 0.0034\n",
      "Epoch 2 batch 2800 train loss: 0.0008 test loss: 0.0033\n",
      "Epoch 2 batch 2900 train loss: 0.0021 test loss: 0.0032\n",
      "Epoch 3 batch 0 train loss: 0.0007 test loss: 0.0032\n",
      "Epoch 3 batch 100 train loss: 0.0039 test loss: 0.0033\n",
      "Epoch 3 batch 200 train loss: 0.0034 test loss: 0.0033\n",
      "Epoch 3 batch 300 train loss: 0.0016 test loss: 0.0033\n",
      "Epoch 3 batch 400 train loss: 0.0007 test loss: 0.0032\n",
      "Epoch 3 batch 500 train loss: 0.0023 test loss: 0.0033\n",
      "Epoch 3 batch 600 train loss: 0.0032 test loss: 0.0034\n",
      "Epoch 3 batch 700 train loss: 0.0022 test loss: 0.0033\n",
      "Epoch 3 batch 800 train loss: 0.0005 test loss: 0.0033\n",
      "Epoch 3 batch 900 train loss: 0.0008 test loss: 0.0033\n",
      "Epoch 3 batch 1000 train loss: 0.0011 test loss: 0.0033\n",
      "Epoch 3 batch 1100 train loss: 0.0009 test loss: 0.0034\n",
      "Epoch 3 batch 1200 train loss: 0.0024 test loss: 0.0033\n",
      "Epoch 3 batch 1300 train loss: 0.0030 test loss: 0.0033\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.30p/ckpt-17\n",
      "Epoch 3 batch 1400 train loss: 0.0006 test loss: 0.0032\n",
      "Epoch 3 batch 1500 train loss: 0.0008 test loss: 0.0033\n",
      "Epoch 3 batch 1600 train loss: 0.0021 test loss: 0.0033\n",
      "Epoch 3 batch 1700 train loss: 0.0009 test loss: 0.0033\n",
      "Epoch 3 batch 1800 train loss: 0.0016 test loss: 0.0033\n",
      "Epoch 3 batch 1900 train loss: 0.0024 test loss: 0.0036\n",
      "Epoch 3 batch 2000 train loss: 0.0007 test loss: 0.0033\n",
      "Epoch 3 batch 2100 train loss: 0.0050 test loss: 0.0033\n",
      "Epoch 3 batch 2200 train loss: 0.0006 test loss: 0.0032\n",
      "Epoch 3 batch 2300 train loss: 0.0005 test loss: 0.0033\n",
      "Epoch 3 batch 2400 train loss: 0.0007 test loss: 0.0033\n",
      "Epoch 3 batch 2500 train loss: 0.0065 test loss: 0.0032\n",
      "Epoch 3 batch 2600 train loss: 0.0007 test loss: 0.0033\n",
      "Epoch 3 batch 2700 train loss: 0.0020 test loss: 0.0033\n",
      "Epoch 3 batch 2800 train loss: 0.0014 test loss: 0.0032\n",
      "Epoch 3 batch 2900 train loss: 0.0011 test loss: 0.0032\n",
      "Epoch 4 batch 0 train loss: 0.0020 test loss: 0.0032\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.30p/ckpt-18\n",
      "Epoch 4 batch 100 train loss: 0.0006 test loss: 0.0031\n",
      "Epoch 4 batch 200 train loss: 0.0028 test loss: 0.0034\n",
      "Epoch 4 batch 300 train loss: 0.0004 test loss: 0.0033\n",
      "Epoch 4 batch 400 train loss: 0.0012 test loss: 0.0034\n",
      "Epoch 4 batch 500 train loss: 0.0017 test loss: 0.0032\n",
      "Epoch 4 batch 600 train loss: 0.0032 test loss: 0.0033\n",
      "Epoch 4 batch 700 train loss: 0.0011 test loss: 0.0033\n",
      "Epoch 4 batch 800 train loss: 0.0021 test loss: 0.0033\n",
      "Epoch 4 batch 900 train loss: 0.0006 test loss: 0.0032\n",
      "Epoch 4 batch 1000 train loss: 0.0015 test loss: 0.0032\n",
      "Epoch 4 batch 1100 train loss: 0.0016 test loss: 0.0033\n",
      "Epoch 4 batch 1200 train loss: 0.0031 test loss: 0.0032\n",
      "Epoch 4 batch 1300 train loss: 0.0004 test loss: 0.0033\n",
      "Epoch 4 batch 1400 train loss: 0.0018 test loss: 0.0032\n",
      "Epoch 4 batch 1500 train loss: 0.0015 test loss: 0.0032\n",
      "Epoch 4 batch 1600 train loss: 0.0005 test loss: 0.0033\n",
      "Epoch 4 batch 1700 train loss: 0.0009 test loss: 0.0035\n",
      "Epoch 4 batch 1800 train loss: 0.0009 test loss: 0.0032\n",
      "Epoch 4 batch 1900 train loss: 0.0041 test loss: 0.0033\n",
      "Epoch 4 batch 2000 train loss: 0.0005 test loss: 0.0032\n",
      "Epoch 4 batch 2100 train loss: 0.0011 test loss: 0.0032\n",
      "Epoch 4 batch 2200 train loss: 0.0004 test loss: 0.0032\n",
      "Epoch 4 batch 2300 train loss: 0.0003 test loss: 0.0032\n",
      "Epoch 4 batch 2400 train loss: 0.0011 test loss: 0.0032\n",
      "Epoch 4 batch 2500 train loss: 0.0003 test loss: 0.0032\n",
      "Epoch 4 batch 2600 train loss: 0.0037 test loss: 0.0032\n",
      "Epoch 4 batch 2700 train loss: 0.0004 test loss: 0.0032\n",
      "Epoch 4 batch 2800 train loss: 0.0005 test loss: 0.0032\n",
      "Epoch 4 batch 2900 train loss: 0.0036 test loss: 0.0032\n",
      "Epoch 5 batch 0 train loss: 0.0017 test loss: 0.0032\n",
      "Epoch 5 batch 100 train loss: 0.0029 test loss: 0.0033\n",
      "Epoch 5 batch 200 train loss: 0.0005 test loss: 0.0033\n",
      "Epoch 5 batch 300 train loss: 0.0021 test loss: 0.0032\n",
      "Epoch 5 batch 400 train loss: 0.0020 test loss: 0.0033\n",
      "Epoch 5 batch 500 train loss: 0.0045 test loss: 0.0032\n",
      "Epoch 5 batch 600 train loss: 0.0025 test loss: 0.0032\n",
      "Epoch 5 batch 700 train loss: 0.0010 test loss: 0.0033\n",
      "Epoch 5 batch 800 train loss: 0.0045 test loss: 0.0032\n",
      "Epoch 5 batch 900 train loss: 0.0015 test loss: 0.0032\n",
      "Epoch 5 batch 1000 train loss: 0.0004 test loss: 0.0033\n",
      "Epoch 5 batch 1100 train loss: 0.0040 test loss: 0.0032\n",
      "Epoch 5 batch 1200 train loss: 0.0017 test loss: 0.0032\n",
      "Epoch 5 batch 1300 train loss: 0.0004 test loss: 0.0032\n",
      "Epoch 5 batch 1400 train loss: 0.0040 test loss: 0.0032\n",
      "Epoch 5 batch 1500 train loss: 0.0024 test loss: 0.0031\n",
      "Epoch 5 batch 1600 train loss: 0.0008 test loss: 0.0033\n",
      "Epoch 5 batch 1700 train loss: 0.0007 test loss: 0.0032\n",
      "Epoch 5 batch 1800 train loss: 0.0016 test loss: 0.0033\n",
      "Epoch 5 batch 1900 train loss: 0.0024 test loss: 0.0033\n",
      "Epoch 5 batch 2000 train loss: 0.0007 test loss: 0.0033\n",
      "Epoch 5 batch 2100 train loss: 0.0024 test loss: 0.0034\n",
      "Epoch 5 batch 2200 train loss: 0.0021 test loss: 0.0033\n",
      "Epoch 5 batch 2300 train loss: 0.0018 test loss: 0.0032\n",
      "Epoch 5 batch 2400 train loss: 0.0004 test loss: 0.0033\n",
      "Epoch 5 batch 2500 train loss: 0.0033 test loss: 0.0033\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.30p/ckpt-19\n",
      "Epoch 5 batch 2600 train loss: 0.0026 test loss: 0.0031\n",
      "Epoch 5 batch 2700 train loss: 0.0005 test loss: 0.0033\n",
      "Epoch 5 batch 2800 train loss: 0.0004 test loss: 0.0033\n",
      "Epoch 5 batch 2900 train loss: 0.0004 test loss: 0.0032\n",
      "Epoch 6 batch 0 train loss: 0.0036 test loss: 0.0031\n",
      "Epoch 6 batch 100 train loss: 0.0023 test loss: 0.0032\n",
      "Epoch 6 batch 200 train loss: 0.0005 test loss: 0.0032\n",
      "Epoch 6 batch 300 train loss: 0.0014 test loss: 0.0032\n",
      "Epoch 6 batch 400 train loss: 0.0035 test loss: 0.0033\n",
      "Epoch 6 batch 500 train loss: 0.0036 test loss: 0.0032\n",
      "Epoch 6 batch 600 train loss: 0.0035 test loss: 0.0033\n",
      "Epoch 6 batch 700 train loss: 0.0050 test loss: 0.0032\n",
      "Epoch 6 batch 800 train loss: 0.0017 test loss: 0.0033\n",
      "Epoch 6 batch 900 train loss: 0.0004 test loss: 0.0033\n",
      "Epoch 6 batch 1000 train loss: 0.0020 test loss: 0.0032\n",
      "Epoch 6 batch 1100 train loss: 0.0024 test loss: 0.0032\n",
      "Epoch 6 batch 1200 train loss: 0.0012 test loss: 0.0032\n",
      "Epoch 6 batch 1300 train loss: 0.0053 test loss: 0.0032\n",
      "Epoch 6 batch 1400 train loss: 0.0009 test loss: 0.0032\n",
      "Epoch 6 batch 1500 train loss: 0.0026 test loss: 0.0033\n",
      "Epoch 6 batch 1600 train loss: 0.0004 test loss: 0.0032\n",
      "Epoch 6 batch 1700 train loss: 0.0004 test loss: 0.0032\n",
      "Epoch 6 batch 1800 train loss: 0.0034 test loss: 0.0032\n",
      "Epoch 6 batch 1900 train loss: 0.0021 test loss: 0.0033\n",
      "Epoch 6 batch 2000 train loss: 0.0005 test loss: 0.0032\n",
      "Epoch 6 batch 2100 train loss: 0.0034 test loss: 0.0032\n",
      "Epoch 6 batch 2200 train loss: 0.0021 test loss: 0.0032\n",
      "Epoch 6 batch 2300 train loss: 0.0012 test loss: 0.0033\n",
      "Epoch 6 batch 2400 train loss: 0.0028 test loss: 0.0033\n",
      "Epoch 6 batch 2500 train loss: 0.0036 test loss: 0.0032\n",
      "Epoch 6 batch 2600 train loss: 0.0004 test loss: 0.0031\n",
      "Epoch 6 batch 2700 train loss: 0.0015 test loss: 0.0032\n",
      "Epoch 6 batch 2800 train loss: 0.0017 test loss: 0.0032\n",
      "Epoch 6 batch 2900 train loss: 0.0003 test loss: 0.0033\n",
      "Epoch 7 batch 0 train loss: 0.0020 test loss: 0.0032\n",
      "Epoch 7 batch 100 train loss: 0.0003 test loss: 0.0032\n",
      "Epoch 7 batch 200 train loss: 0.0030 test loss: 0.0033\n",
      "Epoch 7 batch 300 train loss: 0.0026 test loss: 0.0033\n",
      "Epoch 7 batch 400 train loss: 0.0044 test loss: 0.0032\n",
      "Epoch 7 batch 500 train loss: 0.0006 test loss: 0.0033\n",
      "Epoch 7 batch 600 train loss: 0.0017 test loss: 0.0032\n",
      "Epoch 7 batch 700 train loss: 0.0005 test loss: 0.0032\n",
      "Epoch 7 batch 800 train loss: 0.0011 test loss: 0.0032\n",
      "Epoch 7 batch 900 train loss: 0.0030 test loss: 0.0032\n",
      "Epoch 7 batch 1000 train loss: 0.0005 test loss: 0.0032\n",
      "Epoch 7 batch 1100 train loss: 0.0016 test loss: 0.0032\n",
      "Epoch 7 batch 1200 train loss: 0.0017 test loss: 0.0032\n",
      "Epoch 7 batch 1300 train loss: 0.0025 test loss: 0.0033\n",
      "Epoch 7 batch 1400 train loss: 0.0026 test loss: 0.0032\n",
      "Epoch 7 batch 1500 train loss: 0.0006 test loss: 0.0033\n",
      "Epoch 7 batch 1600 train loss: 0.0040 test loss: 0.0032\n",
      "Epoch 7 batch 1700 train loss: 0.0019 test loss: 0.0032\n",
      "Epoch 7 batch 1800 train loss: 0.0005 test loss: 0.0032\n",
      "Epoch 7 batch 1900 train loss: 0.0004 test loss: 0.0032\n",
      "Epoch 7 batch 2000 train loss: 0.0003 test loss: 0.0032\n",
      "Epoch 7 batch 2100 train loss: 0.0003 test loss: 0.0032\n",
      "Epoch 7 batch 2200 train loss: 0.0016 test loss: 0.0032\n",
      "Epoch 7 batch 2300 train loss: 0.0007 test loss: 0.0032\n",
      "Epoch 7 batch 2400 train loss: 0.0004 test loss: 0.0032\n",
      "Epoch 7 batch 2500 train loss: 0.0003 test loss: 0.0032\n",
      "Epoch 7 batch 2600 train loss: 0.0016 test loss: 0.0032\n",
      "Epoch 7 batch 2700 train loss: 0.0035 test loss: 0.0032\n",
      "Epoch 7 batch 2800 train loss: 0.0036 test loss: 0.0032\n",
      "Epoch 7 batch 2900 train loss: 0.0009 test loss: 0.0032\n",
      "Epoch 8 batch 0 train loss: 0.0014 test loss: 0.0031\n",
      "Epoch 8 batch 100 train loss: 0.0003 test loss: 0.0032\n",
      "Epoch 8 batch 200 train loss: 0.0003 test loss: 0.0032\n",
      "Epoch 8 batch 300 train loss: 0.0028 test loss: 0.0033\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.30p/ckpt-20\n",
      "Epoch 8 batch 400 train loss: 0.0009 test loss: 0.0031\n",
      "Epoch 8 batch 500 train loss: 0.0003 test loss: 0.0033\n",
      "Epoch 8 batch 600 train loss: 0.0013 test loss: 0.0032\n",
      "Epoch 8 batch 700 train loss: 0.0015 test loss: 0.0033\n",
      "Epoch 8 batch 800 train loss: 0.0003 test loss: 0.0032\n",
      "Epoch 8 batch 900 train loss: 0.0003 test loss: 0.0032\n",
      "Epoch 8 batch 1000 train loss: 0.0022 test loss: 0.0032\n",
      "Epoch 8 batch 1100 train loss: 0.0016 test loss: 0.0032\n",
      "Epoch 8 batch 1200 train loss: 0.0010 test loss: 0.0032\n",
      "Epoch 8 batch 1300 train loss: 0.0011 test loss: 0.0032\n",
      "Epoch 8 batch 1400 train loss: 0.0017 test loss: 0.0032\n",
      "Epoch 8 batch 1500 train loss: 0.0004 test loss: 0.0031\n",
      "Epoch 8 batch 1600 train loss: 0.0029 test loss: 0.0032\n",
      "Epoch 8 batch 1700 train loss: 0.0003 test loss: 0.0031\n",
      "Epoch 8 batch 1800 train loss: 0.0015 test loss: 0.0032\n",
      "Epoch 8 batch 1900 train loss: 0.0031 test loss: 0.0032\n",
      "Epoch 8 batch 2000 train loss: 0.0010 test loss: 0.0032\n",
      "Epoch 8 batch 2100 train loss: 0.0006 test loss: 0.0032\n",
      "Epoch 8 batch 2200 train loss: 0.0023 test loss: 0.0033\n",
      "Epoch 8 batch 2300 train loss: 0.0021 test loss: 0.0032\n",
      "Epoch 8 batch 2400 train loss: 0.0003 test loss: 0.0033\n",
      "Epoch 8 batch 2500 train loss: 0.0004 test loss: 0.0032\n",
      "Epoch 8 batch 2600 train loss: 0.0022 test loss: 0.0031\n",
      "Epoch 8 batch 2700 train loss: 0.0011 test loss: 0.0032\n",
      "Epoch 8 batch 2800 train loss: 0.0035 test loss: 0.0031\n",
      "Epoch 8 batch 2900 train loss: 0.0003 test loss: 0.0032\n",
      "Epoch 9 batch 0 train loss: 0.0014 test loss: 0.0032\n",
      "Epoch 9 batch 100 train loss: 0.0003 test loss: 0.0032\n",
      "Epoch 9 batch 200 train loss: 0.0003 test loss: 0.0032\n",
      "Epoch 9 batch 300 train loss: 0.0024 test loss: 0.0032\n",
      "Epoch 9 batch 400 train loss: 0.0024 test loss: 0.0031\n",
      "Epoch 9 batch 500 train loss: 0.0049 test loss: 0.0033\n",
      "Epoch 9 batch 600 train loss: 0.0021 test loss: 0.0032\n",
      "Epoch 9 batch 700 train loss: 0.0006 test loss: 0.0033\n",
      "Epoch 9 batch 800 train loss: 0.0007 test loss: 0.0032\n",
      "Epoch 9 batch 900 train loss: 0.0036 test loss: 0.0032\n",
      "Epoch 9 batch 1000 train loss: 0.0003 test loss: 0.0033\n",
      "Epoch 9 batch 1100 train loss: 0.0055 test loss: 0.0033\n",
      "Epoch 9 batch 1200 train loss: 0.0033 test loss: 0.0032\n",
      "Epoch 9 batch 1300 train loss: 0.0032 test loss: 0.0033\n",
      "Epoch 9 batch 1400 train loss: 0.0015 test loss: 0.0032\n",
      "Epoch 9 batch 1500 train loss: 0.0034 test loss: 0.0033\n",
      "Epoch 9 batch 1600 train loss: 0.0007 test loss: 0.0032\n",
      "Epoch 9 batch 1700 train loss: 0.0015 test loss: 0.0033\n",
      "Epoch 9 batch 1800 train loss: 0.0031 test loss: 0.0032\n",
      "Epoch 9 batch 1900 train loss: 0.0039 test loss: 0.0032\n",
      "Epoch 9 batch 2000 train loss: 0.0039 test loss: 0.0032\n",
      "Epoch 9 batch 2100 train loss: 0.0018 test loss: 0.0032\n",
      "Epoch 9 batch 2200 train loss: 0.0023 test loss: 0.0032\n",
      "Epoch 9 batch 2300 train loss: 0.0019 test loss: 0.0032\n",
      "Epoch 9 batch 2400 train loss: 0.0010 test loss: 0.0031\n",
      "Epoch 9 batch 2500 train loss: 0.0003 test loss: 0.0033\n",
      "Epoch 9 batch 2600 train loss: 0.0008 test loss: 0.0032\n",
      "Epoch 9 batch 2700 train loss: 0.0035 test loss: 0.0032\n",
      "Epoch 9 batch 2800 train loss: 0.0024 test loss: 0.0032\n",
      "Epoch 9 batch 2900 train loss: 0.0006 test loss: 0.0032\n",
      "Epoch 10 batch 0 train loss: 0.0019 test loss: 0.0032\n",
      "Epoch 10 batch 100 train loss: 0.0033 test loss: 0.0032\n",
      "Epoch 10 batch 200 train loss: 0.0009 test loss: 0.0032\n",
      "Epoch 10 batch 300 train loss: 0.0022 test loss: 0.0033\n",
      "Epoch 10 batch 400 train loss: 0.0003 test loss: 0.0032\n",
      "Epoch 10 batch 500 train loss: 0.0028 test loss: 0.0032\n",
      "Epoch 10 batch 600 train loss: 0.0017 test loss: 0.0032\n",
      "Epoch 10 batch 700 train loss: 0.0015 test loss: 0.0032\n",
      "Epoch 10 batch 800 train loss: 0.0003 test loss: 0.0032\n",
      "Epoch 10 batch 900 train loss: 0.0010 test loss: 0.0032\n",
      "Epoch 10 batch 1000 train loss: 0.0017 test loss: 0.0032\n",
      "Epoch 10 batch 1100 train loss: 0.0008 test loss: 0.0032\n",
      "Epoch 10 batch 1200 train loss: 0.0002 test loss: 0.0033\n",
      "Epoch 10 batch 1300 train loss: 0.0010 test loss: 0.0032\n",
      "Epoch 10 batch 1400 train loss: 0.0012 test loss: 0.0032\n",
      "Epoch 10 batch 1500 train loss: 0.0019 test loss: 0.0032\n",
      "Epoch 10 batch 1600 train loss: 0.0003 test loss: 0.0032\n",
      "Epoch 10 batch 1700 train loss: 0.0018 test loss: 0.0032\n",
      "Epoch 10 batch 1800 train loss: 0.0023 test loss: 0.0032\n",
      "Epoch 10 batch 1900 train loss: 0.0014 test loss: 0.0033\n",
      "Epoch 10 batch 2000 train loss: 0.0022 test loss: 0.0032\n",
      "Epoch 10 batch 2100 train loss: 0.0004 test loss: 0.0033\n",
      "Epoch 10 batch 2200 train loss: 0.0016 test loss: 0.0032\n",
      "Epoch 10 batch 2300 train loss: 0.0002 test loss: 0.0032\n",
      "Epoch 10 batch 2400 train loss: 0.0003 test loss: 0.0032\n",
      "Epoch 10 batch 2500 train loss: 0.0010 test loss: 0.0032\n",
      "Epoch 10 batch 2600 train loss: 0.0044 test loss: 0.0032\n",
      "Epoch 10 batch 2700 train loss: 0.0027 test loss: 0.0032\n",
      "Epoch 10 batch 2800 train loss: 0.0003 test loss: 0.0032\n",
      "Epoch 10 batch 2900 train loss: 0.0029 test loss: 0.0031\n",
      "Epoch 11 batch 0 train loss: 0.0003 test loss: 0.0032\n",
      "Epoch 11 batch 100 train loss: 0.0002 test loss: 0.0032\n",
      "Epoch 11 batch 200 train loss: 0.0040 test loss: 0.0033\n",
      "Epoch 11 batch 300 train loss: 0.0003 test loss: 0.0032\n",
      "Epoch 11 batch 400 train loss: 0.0021 test loss: 0.0032\n",
      "Epoch 11 batch 500 train loss: 0.0018 test loss: 0.0033\n",
      "Epoch 11 batch 600 train loss: 0.0013 test loss: 0.0034\n",
      "Epoch 11 batch 700 train loss: 0.0055 test loss: 0.0032\n",
      "Epoch 11 batch 800 train loss: 0.0002 test loss: 0.0032\n",
      "Epoch 11 batch 900 train loss: 0.0015 test loss: 0.0032\n",
      "Epoch 11 batch 1000 train loss: 0.0002 test loss: 0.0032\n",
      "Epoch 11 batch 1100 train loss: 0.0018 test loss: 0.0031\n",
      "Epoch 11 batch 1200 train loss: 0.0030 test loss: 0.0032\n",
      "Epoch 11 batch 1300 train loss: 0.0003 test loss: 0.0032\n",
      "Epoch 11 batch 1400 train loss: 0.0002 test loss: 0.0032\n",
      "Epoch 11 batch 1500 train loss: 0.0032 test loss: 0.0032\n",
      "Epoch 11 batch 1600 train loss: 0.0018 test loss: 0.0032\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.30p/ckpt-21\n",
      "Epoch 11 batch 1700 train loss: 0.0020 test loss: 0.0031\n",
      "Epoch 11 batch 1800 train loss: 0.0028 test loss: 0.0032\n",
      "Epoch 11 batch 1900 train loss: 0.0006 test loss: 0.0032\n",
      "Epoch 11 batch 2000 train loss: 0.0003 test loss: 0.0032\n",
      "Epoch 11 batch 2100 train loss: 0.0002 test loss: 0.0032\n",
      "Epoch 11 batch 2200 train loss: 0.0014 test loss: 0.0031\n",
      "Epoch 11 batch 2300 train loss: 0.0007 test loss: 0.0032\n",
      "Epoch 11 batch 2400 train loss: 0.0010 test loss: 0.0032\n",
      "Epoch 11 batch 2500 train loss: 0.0008 test loss: 0.0032\n",
      "Epoch 11 batch 2600 train loss: 0.0003 test loss: 0.0032\n",
      "Epoch 11 batch 2700 train loss: 0.0022 test loss: 0.0032\n",
      "Epoch 11 batch 2800 train loss: 0.0010 test loss: 0.0032\n",
      "Epoch 11 batch 2900 train loss: 0.0028 test loss: 0.0031\n",
      "Epoch 12 batch 0 train loss: 0.0002 test loss: 0.0032\n",
      "Epoch 12 batch 100 train loss: 0.0022 test loss: 0.0032\n",
      "Epoch 12 batch 200 train loss: 0.0009 test loss: 0.0032\n",
      "Epoch 12 batch 300 train loss: 0.0027 test loss: 0.0032\n",
      "Epoch 12 batch 400 train loss: 0.0026 test loss: 0.0032\n",
      "Epoch 12 batch 500 train loss: 0.0035 test loss: 0.0032\n",
      "Epoch 12 batch 600 train loss: 0.0042 test loss: 0.0032\n",
      "Epoch 12 batch 700 train loss: 0.0002 test loss: 0.0032\n",
      "Epoch 12 batch 800 train loss: 0.0037 test loss: 0.0032\n",
      "Epoch 12 batch 900 train loss: 0.0010 test loss: 0.0032\n",
      "Epoch 12 batch 1000 train loss: 0.0022 test loss: 0.0032\n",
      "Epoch 12 batch 1100 train loss: 0.0011 test loss: 0.0032\n",
      "Epoch 12 batch 1200 train loss: 0.0033 test loss: 0.0032\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.30p/ckpt-22\n",
      "Epoch 12 batch 1300 train loss: 0.0009 test loss: 0.0031\n",
      "Epoch 12 batch 1400 train loss: 0.0002 test loss: 0.0032\n",
      "Epoch 12 batch 1500 train loss: 0.0028 test loss: 0.0032\n",
      "Epoch 12 batch 1600 train loss: 0.0017 test loss: 0.0032\n",
      "Epoch 12 batch 1700 train loss: 0.0002 test loss: 0.0032\n",
      "Epoch 12 batch 1800 train loss: 0.0031 test loss: 0.0031\n",
      "Epoch 12 batch 1900 train loss: 0.0025 test loss: 0.0033\n",
      "Epoch 12 batch 2000 train loss: 0.0008 test loss: 0.0032\n",
      "Epoch 12 batch 2100 train loss: 0.0030 test loss: 0.0032\n",
      "Epoch 12 batch 2200 train loss: 0.0011 test loss: 0.0032\n",
      "Epoch 12 batch 2300 train loss: 0.0024 test loss: 0.0031\n",
      "Epoch 12 batch 2400 train loss: 0.0009 test loss: 0.0032\n",
      "Epoch 12 batch 2500 train loss: 0.0002 test loss: 0.0032\n",
      "Epoch 12 batch 2600 train loss: 0.0002 test loss: 0.0031\n",
      "Epoch 12 batch 2700 train loss: 0.0002 test loss: 0.0032\n",
      "Epoch 12 batch 2800 train loss: 0.0025 test loss: 0.0032\n",
      "Epoch 12 batch 2900 train loss: 0.0002 test loss: 0.0032\n",
      "Epoch 13 batch 0 train loss: 0.0010 test loss: 0.0031\n",
      "Epoch 13 batch 100 train loss: 0.0002 test loss: 0.0032\n",
      "Epoch 13 batch 200 train loss: 0.0018 test loss: 0.0032\n",
      "Epoch 13 batch 300 train loss: 0.0040 test loss: 0.0031\n",
      "Epoch 13 batch 400 train loss: 0.0002 test loss: 0.0032\n",
      "Epoch 13 batch 500 train loss: 0.0002 test loss: 0.0032\n",
      "Epoch 13 batch 600 train loss: 0.0006 test loss: 0.0032\n",
      "Epoch 13 batch 700 train loss: 0.0006 test loss: 0.0032\n",
      "Epoch 13 batch 800 train loss: 0.0003 test loss: 0.0032\n",
      "Epoch 13 batch 900 train loss: 0.0003 test loss: 0.0032\n",
      "Epoch 13 batch 1000 train loss: 0.0002 test loss: 0.0032\n",
      "Epoch 13 batch 1100 train loss: 0.0013 test loss: 0.0032\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.30p/ckpt-23\n",
      "Epoch 13 batch 1200 train loss: 0.0002 test loss: 0.0031\n",
      "Epoch 13 batch 1300 train loss: 0.0003 test loss: 0.0031\n",
      "Epoch 13 batch 1400 train loss: 0.0003 test loss: 0.0031\n",
      "Epoch 13 batch 1500 train loss: 0.0016 test loss: 0.0031\n",
      "Epoch 13 batch 1600 train loss: 0.0010 test loss: 0.0032\n",
      "Epoch 13 batch 1700 train loss: 0.0021 test loss: 0.0032\n",
      "Epoch 13 batch 1800 train loss: 0.0023 test loss: 0.0032\n",
      "Epoch 13 batch 1900 train loss: 0.0035 test loss: 0.0033\n",
      "Epoch 13 batch 2000 train loss: 0.0020 test loss: 0.0033\n",
      "Epoch 13 batch 2100 train loss: 0.0002 test loss: 0.0032\n",
      "Epoch 13 batch 2200 train loss: 0.0010 test loss: 0.0032\n",
      "Epoch 13 batch 2300 train loss: 0.0020 test loss: 0.0032\n",
      "Epoch 13 batch 2400 train loss: 0.0025 test loss: 0.0032\n",
      "Epoch 13 batch 2500 train loss: 0.0017 test loss: 0.0032\n",
      "Epoch 13 batch 2600 train loss: 0.0008 test loss: 0.0032\n",
      "Epoch 13 batch 2700 train loss: 0.0019 test loss: 0.0032\n",
      "Epoch 13 batch 2800 train loss: 0.0041 test loss: 0.0032\n",
      "Epoch 13 batch 2900 train loss: 0.0002 test loss: 0.0032\n",
      "Epoch 14 batch 0 train loss: 0.0002 test loss: 0.0032\n",
      "Epoch 14 batch 100 train loss: 0.0021 test loss: 0.0031\n",
      "Epoch 14 batch 200 train loss: 0.0032 test loss: 0.0032\n",
      "Epoch 14 batch 300 train loss: 0.0003 test loss: 0.0032\n",
      "Epoch 14 batch 400 train loss: 0.0003 test loss: 0.0032\n",
      "Epoch 14 batch 500 train loss: 0.0011 test loss: 0.0032\n",
      "Epoch 14 batch 600 train loss: 0.0057 test loss: 0.0032\n",
      "Epoch 14 batch 700 train loss: 0.0011 test loss: 0.0032\n",
      "Epoch 14 batch 800 train loss: 0.0054 test loss: 0.0032\n",
      "Epoch 14 batch 900 train loss: 0.0012 test loss: 0.0032\n",
      "Epoch 14 batch 1000 train loss: 0.0020 test loss: 0.0032\n",
      "Epoch 14 batch 1100 train loss: 0.0006 test loss: 0.0032\n",
      "Epoch 14 batch 1200 train loss: 0.0032 test loss: 0.0031\n",
      "Epoch 14 batch 1300 train loss: 0.0003 test loss: 0.0031\n",
      "Epoch 14 batch 1400 train loss: 0.0013 test loss: 0.0032\n",
      "Epoch 14 batch 1500 train loss: 0.0013 test loss: 0.0031\n",
      "Epoch 14 batch 1600 train loss: 0.0032 test loss: 0.0032\n",
      "Epoch 14 batch 1700 train loss: 0.0012 test loss: 0.0032\n",
      "Epoch 14 batch 1800 train loss: 0.0023 test loss: 0.0032\n",
      "Epoch 14 batch 1900 train loss: 0.0024 test loss: 0.0032\n",
      "Epoch 14 batch 2000 train loss: 0.0018 test loss: 0.0032\n",
      "Epoch 14 batch 2100 train loss: 0.0004 test loss: 0.0032\n",
      "Epoch 14 batch 2200 train loss: 0.0003 test loss: 0.0031\n",
      "Epoch 14 batch 2300 train loss: 0.0022 test loss: 0.0032\n",
      "Epoch 14 batch 2400 train loss: 0.0019 test loss: 0.0032\n",
      "Epoch 14 batch 2500 train loss: 0.0044 test loss: 0.0032\n",
      "Epoch 14 batch 2600 train loss: 0.0065 test loss: 0.0031\n",
      "Epoch 14 batch 2700 train loss: 0.0002 test loss: 0.0033\n",
      "Epoch 14 batch 2800 train loss: 0.0027 test loss: 0.0032\n",
      "Epoch 14 batch 2900 train loss: 0.0033 test loss: 0.0032\n",
      "Epoch 15 batch 0 train loss: 0.0009 test loss: 0.0032\n",
      "Epoch 15 batch 100 train loss: 0.0032 test loss: 0.0032\n",
      "Epoch 15 batch 200 train loss: 0.0003 test loss: 0.0031\n",
      "Epoch 15 batch 300 train loss: 0.0026 test loss: 0.0032\n",
      "Epoch 15 batch 400 train loss: 0.0010 test loss: 0.0032\n",
      "Epoch 15 batch 500 train loss: 0.0035 test loss: 0.0031\n",
      "Epoch 15 batch 600 train loss: 0.0002 test loss: 0.0032\n",
      "Epoch 15 batch 700 train loss: 0.0015 test loss: 0.0031\n",
      "Epoch 15 batch 800 train loss: 0.0038 test loss: 0.0032\n",
      "Epoch 15 batch 900 train loss: 0.0011 test loss: 0.0031\n",
      "Epoch 15 batch 1000 train loss: 0.0002 test loss: 0.0032\n",
      "Epoch 15 batch 1100 train loss: 0.0002 test loss: 0.0032\n",
      "Epoch 15 batch 1200 train loss: 0.0012 test loss: 0.0032\n",
      "Epoch 15 batch 1300 train loss: 0.0002 test loss: 0.0032\n",
      "Epoch 15 batch 1400 train loss: 0.0026 test loss: 0.0031\n",
      "Epoch 15 batch 1500 train loss: 0.0027 test loss: 0.0032\n",
      "Epoch 15 batch 1600 train loss: 0.0021 test loss: 0.0032\n",
      "Epoch 15 batch 1700 train loss: 0.0002 test loss: 0.0032\n",
      "Epoch 15 batch 1800 train loss: 0.0016 test loss: 0.0032\n",
      "Epoch 15 batch 1900 train loss: 0.0013 test loss: 0.0032\n",
      "Epoch 15 batch 2000 train loss: 0.0006 test loss: 0.0032\n",
      "Epoch 15 batch 2100 train loss: 0.0002 test loss: 0.0032\n",
      "Epoch 15 batch 2200 train loss: 0.0010 test loss: 0.0031\n",
      "Epoch 15 batch 2300 train loss: 0.0021 test loss: 0.0032\n",
      "Epoch 15 batch 2400 train loss: 0.0004 test loss: 0.0032\n",
      "Epoch 15 batch 2500 train loss: 0.0020 test loss: 0.0032\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.30p/ckpt-24\n",
      "Epoch 15 batch 2600 train loss: 0.0011 test loss: 0.0031\n",
      "Epoch 15 batch 2700 train loss: 0.0018 test loss: 0.0031\n",
      "Epoch 15 batch 2800 train loss: 0.0049 test loss: 0.0031\n",
      "Epoch 15 batch 2900 train loss: 0.0027 test loss: 0.0031\n",
      "Epoch 16 batch 0 train loss: 0.0032 test loss: 0.0032\n",
      "Epoch 16 batch 100 train loss: 0.0009 test loss: 0.0032\n",
      "Epoch 16 batch 200 train loss: 0.0024 test loss: 0.0033\n",
      "Epoch 16 batch 300 train loss: 0.0003 test loss: 0.0032\n",
      "Epoch 16 batch 400 train loss: 0.0003 test loss: 0.0032\n",
      "Epoch 16 batch 500 train loss: 0.0024 test loss: 0.0031\n",
      "Epoch 16 batch 600 train loss: 0.0017 test loss: 0.0031\n",
      "Epoch 16 batch 700 train loss: 0.0024 test loss: 0.0032\n",
      "Epoch 16 batch 800 train loss: 0.0013 test loss: 0.0032\n",
      "Epoch 16 batch 900 train loss: 0.0004 test loss: 0.0031\n",
      "Epoch 16 batch 1000 train loss: 0.0020 test loss: 0.0032\n",
      "Epoch 16 batch 1100 train loss: 0.0005 test loss: 0.0031\n",
      "Epoch 16 batch 1200 train loss: 0.0016 test loss: 0.0032\n",
      "Epoch 16 batch 1300 train loss: 0.0045 test loss: 0.0031\n",
      "Epoch 16 batch 1400 train loss: 0.0004 test loss: 0.0031\n",
      "Epoch 16 batch 1500 train loss: 0.0003 test loss: 0.0032\n",
      "Epoch 16 batch 1600 train loss: 0.0026 test loss: 0.0031\n",
      "Epoch 16 batch 1700 train loss: 0.0014 test loss: 0.0032\n",
      "Epoch 16 batch 1800 train loss: 0.0019 test loss: 0.0032\n",
      "Epoch 16 batch 1900 train loss: 0.0002 test loss: 0.0031\n",
      "Epoch 16 batch 2000 train loss: 0.0008 test loss: 0.0032\n",
      "Epoch 16 batch 2100 train loss: 0.0003 test loss: 0.0032\n",
      "Epoch 16 batch 2200 train loss: 0.0002 test loss: 0.0031\n",
      "Epoch 16 batch 2300 train loss: 0.0009 test loss: 0.0032\n",
      "Epoch 16 batch 2400 train loss: 0.0009 test loss: 0.0032\n",
      "Epoch 16 batch 2500 train loss: 0.0023 test loss: 0.0031\n",
      "Epoch 16 batch 2600 train loss: 0.0011 test loss: 0.0032\n",
      "Epoch 16 batch 2700 train loss: 0.0024 test loss: 0.0032\n",
      "Epoch 16 batch 2800 train loss: 0.0002 test loss: 0.0032\n",
      "Epoch 16 batch 2900 train loss: 0.0040 test loss: 0.0031\n",
      "Epoch 17 batch 0 train loss: 0.0020 test loss: 0.0032\n",
      "Epoch 17 batch 100 train loss: 0.0008 test loss: 0.0032\n",
      "Epoch 17 batch 200 train loss: 0.0049 test loss: 0.0032\n",
      "Epoch 17 batch 300 train loss: 0.0022 test loss: 0.0031\n",
      "Epoch 17 batch 400 train loss: 0.0002 test loss: 0.0031\n",
      "Epoch 17 batch 500 train loss: 0.0004 test loss: 0.0031\n",
      "Epoch 17 batch 600 train loss: 0.0023 test loss: 0.0031\n",
      "Epoch 17 batch 700 train loss: 0.0012 test loss: 0.0032\n",
      "Epoch 17 batch 800 train loss: 0.0013 test loss: 0.0032\n",
      "Epoch 17 batch 900 train loss: 0.0010 test loss: 0.0031\n",
      "Epoch 17 batch 1000 train loss: 0.0008 test loss: 0.0032\n",
      "Epoch 17 batch 1100 train loss: 0.0006 test loss: 0.0032\n",
      "Epoch 17 batch 1200 train loss: 0.0002 test loss: 0.0032\n",
      "Epoch 17 batch 1300 train loss: 0.0031 test loss: 0.0032\n",
      "early stop.\n",
      "Checkpoint 24 restored!!\n",
      "2446/2446 [==============================] - 18s 7ms/step - loss: 0.0033\n",
      "Training for loss rate 0.40 start.\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.40p/ckpt-1\n",
      "Epoch 0 batch 0 train loss: 0.0846 test loss: 0.1174\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.40p/ckpt-2\n",
      "Epoch 0 batch 100 train loss: 0.0075 test loss: 0.0157\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.40p/ckpt-3\n",
      "Epoch 0 batch 200 train loss: 0.0133 test loss: 0.0124\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.40p/ckpt-4\n",
      "Epoch 0 batch 300 train loss: 0.0054 test loss: 0.0113\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.40p/ckpt-5\n",
      "Epoch 0 batch 400 train loss: 0.0074 test loss: 0.0101\n",
      "Epoch 0 batch 500 train loss: 0.0045 test loss: 0.0102\n",
      "Epoch 0 batch 600 train loss: 0.0083 test loss: 0.0102\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.40p/ckpt-6\n",
      "Epoch 0 batch 700 train loss: 0.0047 test loss: 0.0100\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.40p/ckpt-7\n",
      "Epoch 0 batch 800 train loss: 0.0062 test loss: 0.0090\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.40p/ckpt-8\n",
      "Epoch 0 batch 900 train loss: 0.0027 test loss: 0.0086\n",
      "Epoch 0 batch 1000 train loss: 0.0064 test loss: 0.0089\n",
      "Epoch 0 batch 1100 train loss: 0.0089 test loss: 0.0091\n",
      "Epoch 0 batch 1200 train loss: 0.0069 test loss: 0.0089\n",
      "Epoch 0 batch 1300 train loss: 0.0044 test loss: 0.0089\n",
      "Epoch 0 batch 1400 train loss: 0.0038 test loss: 0.0087\n",
      "Epoch 0 batch 1500 train loss: 0.0068 test loss: 0.0088\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.40p/ckpt-9\n",
      "Epoch 0 batch 1600 train loss: 0.0035 test loss: 0.0086\n",
      "Epoch 0 batch 1700 train loss: 0.0046 test loss: 0.0089\n",
      "Epoch 0 batch 1800 train loss: 0.0052 test loss: 0.0086\n",
      "Epoch 0 batch 1900 train loss: 0.0037 test loss: 0.0087\n",
      "Epoch 0 batch 2000 train loss: 0.0033 test loss: 0.0087\n",
      "Epoch 0 batch 2100 train loss: 0.0013 test loss: 0.0087\n",
      "Epoch 0 batch 2200 train loss: 0.0091 test loss: 0.0087\n",
      "Epoch 0 batch 2300 train loss: 0.0067 test loss: 0.0087\n",
      "Epoch 0 batch 2400 train loss: 0.0050 test loss: 0.0087\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.40p/ckpt-10\n",
      "Epoch 0 batch 2500 train loss: 0.0068 test loss: 0.0083\n",
      "Epoch 0 batch 2600 train loss: 0.0020 test loss: 0.0084\n",
      "Epoch 0 batch 2700 train loss: 0.0038 test loss: 0.0084\n",
      "Epoch 0 batch 2800 train loss: 0.0016 test loss: 0.0089\n",
      "Epoch 0 batch 2900 train loss: 0.0044 test loss: 0.0088\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.40p/ckpt-11\n",
      "Epoch 1 batch 0 train loss: 0.0036 test loss: 0.0081\n",
      "Epoch 1 batch 100 train loss: 0.0058 test loss: 0.0087\n",
      "Epoch 1 batch 200 train loss: 0.0052 test loss: 0.0085\n",
      "Epoch 1 batch 300 train loss: 0.0057 test loss: 0.0088\n",
      "Epoch 1 batch 400 train loss: 0.0051 test loss: 0.0082\n",
      "Epoch 1 batch 500 train loss: 0.0035 test loss: 0.0084\n",
      "Epoch 1 batch 600 train loss: 0.0089 test loss: 0.0088\n",
      "Epoch 1 batch 700 train loss: 0.0047 test loss: 0.0083\n",
      "Epoch 1 batch 800 train loss: 0.0041 test loss: 0.0086\n",
      "Epoch 1 batch 900 train loss: 0.0043 test loss: 0.0084\n",
      "Epoch 1 batch 1000 train loss: 0.0014 test loss: 0.0083\n",
      "Epoch 1 batch 1100 train loss: 0.0036 test loss: 0.0083\n",
      "Epoch 1 batch 1200 train loss: 0.0022 test loss: 0.0083\n",
      "Epoch 1 batch 1300 train loss: 0.0035 test loss: 0.0082\n",
      "Epoch 1 batch 1400 train loss: 0.0086 test loss: 0.0084\n",
      "Epoch 1 batch 1500 train loss: 0.0059 test loss: 0.0081\n",
      "Epoch 1 batch 1600 train loss: 0.0010 test loss: 0.0082\n",
      "Epoch 1 batch 1700 train loss: 0.0051 test loss: 0.0084\n",
      "Epoch 1 batch 1800 train loss: 0.0055 test loss: 0.0081\n",
      "Epoch 1 batch 1900 train loss: 0.0079 test loss: 0.0084\n",
      "Epoch 1 batch 2000 train loss: 0.0107 test loss: 0.0083\n",
      "Epoch 1 batch 2100 train loss: 0.0079 test loss: 0.0083\n",
      "Epoch 1 batch 2200 train loss: 0.0046 test loss: 0.0082\n",
      "Epoch 1 batch 2300 train loss: 0.0024 test loss: 0.0083\n",
      "Epoch 1 batch 2400 train loss: 0.0049 test loss: 0.0082\n",
      "Epoch 1 batch 2500 train loss: 0.0024 test loss: 0.0082\n",
      "Epoch 1 batch 2600 train loss: 0.0134 test loss: 0.0081\n",
      "Epoch 1 batch 2700 train loss: 0.0028 test loss: 0.0085\n",
      "Epoch 1 batch 2800 train loss: 0.0034 test loss: 0.0086\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.40p/ckpt-12\n",
      "Epoch 1 batch 2900 train loss: 0.0021 test loss: 0.0080\n",
      "Epoch 2 batch 0 train loss: 0.0013 test loss: 0.0081\n",
      "Epoch 2 batch 100 train loss: 0.0041 test loss: 0.0083\n",
      "Epoch 2 batch 200 train loss: 0.0024 test loss: 0.0082\n",
      "Epoch 2 batch 300 train loss: 0.0043 test loss: 0.0083\n",
      "Epoch 2 batch 400 train loss: 0.0010 test loss: 0.0083\n",
      "Epoch 2 batch 500 train loss: 0.0041 test loss: 0.0082\n",
      "Epoch 2 batch 600 train loss: 0.0055 test loss: 0.0082\n",
      "Epoch 2 batch 700 train loss: 0.0010 test loss: 0.0082\n",
      "Epoch 2 batch 800 train loss: 0.0077 test loss: 0.0082\n",
      "Epoch 2 batch 900 train loss: 0.0019 test loss: 0.0084\n",
      "Epoch 2 batch 1000 train loss: 0.0048 test loss: 0.0082\n",
      "Epoch 2 batch 1100 train loss: 0.0061 test loss: 0.0081\n",
      "Epoch 2 batch 1200 train loss: 0.0027 test loss: 0.0081\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.40p/ckpt-13\n",
      "Epoch 2 batch 1300 train loss: 0.0062 test loss: 0.0080\n",
      "Epoch 2 batch 1400 train loss: 0.0035 test loss: 0.0083\n",
      "Epoch 2 batch 1500 train loss: 0.0040 test loss: 0.0082\n",
      "Epoch 2 batch 1600 train loss: 0.0009 test loss: 0.0083\n",
      "Epoch 2 batch 1700 train loss: 0.0018 test loss: 0.0082\n",
      "Epoch 2 batch 1800 train loss: 0.0021 test loss: 0.0082\n",
      "Epoch 2 batch 1900 train loss: 0.0029 test loss: 0.0083\n",
      "Epoch 2 batch 2000 train loss: 0.0069 test loss: 0.0081\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.40p/ckpt-14\n",
      "Epoch 2 batch 2100 train loss: 0.0059 test loss: 0.0080\n",
      "Epoch 2 batch 2200 train loss: 0.0037 test loss: 0.0080\n",
      "Epoch 2 batch 2300 train loss: 0.0037 test loss: 0.0081\n",
      "Epoch 2 batch 2400 train loss: 0.0072 test loss: 0.0083\n",
      "Epoch 2 batch 2500 train loss: 0.0037 test loss: 0.0081\n",
      "Epoch 2 batch 2600 train loss: 0.0036 test loss: 0.0080\n",
      "Epoch 2 batch 2700 train loss: 0.0012 test loss: 0.0081\n",
      "Epoch 2 batch 2800 train loss: 0.0112 test loss: 0.0080\n",
      "Epoch 2 batch 2900 train loss: 0.0028 test loss: 0.0080\n",
      "Epoch 3 batch 0 train loss: 0.0034 test loss: 0.0082\n",
      "Epoch 3 batch 100 train loss: 0.0051 test loss: 0.0083\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.40p/ckpt-15\n",
      "Epoch 3 batch 200 train loss: 0.0033 test loss: 0.0079\n",
      "Epoch 3 batch 300 train loss: 0.0050 test loss: 0.0080\n",
      "Epoch 3 batch 400 train loss: 0.0023 test loss: 0.0082\n",
      "Epoch 3 batch 500 train loss: 0.0070 test loss: 0.0082\n",
      "Epoch 3 batch 600 train loss: 0.0104 test loss: 0.0082\n",
      "Epoch 3 batch 700 train loss: 0.0013 test loss: 0.0082\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.40p/ckpt-16\n",
      "Epoch 3 batch 800 train loss: 0.0026 test loss: 0.0079\n",
      "Epoch 3 batch 900 train loss: 0.0007 test loss: 0.0081\n",
      "Epoch 3 batch 1000 train loss: 0.0039 test loss: 0.0081\n",
      "Epoch 3 batch 1100 train loss: 0.0055 test loss: 0.0081\n",
      "Epoch 3 batch 1200 train loss: 0.0024 test loss: 0.0082\n",
      "Epoch 3 batch 1300 train loss: 0.0062 test loss: 0.0082\n",
      "Epoch 3 batch 1400 train loss: 0.0041 test loss: 0.0084\n",
      "Epoch 3 batch 1500 train loss: 0.0031 test loss: 0.0080\n",
      "Epoch 3 batch 1600 train loss: 0.0033 test loss: 0.0080\n",
      "Epoch 3 batch 1700 train loss: 0.0027 test loss: 0.0081\n",
      "Epoch 3 batch 1800 train loss: 0.0014 test loss: 0.0081\n",
      "Epoch 3 batch 1900 train loss: 0.0015 test loss: 0.0080\n",
      "Epoch 3 batch 2000 train loss: 0.0073 test loss: 0.0082\n",
      "Epoch 3 batch 2100 train loss: 0.0025 test loss: 0.0080\n",
      "Epoch 3 batch 2200 train loss: 0.0011 test loss: 0.0082\n",
      "Epoch 3 batch 2300 train loss: 0.0023 test loss: 0.0083\n",
      "Epoch 3 batch 2400 train loss: 0.0028 test loss: 0.0081\n",
      "Epoch 3 batch 2500 train loss: 0.0068 test loss: 0.0080\n",
      "Epoch 3 batch 2600 train loss: 0.0052 test loss: 0.0080\n",
      "Epoch 3 batch 2700 train loss: 0.0017 test loss: 0.0081\n",
      "Epoch 3 batch 2800 train loss: 0.0039 test loss: 0.0080\n",
      "Epoch 3 batch 2900 train loss: 0.0049 test loss: 0.0081\n",
      "Epoch 4 batch 0 train loss: 0.0028 test loss: 0.0080\n",
      "Epoch 4 batch 100 train loss: 0.0093 test loss: 0.0080\n",
      "Epoch 4 batch 200 train loss: 0.0034 test loss: 0.0081\n",
      "Epoch 4 batch 300 train loss: 0.0111 test loss: 0.0079\n",
      "Epoch 4 batch 400 train loss: 0.0022 test loss: 0.0081\n",
      "Epoch 4 batch 500 train loss: 0.0045 test loss: 0.0083\n",
      "Epoch 4 batch 600 train loss: 0.0032 test loss: 0.0081\n",
      "Epoch 4 batch 700 train loss: 0.0006 test loss: 0.0083\n",
      "Epoch 4 batch 800 train loss: 0.0038 test loss: 0.0081\n",
      "Epoch 4 batch 900 train loss: 0.0028 test loss: 0.0081\n",
      "Epoch 4 batch 1000 train loss: 0.0053 test loss: 0.0081\n",
      "Epoch 4 batch 1100 train loss: 0.0018 test loss: 0.0082\n",
      "Epoch 4 batch 1200 train loss: 0.0027 test loss: 0.0081\n",
      "Epoch 4 batch 1300 train loss: 0.0021 test loss: 0.0082\n",
      "Epoch 4 batch 1400 train loss: 0.0073 test loss: 0.0080\n",
      "Epoch 4 batch 1500 train loss: 0.0024 test loss: 0.0081\n",
      "Epoch 4 batch 1600 train loss: 0.0023 test loss: 0.0079\n",
      "Epoch 4 batch 1700 train loss: 0.0041 test loss: 0.0081\n",
      "Epoch 4 batch 1800 train loss: 0.0029 test loss: 0.0079\n",
      "Epoch 4 batch 1900 train loss: 0.0039 test loss: 0.0082\n",
      "Epoch 4 batch 2000 train loss: 0.0035 test loss: 0.0081\n",
      "Epoch 4 batch 2100 train loss: 0.0053 test loss: 0.0080\n",
      "Epoch 4 batch 2200 train loss: 0.0029 test loss: 0.0080\n",
      "Epoch 4 batch 2300 train loss: 0.0036 test loss: 0.0082\n",
      "Epoch 4 batch 2400 train loss: 0.0043 test loss: 0.0081\n",
      "Epoch 4 batch 2500 train loss: 0.0035 test loss: 0.0081\n",
      "Epoch 4 batch 2600 train loss: 0.0036 test loss: 0.0080\n",
      "Epoch 4 batch 2700 train loss: 0.0032 test loss: 0.0081\n",
      "Epoch 4 batch 2800 train loss: 0.0007 test loss: 0.0081\n",
      "Epoch 4 batch 2900 train loss: 0.0036 test loss: 0.0081\n",
      "Epoch 5 batch 0 train loss: 0.0020 test loss: 0.0082\n",
      "Epoch 5 batch 100 train loss: 0.0005 test loss: 0.0081\n",
      "Epoch 5 batch 200 train loss: 0.0079 test loss: 0.0081\n",
      "Epoch 5 batch 300 train loss: 0.0022 test loss: 0.0079\n",
      "Epoch 5 batch 400 train loss: 0.0058 test loss: 0.0080\n",
      "Epoch 5 batch 500 train loss: 0.0006 test loss: 0.0082\n",
      "Epoch 5 batch 600 train loss: 0.0057 test loss: 0.0082\n",
      "Epoch 5 batch 700 train loss: 0.0028 test loss: 0.0082\n",
      "Epoch 5 batch 800 train loss: 0.0031 test loss: 0.0080\n",
      "Epoch 5 batch 900 train loss: 0.0047 test loss: 0.0081\n",
      "Epoch 5 batch 1000 train loss: 0.0054 test loss: 0.0080\n",
      "Epoch 5 batch 1100 train loss: 0.0045 test loss: 0.0080\n",
      "Epoch 5 batch 1200 train loss: 0.0055 test loss: 0.0080\n",
      "Epoch 5 batch 1300 train loss: 0.0066 test loss: 0.0081\n",
      "Epoch 5 batch 1400 train loss: 0.0045 test loss: 0.0080\n",
      "Epoch 5 batch 1500 train loss: 0.0032 test loss: 0.0080\n",
      "Epoch 5 batch 1600 train loss: 0.0036 test loss: 0.0080\n",
      "Epoch 5 batch 1700 train loss: 0.0013 test loss: 0.0081\n",
      "Epoch 5 batch 1800 train loss: 0.0051 test loss: 0.0080\n",
      "Epoch 5 batch 1900 train loss: 0.0031 test loss: 0.0080\n",
      "Epoch 5 batch 2000 train loss: 0.0067 test loss: 0.0081\n",
      "Epoch 5 batch 2100 train loss: 0.0025 test loss: 0.0080\n",
      "Epoch 5 batch 2200 train loss: 0.0064 test loss: 0.0079\n",
      "Epoch 5 batch 2300 train loss: 0.0010 test loss: 0.0080\n",
      "Epoch 5 batch 2400 train loss: 0.0017 test loss: 0.0080\n",
      "Epoch 5 batch 2500 train loss: 0.0046 test loss: 0.0081\n",
      "Epoch 5 batch 2600 train loss: 0.0079 test loss: 0.0080\n",
      "Epoch 5 batch 2700 train loss: 0.0042 test loss: 0.0080\n",
      "Epoch 5 batch 2800 train loss: 0.0053 test loss: 0.0081\n",
      "Epoch 5 batch 2900 train loss: 0.0041 test loss: 0.0080\n",
      "Epoch 6 batch 0 train loss: 0.0041 test loss: 0.0080\n",
      "Epoch 6 batch 100 train loss: 0.0055 test loss: 0.0080\n",
      "Epoch 6 batch 200 train loss: 0.0009 test loss: 0.0079\n",
      "Epoch 6 batch 300 train loss: 0.0027 test loss: 0.0079\n",
      "Epoch 6 batch 400 train loss: 0.0047 test loss: 0.0079\n",
      "Epoch 6 batch 500 train loss: 0.0018 test loss: 0.0081\n",
      "Epoch 6 batch 600 train loss: 0.0012 test loss: 0.0081\n",
      "Epoch 6 batch 700 train loss: 0.0042 test loss: 0.0080\n",
      "Epoch 6 batch 800 train loss: 0.0050 test loss: 0.0080\n",
      "Epoch 6 batch 900 train loss: 0.0044 test loss: 0.0081\n",
      "Epoch 6 batch 1000 train loss: 0.0027 test loss: 0.0079\n",
      "Epoch 6 batch 1100 train loss: 0.0050 test loss: 0.0082\n",
      "Epoch 6 batch 1200 train loss: 0.0036 test loss: 0.0080\n",
      "Epoch 6 batch 1300 train loss: 0.0009 test loss: 0.0081\n",
      "Epoch 6 batch 1400 train loss: 0.0025 test loss: 0.0082\n",
      "Epoch 6 batch 1500 train loss: 0.0059 test loss: 0.0081\n",
      "Epoch 6 batch 1600 train loss: 0.0070 test loss: 0.0080\n",
      "Epoch 6 batch 1700 train loss: 0.0021 test loss: 0.0082\n",
      "Epoch 6 batch 1800 train loss: 0.0029 test loss: 0.0081\n",
      "Epoch 6 batch 1900 train loss: 0.0018 test loss: 0.0080\n",
      "Epoch 6 batch 2000 train loss: 0.0053 test loss: 0.0082\n",
      "Epoch 6 batch 2100 train loss: 0.0029 test loss: 0.0080\n",
      "Epoch 6 batch 2200 train loss: 0.0044 test loss: 0.0080\n",
      "Epoch 6 batch 2300 train loss: 0.0025 test loss: 0.0081\n",
      "Epoch 6 batch 2400 train loss: 0.0045 test loss: 0.0081\n",
      "Epoch 6 batch 2500 train loss: 0.0041 test loss: 0.0080\n",
      "Epoch 6 batch 2600 train loss: 0.0041 test loss: 0.0079\n",
      "Epoch 6 batch 2700 train loss: 0.0052 test loss: 0.0081\n",
      "Epoch 6 batch 2800 train loss: 0.0056 test loss: 0.0080\n",
      "Epoch 6 batch 2900 train loss: 0.0016 test loss: 0.0079\n",
      "Epoch 7 batch 0 train loss: 0.0095 test loss: 0.0081\n",
      "Epoch 7 batch 100 train loss: 0.0024 test loss: 0.0080\n",
      "Epoch 7 batch 200 train loss: 0.0034 test loss: 0.0079\n",
      "Epoch 7 batch 300 train loss: 0.0025 test loss: 0.0080\n",
      "Epoch 7 batch 400 train loss: 0.0075 test loss: 0.0079\n",
      "Epoch 7 batch 500 train loss: 0.0035 test loss: 0.0081\n",
      "Epoch 7 batch 600 train loss: 0.0066 test loss: 0.0080\n",
      "Epoch 7 batch 700 train loss: 0.0033 test loss: 0.0080\n",
      "Epoch 7 batch 800 train loss: 0.0060 test loss: 0.0080\n",
      "Epoch 7 batch 900 train loss: 0.0037 test loss: 0.0080\n",
      "Epoch 7 batch 1000 train loss: 0.0043 test loss: 0.0080\n",
      "Epoch 7 batch 1100 train loss: 0.0043 test loss: 0.0080\n",
      "Epoch 7 batch 1200 train loss: 0.0045 test loss: 0.0081\n",
      "Epoch 7 batch 1300 train loss: 0.0044 test loss: 0.0080\n",
      "Epoch 7 batch 1400 train loss: 0.0030 test loss: 0.0079\n",
      "Epoch 7 batch 1500 train loss: 0.0012 test loss: 0.0080\n",
      "Epoch 7 batch 1600 train loss: 0.0045 test loss: 0.0079\n",
      "Epoch 7 batch 1700 train loss: 0.0013 test loss: 0.0080\n",
      "Epoch 7 batch 1800 train loss: 0.0053 test loss: 0.0080\n",
      "Epoch 7 batch 1900 train loss: 0.0041 test loss: 0.0082\n",
      "Epoch 7 batch 2000 train loss: 0.0031 test loss: 0.0079\n",
      "Epoch 7 batch 2100 train loss: 0.0059 test loss: 0.0080\n",
      "Epoch 7 batch 2200 train loss: 0.0005 test loss: 0.0080\n",
      "Epoch 7 batch 2300 train loss: 0.0068 test loss: 0.0080\n",
      "Epoch 7 batch 2400 train loss: 0.0025 test loss: 0.0080\n",
      "Epoch 7 batch 2500 train loss: 0.0070 test loss: 0.0080\n",
      "Epoch 7 batch 2600 train loss: 0.0004 test loss: 0.0079\n",
      "Epoch 7 batch 2700 train loss: 0.0014 test loss: 0.0081\n",
      "Epoch 7 batch 2800 train loss: 0.0026 test loss: 0.0080\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.40p/ckpt-17\n",
      "Epoch 7 batch 2900 train loss: 0.0024 test loss: 0.0079\n",
      "Epoch 8 batch 0 train loss: 0.0046 test loss: 0.0080\n",
      "Epoch 8 batch 100 train loss: 0.0004 test loss: 0.0080\n",
      "Epoch 8 batch 200 train loss: 0.0092 test loss: 0.0080\n",
      "Epoch 8 batch 300 train loss: 0.0038 test loss: 0.0080\n",
      "Epoch 8 batch 400 train loss: 0.0065 test loss: 0.0081\n",
      "Epoch 8 batch 500 train loss: 0.0054 test loss: 0.0080\n",
      "Epoch 8 batch 600 train loss: 0.0054 test loss: 0.0081\n",
      "Epoch 8 batch 700 train loss: 0.0042 test loss: 0.0080\n",
      "Epoch 8 batch 800 train loss: 0.0059 test loss: 0.0080\n",
      "Epoch 8 batch 900 train loss: 0.0035 test loss: 0.0081\n",
      "Epoch 8 batch 1000 train loss: 0.0033 test loss: 0.0080\n",
      "Epoch 8 batch 1100 train loss: 0.0029 test loss: 0.0081\n",
      "Epoch 8 batch 1200 train loss: 0.0011 test loss: 0.0079\n",
      "Epoch 8 batch 1300 train loss: 0.0022 test loss: 0.0080\n",
      "Epoch 8 batch 1400 train loss: 0.0003 test loss: 0.0081\n",
      "Epoch 8 batch 1500 train loss: 0.0068 test loss: 0.0079\n",
      "Epoch 8 batch 1600 train loss: 0.0051 test loss: 0.0079\n",
      "Epoch 8 batch 1700 train loss: 0.0037 test loss: 0.0080\n",
      "Epoch 8 batch 1800 train loss: 0.0027 test loss: 0.0080\n",
      "Epoch 8 batch 1900 train loss: 0.0051 test loss: 0.0080\n",
      "Epoch 8 batch 2000 train loss: 0.0022 test loss: 0.0081\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.40p/ckpt-18\n",
      "Epoch 8 batch 2100 train loss: 0.0017 test loss: 0.0078\n",
      "Epoch 8 batch 2200 train loss: 0.0015 test loss: 0.0079\n",
      "Epoch 8 batch 2300 train loss: 0.0015 test loss: 0.0080\n",
      "Epoch 8 batch 2400 train loss: 0.0060 test loss: 0.0080\n",
      "Epoch 8 batch 2500 train loss: 0.0004 test loss: 0.0081\n",
      "Epoch 8 batch 2600 train loss: 0.0052 test loss: 0.0079\n",
      "Epoch 8 batch 2700 train loss: 0.0009 test loss: 0.0080\n",
      "Epoch 8 batch 2800 train loss: 0.0031 test loss: 0.0079\n",
      "Epoch 8 batch 2900 train loss: 0.0057 test loss: 0.0079\n",
      "Epoch 9 batch 0 train loss: 0.0014 test loss: 0.0079\n",
      "Epoch 9 batch 100 train loss: 0.0033 test loss: 0.0081\n",
      "Epoch 9 batch 200 train loss: 0.0054 test loss: 0.0080\n",
      "Epoch 9 batch 300 train loss: 0.0014 test loss: 0.0079\n",
      "Epoch 9 batch 400 train loss: 0.0003 test loss: 0.0080\n",
      "Epoch 9 batch 500 train loss: 0.0008 test loss: 0.0079\n",
      "Epoch 9 batch 600 train loss: 0.0048 test loss: 0.0079\n",
      "Epoch 9 batch 700 train loss: 0.0026 test loss: 0.0080\n",
      "Epoch 9 batch 800 train loss: 0.0070 test loss: 0.0080\n",
      "Epoch 9 batch 900 train loss: 0.0050 test loss: 0.0078\n",
      "Epoch 9 batch 1000 train loss: 0.0016 test loss: 0.0079\n",
      "Epoch 9 batch 1100 train loss: 0.0063 test loss: 0.0080\n",
      "Epoch 9 batch 1200 train loss: 0.0021 test loss: 0.0079\n",
      "Epoch 9 batch 1300 train loss: 0.0067 test loss: 0.0080\n",
      "Epoch 9 batch 1400 train loss: 0.0019 test loss: 0.0080\n",
      "Epoch 9 batch 1500 train loss: 0.0011 test loss: 0.0080\n",
      "Epoch 9 batch 1600 train loss: 0.0034 test loss: 0.0080\n",
      "Epoch 9 batch 1700 train loss: 0.0013 test loss: 0.0080\n",
      "Epoch 9 batch 1800 train loss: 0.0029 test loss: 0.0080\n",
      "Epoch 9 batch 1900 train loss: 0.0050 test loss: 0.0080\n",
      "Epoch 9 batch 2000 train loss: 0.0017 test loss: 0.0081\n",
      "Epoch 9 batch 2100 train loss: 0.0044 test loss: 0.0080\n",
      "Epoch 9 batch 2200 train loss: 0.0050 test loss: 0.0079\n",
      "Epoch 9 batch 2300 train loss: 0.0031 test loss: 0.0081\n",
      "Epoch 9 batch 2400 train loss: 0.0038 test loss: 0.0080\n",
      "Epoch 9 batch 2500 train loss: 0.0041 test loss: 0.0080\n",
      "Epoch 9 batch 2600 train loss: 0.0044 test loss: 0.0080\n",
      "Epoch 9 batch 2700 train loss: 0.0045 test loss: 0.0082\n",
      "Epoch 9 batch 2800 train loss: 0.0023 test loss: 0.0080\n",
      "Epoch 9 batch 2900 train loss: 0.0013 test loss: 0.0079\n",
      "Epoch 10 batch 0 train loss: 0.0028 test loss: 0.0080\n",
      "Epoch 10 batch 100 train loss: 0.0055 test loss: 0.0080\n",
      "Epoch 10 batch 200 train loss: 0.0054 test loss: 0.0080\n",
      "Epoch 10 batch 300 train loss: 0.0010 test loss: 0.0079\n",
      "Epoch 10 batch 400 train loss: 0.0077 test loss: 0.0079\n",
      "Epoch 10 batch 500 train loss: 0.0013 test loss: 0.0080\n",
      "Epoch 10 batch 600 train loss: 0.0086 test loss: 0.0081\n",
      "Epoch 10 batch 700 train loss: 0.0049 test loss: 0.0080\n",
      "Epoch 10 batch 800 train loss: 0.0063 test loss: 0.0080\n",
      "Epoch 10 batch 900 train loss: 0.0034 test loss: 0.0079\n",
      "Epoch 10 batch 1000 train loss: 0.0025 test loss: 0.0080\n",
      "Epoch 10 batch 1100 train loss: 0.0061 test loss: 0.0080\n",
      "Epoch 10 batch 1200 train loss: 0.0013 test loss: 0.0080\n",
      "Epoch 10 batch 1300 train loss: 0.0050 test loss: 0.0080\n",
      "Epoch 10 batch 1400 train loss: 0.0053 test loss: 0.0080\n",
      "Epoch 10 batch 1500 train loss: 0.0022 test loss: 0.0080\n",
      "Epoch 10 batch 1600 train loss: 0.0015 test loss: 0.0079\n",
      "Epoch 10 batch 1700 train loss: 0.0010 test loss: 0.0080\n",
      "Epoch 10 batch 1800 train loss: 0.0058 test loss: 0.0081\n",
      "Epoch 10 batch 1900 train loss: 0.0034 test loss: 0.0080\n",
      "Epoch 10 batch 2000 train loss: 0.0074 test loss: 0.0081\n",
      "Epoch 10 batch 2100 train loss: 0.0019 test loss: 0.0079\n",
      "Epoch 10 batch 2200 train loss: 0.0004 test loss: 0.0079\n",
      "Epoch 10 batch 2300 train loss: 0.0049 test loss: 0.0080\n",
      "Epoch 10 batch 2400 train loss: 0.0027 test loss: 0.0081\n",
      "Epoch 10 batch 2500 train loss: 0.0027 test loss: 0.0079\n",
      "Epoch 10 batch 2600 train loss: 0.0035 test loss: 0.0080\n",
      "Epoch 10 batch 2700 train loss: 0.0020 test loss: 0.0080\n",
      "Epoch 10 batch 2800 train loss: 0.0023 test loss: 0.0080\n",
      "Epoch 10 batch 2900 train loss: 0.0061 test loss: 0.0079\n",
      "Epoch 11 batch 0 train loss: 0.0038 test loss: 0.0079\n",
      "Epoch 11 batch 100 train loss: 0.0047 test loss: 0.0080\n",
      "Epoch 11 batch 200 train loss: 0.0024 test loss: 0.0080\n",
      "Epoch 11 batch 300 train loss: 0.0042 test loss: 0.0079\n",
      "Epoch 11 batch 400 train loss: 0.0004 test loss: 0.0080\n",
      "Epoch 11 batch 500 train loss: 0.0072 test loss: 0.0081\n",
      "Epoch 11 batch 600 train loss: 0.0038 test loss: 0.0079\n",
      "Epoch 11 batch 700 train loss: 0.0073 test loss: 0.0080\n",
      "Epoch 11 batch 800 train loss: 0.0021 test loss: 0.0080\n",
      "Epoch 11 batch 900 train loss: 0.0019 test loss: 0.0080\n",
      "Epoch 11 batch 1000 train loss: 0.0093 test loss: 0.0079\n",
      "Epoch 11 batch 1100 train loss: 0.0025 test loss: 0.0080\n",
      "Epoch 11 batch 1200 train loss: 0.0057 test loss: 0.0081\n",
      "Epoch 11 batch 1300 train loss: 0.0054 test loss: 0.0081\n",
      "Epoch 11 batch 1400 train loss: 0.0044 test loss: 0.0080\n",
      "Epoch 11 batch 1500 train loss: 0.0054 test loss: 0.0080\n",
      "Epoch 11 batch 1600 train loss: 0.0077 test loss: 0.0080\n",
      "Epoch 11 batch 1700 train loss: 0.0046 test loss: 0.0080\n",
      "Epoch 11 batch 1800 train loss: 0.0051 test loss: 0.0080\n",
      "Epoch 11 batch 1900 train loss: 0.0013 test loss: 0.0080\n",
      "Epoch 11 batch 2000 train loss: 0.0050 test loss: 0.0080\n",
      "Epoch 11 batch 2100 train loss: 0.0053 test loss: 0.0079\n",
      "Epoch 11 batch 2200 train loss: 0.0047 test loss: 0.0080\n",
      "Epoch 11 batch 2300 train loss: 0.0012 test loss: 0.0079\n",
      "Epoch 11 batch 2400 train loss: 0.0067 test loss: 0.0080\n",
      "Epoch 11 batch 2500 train loss: 0.0065 test loss: 0.0079\n",
      "Epoch 11 batch 2600 train loss: 0.0004 test loss: 0.0078\n",
      "Epoch 11 batch 2700 train loss: 0.0008 test loss: 0.0080\n",
      "Epoch 11 batch 2800 train loss: 0.0025 test loss: 0.0080\n",
      "Epoch 11 batch 2900 train loss: 0.0060 test loss: 0.0079\n",
      "Epoch 12 batch 0 train loss: 0.0079 test loss: 0.0079\n",
      "Epoch 12 batch 100 train loss: 0.0062 test loss: 0.0080\n",
      "Epoch 12 batch 200 train loss: 0.0084 test loss: 0.0080\n",
      "Epoch 12 batch 300 train loss: 0.0030 test loss: 0.0079\n",
      "Epoch 12 batch 400 train loss: 0.0063 test loss: 0.0080\n",
      "Epoch 12 batch 500 train loss: 0.0020 test loss: 0.0080\n",
      "Epoch 12 batch 600 train loss: 0.0010 test loss: 0.0079\n",
      "Epoch 12 batch 700 train loss: 0.0041 test loss: 0.0080\n",
      "Epoch 12 batch 800 train loss: 0.0055 test loss: 0.0080\n",
      "Epoch 12 batch 900 train loss: 0.0028 test loss: 0.0079\n",
      "Epoch 12 batch 1000 train loss: 0.0016 test loss: 0.0080\n",
      "Epoch 12 batch 1100 train loss: 0.0021 test loss: 0.0080\n",
      "Epoch 12 batch 1200 train loss: 0.0061 test loss: 0.0080\n",
      "Epoch 12 batch 1300 train loss: 0.0042 test loss: 0.0080\n",
      "Epoch 12 batch 1400 train loss: 0.0046 test loss: 0.0080\n",
      "Epoch 12 batch 1500 train loss: 0.0017 test loss: 0.0079\n",
      "Epoch 12 batch 1600 train loss: 0.0027 test loss: 0.0079\n",
      "Epoch 12 batch 1700 train loss: 0.0019 test loss: 0.0080\n",
      "Epoch 12 batch 1800 train loss: 0.0032 test loss: 0.0080\n",
      "Epoch 12 batch 1900 train loss: 0.0004 test loss: 0.0081\n",
      "Epoch 12 batch 2000 train loss: 0.0010 test loss: 0.0080\n",
      "Epoch 12 batch 2100 train loss: 0.0036 test loss: 0.0079\n",
      "Epoch 12 batch 2200 train loss: 0.0043 test loss: 0.0080\n",
      "Epoch 12 batch 2300 train loss: 0.0028 test loss: 0.0079\n",
      "Epoch 12 batch 2400 train loss: 0.0042 test loss: 0.0080\n",
      "Epoch 12 batch 2500 train loss: 0.0011 test loss: 0.0080\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.40p/ckpt-19\n",
      "Epoch 12 batch 2600 train loss: 0.0044 test loss: 0.0078\n",
      "Epoch 12 batch 2700 train loss: 0.0032 test loss: 0.0081\n",
      "Epoch 12 batch 2800 train loss: 0.0006 test loss: 0.0079\n",
      "Epoch 12 batch 2900 train loss: 0.0061 test loss: 0.0080\n",
      "Epoch 13 batch 0 train loss: 0.0006 test loss: 0.0079\n",
      "Epoch 13 batch 100 train loss: 0.0027 test loss: 0.0079\n",
      "Epoch 13 batch 200 train loss: 0.0028 test loss: 0.0080\n",
      "Epoch 13 batch 300 train loss: 0.0052 test loss: 0.0080\n",
      "Epoch 13 batch 400 train loss: 0.0021 test loss: 0.0080\n",
      "Epoch 13 batch 500 train loss: 0.0048 test loss: 0.0080\n",
      "Epoch 13 batch 600 train loss: 0.0073 test loss: 0.0080\n",
      "Epoch 13 batch 700 train loss: 0.0009 test loss: 0.0079\n",
      "Epoch 13 batch 800 train loss: 0.0021 test loss: 0.0080\n",
      "Epoch 13 batch 900 train loss: 0.0031 test loss: 0.0079\n",
      "Epoch 13 batch 1000 train loss: 0.0066 test loss: 0.0079\n",
      "Epoch 13 batch 1100 train loss: 0.0011 test loss: 0.0081\n",
      "Epoch 13 batch 1200 train loss: 0.0084 test loss: 0.0080\n",
      "Epoch 13 batch 1300 train loss: 0.0052 test loss: 0.0080\n",
      "Epoch 13 batch 1400 train loss: 0.0025 test loss: 0.0080\n",
      "Epoch 13 batch 1500 train loss: 0.0055 test loss: 0.0079\n",
      "Epoch 13 batch 1600 train loss: 0.0024 test loss: 0.0080\n",
      "Epoch 13 batch 1700 train loss: 0.0039 test loss: 0.0079\n",
      "Epoch 13 batch 1800 train loss: 0.0043 test loss: 0.0079\n",
      "Epoch 13 batch 1900 train loss: 0.0004 test loss: 0.0079\n",
      "Epoch 13 batch 2000 train loss: 0.0006 test loss: 0.0080\n",
      "Epoch 13 batch 2100 train loss: 0.0085 test loss: 0.0079\n",
      "Epoch 13 batch 2200 train loss: 0.0022 test loss: 0.0080\n",
      "Epoch 13 batch 2300 train loss: 0.0056 test loss: 0.0080\n",
      "Epoch 13 batch 2400 train loss: 0.0025 test loss: 0.0079\n",
      "Epoch 13 batch 2500 train loss: 0.0059 test loss: 0.0079\n",
      "Epoch 13 batch 2600 train loss: 0.0024 test loss: 0.0079\n",
      "Epoch 13 batch 2700 train loss: 0.0050 test loss: 0.0080\n",
      "Epoch 13 batch 2800 train loss: 0.0069 test loss: 0.0080\n",
      "Epoch 13 batch 2900 train loss: 0.0008 test loss: 0.0080\n",
      "Epoch 14 batch 0 train loss: 0.0036 test loss: 0.0080\n",
      "Epoch 14 batch 100 train loss: 0.0071 test loss: 0.0079\n",
      "Epoch 14 batch 200 train loss: 0.0062 test loss: 0.0081\n",
      "Epoch 14 batch 300 train loss: 0.0039 test loss: 0.0080\n",
      "Epoch 14 batch 400 train loss: 0.0091 test loss: 0.0079\n",
      "Epoch 14 batch 500 train loss: 0.0008 test loss: 0.0080\n",
      "Epoch 14 batch 600 train loss: 0.0070 test loss: 0.0079\n",
      "Epoch 14 batch 700 train loss: 0.0035 test loss: 0.0080\n",
      "Epoch 14 batch 800 train loss: 0.0054 test loss: 0.0080\n",
      "Epoch 14 batch 900 train loss: 0.0061 test loss: 0.0080\n",
      "Epoch 14 batch 1000 train loss: 0.0026 test loss: 0.0080\n",
      "Epoch 14 batch 1100 train loss: 0.0022 test loss: 0.0081\n",
      "Epoch 14 batch 1200 train loss: 0.0008 test loss: 0.0079\n",
      "Epoch 14 batch 1300 train loss: 0.0027 test loss: 0.0081\n",
      "Epoch 14 batch 1400 train loss: 0.0053 test loss: 0.0080\n",
      "Epoch 14 batch 1500 train loss: 0.0048 test loss: 0.0080\n",
      "Epoch 14 batch 1600 train loss: 0.0052 test loss: 0.0079\n",
      "Epoch 14 batch 1700 train loss: 0.0009 test loss: 0.0080\n",
      "Epoch 14 batch 1800 train loss: 0.0061 test loss: 0.0081\n",
      "Epoch 14 batch 1900 train loss: 0.0029 test loss: 0.0080\n",
      "Epoch 14 batch 2000 train loss: 0.0008 test loss: 0.0080\n",
      "Epoch 14 batch 2100 train loss: 0.0011 test loss: 0.0080\n",
      "Epoch 14 batch 2200 train loss: 0.0036 test loss: 0.0079\n",
      "Epoch 14 batch 2300 train loss: 0.0027 test loss: 0.0079\n",
      "Epoch 14 batch 2400 train loss: 0.0016 test loss: 0.0079\n",
      "Epoch 14 batch 2500 train loss: 0.0070 test loss: 0.0079\n",
      "Epoch 14 batch 2600 train loss: 0.0049 test loss: 0.0079\n",
      "Epoch 14 batch 2700 train loss: 0.0031 test loss: 0.0079\n",
      "Epoch 14 batch 2800 train loss: 0.0038 test loss: 0.0079\n",
      "Epoch 14 batch 2900 train loss: 0.0015 test loss: 0.0079\n",
      "Epoch 15 batch 0 train loss: 0.0060 test loss: 0.0079\n",
      "Epoch 15 batch 100 train loss: 0.0020 test loss: 0.0080\n",
      "Epoch 15 batch 200 train loss: 0.0022 test loss: 0.0080\n",
      "Epoch 15 batch 300 train loss: 0.0002 test loss: 0.0079\n",
      "Epoch 15 batch 400 train loss: 0.0002 test loss: 0.0079\n",
      "Epoch 15 batch 500 train loss: 0.0054 test loss: 0.0080\n",
      "Epoch 15 batch 600 train loss: 0.0028 test loss: 0.0080\n",
      "Epoch 15 batch 700 train loss: 0.0047 test loss: 0.0080\n",
      "Epoch 15 batch 800 train loss: 0.0013 test loss: 0.0079\n",
      "Epoch 15 batch 900 train loss: 0.0043 test loss: 0.0079\n",
      "Epoch 15 batch 1000 train loss: 0.0113 test loss: 0.0080\n",
      "Epoch 15 batch 1100 train loss: 0.0049 test loss: 0.0081\n",
      "Epoch 15 batch 1200 train loss: 0.0031 test loss: 0.0080\n",
      "Epoch 15 batch 1300 train loss: 0.0049 test loss: 0.0080\n",
      "Epoch 15 batch 1400 train loss: 0.0031 test loss: 0.0080\n",
      "Epoch 15 batch 1500 train loss: 0.0037 test loss: 0.0079\n",
      "Epoch 15 batch 1600 train loss: 0.0011 test loss: 0.0080\n",
      "Epoch 15 batch 1700 train loss: 0.0031 test loss: 0.0079\n",
      "Epoch 15 batch 1800 train loss: 0.0054 test loss: 0.0079\n",
      "Epoch 15 batch 1900 train loss: 0.0026 test loss: 0.0080\n",
      "Epoch 15 batch 2000 train loss: 0.0006 test loss: 0.0079\n",
      "Epoch 15 batch 2100 train loss: 0.0032 test loss: 0.0079\n",
      "Epoch 15 batch 2200 train loss: 0.0018 test loss: 0.0079\n",
      "Epoch 15 batch 2300 train loss: 0.0050 test loss: 0.0079\n",
      "Epoch 15 batch 2400 train loss: 0.0003 test loss: 0.0081\n",
      "Epoch 15 batch 2500 train loss: 0.0064 test loss: 0.0079\n",
      "Epoch 15 batch 2600 train loss: 0.0099 test loss: 0.0079\n",
      "Epoch 15 batch 2700 train loss: 0.0092 test loss: 0.0080\n",
      "Epoch 15 batch 2800 train loss: 0.0032 test loss: 0.0081\n",
      "Epoch 15 batch 2900 train loss: 0.0033 test loss: 0.0080\n",
      "Epoch 16 batch 0 train loss: 0.0034 test loss: 0.0078\n",
      "Epoch 16 batch 100 train loss: 0.0018 test loss: 0.0079\n",
      "Epoch 16 batch 200 train loss: 0.0047 test loss: 0.0080\n",
      "Epoch 16 batch 300 train loss: 0.0045 test loss: 0.0079\n",
      "Epoch 16 batch 400 train loss: 0.0023 test loss: 0.0079\n",
      "Epoch 16 batch 500 train loss: 0.0094 test loss: 0.0081\n",
      "Epoch 16 batch 600 train loss: 0.0006 test loss: 0.0080\n",
      "Epoch 16 batch 700 train loss: 0.0009 test loss: 0.0080\n",
      "Epoch 16 batch 800 train loss: 0.0021 test loss: 0.0080\n",
      "Epoch 16 batch 900 train loss: 0.0003 test loss: 0.0080\n",
      "Epoch 16 batch 1000 train loss: 0.0065 test loss: 0.0080\n",
      "Epoch 16 batch 1100 train loss: 0.0064 test loss: 0.0080\n",
      "Epoch 16 batch 1200 train loss: 0.0051 test loss: 0.0080\n",
      "Epoch 16 batch 1300 train loss: 0.0067 test loss: 0.0080\n",
      "Epoch 16 batch 1400 train loss: 0.0003 test loss: 0.0079\n",
      "Epoch 16 batch 1500 train loss: 0.0032 test loss: 0.0079\n",
      "Epoch 16 batch 1600 train loss: 0.0070 test loss: 0.0079\n",
      "Epoch 16 batch 1700 train loss: 0.0010 test loss: 0.0079\n",
      "Epoch 16 batch 1800 train loss: 0.0044 test loss: 0.0080\n",
      "Epoch 16 batch 1900 train loss: 0.0031 test loss: 0.0081\n",
      "Epoch 16 batch 2000 train loss: 0.0048 test loss: 0.0081\n",
      "Epoch 16 batch 2100 train loss: 0.0035 test loss: 0.0079\n",
      "Epoch 16 batch 2200 train loss: 0.0013 test loss: 0.0079\n",
      "Epoch 16 batch 2300 train loss: 0.0046 test loss: 0.0079\n",
      "Epoch 16 batch 2400 train loss: 0.0052 test loss: 0.0079\n",
      "Epoch 16 batch 2500 train loss: 0.0061 test loss: 0.0080\n",
      "Epoch 16 batch 2600 train loss: 0.0012 test loss: 0.0079\n",
      "Epoch 16 batch 2700 train loss: 0.0003 test loss: 0.0080\n",
      "Epoch 16 batch 2800 train loss: 0.0023 test loss: 0.0080\n",
      "Epoch 16 batch 2900 train loss: 0.0034 test loss: 0.0080\n",
      "Epoch 17 batch 0 train loss: 0.0021 test loss: 0.0079\n",
      "Epoch 17 batch 100 train loss: 0.0008 test loss: 0.0080\n",
      "Epoch 17 batch 200 train loss: 0.0053 test loss: 0.0079\n",
      "Epoch 17 batch 300 train loss: 0.0042 test loss: 0.0079\n",
      "Epoch 17 batch 400 train loss: 0.0014 test loss: 0.0080\n",
      "Epoch 17 batch 500 train loss: 0.0052 test loss: 0.0081\n",
      "Epoch 17 batch 600 train loss: 0.0029 test loss: 0.0080\n",
      "Epoch 17 batch 700 train loss: 0.0055 test loss: 0.0080\n",
      "Epoch 17 batch 800 train loss: 0.0005 test loss: 0.0080\n",
      "early stop.\n",
      "Checkpoint 19 restored!!\n",
      "2446/2446 [==============================] - 18s 7ms/step - loss: 0.0053\n",
      "Training for loss rate 0.50 start.\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.50p/ckpt-1\n",
      "Epoch 0 batch 0 train loss: 0.1125 test loss: 0.1581\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.50p/ckpt-2\n",
      "Epoch 0 batch 100 train loss: 0.0119 test loss: 0.0180\n",
      "Epoch 0 batch 200 train loss: 0.0120 test loss: 0.0186\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.50p/ckpt-3\n",
      "Epoch 0 batch 300 train loss: 0.0111 test loss: 0.0154\n",
      "Epoch 0 batch 400 train loss: 0.0113 test loss: 0.0164\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.50p/ckpt-4\n",
      "Epoch 0 batch 500 train loss: 0.0066 test loss: 0.0148\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.50p/ckpt-5\n",
      "Epoch 0 batch 600 train loss: 0.0088 test loss: 0.0139\n",
      "Epoch 0 batch 700 train loss: 0.0060 test loss: 0.0143\n",
      "Epoch 0 batch 800 train loss: 0.0139 test loss: 0.0140\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.50p/ckpt-6\n",
      "Epoch 0 batch 900 train loss: 0.0084 test loss: 0.0137\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.50p/ckpt-7\n",
      "Epoch 0 batch 1000 train loss: 0.0085 test loss: 0.0132\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.50p/ckpt-8\n",
      "Epoch 0 batch 1100 train loss: 0.0065 test loss: 0.0128\n",
      "Epoch 0 batch 1200 train loss: 0.0141 test loss: 0.0133\n",
      "Epoch 0 batch 1300 train loss: 0.0076 test loss: 0.0134\n",
      "Epoch 0 batch 1400 train loss: 0.0051 test loss: 0.0131\n",
      "Epoch 0 batch 1500 train loss: 0.0089 test loss: 0.0128\n",
      "Epoch 0 batch 1600 train loss: 0.0111 test loss: 0.0132\n",
      "Epoch 0 batch 1700 train loss: 0.0100 test loss: 0.0133\n",
      "Epoch 0 batch 1800 train loss: 0.0108 test loss: 0.0131\n",
      "Epoch 0 batch 1900 train loss: 0.0090 test loss: 0.0129\n",
      "Epoch 0 batch 2000 train loss: 0.0064 test loss: 0.0132\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.50p/ckpt-9\n",
      "Epoch 0 batch 2100 train loss: 0.0114 test loss: 0.0126\n",
      "Epoch 0 batch 2200 train loss: 0.0095 test loss: 0.0138\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.50p/ckpt-10\n",
      "Epoch 0 batch 2300 train loss: 0.0102 test loss: 0.0125\n",
      "Epoch 0 batch 2400 train loss: 0.0132 test loss: 0.0130\n",
      "Epoch 0 batch 2500 train loss: 0.0055 test loss: 0.0132\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.50p/ckpt-11\n",
      "Epoch 0 batch 2600 train loss: 0.0109 test loss: 0.0123\n",
      "Epoch 0 batch 2700 train loss: 0.0125 test loss: 0.0130\n",
      "Epoch 0 batch 2800 train loss: 0.0118 test loss: 0.0129\n",
      "Epoch 0 batch 2900 train loss: 0.0108 test loss: 0.0130\n",
      "Epoch 1 batch 0 train loss: 0.0092 test loss: 0.0129\n",
      "Epoch 1 batch 100 train loss: 0.0091 test loss: 0.0128\n",
      "Epoch 1 batch 200 train loss: 0.0087 test loss: 0.0124\n",
      "Epoch 1 batch 300 train loss: 0.0114 test loss: 0.0129\n",
      "Epoch 1 batch 400 train loss: 0.0062 test loss: 0.0128\n",
      "Epoch 1 batch 500 train loss: 0.0100 test loss: 0.0127\n",
      "Epoch 1 batch 600 train loss: 0.0093 test loss: 0.0126\n",
      "Epoch 1 batch 700 train loss: 0.0104 test loss: 0.0128\n",
      "Epoch 1 batch 800 train loss: 0.0106 test loss: 0.0126\n",
      "Epoch 1 batch 900 train loss: 0.0122 test loss: 0.0131\n",
      "Epoch 1 batch 1000 train loss: 0.0061 test loss: 0.0128\n",
      "Epoch 1 batch 1100 train loss: 0.0053 test loss: 0.0126\n",
      "Epoch 1 batch 1200 train loss: 0.0066 test loss: 0.0126\n",
      "Epoch 1 batch 1300 train loss: 0.0090 test loss: 0.0127\n",
      "Epoch 1 batch 1400 train loss: 0.0072 test loss: 0.0126\n",
      "Epoch 1 batch 1500 train loss: 0.0059 test loss: 0.0125\n",
      "Epoch 1 batch 1600 train loss: 0.0126 test loss: 0.0126\n",
      "Epoch 1 batch 1700 train loss: 0.0093 test loss: 0.0125\n",
      "Epoch 1 batch 1800 train loss: 0.0122 test loss: 0.0128\n",
      "Epoch 1 batch 1900 train loss: 0.0134 test loss: 0.0127\n",
      "Epoch 1 batch 2000 train loss: 0.0051 test loss: 0.0125\n",
      "Epoch 1 batch 2100 train loss: 0.0078 test loss: 0.0126\n",
      "Epoch 1 batch 2200 train loss: 0.0094 test loss: 0.0126\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.50p/ckpt-12\n",
      "Epoch 1 batch 2300 train loss: 0.0064 test loss: 0.0122\n",
      "Epoch 1 batch 2400 train loss: 0.0089 test loss: 0.0128\n",
      "Epoch 1 batch 2500 train loss: 0.0019 test loss: 0.0126\n",
      "Epoch 1 batch 2600 train loss: 0.0063 test loss: 0.0126\n",
      "Epoch 1 batch 2700 train loss: 0.0045 test loss: 0.0126\n",
      "Epoch 1 batch 2800 train loss: 0.0103 test loss: 0.0125\n",
      "Epoch 1 batch 2900 train loss: 0.0144 test loss: 0.0127\n",
      "Epoch 2 batch 0 train loss: 0.0059 test loss: 0.0123\n",
      "Epoch 2 batch 100 train loss: 0.0080 test loss: 0.0127\n",
      "Epoch 2 batch 200 train loss: 0.0114 test loss: 0.0124\n",
      "Epoch 2 batch 300 train loss: 0.0080 test loss: 0.0122\n",
      "Epoch 2 batch 400 train loss: 0.0087 test loss: 0.0123\n",
      "Epoch 2 batch 500 train loss: 0.0087 test loss: 0.0123\n",
      "Epoch 2 batch 600 train loss: 0.0121 test loss: 0.0129\n",
      "Epoch 2 batch 700 train loss: 0.0139 test loss: 0.0130\n",
      "Epoch 2 batch 800 train loss: 0.0056 test loss: 0.0125\n",
      "Epoch 2 batch 900 train loss: 0.0023 test loss: 0.0125\n",
      "Epoch 2 batch 1000 train loss: 0.0058 test loss: 0.0123\n",
      "Epoch 2 batch 1100 train loss: 0.0094 test loss: 0.0127\n",
      "Epoch 2 batch 1200 train loss: 0.0070 test loss: 0.0124\n",
      "Epoch 2 batch 1300 train loss: 0.0074 test loss: 0.0127\n",
      "Epoch 2 batch 1400 train loss: 0.0039 test loss: 0.0127\n",
      "Epoch 2 batch 1500 train loss: 0.0091 test loss: 0.0128\n",
      "Epoch 2 batch 1600 train loss: 0.0060 test loss: 0.0123\n",
      "Epoch 2 batch 1700 train loss: 0.0040 test loss: 0.0125\n",
      "Epoch 2 batch 1800 train loss: 0.0170 test loss: 0.0128\n",
      "Epoch 2 batch 1900 train loss: 0.0151 test loss: 0.0126\n",
      "Epoch 2 batch 2000 train loss: 0.0031 test loss: 0.0126\n",
      "Epoch 2 batch 2100 train loss: 0.0100 test loss: 0.0124\n",
      "Epoch 2 batch 2200 train loss: 0.0086 test loss: 0.0123\n",
      "Epoch 2 batch 2300 train loss: 0.0044 test loss: 0.0126\n",
      "Epoch 2 batch 2400 train loss: 0.0078 test loss: 0.0123\n",
      "Epoch 2 batch 2500 train loss: 0.0087 test loss: 0.0123\n",
      "Epoch 2 batch 2600 train loss: 0.0077 test loss: 0.0123\n",
      "Epoch 2 batch 2700 train loss: 0.0083 test loss: 0.0123\n",
      "Epoch 2 batch 2800 train loss: 0.0061 test loss: 0.0126\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.50p/ckpt-13\n",
      "Epoch 2 batch 2900 train loss: 0.0117 test loss: 0.0121\n",
      "Epoch 3 batch 0 train loss: 0.0062 test loss: 0.0125\n",
      "Epoch 3 batch 100 train loss: 0.0087 test loss: 0.0123\n",
      "Epoch 3 batch 200 train loss: 0.0097 test loss: 0.0126\n",
      "Epoch 3 batch 300 train loss: 0.0019 test loss: 0.0125\n",
      "Epoch 3 batch 400 train loss: 0.0060 test loss: 0.0125\n",
      "Epoch 3 batch 500 train loss: 0.0022 test loss: 0.0124\n",
      "Epoch 3 batch 600 train loss: 0.0133 test loss: 0.0123\n",
      "Epoch 3 batch 700 train loss: 0.0121 test loss: 0.0129\n",
      "Epoch 3 batch 800 train loss: 0.0034 test loss: 0.0125\n",
      "Epoch 3 batch 900 train loss: 0.0070 test loss: 0.0125\n",
      "Epoch 3 batch 1000 train loss: 0.0084 test loss: 0.0121\n",
      "Epoch 3 batch 1100 train loss: 0.0113 test loss: 0.0124\n",
      "Epoch 3 batch 1200 train loss: 0.0110 test loss: 0.0123\n",
      "Epoch 3 batch 1300 train loss: 0.0125 test loss: 0.0125\n",
      "Epoch 3 batch 1400 train loss: 0.0038 test loss: 0.0122\n",
      "Epoch 3 batch 1500 train loss: 0.0107 test loss: 0.0123\n",
      "Epoch 3 batch 1600 train loss: 0.0143 test loss: 0.0124\n",
      "Epoch 3 batch 1700 train loss: 0.0102 test loss: 0.0128\n",
      "Epoch 3 batch 1800 train loss: 0.0047 test loss: 0.0124\n",
      "Epoch 3 batch 1900 train loss: 0.0071 test loss: 0.0125\n",
      "Epoch 3 batch 2000 train loss: 0.0046 test loss: 0.0125\n",
      "Epoch 3 batch 2100 train loss: 0.0053 test loss: 0.0127\n",
      "Epoch 3 batch 2200 train loss: 0.0075 test loss: 0.0124\n",
      "Epoch 3 batch 2300 train loss: 0.0060 test loss: 0.0126\n",
      "Epoch 3 batch 2400 train loss: 0.0057 test loss: 0.0124\n",
      "Epoch 3 batch 2500 train loss: 0.0094 test loss: 0.0123\n",
      "Epoch 3 batch 2600 train loss: 0.0075 test loss: 0.0123\n",
      "Epoch 3 batch 2700 train loss: 0.0099 test loss: 0.0124\n",
      "Epoch 3 batch 2800 train loss: 0.0103 test loss: 0.0122\n",
      "Epoch 3 batch 2900 train loss: 0.0092 test loss: 0.0123\n",
      "Epoch 4 batch 0 train loss: 0.0042 test loss: 0.0122\n",
      "Epoch 4 batch 100 train loss: 0.0086 test loss: 0.0123\n",
      "Epoch 4 batch 200 train loss: 0.0113 test loss: 0.0123\n",
      "Epoch 4 batch 300 train loss: 0.0031 test loss: 0.0122\n",
      "Epoch 4 batch 400 train loss: 0.0089 test loss: 0.0122\n",
      "Epoch 4 batch 500 train loss: 0.0059 test loss: 0.0125\n",
      "Epoch 4 batch 600 train loss: 0.0037 test loss: 0.0121\n",
      "Epoch 4 batch 700 train loss: 0.0049 test loss: 0.0124\n",
      "Epoch 4 batch 800 train loss: 0.0026 test loss: 0.0123\n",
      "Epoch 4 batch 900 train loss: 0.0069 test loss: 0.0122\n",
      "Epoch 4 batch 1000 train loss: 0.0109 test loss: 0.0123\n",
      "Epoch 4 batch 1100 train loss: 0.0099 test loss: 0.0124\n",
      "Epoch 4 batch 1200 train loss: 0.0103 test loss: 0.0125\n",
      "Epoch 4 batch 1300 train loss: 0.0063 test loss: 0.0122\n",
      "Epoch 4 batch 1400 train loss: 0.0087 test loss: 0.0124\n",
      "Epoch 4 batch 1500 train loss: 0.0089 test loss: 0.0126\n",
      "Epoch 4 batch 1600 train loss: 0.0079 test loss: 0.0121\n",
      "Epoch 4 batch 1700 train loss: 0.0067 test loss: 0.0126\n",
      "Epoch 4 batch 1800 train loss: 0.0078 test loss: 0.0126\n",
      "Epoch 4 batch 1900 train loss: 0.0034 test loss: 0.0124\n",
      "Epoch 4 batch 2000 train loss: 0.0069 test loss: 0.0125\n",
      "Epoch 4 batch 2100 train loss: 0.0055 test loss: 0.0123\n",
      "Epoch 4 batch 2200 train loss: 0.0064 test loss: 0.0123\n",
      "Epoch 4 batch 2300 train loss: 0.0066 test loss: 0.0124\n",
      "Epoch 4 batch 2400 train loss: 0.0024 test loss: 0.0127\n",
      "Epoch 4 batch 2500 train loss: 0.0095 test loss: 0.0123\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.50p/ckpt-14\n",
      "Epoch 4 batch 2600 train loss: 0.0122 test loss: 0.0121\n",
      "Epoch 4 batch 2700 train loss: 0.0091 test loss: 0.0124\n",
      "Epoch 4 batch 2800 train loss: 0.0103 test loss: 0.0124\n",
      "Epoch 4 batch 2900 train loss: 0.0073 test loss: 0.0124\n",
      "Epoch 5 batch 0 train loss: 0.0066 test loss: 0.0122\n",
      "Epoch 5 batch 100 train loss: 0.0046 test loss: 0.0123\n",
      "Epoch 5 batch 200 train loss: 0.0101 test loss: 0.0123\n",
      "Epoch 5 batch 300 train loss: 0.0070 test loss: 0.0122\n",
      "Epoch 5 batch 400 train loss: 0.0028 test loss: 0.0128\n",
      "Epoch 5 batch 500 train loss: 0.0103 test loss: 0.0124\n",
      "Epoch 5 batch 600 train loss: 0.0083 test loss: 0.0124\n",
      "Epoch 5 batch 700 train loss: 0.0102 test loss: 0.0123\n",
      "Epoch 5 batch 800 train loss: 0.0065 test loss: 0.0124\n",
      "Epoch 5 batch 900 train loss: 0.0085 test loss: 0.0124\n",
      "Epoch 5 batch 1000 train loss: 0.0032 test loss: 0.0125\n",
      "Epoch 5 batch 1100 train loss: 0.0090 test loss: 0.0125\n",
      "Epoch 5 batch 1200 train loss: 0.0065 test loss: 0.0125\n",
      "Epoch 5 batch 1300 train loss: 0.0050 test loss: 0.0127\n",
      "Epoch 5 batch 1400 train loss: 0.0084 test loss: 0.0124\n",
      "Epoch 5 batch 1500 train loss: 0.0088 test loss: 0.0125\n",
      "Epoch 5 batch 1600 train loss: 0.0089 test loss: 0.0123\n",
      "Epoch 5 batch 1700 train loss: 0.0038 test loss: 0.0124\n",
      "Epoch 5 batch 1800 train loss: 0.0062 test loss: 0.0124\n",
      "Epoch 5 batch 1900 train loss: 0.0088 test loss: 0.0124\n",
      "Epoch 5 batch 2000 train loss: 0.0069 test loss: 0.0126\n",
      "Epoch 5 batch 2100 train loss: 0.0080 test loss: 0.0122\n",
      "Epoch 5 batch 2200 train loss: 0.0064 test loss: 0.0124\n",
      "Epoch 5 batch 2300 train loss: 0.0042 test loss: 0.0122\n",
      "Epoch 5 batch 2400 train loss: 0.0085 test loss: 0.0122\n",
      "Epoch 5 batch 2500 train loss: 0.0149 test loss: 0.0124\n",
      "Epoch 5 batch 2600 train loss: 0.0053 test loss: 0.0123\n",
      "Epoch 5 batch 2700 train loss: 0.0072 test loss: 0.0125\n",
      "Epoch 5 batch 2800 train loss: 0.0054 test loss: 0.0122\n",
      "Epoch 5 batch 2900 train loss: 0.0014 test loss: 0.0123\n",
      "Epoch 6 batch 0 train loss: 0.0074 test loss: 0.0122\n",
      "Epoch 6 batch 100 train loss: 0.0076 test loss: 0.0122\n",
      "Epoch 6 batch 200 train loss: 0.0121 test loss: 0.0123\n",
      "Epoch 6 batch 300 train loss: 0.0054 test loss: 0.0123\n",
      "Epoch 6 batch 400 train loss: 0.0029 test loss: 0.0124\n",
      "Epoch 6 batch 500 train loss: 0.0090 test loss: 0.0124\n",
      "Epoch 6 batch 600 train loss: 0.0028 test loss: 0.0121\n",
      "Epoch 6 batch 700 train loss: 0.0051 test loss: 0.0125\n",
      "Epoch 6 batch 800 train loss: 0.0064 test loss: 0.0125\n",
      "Epoch 6 batch 900 train loss: 0.0082 test loss: 0.0125\n",
      "Epoch 6 batch 1000 train loss: 0.0119 test loss: 0.0123\n",
      "Epoch 6 batch 1100 train loss: 0.0149 test loss: 0.0125\n",
      "Epoch 6 batch 1200 train loss: 0.0032 test loss: 0.0123\n",
      "Epoch 6 batch 1300 train loss: 0.0052 test loss: 0.0123\n",
      "Epoch 6 batch 1400 train loss: 0.0114 test loss: 0.0123\n",
      "Epoch 6 batch 1500 train loss: 0.0052 test loss: 0.0124\n",
      "Epoch 6 batch 1600 train loss: 0.0122 test loss: 0.0123\n",
      "Epoch 6 batch 1700 train loss: 0.0065 test loss: 0.0124\n",
      "Epoch 6 batch 1800 train loss: 0.0016 test loss: 0.0124\n",
      "Epoch 6 batch 1900 train loss: 0.0118 test loss: 0.0123\n",
      "Epoch 6 batch 2000 train loss: 0.0092 test loss: 0.0126\n",
      "Epoch 6 batch 2100 train loss: 0.0145 test loss: 0.0124\n",
      "Epoch 6 batch 2200 train loss: 0.0123 test loss: 0.0126\n",
      "Epoch 6 batch 2300 train loss: 0.0082 test loss: 0.0125\n",
      "Epoch 6 batch 2400 train loss: 0.0084 test loss: 0.0124\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.50p/ckpt-15\n",
      "Epoch 6 batch 2500 train loss: 0.0114 test loss: 0.0121\n",
      "Epoch 6 batch 2600 train loss: 0.0058 test loss: 0.0122\n",
      "Epoch 6 batch 2700 train loss: 0.0060 test loss: 0.0124\n",
      "Epoch 6 batch 2800 train loss: 0.0077 test loss: 0.0125\n",
      "Epoch 6 batch 2900 train loss: 0.0055 test loss: 0.0125\n",
      "Epoch 7 batch 0 train loss: 0.0046 test loss: 0.0124\n",
      "Epoch 7 batch 100 train loss: 0.0101 test loss: 0.0122\n",
      "Epoch 7 batch 200 train loss: 0.0053 test loss: 0.0125\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.50p/ckpt-16\n",
      "Epoch 7 batch 300 train loss: 0.0065 test loss: 0.0121\n",
      "Epoch 7 batch 400 train loss: 0.0060 test loss: 0.0122\n",
      "Epoch 7 batch 500 train loss: 0.0043 test loss: 0.0124\n",
      "Epoch 7 batch 600 train loss: 0.0073 test loss: 0.0123\n",
      "Epoch 7 batch 700 train loss: 0.0040 test loss: 0.0125\n",
      "Epoch 7 batch 800 train loss: 0.0122 test loss: 0.0123\n",
      "Epoch 7 batch 900 train loss: 0.0096 test loss: 0.0123\n",
      "Epoch 7 batch 1000 train loss: 0.0059 test loss: 0.0123\n",
      "Epoch 7 batch 1100 train loss: 0.0162 test loss: 0.0124\n",
      "Epoch 7 batch 1200 train loss: 0.0101 test loss: 0.0123\n",
      "Epoch 7 batch 1300 train loss: 0.0024 test loss: 0.0123\n",
      "Epoch 7 batch 1400 train loss: 0.0007 test loss: 0.0125\n",
      "Epoch 7 batch 1500 train loss: 0.0056 test loss: 0.0124\n",
      "Epoch 7 batch 1600 train loss: 0.0121 test loss: 0.0123\n",
      "Epoch 7 batch 1700 train loss: 0.0045 test loss: 0.0126\n",
      "Epoch 7 batch 1800 train loss: 0.0101 test loss: 0.0124\n",
      "Epoch 7 batch 1900 train loss: 0.0105 test loss: 0.0125\n",
      "Epoch 7 batch 2000 train loss: 0.0095 test loss: 0.0125\n",
      "Epoch 7 batch 2100 train loss: 0.0113 test loss: 0.0121\n",
      "Epoch 7 batch 2200 train loss: 0.0114 test loss: 0.0123\n",
      "Epoch 7 batch 2300 train loss: 0.0099 test loss: 0.0123\n",
      "Epoch 7 batch 2400 train loss: 0.0128 test loss: 0.0124\n",
      "Epoch 7 batch 2500 train loss: 0.0045 test loss: 0.0123\n",
      "Epoch 7 batch 2600 train loss: 0.0054 test loss: 0.0123\n",
      "Epoch 7 batch 2700 train loss: 0.0056 test loss: 0.0122\n",
      "Epoch 7 batch 2800 train loss: 0.0045 test loss: 0.0122\n",
      "Epoch 7 batch 2900 train loss: 0.0070 test loss: 0.0124\n",
      "Epoch 8 batch 0 train loss: 0.0100 test loss: 0.0121\n",
      "Epoch 8 batch 100 train loss: 0.0030 test loss: 0.0123\n",
      "Epoch 8 batch 200 train loss: 0.0084 test loss: 0.0122\n",
      "Epoch 8 batch 300 train loss: 0.0104 test loss: 0.0123\n",
      "Epoch 8 batch 400 train loss: 0.0035 test loss: 0.0124\n",
      "Epoch 8 batch 500 train loss: 0.0035 test loss: 0.0122\n",
      "Epoch 8 batch 600 train loss: 0.0073 test loss: 0.0124\n",
      "Epoch 8 batch 700 train loss: 0.0062 test loss: 0.0124\n",
      "Epoch 8 batch 800 train loss: 0.0041 test loss: 0.0124\n",
      "Epoch 8 batch 900 train loss: 0.0088 test loss: 0.0123\n",
      "Epoch 8 batch 1000 train loss: 0.0100 test loss: 0.0123\n",
      "Epoch 8 batch 1100 train loss: 0.0111 test loss: 0.0123\n",
      "Epoch 8 batch 1200 train loss: 0.0048 test loss: 0.0128\n",
      "Epoch 8 batch 1300 train loss: 0.0080 test loss: 0.0127\n",
      "Epoch 8 batch 1400 train loss: 0.0021 test loss: 0.0125\n",
      "Epoch 8 batch 1500 train loss: 0.0049 test loss: 0.0124\n",
      "Epoch 8 batch 1600 train loss: 0.0158 test loss: 0.0123\n",
      "Epoch 8 batch 1700 train loss: 0.0096 test loss: 0.0125\n",
      "Epoch 8 batch 1800 train loss: 0.0062 test loss: 0.0124\n",
      "Epoch 8 batch 1900 train loss: 0.0086 test loss: 0.0122\n",
      "Epoch 8 batch 2000 train loss: 0.0111 test loss: 0.0126\n",
      "Epoch 8 batch 2100 train loss: 0.0045 test loss: 0.0122\n",
      "Epoch 8 batch 2200 train loss: 0.0046 test loss: 0.0123\n",
      "Epoch 8 batch 2300 train loss: 0.0034 test loss: 0.0124\n",
      "Epoch 8 batch 2400 train loss: 0.0055 test loss: 0.0121\n",
      "Epoch 8 batch 2500 train loss: 0.0071 test loss: 0.0123\n",
      "Epoch 8 batch 2600 train loss: 0.0089 test loss: 0.0123\n",
      "Epoch 8 batch 2700 train loss: 0.0022 test loss: 0.0124\n",
      "Epoch 8 batch 2800 train loss: 0.0088 test loss: 0.0126\n",
      "Epoch 8 batch 2900 train loss: 0.0090 test loss: 0.0122\n",
      "Epoch 9 batch 0 train loss: 0.0033 test loss: 0.0122\n",
      "Epoch 9 batch 100 train loss: 0.0083 test loss: 0.0124\n",
      "Epoch 9 batch 200 train loss: 0.0089 test loss: 0.0123\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.50p/ckpt-17\n",
      "Epoch 9 batch 300 train loss: 0.0067 test loss: 0.0120\n",
      "Epoch 9 batch 400 train loss: 0.0027 test loss: 0.0122\n",
      "Epoch 9 batch 500 train loss: 0.0104 test loss: 0.0123\n",
      "Epoch 9 batch 600 train loss: 0.0211 test loss: 0.0125\n",
      "Epoch 9 batch 700 train loss: 0.0061 test loss: 0.0123\n",
      "Epoch 9 batch 800 train loss: 0.0106 test loss: 0.0125\n",
      "Epoch 9 batch 900 train loss: 0.0056 test loss: 0.0123\n",
      "Epoch 9 batch 1000 train loss: 0.0049 test loss: 0.0122\n",
      "Epoch 9 batch 1100 train loss: 0.0076 test loss: 0.0125\n",
      "Epoch 9 batch 1200 train loss: 0.0090 test loss: 0.0122\n",
      "Epoch 9 batch 1300 train loss: 0.0057 test loss: 0.0124\n",
      "Epoch 9 batch 1400 train loss: 0.0089 test loss: 0.0123\n",
      "Epoch 9 batch 1500 train loss: 0.0059 test loss: 0.0124\n",
      "Epoch 9 batch 1600 train loss: 0.0080 test loss: 0.0124\n",
      "Epoch 9 batch 1700 train loss: 0.0076 test loss: 0.0124\n",
      "Epoch 9 batch 1800 train loss: 0.0044 test loss: 0.0123\n",
      "Epoch 9 batch 1900 train loss: 0.0070 test loss: 0.0125\n",
      "Epoch 9 batch 2000 train loss: 0.0055 test loss: 0.0124\n",
      "Epoch 9 batch 2100 train loss: 0.0095 test loss: 0.0123\n",
      "Epoch 9 batch 2200 train loss: 0.0069 test loss: 0.0122\n",
      "Epoch 9 batch 2300 train loss: 0.0060 test loss: 0.0123\n",
      "Epoch 9 batch 2400 train loss: 0.0110 test loss: 0.0123\n",
      "Epoch 9 batch 2500 train loss: 0.0140 test loss: 0.0123\n",
      "Epoch 9 batch 2600 train loss: 0.0040 test loss: 0.0123\n",
      "Epoch 9 batch 2700 train loss: 0.0131 test loss: 0.0124\n",
      "Epoch 9 batch 2800 train loss: 0.0012 test loss: 0.0124\n",
      "Epoch 9 batch 2900 train loss: 0.0058 test loss: 0.0123\n",
      "Epoch 10 batch 0 train loss: 0.0085 test loss: 0.0123\n",
      "Epoch 10 batch 100 train loss: 0.0091 test loss: 0.0123\n",
      "Epoch 10 batch 200 train loss: 0.0043 test loss: 0.0122\n",
      "Epoch 10 batch 300 train loss: 0.0057 test loss: 0.0122\n",
      "Epoch 10 batch 400 train loss: 0.0121 test loss: 0.0123\n",
      "Epoch 10 batch 500 train loss: 0.0078 test loss: 0.0123\n",
      "Epoch 10 batch 600 train loss: 0.0076 test loss: 0.0123\n",
      "Epoch 10 batch 700 train loss: 0.0066 test loss: 0.0124\n",
      "Epoch 10 batch 800 train loss: 0.0072 test loss: 0.0124\n",
      "Epoch 10 batch 900 train loss: 0.0139 test loss: 0.0122\n",
      "Epoch 10 batch 1000 train loss: 0.0087 test loss: 0.0124\n",
      "Epoch 10 batch 1100 train loss: 0.0019 test loss: 0.0122\n",
      "Epoch 10 batch 1200 train loss: 0.0107 test loss: 0.0123\n",
      "Epoch 10 batch 1300 train loss: 0.0081 test loss: 0.0125\n",
      "Epoch 10 batch 1400 train loss: 0.0126 test loss: 0.0125\n",
      "Epoch 10 batch 1500 train loss: 0.0080 test loss: 0.0124\n",
      "Epoch 10 batch 1600 train loss: 0.0070 test loss: 0.0121\n",
      "Epoch 10 batch 1700 train loss: 0.0086 test loss: 0.0124\n",
      "Epoch 10 batch 1800 train loss: 0.0070 test loss: 0.0122\n",
      "Epoch 10 batch 1900 train loss: 0.0062 test loss: 0.0123\n",
      "Epoch 10 batch 2000 train loss: 0.0086 test loss: 0.0123\n",
      "Epoch 10 batch 2100 train loss: 0.0053 test loss: 0.0122\n",
      "Epoch 10 batch 2200 train loss: 0.0073 test loss: 0.0124\n",
      "Epoch 10 batch 2300 train loss: 0.0108 test loss: 0.0124\n",
      "Epoch 10 batch 2400 train loss: 0.0071 test loss: 0.0124\n",
      "Epoch 10 batch 2500 train loss: 0.0120 test loss: 0.0122\n",
      "Epoch 10 batch 2600 train loss: 0.0052 test loss: 0.0124\n",
      "Epoch 10 batch 2700 train loss: 0.0082 test loss: 0.0124\n",
      "Epoch 10 batch 2800 train loss: 0.0041 test loss: 0.0125\n",
      "Epoch 10 batch 2900 train loss: 0.0065 test loss: 0.0123\n",
      "Epoch 11 batch 0 train loss: 0.0037 test loss: 0.0121\n",
      "Epoch 11 batch 100 train loss: 0.0050 test loss: 0.0121\n",
      "Epoch 11 batch 200 train loss: 0.0070 test loss: 0.0122\n",
      "Epoch 11 batch 300 train loss: 0.0101 test loss: 0.0122\n",
      "Epoch 11 batch 400 train loss: 0.0073 test loss: 0.0123\n",
      "Epoch 11 batch 500 train loss: 0.0062 test loss: 0.0123\n",
      "Epoch 11 batch 600 train loss: 0.0045 test loss: 0.0123\n",
      "Epoch 11 batch 700 train loss: 0.0049 test loss: 0.0125\n",
      "Epoch 11 batch 800 train loss: 0.0028 test loss: 0.0123\n",
      "Epoch 11 batch 900 train loss: 0.0052 test loss: 0.0124\n",
      "Epoch 11 batch 1000 train loss: 0.0022 test loss: 0.0125\n",
      "Epoch 11 batch 1100 train loss: 0.0055 test loss: 0.0123\n",
      "Epoch 11 batch 1200 train loss: 0.0082 test loss: 0.0122\n",
      "Epoch 11 batch 1300 train loss: 0.0083 test loss: 0.0126\n",
      "Epoch 11 batch 1400 train loss: 0.0091 test loss: 0.0122\n",
      "Epoch 11 batch 1500 train loss: 0.0034 test loss: 0.0122\n",
      "Epoch 11 batch 1600 train loss: 0.0132 test loss: 0.0122\n",
      "Epoch 11 batch 1700 train loss: 0.0029 test loss: 0.0126\n",
      "Epoch 11 batch 1800 train loss: 0.0080 test loss: 0.0125\n",
      "Epoch 11 batch 1900 train loss: 0.0035 test loss: 0.0123\n",
      "Epoch 11 batch 2000 train loss: 0.0072 test loss: 0.0124\n",
      "Epoch 11 batch 2100 train loss: 0.0124 test loss: 0.0123\n",
      "Epoch 11 batch 2200 train loss: 0.0126 test loss: 0.0123\n",
      "Epoch 11 batch 2300 train loss: 0.0073 test loss: 0.0122\n",
      "Epoch 11 batch 2400 train loss: 0.0035 test loss: 0.0122\n",
      "Epoch 11 batch 2500 train loss: 0.0066 test loss: 0.0124\n",
      "Epoch 11 batch 2600 train loss: 0.0028 test loss: 0.0124\n",
      "Epoch 11 batch 2700 train loss: 0.0097 test loss: 0.0123\n",
      "Epoch 11 batch 2800 train loss: 0.0037 test loss: 0.0124\n",
      "Epoch 11 batch 2900 train loss: 0.0046 test loss: 0.0124\n",
      "Epoch 12 batch 0 train loss: 0.0076 test loss: 0.0121\n",
      "Epoch 12 batch 100 train loss: 0.0023 test loss: 0.0125\n",
      "Epoch 12 batch 200 train loss: 0.0044 test loss: 0.0122\n",
      "Epoch 12 batch 300 train loss: 0.0046 test loss: 0.0122\n",
      "Epoch 12 batch 400 train loss: 0.0062 test loss: 0.0124\n",
      "Epoch 12 batch 500 train loss: 0.0087 test loss: 0.0122\n",
      "Epoch 12 batch 600 train loss: 0.0105 test loss: 0.0122\n",
      "Epoch 12 batch 700 train loss: 0.0043 test loss: 0.0123\n",
      "Epoch 12 batch 800 train loss: 0.0084 test loss: 0.0122\n",
      "Epoch 12 batch 900 train loss: 0.0045 test loss: 0.0125\n",
      "Epoch 12 batch 1000 train loss: 0.0073 test loss: 0.0122\n",
      "Epoch 12 batch 1100 train loss: 0.0083 test loss: 0.0125\n",
      "Epoch 12 batch 1200 train loss: 0.0053 test loss: 0.0121\n",
      "Epoch 12 batch 1300 train loss: 0.0087 test loss: 0.0124\n",
      "Epoch 12 batch 1400 train loss: 0.0073 test loss: 0.0124\n",
      "Epoch 12 batch 1500 train loss: 0.0066 test loss: 0.0124\n",
      "Epoch 12 batch 1600 train loss: 0.0105 test loss: 0.0122\n",
      "Epoch 12 batch 1700 train loss: 0.0087 test loss: 0.0125\n",
      "Epoch 12 batch 1800 train loss: 0.0030 test loss: 0.0124\n",
      "Epoch 12 batch 1900 train loss: 0.0100 test loss: 0.0122\n",
      "Epoch 12 batch 2000 train loss: 0.0092 test loss: 0.0126\n",
      "Epoch 12 batch 2100 train loss: 0.0079 test loss: 0.0122\n",
      "Epoch 12 batch 2200 train loss: 0.0063 test loss: 0.0122\n",
      "Epoch 12 batch 2300 train loss: 0.0081 test loss: 0.0124\n",
      "Epoch 12 batch 2400 train loss: 0.0090 test loss: 0.0125\n",
      "Epoch 12 batch 2500 train loss: 0.0066 test loss: 0.0123\n",
      "Epoch 12 batch 2600 train loss: 0.0081 test loss: 0.0124\n",
      "Epoch 12 batch 2700 train loss: 0.0131 test loss: 0.0124\n",
      "Epoch 12 batch 2800 train loss: 0.0032 test loss: 0.0122\n",
      "Epoch 12 batch 2900 train loss: 0.0084 test loss: 0.0122\n",
      "Epoch 13 batch 0 train loss: 0.0114 test loss: 0.0123\n",
      "Epoch 13 batch 100 train loss: 0.0023 test loss: 0.0123\n",
      "Epoch 13 batch 200 train loss: 0.0031 test loss: 0.0122\n",
      "Epoch 13 batch 300 train loss: 0.0102 test loss: 0.0122\n",
      "Epoch 13 batch 400 train loss: 0.0113 test loss: 0.0123\n",
      "Epoch 13 batch 500 train loss: 0.0119 test loss: 0.0124\n",
      "Epoch 13 batch 600 train loss: 0.0055 test loss: 0.0121\n",
      "Epoch 13 batch 700 train loss: 0.0058 test loss: 0.0124\n",
      "Epoch 13 batch 800 train loss: 0.0024 test loss: 0.0125\n",
      "Epoch 13 batch 900 train loss: 0.0069 test loss: 0.0123\n",
      "Epoch 13 batch 1000 train loss: 0.0051 test loss: 0.0123\n",
      "Epoch 13 batch 1100 train loss: 0.0184 test loss: 0.0122\n",
      "Epoch 13 batch 1200 train loss: 0.0051 test loss: 0.0124\n",
      "Epoch 13 batch 1300 train loss: 0.0075 test loss: 0.0122\n",
      "Epoch 13 batch 1400 train loss: 0.0041 test loss: 0.0123\n",
      "Epoch 13 batch 1500 train loss: 0.0060 test loss: 0.0122\n",
      "Epoch 13 batch 1600 train loss: 0.0031 test loss: 0.0121\n",
      "Epoch 13 batch 1700 train loss: 0.0043 test loss: 0.0124\n",
      "Epoch 13 batch 1800 train loss: 0.0090 test loss: 0.0123\n",
      "Epoch 13 batch 1900 train loss: 0.0113 test loss: 0.0124\n",
      "Epoch 13 batch 2000 train loss: 0.0073 test loss: 0.0123\n",
      "Epoch 13 batch 2100 train loss: 0.0081 test loss: 0.0124\n",
      "Epoch 13 batch 2200 train loss: 0.0089 test loss: 0.0122\n",
      "Epoch 13 batch 2300 train loss: 0.0036 test loss: 0.0123\n",
      "Epoch 13 batch 2400 train loss: 0.0066 test loss: 0.0124\n",
      "Epoch 13 batch 2500 train loss: 0.0112 test loss: 0.0121\n",
      "Epoch 13 batch 2600 train loss: 0.0098 test loss: 0.0122\n",
      "Epoch 13 batch 2700 train loss: 0.0032 test loss: 0.0122\n",
      "Epoch 13 batch 2800 train loss: 0.0055 test loss: 0.0124\n",
      "Epoch 13 batch 2900 train loss: 0.0050 test loss: 0.0122\n",
      "Epoch 14 batch 0 train loss: 0.0051 test loss: 0.0125\n",
      "Epoch 14 batch 100 train loss: 0.0067 test loss: 0.0123\n",
      "Epoch 14 batch 200 train loss: 0.0048 test loss: 0.0123\n",
      "Epoch 14 batch 300 train loss: 0.0064 test loss: 0.0122\n",
      "Epoch 14 batch 400 train loss: 0.0139 test loss: 0.0122\n",
      "Epoch 14 batch 500 train loss: 0.0181 test loss: 0.0123\n",
      "Epoch 14 batch 600 train loss: 0.0100 test loss: 0.0121\n",
      "Epoch 14 batch 700 train loss: 0.0021 test loss: 0.0123\n",
      "Epoch 14 batch 800 train loss: 0.0154 test loss: 0.0124\n",
      "Epoch 14 batch 900 train loss: 0.0070 test loss: 0.0122\n",
      "Epoch 14 batch 1000 train loss: 0.0049 test loss: 0.0123\n",
      "Epoch 14 batch 1100 train loss: 0.0038 test loss: 0.0122\n",
      "Epoch 14 batch 1200 train loss: 0.0064 test loss: 0.0124\n",
      "Epoch 14 batch 1300 train loss: 0.0171 test loss: 0.0123\n",
      "Epoch 14 batch 1400 train loss: 0.0118 test loss: 0.0123\n",
      "Epoch 14 batch 1500 train loss: 0.0034 test loss: 0.0125\n",
      "Epoch 14 batch 1600 train loss: 0.0074 test loss: 0.0122\n",
      "Epoch 14 batch 1700 train loss: 0.0059 test loss: 0.0124\n",
      "Epoch 14 batch 1800 train loss: 0.0089 test loss: 0.0122\n",
      "Epoch 14 batch 1900 train loss: 0.0058 test loss: 0.0124\n",
      "Epoch 14 batch 2000 train loss: 0.0072 test loss: 0.0125\n",
      "Epoch 14 batch 2100 train loss: 0.0072 test loss: 0.0122\n",
      "Epoch 14 batch 2200 train loss: 0.0048 test loss: 0.0123\n",
      "Epoch 14 batch 2300 train loss: 0.0094 test loss: 0.0125\n",
      "Epoch 14 batch 2400 train loss: 0.0059 test loss: 0.0123\n",
      "Epoch 14 batch 2500 train loss: 0.0139 test loss: 0.0122\n",
      "Epoch 14 batch 2600 train loss: 0.0058 test loss: 0.0123\n",
      "Epoch 14 batch 2700 train loss: 0.0132 test loss: 0.0123\n",
      "Epoch 14 batch 2800 train loss: 0.0054 test loss: 0.0124\n",
      "Epoch 14 batch 2900 train loss: 0.0111 test loss: 0.0122\n",
      "Epoch 15 batch 0 train loss: 0.0017 test loss: 0.0122\n",
      "Epoch 15 batch 100 train loss: 0.0119 test loss: 0.0123\n",
      "Epoch 15 batch 200 train loss: 0.0099 test loss: 0.0122\n",
      "Epoch 15 batch 300 train loss: 0.0114 test loss: 0.0122\n",
      "Epoch 15 batch 400 train loss: 0.0128 test loss: 0.0124\n",
      "Epoch 15 batch 500 train loss: 0.0106 test loss: 0.0125\n",
      "Epoch 15 batch 600 train loss: 0.0036 test loss: 0.0123\n",
      "Epoch 15 batch 700 train loss: 0.0038 test loss: 0.0123\n",
      "Epoch 15 batch 800 train loss: 0.0046 test loss: 0.0122\n",
      "Epoch 15 batch 900 train loss: 0.0070 test loss: 0.0123\n",
      "Epoch 15 batch 1000 train loss: 0.0038 test loss: 0.0123\n",
      "Epoch 15 batch 1100 train loss: 0.0121 test loss: 0.0123\n",
      "Epoch 15 batch 1200 train loss: 0.0043 test loss: 0.0122\n",
      "Epoch 15 batch 1300 train loss: 0.0071 test loss: 0.0123\n",
      "Epoch 15 batch 1400 train loss: 0.0100 test loss: 0.0125\n",
      "Epoch 15 batch 1500 train loss: 0.0098 test loss: 0.0124\n",
      "Epoch 15 batch 1600 train loss: 0.0025 test loss: 0.0123\n",
      "Epoch 15 batch 1700 train loss: 0.0085 test loss: 0.0123\n",
      "Epoch 15 batch 1800 train loss: 0.0056 test loss: 0.0122\n",
      "Epoch 15 batch 1900 train loss: 0.0052 test loss: 0.0122\n",
      "Epoch 15 batch 2000 train loss: 0.0065 test loss: 0.0123\n",
      "Epoch 15 batch 2100 train loss: 0.0041 test loss: 0.0123\n",
      "Epoch 15 batch 2200 train loss: 0.0092 test loss: 0.0123\n",
      "Epoch 15 batch 2300 train loss: 0.0038 test loss: 0.0124\n",
      "Epoch 15 batch 2400 train loss: 0.0042 test loss: 0.0123\n",
      "Epoch 15 batch 2500 train loss: 0.0132 test loss: 0.0124\n",
      "Epoch 15 batch 2600 train loss: 0.0121 test loss: 0.0123\n",
      "Epoch 15 batch 2700 train loss: 0.0079 test loss: 0.0122\n",
      "Epoch 15 batch 2800 train loss: 0.0124 test loss: 0.0124\n",
      "Epoch 15 batch 2900 train loss: 0.0046 test loss: 0.0125\n",
      "Epoch 16 batch 0 train loss: 0.0047 test loss: 0.0123\n",
      "Epoch 16 batch 100 train loss: 0.0049 test loss: 0.0122\n",
      "Epoch 16 batch 200 train loss: 0.0050 test loss: 0.0123\n",
      "Epoch 16 batch 300 train loss: 0.0123 test loss: 0.0122\n",
      "Epoch 16 batch 400 train loss: 0.0090 test loss: 0.0124\n",
      "Epoch 16 batch 500 train loss: 0.0066 test loss: 0.0123\n",
      "Epoch 16 batch 600 train loss: 0.0060 test loss: 0.0123\n",
      "Epoch 16 batch 700 train loss: 0.0020 test loss: 0.0125\n",
      "Epoch 16 batch 800 train loss: 0.0132 test loss: 0.0123\n",
      "Epoch 16 batch 900 train loss: 0.0032 test loss: 0.0124\n",
      "Epoch 16 batch 1000 train loss: 0.0076 test loss: 0.0121\n",
      "Epoch 16 batch 1100 train loss: 0.0004 test loss: 0.0124\n",
      "Epoch 16 batch 1200 train loss: 0.0036 test loss: 0.0122\n",
      "Epoch 16 batch 1300 train loss: 0.0066 test loss: 0.0122\n",
      "Epoch 16 batch 1400 train loss: 0.0068 test loss: 0.0124\n",
      "Epoch 16 batch 1500 train loss: 0.0080 test loss: 0.0122\n",
      "Epoch 16 batch 1600 train loss: 0.0040 test loss: 0.0122\n",
      "Epoch 16 batch 1700 train loss: 0.0022 test loss: 0.0124\n",
      "Epoch 16 batch 1800 train loss: 0.0076 test loss: 0.0124\n",
      "Epoch 16 batch 1900 train loss: 0.0104 test loss: 0.0125\n",
      "Epoch 16 batch 2000 train loss: 0.0065 test loss: 0.0123\n",
      "Epoch 16 batch 2100 train loss: 0.0081 test loss: 0.0124\n",
      "Epoch 16 batch 2200 train loss: 0.0098 test loss: 0.0123\n",
      "Epoch 16 batch 2300 train loss: 0.0114 test loss: 0.0121\n",
      "Epoch 16 batch 2400 train loss: 0.0107 test loss: 0.0121\n",
      "Epoch 16 batch 2500 train loss: 0.0023 test loss: 0.0123\n",
      "Epoch 16 batch 2600 train loss: 0.0047 test loss: 0.0123\n",
      "Epoch 16 batch 2700 train loss: 0.0096 test loss: 0.0122\n",
      "Epoch 16 batch 2800 train loss: 0.0091 test loss: 0.0121\n",
      "Epoch 16 batch 2900 train loss: 0.0035 test loss: 0.0122\n",
      "Epoch 17 batch 0 train loss: 0.0071 test loss: 0.0124\n",
      "Epoch 17 batch 100 train loss: 0.0113 test loss: 0.0124\n",
      "Epoch 17 batch 200 train loss: 0.0137 test loss: 0.0123\n",
      "Epoch 17 batch 300 train loss: 0.0047 test loss: 0.0124\n",
      "Epoch 17 batch 400 train loss: 0.0140 test loss: 0.0122\n",
      "Epoch 17 batch 500 train loss: 0.0135 test loss: 0.0123\n",
      "Epoch 17 batch 600 train loss: 0.0125 test loss: 0.0121\n",
      "early stop.\n",
      "Checkpoint 17 restored!!\n",
      "2446/2446 [==============================] - 18s 7ms/step - loss: 0.0087\n",
      "Training for loss rate 0.60 start.\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.60p/ckpt-1\n",
      "Epoch 0 batch 0 train loss: 0.0704 test loss: 0.1086\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.60p/ckpt-2\n",
      "Epoch 0 batch 100 train loss: 0.0231 test loss: 0.0283\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.60p/ckpt-3\n",
      "Epoch 0 batch 200 train loss: 0.0130 test loss: 0.0254\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.60p/ckpt-4\n",
      "Epoch 0 batch 300 train loss: 0.0114 test loss: 0.0250\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.60p/ckpt-5\n",
      "Epoch 0 batch 400 train loss: 0.0172 test loss: 0.0236\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.60p/ckpt-6\n",
      "Epoch 0 batch 500 train loss: 0.0123 test loss: 0.0235\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.60p/ckpt-7\n",
      "Epoch 0 batch 600 train loss: 0.0111 test loss: 0.0230\n",
      "Epoch 0 batch 700 train loss: 0.0213 test loss: 0.0244\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.60p/ckpt-8\n",
      "Epoch 0 batch 800 train loss: 0.0099 test loss: 0.0227\n",
      "Epoch 0 batch 900 train loss: 0.0104 test loss: 0.0233\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.60p/ckpt-9\n",
      "Epoch 0 batch 1000 train loss: 0.0156 test loss: 0.0220\n",
      "Epoch 0 batch 1100 train loss: 0.0090 test loss: 0.0224\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.60p/ckpt-10\n",
      "Epoch 0 batch 1200 train loss: 0.0185 test loss: 0.0216\n",
      "Epoch 0 batch 1300 train loss: 0.0138 test loss: 0.0222\n",
      "Epoch 0 batch 1400 train loss: 0.0149 test loss: 0.0220\n",
      "Epoch 0 batch 1500 train loss: 0.0119 test loss: 0.0216\n",
      "Epoch 0 batch 1600 train loss: 0.0067 test loss: 0.0225\n",
      "Epoch 0 batch 1700 train loss: 0.0136 test loss: 0.0228\n",
      "Epoch 0 batch 1800 train loss: 0.0131 test loss: 0.0223\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.60p/ckpt-11\n",
      "Epoch 0 batch 1900 train loss: 0.0176 test loss: 0.0214\n",
      "Epoch 0 batch 2000 train loss: 0.0084 test loss: 0.0224\n",
      "Epoch 0 batch 2100 train loss: 0.0125 test loss: 0.0220\n",
      "Epoch 0 batch 2200 train loss: 0.0062 test loss: 0.0226\n",
      "Epoch 0 batch 2300 train loss: 0.0187 test loss: 0.0218\n",
      "Epoch 0 batch 2400 train loss: 0.0221 test loss: 0.0215\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.60p/ckpt-12\n",
      "Epoch 0 batch 2500 train loss: 0.0111 test loss: 0.0212\n",
      "Epoch 0 batch 2600 train loss: 0.0149 test loss: 0.0224\n",
      "Epoch 0 batch 2700 train loss: 0.0128 test loss: 0.0220\n",
      "Epoch 0 batch 2800 train loss: 0.0157 test loss: 0.0215\n",
      "Epoch 0 batch 2900 train loss: 0.0106 test loss: 0.0221\n",
      "Epoch 1 batch 0 train loss: 0.0109 test loss: 0.0217\n",
      "Epoch 1 batch 100 train loss: 0.0224 test loss: 0.0217\n",
      "Epoch 1 batch 200 train loss: 0.0105 test loss: 0.0218\n",
      "Epoch 1 batch 300 train loss: 0.0212 test loss: 0.0219\n",
      "Epoch 1 batch 400 train loss: 0.0131 test loss: 0.0220\n",
      "Epoch 1 batch 500 train loss: 0.0131 test loss: 0.0216\n",
      "Epoch 1 batch 600 train loss: 0.0156 test loss: 0.0220\n",
      "Epoch 1 batch 700 train loss: 0.0102 test loss: 0.0213\n",
      "Epoch 1 batch 800 train loss: 0.0069 test loss: 0.0218\n",
      "Epoch 1 batch 900 train loss: 0.0181 test loss: 0.0216\n",
      "Epoch 1 batch 1000 train loss: 0.0105 test loss: 0.0219\n",
      "Epoch 1 batch 1100 train loss: 0.0150 test loss: 0.0224\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.60p/ckpt-13\n",
      "Epoch 1 batch 1200 train loss: 0.0176 test loss: 0.0211\n",
      "Epoch 1 batch 1300 train loss: 0.0141 test loss: 0.0217\n",
      "Epoch 1 batch 1400 train loss: 0.0138 test loss: 0.0214\n",
      "Epoch 1 batch 1500 train loss: 0.0169 test loss: 0.0213\n",
      "Epoch 1 batch 1600 train loss: 0.0121 test loss: 0.0212\n",
      "Epoch 1 batch 1700 train loss: 0.0153 test loss: 0.0220\n",
      "Epoch 1 batch 1800 train loss: 0.0096 test loss: 0.0218\n",
      "Epoch 1 batch 1900 train loss: 0.0149 test loss: 0.0216\n",
      "Epoch 1 batch 2000 train loss: 0.0186 test loss: 0.0220\n",
      "Epoch 1 batch 2100 train loss: 0.0176 test loss: 0.0214\n",
      "Epoch 1 batch 2200 train loss: 0.0123 test loss: 0.0218\n",
      "Epoch 1 batch 2300 train loss: 0.0104 test loss: 0.0216\n",
      "Epoch 1 batch 2400 train loss: 0.0158 test loss: 0.0212\n",
      "Epoch 1 batch 2500 train loss: 0.0093 test loss: 0.0217\n",
      "Epoch 1 batch 2600 train loss: 0.0141 test loss: 0.0212\n",
      "Epoch 1 batch 2700 train loss: 0.0263 test loss: 0.0217\n",
      "Epoch 1 batch 2800 train loss: 0.0106 test loss: 0.0217\n",
      "Epoch 1 batch 2900 train loss: 0.0144 test loss: 0.0214\n",
      "Epoch 2 batch 0 train loss: 0.0090 test loss: 0.0214\n",
      "Epoch 2 batch 100 train loss: 0.0146 test loss: 0.0216\n",
      "Epoch 2 batch 200 train loss: 0.0110 test loss: 0.0213\n",
      "Epoch 2 batch 300 train loss: 0.0097 test loss: 0.0213\n",
      "Epoch 2 batch 400 train loss: 0.0072 test loss: 0.0213\n",
      "Epoch 2 batch 500 train loss: 0.0077 test loss: 0.0213\n",
      "Epoch 2 batch 600 train loss: 0.0154 test loss: 0.0215\n",
      "Epoch 2 batch 700 train loss: 0.0125 test loss: 0.0216\n",
      "Epoch 2 batch 800 train loss: 0.0135 test loss: 0.0215\n",
      "Epoch 2 batch 900 train loss: 0.0203 test loss: 0.0213\n",
      "Epoch 2 batch 1000 train loss: 0.0167 test loss: 0.0213\n",
      "Epoch 2 batch 1100 train loss: 0.0089 test loss: 0.0214\n",
      "Epoch 2 batch 1200 train loss: 0.0223 test loss: 0.0216\n",
      "Epoch 2 batch 1300 train loss: 0.0186 test loss: 0.0214\n",
      "Epoch 2 batch 1400 train loss: 0.0156 test loss: 0.0212\n",
      "Epoch 2 batch 1500 train loss: 0.0145 test loss: 0.0213\n",
      "Epoch 2 batch 1600 train loss: 0.0076 test loss: 0.0223\n",
      "Epoch 2 batch 1700 train loss: 0.0061 test loss: 0.0215\n",
      "Epoch 2 batch 1800 train loss: 0.0124 test loss: 0.0214\n",
      "Epoch 2 batch 1900 train loss: 0.0113 test loss: 0.0214\n",
      "Epoch 2 batch 2000 train loss: 0.0092 test loss: 0.0215\n",
      "Epoch 2 batch 2100 train loss: 0.0119 test loss: 0.0212\n",
      "Epoch 2 batch 2200 train loss: 0.0130 test loss: 0.0215\n",
      "Epoch 2 batch 2300 train loss: 0.0145 test loss: 0.0214\n",
      "Epoch 2 batch 2400 train loss: 0.0157 test loss: 0.0216\n",
      "Epoch 2 batch 2500 train loss: 0.0080 test loss: 0.0211\n",
      "Epoch 2 batch 2600 train loss: 0.0111 test loss: 0.0214\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.60p/ckpt-14\n",
      "Epoch 2 batch 2700 train loss: 0.0095 test loss: 0.0207\n",
      "Epoch 2 batch 2800 train loss: 0.0179 test loss: 0.0213\n",
      "Epoch 2 batch 2900 train loss: 0.0151 test loss: 0.0214\n",
      "Epoch 3 batch 0 train loss: 0.0178 test loss: 0.0215\n",
      "Epoch 3 batch 100 train loss: 0.0135 test loss: 0.0214\n",
      "Epoch 3 batch 200 train loss: 0.0149 test loss: 0.0216\n",
      "Epoch 3 batch 300 train loss: 0.0120 test loss: 0.0216\n",
      "Epoch 3 batch 400 train loss: 0.0133 test loss: 0.0220\n",
      "Epoch 3 batch 500 train loss: 0.0126 test loss: 0.0211\n",
      "Epoch 3 batch 600 train loss: 0.0135 test loss: 0.0213\n",
      "Epoch 3 batch 700 train loss: 0.0078 test loss: 0.0210\n",
      "Epoch 3 batch 800 train loss: 0.0210 test loss: 0.0212\n",
      "Epoch 3 batch 900 train loss: 0.0160 test loss: 0.0215\n",
      "Epoch 3 batch 1000 train loss: 0.0121 test loss: 0.0212\n",
      "Epoch 3 batch 1100 train loss: 0.0123 test loss: 0.0215\n",
      "Epoch 3 batch 1200 train loss: 0.0076 test loss: 0.0213\n",
      "Epoch 3 batch 1300 train loss: 0.0088 test loss: 0.0220\n",
      "Epoch 3 batch 1400 train loss: 0.0092 test loss: 0.0212\n",
      "Epoch 3 batch 1500 train loss: 0.0103 test loss: 0.0218\n",
      "Epoch 3 batch 1600 train loss: 0.0156 test loss: 0.0212\n",
      "Epoch 3 batch 1700 train loss: 0.0115 test loss: 0.0214\n",
      "Epoch 3 batch 1800 train loss: 0.0160 test loss: 0.0211\n",
      "Epoch 3 batch 1900 train loss: 0.0087 test loss: 0.0214\n",
      "Epoch 3 batch 2000 train loss: 0.0065 test loss: 0.0218\n",
      "Epoch 3 batch 2100 train loss: 0.0153 test loss: 0.0207\n",
      "Epoch 3 batch 2200 train loss: 0.0125 test loss: 0.0217\n",
      "Epoch 3 batch 2300 train loss: 0.0065 test loss: 0.0211\n",
      "Epoch 3 batch 2400 train loss: 0.0190 test loss: 0.0210\n",
      "Epoch 3 batch 2500 train loss: 0.0158 test loss: 0.0214\n",
      "Epoch 3 batch 2600 train loss: 0.0199 test loss: 0.0210\n",
      "Epoch 3 batch 2700 train loss: 0.0040 test loss: 0.0217\n",
      "Epoch 3 batch 2800 train loss: 0.0155 test loss: 0.0212\n",
      "Epoch 3 batch 2900 train loss: 0.0099 test loss: 0.0212\n",
      "Epoch 4 batch 0 train loss: 0.0071 test loss: 0.0211\n",
      "Epoch 4 batch 100 train loss: 0.0062 test loss: 0.0216\n",
      "Epoch 4 batch 200 train loss: 0.0100 test loss: 0.0212\n",
      "Epoch 4 batch 300 train loss: 0.0160 test loss: 0.0213\n",
      "Epoch 4 batch 400 train loss: 0.0095 test loss: 0.0215\n",
      "Epoch 4 batch 500 train loss: 0.0145 test loss: 0.0212\n",
      "Epoch 4 batch 600 train loss: 0.0129 test loss: 0.0210\n",
      "Epoch 4 batch 700 train loss: 0.0064 test loss: 0.0217\n",
      "Epoch 4 batch 800 train loss: 0.0072 test loss: 0.0212\n",
      "Epoch 4 batch 900 train loss: 0.0118 test loss: 0.0213\n",
      "Epoch 4 batch 1000 train loss: 0.0118 test loss: 0.0212\n",
      "Epoch 4 batch 1100 train loss: 0.0220 test loss: 0.0214\n",
      "Epoch 4 batch 1200 train loss: 0.0053 test loss: 0.0213\n",
      "Epoch 4 batch 1300 train loss: 0.0100 test loss: 0.0214\n",
      "Epoch 4 batch 1400 train loss: 0.0087 test loss: 0.0215\n",
      "Epoch 4 batch 1500 train loss: 0.0063 test loss: 0.0216\n",
      "Epoch 4 batch 1600 train loss: 0.0088 test loss: 0.0211\n",
      "Epoch 4 batch 1700 train loss: 0.0089 test loss: 0.0215\n",
      "Epoch 4 batch 1800 train loss: 0.0113 test loss: 0.0213\n",
      "Epoch 4 batch 1900 train loss: 0.0119 test loss: 0.0214\n",
      "Epoch 4 batch 2000 train loss: 0.0191 test loss: 0.0215\n",
      "Epoch 4 batch 2100 train loss: 0.0070 test loss: 0.0214\n",
      "Epoch 4 batch 2200 train loss: 0.0114 test loss: 0.0214\n",
      "Epoch 4 batch 2300 train loss: 0.0062 test loss: 0.0212\n",
      "Epoch 4 batch 2400 train loss: 0.0158 test loss: 0.0215\n",
      "Epoch 4 batch 2500 train loss: 0.0214 test loss: 0.0213\n",
      "Epoch 4 batch 2600 train loss: 0.0131 test loss: 0.0214\n",
      "Epoch 4 batch 2700 train loss: 0.0120 test loss: 0.0213\n",
      "Epoch 4 batch 2800 train loss: 0.0141 test loss: 0.0210\n",
      "Epoch 4 batch 2900 train loss: 0.0119 test loss: 0.0215\n",
      "Epoch 5 batch 0 train loss: 0.0126 test loss: 0.0215\n",
      "Epoch 5 batch 100 train loss: 0.0055 test loss: 0.0214\n",
      "Epoch 5 batch 200 train loss: 0.0066 test loss: 0.0213\n",
      "Epoch 5 batch 300 train loss: 0.0091 test loss: 0.0212\n",
      "Epoch 5 batch 400 train loss: 0.0101 test loss: 0.0211\n",
      "Epoch 5 batch 500 train loss: 0.0084 test loss: 0.0211\n",
      "Epoch 5 batch 600 train loss: 0.0155 test loss: 0.0210\n",
      "Epoch 5 batch 700 train loss: 0.0133 test loss: 0.0216\n",
      "Epoch 5 batch 800 train loss: 0.0106 test loss: 0.0214\n",
      "Epoch 5 batch 900 train loss: 0.0052 test loss: 0.0212\n",
      "Epoch 5 batch 1000 train loss: 0.0067 test loss: 0.0210\n",
      "Epoch 5 batch 1100 train loss: 0.0089 test loss: 0.0213\n",
      "Epoch 5 batch 1200 train loss: 0.0071 test loss: 0.0210\n",
      "Epoch 5 batch 1300 train loss: 0.0146 test loss: 0.0214\n",
      "Epoch 5 batch 1400 train loss: 0.0187 test loss: 0.0211\n",
      "Epoch 5 batch 1500 train loss: 0.0035 test loss: 0.0214\n",
      "Epoch 5 batch 1600 train loss: 0.0144 test loss: 0.0210\n",
      "Epoch 5 batch 1700 train loss: 0.0177 test loss: 0.0215\n",
      "Epoch 5 batch 1800 train loss: 0.0114 test loss: 0.0209\n",
      "Epoch 5 batch 1900 train loss: 0.0125 test loss: 0.0213\n",
      "Epoch 5 batch 2000 train loss: 0.0182 test loss: 0.0220\n",
      "Epoch 5 batch 2100 train loss: 0.0106 test loss: 0.0212\n",
      "Epoch 5 batch 2200 train loss: 0.0113 test loss: 0.0212\n",
      "Epoch 5 batch 2300 train loss: 0.0161 test loss: 0.0216\n",
      "Epoch 5 batch 2400 train loss: 0.0113 test loss: 0.0217\n",
      "Epoch 5 batch 2500 train loss: 0.0081 test loss: 0.0214\n",
      "Epoch 5 batch 2600 train loss: 0.0107 test loss: 0.0214\n",
      "Epoch 5 batch 2700 train loss: 0.0027 test loss: 0.0211\n",
      "Epoch 5 batch 2800 train loss: 0.0093 test loss: 0.0210\n",
      "Epoch 5 batch 2900 train loss: 0.0133 test loss: 0.0210\n",
      "Epoch 6 batch 0 train loss: 0.0095 test loss: 0.0215\n",
      "Epoch 6 batch 100 train loss: 0.0150 test loss: 0.0212\n",
      "Epoch 6 batch 200 train loss: 0.0105 test loss: 0.0212\n",
      "Epoch 6 batch 300 train loss: 0.0089 test loss: 0.0213\n",
      "Epoch 6 batch 400 train loss: 0.0228 test loss: 0.0214\n",
      "Epoch 6 batch 500 train loss: 0.0176 test loss: 0.0210\n",
      "Epoch 6 batch 600 train loss: 0.0120 test loss: 0.0208\n",
      "Epoch 6 batch 700 train loss: 0.0104 test loss: 0.0216\n",
      "Epoch 6 batch 800 train loss: 0.0127 test loss: 0.0214\n",
      "Epoch 6 batch 900 train loss: 0.0104 test loss: 0.0212\n",
      "Epoch 6 batch 1000 train loss: 0.0191 test loss: 0.0212\n",
      "Epoch 6 batch 1100 train loss: 0.0172 test loss: 0.0212\n",
      "Epoch 6 batch 1200 train loss: 0.0149 test loss: 0.0213\n",
      "Epoch 6 batch 1300 train loss: 0.0110 test loss: 0.0215\n",
      "Epoch 6 batch 1400 train loss: 0.0137 test loss: 0.0214\n",
      "Epoch 6 batch 1500 train loss: 0.0057 test loss: 0.0219\n",
      "Epoch 6 batch 1600 train loss: 0.0130 test loss: 0.0210\n",
      "Epoch 6 batch 1700 train loss: 0.0157 test loss: 0.0213\n",
      "Epoch 6 batch 1800 train loss: 0.0138 test loss: 0.0214\n",
      "Epoch 6 batch 1900 train loss: 0.0088 test loss: 0.0210\n",
      "Epoch 6 batch 2000 train loss: 0.0144 test loss: 0.0216\n",
      "Epoch 6 batch 2100 train loss: 0.0132 test loss: 0.0209\n",
      "Epoch 6 batch 2200 train loss: 0.0095 test loss: 0.0212\n",
      "Epoch 6 batch 2300 train loss: 0.0154 test loss: 0.0213\n",
      "Epoch 6 batch 2400 train loss: 0.0195 test loss: 0.0215\n",
      "Epoch 6 batch 2500 train loss: 0.0029 test loss: 0.0209\n",
      "Epoch 6 batch 2600 train loss: 0.0066 test loss: 0.0212\n",
      "Epoch 6 batch 2700 train loss: 0.0207 test loss: 0.0215\n",
      "Epoch 6 batch 2800 train loss: 0.0094 test loss: 0.0212\n",
      "Epoch 6 batch 2900 train loss: 0.0092 test loss: 0.0213\n",
      "Epoch 7 batch 0 train loss: 0.0165 test loss: 0.0212\n",
      "Epoch 7 batch 100 train loss: 0.0109 test loss: 0.0215\n",
      "Epoch 7 batch 200 train loss: 0.0097 test loss: 0.0214\n",
      "Epoch 7 batch 300 train loss: 0.0138 test loss: 0.0213\n",
      "Epoch 7 batch 400 train loss: 0.0104 test loss: 0.0215\n",
      "Epoch 7 batch 500 train loss: 0.0095 test loss: 0.0211\n",
      "Epoch 7 batch 600 train loss: 0.0153 test loss: 0.0210\n",
      "Epoch 7 batch 700 train loss: 0.0088 test loss: 0.0213\n",
      "Epoch 7 batch 800 train loss: 0.0078 test loss: 0.0213\n",
      "Epoch 7 batch 900 train loss: 0.0116 test loss: 0.0215\n",
      "Epoch 7 batch 1000 train loss: 0.0121 test loss: 0.0213\n",
      "Epoch 7 batch 1100 train loss: 0.0082 test loss: 0.0211\n",
      "Epoch 7 batch 1200 train loss: 0.0088 test loss: 0.0214\n",
      "Epoch 7 batch 1300 train loss: 0.0137 test loss: 0.0214\n",
      "Epoch 7 batch 1400 train loss: 0.0094 test loss: 0.0214\n",
      "Epoch 7 batch 1500 train loss: 0.0096 test loss: 0.0214\n",
      "Epoch 7 batch 1600 train loss: 0.0064 test loss: 0.0212\n",
      "Epoch 7 batch 1700 train loss: 0.0139 test loss: 0.0216\n",
      "Epoch 7 batch 1800 train loss: 0.0071 test loss: 0.0216\n",
      "Epoch 7 batch 1900 train loss: 0.0144 test loss: 0.0213\n",
      "Epoch 7 batch 2000 train loss: 0.0108 test loss: 0.0214\n",
      "Epoch 7 batch 2100 train loss: 0.0162 test loss: 0.0211\n",
      "Epoch 7 batch 2200 train loss: 0.0054 test loss: 0.0212\n",
      "Epoch 7 batch 2300 train loss: 0.0218 test loss: 0.0212\n",
      "Epoch 7 batch 2400 train loss: 0.0109 test loss: 0.0215\n",
      "Epoch 7 batch 2500 train loss: 0.0086 test loss: 0.0208\n",
      "Epoch 7 batch 2600 train loss: 0.0068 test loss: 0.0210\n",
      "Epoch 7 batch 2700 train loss: 0.0109 test loss: 0.0217\n",
      "Epoch 7 batch 2800 train loss: 0.0064 test loss: 0.0214\n",
      "Epoch 7 batch 2900 train loss: 0.0094 test loss: 0.0214\n",
      "Epoch 8 batch 0 train loss: 0.0151 test loss: 0.0214\n",
      "Epoch 8 batch 100 train loss: 0.0145 test loss: 0.0213\n",
      "Epoch 8 batch 200 train loss: 0.0115 test loss: 0.0212\n",
      "Epoch 8 batch 300 train loss: 0.0114 test loss: 0.0212\n",
      "Epoch 8 batch 400 train loss: 0.0157 test loss: 0.0216\n",
      "Epoch 8 batch 500 train loss: 0.0093 test loss: 0.0213\n",
      "Epoch 8 batch 600 train loss: 0.0136 test loss: 0.0214\n",
      "Epoch 8 batch 700 train loss: 0.0065 test loss: 0.0212\n",
      "Epoch 8 batch 800 train loss: 0.0144 test loss: 0.0213\n",
      "Epoch 8 batch 900 train loss: 0.0083 test loss: 0.0213\n",
      "Epoch 8 batch 1000 train loss: 0.0133 test loss: 0.0211\n",
      "Epoch 8 batch 1100 train loss: 0.0112 test loss: 0.0214\n",
      "Epoch 8 batch 1200 train loss: 0.0092 test loss: 0.0212\n",
      "Epoch 8 batch 1300 train loss: 0.0139 test loss: 0.0214\n",
      "Epoch 8 batch 1400 train loss: 0.0188 test loss: 0.0216\n",
      "Epoch 8 batch 1500 train loss: 0.0059 test loss: 0.0213\n",
      "Epoch 8 batch 1600 train loss: 0.0097 test loss: 0.0213\n",
      "Epoch 8 batch 1700 train loss: 0.0165 test loss: 0.0212\n",
      "Epoch 8 batch 1800 train loss: 0.0086 test loss: 0.0213\n",
      "Epoch 8 batch 1900 train loss: 0.0097 test loss: 0.0213\n",
      "Epoch 8 batch 2000 train loss: 0.0076 test loss: 0.0211\n",
      "Epoch 8 batch 2100 train loss: 0.0081 test loss: 0.0215\n",
      "Epoch 8 batch 2200 train loss: 0.0098 test loss: 0.0212\n",
      "Epoch 8 batch 2300 train loss: 0.0116 test loss: 0.0217\n",
      "Epoch 8 batch 2400 train loss: 0.0207 test loss: 0.0213\n",
      "Epoch 8 batch 2500 train loss: 0.0109 test loss: 0.0211\n",
      "Epoch 8 batch 2600 train loss: 0.0106 test loss: 0.0212\n",
      "Epoch 8 batch 2700 train loss: 0.0155 test loss: 0.0216\n",
      "Epoch 8 batch 2800 train loss: 0.0076 test loss: 0.0212\n",
      "Epoch 8 batch 2900 train loss: 0.0170 test loss: 0.0211\n",
      "Epoch 9 batch 0 train loss: 0.0142 test loss: 0.0214\n",
      "Epoch 9 batch 100 train loss: 0.0139 test loss: 0.0212\n",
      "Epoch 9 batch 200 train loss: 0.0191 test loss: 0.0213\n",
      "Epoch 9 batch 300 train loss: 0.0113 test loss: 0.0212\n",
      "Epoch 9 batch 400 train loss: 0.0099 test loss: 0.0214\n",
      "Epoch 9 batch 500 train loss: 0.0182 test loss: 0.0214\n",
      "Epoch 9 batch 600 train loss: 0.0085 test loss: 0.0213\n",
      "Epoch 9 batch 700 train loss: 0.0131 test loss: 0.0213\n",
      "Epoch 9 batch 800 train loss: 0.0067 test loss: 0.0213\n",
      "Epoch 9 batch 900 train loss: 0.0120 test loss: 0.0214\n",
      "Epoch 9 batch 1000 train loss: 0.0219 test loss: 0.0211\n",
      "Epoch 9 batch 1100 train loss: 0.0105 test loss: 0.0213\n",
      "Epoch 9 batch 1200 train loss: 0.0131 test loss: 0.0211\n",
      "Epoch 9 batch 1300 train loss: 0.0126 test loss: 0.0216\n",
      "Epoch 9 batch 1400 train loss: 0.0092 test loss: 0.0214\n",
      "Epoch 9 batch 1500 train loss: 0.0150 test loss: 0.0214\n",
      "Epoch 9 batch 1600 train loss: 0.0118 test loss: 0.0212\n",
      "Epoch 9 batch 1700 train loss: 0.0098 test loss: 0.0213\n",
      "Epoch 9 batch 1800 train loss: 0.0102 test loss: 0.0213\n",
      "Epoch 9 batch 1900 train loss: 0.0118 test loss: 0.0213\n",
      "Epoch 9 batch 2000 train loss: 0.0190 test loss: 0.0214\n",
      "Epoch 9 batch 2100 train loss: 0.0094 test loss: 0.0209\n",
      "Epoch 9 batch 2200 train loss: 0.0090 test loss: 0.0212\n",
      "Epoch 9 batch 2300 train loss: 0.0169 test loss: 0.0212\n",
      "Epoch 9 batch 2400 train loss: 0.0172 test loss: 0.0213\n",
      "Epoch 9 batch 2500 train loss: 0.0147 test loss: 0.0211\n",
      "Epoch 9 batch 2600 train loss: 0.0137 test loss: 0.0212\n",
      "Epoch 9 batch 2700 train loss: 0.0090 test loss: 0.0215\n",
      "Epoch 9 batch 2800 train loss: 0.0094 test loss: 0.0212\n",
      "Epoch 9 batch 2900 train loss: 0.0083 test loss: 0.0212\n",
      "Epoch 10 batch 0 train loss: 0.0154 test loss: 0.0216\n",
      "Epoch 10 batch 100 train loss: 0.0105 test loss: 0.0218\n",
      "Epoch 10 batch 200 train loss: 0.0217 test loss: 0.0213\n",
      "Epoch 10 batch 300 train loss: 0.0150 test loss: 0.0210\n",
      "Epoch 10 batch 400 train loss: 0.0129 test loss: 0.0213\n",
      "Epoch 10 batch 500 train loss: 0.0109 test loss: 0.0213\n",
      "Epoch 10 batch 600 train loss: 0.0102 test loss: 0.0210\n",
      "Epoch 10 batch 700 train loss: 0.0096 test loss: 0.0217\n",
      "Epoch 10 batch 800 train loss: 0.0141 test loss: 0.0216\n",
      "Epoch 10 batch 900 train loss: 0.0112 test loss: 0.0213\n",
      "Epoch 10 batch 1000 train loss: 0.0080 test loss: 0.0210\n",
      "Epoch 10 batch 1100 train loss: 0.0051 test loss: 0.0214\n",
      "Epoch 10 batch 1200 train loss: 0.0080 test loss: 0.0213\n",
      "Epoch 10 batch 1300 train loss: 0.0117 test loss: 0.0213\n",
      "Epoch 10 batch 1400 train loss: 0.0116 test loss: 0.0212\n",
      "Epoch 10 batch 1500 train loss: 0.0119 test loss: 0.0217\n",
      "Epoch 10 batch 1600 train loss: 0.0110 test loss: 0.0213\n",
      "Epoch 10 batch 1700 train loss: 0.0074 test loss: 0.0211\n",
      "Epoch 10 batch 1800 train loss: 0.0138 test loss: 0.0212\n",
      "Epoch 10 batch 1900 train loss: 0.0095 test loss: 0.0212\n",
      "Epoch 10 batch 2000 train loss: 0.0125 test loss: 0.0213\n",
      "Epoch 10 batch 2100 train loss: 0.0110 test loss: 0.0209\n",
      "Epoch 10 batch 2200 train loss: 0.0089 test loss: 0.0216\n",
      "Epoch 10 batch 2300 train loss: 0.0142 test loss: 0.0209\n",
      "Epoch 10 batch 2400 train loss: 0.0176 test loss: 0.0213\n",
      "Epoch 10 batch 2500 train loss: 0.0119 test loss: 0.0210\n",
      "Epoch 10 batch 2600 train loss: 0.0115 test loss: 0.0213\n",
      "Epoch 10 batch 2700 train loss: 0.0168 test loss: 0.0216\n",
      "Epoch 10 batch 2800 train loss: 0.0101 test loss: 0.0213\n",
      "Epoch 10 batch 2900 train loss: 0.0077 test loss: 0.0209\n",
      "Epoch 11 batch 0 train loss: 0.0104 test loss: 0.0212\n",
      "Epoch 11 batch 100 train loss: 0.0131 test loss: 0.0213\n",
      "Epoch 11 batch 200 train loss: 0.0272 test loss: 0.0210\n",
      "Epoch 11 batch 300 train loss: 0.0137 test loss: 0.0216\n",
      "Epoch 11 batch 400 train loss: 0.0083 test loss: 0.0211\n",
      "Epoch 11 batch 500 train loss: 0.0081 test loss: 0.0214\n",
      "Epoch 11 batch 600 train loss: 0.0158 test loss: 0.0212\n",
      "Epoch 11 batch 700 train loss: 0.0127 test loss: 0.0216\n",
      "Epoch 11 batch 800 train loss: 0.0085 test loss: 0.0213\n",
      "Epoch 11 batch 900 train loss: 0.0123 test loss: 0.0214\n",
      "Epoch 11 batch 1000 train loss: 0.0129 test loss: 0.0214\n",
      "Epoch 11 batch 1100 train loss: 0.0187 test loss: 0.0209\n",
      "Epoch 11 batch 1200 train loss: 0.0160 test loss: 0.0212\n",
      "Epoch 11 batch 1300 train loss: 0.0119 test loss: 0.0213\n",
      "Epoch 11 batch 1400 train loss: 0.0065 test loss: 0.0214\n",
      "Epoch 11 batch 1500 train loss: 0.0054 test loss: 0.0215\n",
      "Epoch 11 batch 1600 train loss: 0.0157 test loss: 0.0210\n",
      "Epoch 11 batch 1700 train loss: 0.0142 test loss: 0.0213\n",
      "Epoch 11 batch 1800 train loss: 0.0045 test loss: 0.0214\n",
      "Epoch 11 batch 1900 train loss: 0.0110 test loss: 0.0213\n",
      "Epoch 11 batch 2000 train loss: 0.0054 test loss: 0.0215\n",
      "Epoch 11 batch 2100 train loss: 0.0082 test loss: 0.0212\n",
      "Epoch 11 batch 2200 train loss: 0.0096 test loss: 0.0215\n",
      "Epoch 11 batch 2300 train loss: 0.0075 test loss: 0.0215\n",
      "Epoch 11 batch 2400 train loss: 0.0120 test loss: 0.0214\n",
      "Epoch 11 batch 2500 train loss: 0.0162 test loss: 0.0212\n",
      "Epoch 11 batch 2600 train loss: 0.0099 test loss: 0.0212\n",
      "Epoch 11 batch 2700 train loss: 0.0055 test loss: 0.0215\n",
      "Epoch 11 batch 2800 train loss: 0.0110 test loss: 0.0215\n",
      "Epoch 11 batch 2900 train loss: 0.0120 test loss: 0.0218\n",
      "Epoch 12 batch 0 train loss: 0.0112 test loss: 0.0210\n",
      "Epoch 12 batch 100 train loss: 0.0230 test loss: 0.0214\n",
      "Epoch 12 batch 200 train loss: 0.0077 test loss: 0.0213\n",
      "Epoch 12 batch 300 train loss: 0.0154 test loss: 0.0211\n",
      "Epoch 12 batch 400 train loss: 0.0105 test loss: 0.0213\n",
      "Epoch 12 batch 500 train loss: 0.0064 test loss: 0.0214\n",
      "Epoch 12 batch 600 train loss: 0.0120 test loss: 0.0215\n",
      "Epoch 12 batch 700 train loss: 0.0082 test loss: 0.0212\n",
      "Epoch 12 batch 800 train loss: 0.0133 test loss: 0.0215\n",
      "Epoch 12 batch 900 train loss: 0.0082 test loss: 0.0212\n",
      "Epoch 12 batch 1000 train loss: 0.0067 test loss: 0.0214\n",
      "Epoch 12 batch 1100 train loss: 0.0104 test loss: 0.0215\n",
      "Epoch 12 batch 1200 train loss: 0.0155 test loss: 0.0213\n",
      "Epoch 12 batch 1300 train loss: 0.0163 test loss: 0.0213\n",
      "Epoch 12 batch 1400 train loss: 0.0074 test loss: 0.0211\n",
      "Epoch 12 batch 1500 train loss: 0.0152 test loss: 0.0212\n",
      "Epoch 12 batch 1600 train loss: 0.0126 test loss: 0.0213\n",
      "Epoch 12 batch 1700 train loss: 0.0115 test loss: 0.0215\n",
      "Epoch 12 batch 1800 train loss: 0.0067 test loss: 0.0212\n",
      "Epoch 12 batch 1900 train loss: 0.0153 test loss: 0.0214\n",
      "Epoch 12 batch 2000 train loss: 0.0147 test loss: 0.0216\n",
      "Epoch 12 batch 2100 train loss: 0.0152 test loss: 0.0209\n",
      "Epoch 12 batch 2200 train loss: 0.0187 test loss: 0.0211\n",
      "Epoch 12 batch 2300 train loss: 0.0083 test loss: 0.0211\n",
      "Epoch 12 batch 2400 train loss: 0.0089 test loss: 0.0214\n",
      "Epoch 12 batch 2500 train loss: 0.0135 test loss: 0.0211\n",
      "Epoch 12 batch 2600 train loss: 0.0137 test loss: 0.0212\n",
      "Epoch 12 batch 2700 train loss: 0.0178 test loss: 0.0216\n",
      "Epoch 12 batch 2800 train loss: 0.0056 test loss: 0.0213\n",
      "Epoch 12 batch 2900 train loss: 0.0140 test loss: 0.0212\n",
      "Epoch 13 batch 0 train loss: 0.0164 test loss: 0.0212\n",
      "Epoch 13 batch 100 train loss: 0.0086 test loss: 0.0214\n",
      "Epoch 13 batch 200 train loss: 0.0109 test loss: 0.0214\n",
      "Epoch 13 batch 300 train loss: 0.0094 test loss: 0.0212\n",
      "Epoch 13 batch 400 train loss: 0.0098 test loss: 0.0213\n",
      "Epoch 13 batch 500 train loss: 0.0180 test loss: 0.0212\n",
      "Epoch 13 batch 600 train loss: 0.0047 test loss: 0.0211\n",
      "Epoch 13 batch 700 train loss: 0.0192 test loss: 0.0211\n",
      "Epoch 13 batch 800 train loss: 0.0129 test loss: 0.0215\n",
      "Epoch 13 batch 900 train loss: 0.0080 test loss: 0.0212\n",
      "Epoch 13 batch 1000 train loss: 0.0116 test loss: 0.0213\n",
      "Epoch 13 batch 1100 train loss: 0.0101 test loss: 0.0214\n",
      "Epoch 13 batch 1200 train loss: 0.0207 test loss: 0.0212\n",
      "Epoch 13 batch 1300 train loss: 0.0076 test loss: 0.0214\n",
      "Epoch 13 batch 1400 train loss: 0.0104 test loss: 0.0215\n",
      "Epoch 13 batch 1500 train loss: 0.0129 test loss: 0.0211\n",
      "Epoch 13 batch 1600 train loss: 0.0135 test loss: 0.0210\n",
      "Epoch 13 batch 1700 train loss: 0.0118 test loss: 0.0213\n",
      "Epoch 13 batch 1800 train loss: 0.0104 test loss: 0.0213\n",
      "Epoch 13 batch 1900 train loss: 0.0105 test loss: 0.0214\n",
      "Epoch 13 batch 2000 train loss: 0.0080 test loss: 0.0215\n",
      "Epoch 13 batch 2100 train loss: 0.0108 test loss: 0.0214\n",
      "Epoch 13 batch 2200 train loss: 0.0136 test loss: 0.0214\n",
      "Epoch 13 batch 2300 train loss: 0.0177 test loss: 0.0212\n",
      "Epoch 13 batch 2400 train loss: 0.0152 test loss: 0.0211\n",
      "Epoch 13 batch 2500 train loss: 0.0136 test loss: 0.0212\n",
      "Epoch 13 batch 2600 train loss: 0.0152 test loss: 0.0213\n",
      "Epoch 13 batch 2700 train loss: 0.0138 test loss: 0.0214\n",
      "Epoch 13 batch 2800 train loss: 0.0078 test loss: 0.0209\n",
      "Epoch 13 batch 2900 train loss: 0.0085 test loss: 0.0213\n",
      "Epoch 14 batch 0 train loss: 0.0116 test loss: 0.0210\n",
      "Epoch 14 batch 100 train loss: 0.0077 test loss: 0.0212\n",
      "Epoch 14 batch 200 train loss: 0.0087 test loss: 0.0215\n",
      "Epoch 14 batch 300 train loss: 0.0105 test loss: 0.0212\n",
      "Epoch 14 batch 400 train loss: 0.0093 test loss: 0.0214\n",
      "Epoch 14 batch 500 train loss: 0.0118 test loss: 0.0211\n",
      "Epoch 14 batch 600 train loss: 0.0029 test loss: 0.0211\n",
      "Epoch 14 batch 700 train loss: 0.0147 test loss: 0.0214\n",
      "Epoch 14 batch 800 train loss: 0.0207 test loss: 0.0212\n",
      "Epoch 14 batch 900 train loss: 0.0107 test loss: 0.0214\n",
      "Epoch 14 batch 1000 train loss: 0.0063 test loss: 0.0214\n",
      "Epoch 14 batch 1100 train loss: 0.0151 test loss: 0.0217\n",
      "Epoch 14 batch 1200 train loss: 0.0171 test loss: 0.0211\n",
      "Epoch 14 batch 1300 train loss: 0.0246 test loss: 0.0210\n",
      "Epoch 14 batch 1400 train loss: 0.0125 test loss: 0.0216\n",
      "Epoch 14 batch 1500 train loss: 0.0070 test loss: 0.0211\n",
      "Epoch 14 batch 1600 train loss: 0.0089 test loss: 0.0211\n",
      "Epoch 14 batch 1700 train loss: 0.0118 test loss: 0.0218\n",
      "Epoch 14 batch 1800 train loss: 0.0122 test loss: 0.0212\n",
      "Epoch 14 batch 1900 train loss: 0.0123 test loss: 0.0214\n",
      "Epoch 14 batch 2000 train loss: 0.0092 test loss: 0.0215\n",
      "Epoch 14 batch 2100 train loss: 0.0126 test loss: 0.0209\n",
      "Epoch 14 batch 2200 train loss: 0.0092 test loss: 0.0214\n",
      "Epoch 14 batch 2300 train loss: 0.0149 test loss: 0.0211\n",
      "Epoch 14 batch 2400 train loss: 0.0098 test loss: 0.0215\n",
      "Epoch 14 batch 2500 train loss: 0.0068 test loss: 0.0212\n",
      "Epoch 14 batch 2600 train loss: 0.0152 test loss: 0.0210\n",
      "Epoch 14 batch 2700 train loss: 0.0069 test loss: 0.0213\n",
      "Epoch 14 batch 2800 train loss: 0.0113 test loss: 0.0211\n",
      "Epoch 14 batch 2900 train loss: 0.0092 test loss: 0.0211\n",
      "Epoch 15 batch 0 train loss: 0.0149 test loss: 0.0214\n",
      "Epoch 15 batch 100 train loss: 0.0133 test loss: 0.0211\n",
      "Epoch 15 batch 200 train loss: 0.0106 test loss: 0.0214\n",
      "Epoch 15 batch 300 train loss: 0.0109 test loss: 0.0216\n",
      "Epoch 15 batch 400 train loss: 0.0076 test loss: 0.0211\n",
      "Epoch 15 batch 500 train loss: 0.0097 test loss: 0.0211\n",
      "Epoch 15 batch 600 train loss: 0.0089 test loss: 0.0214\n",
      "Epoch 15 batch 700 train loss: 0.0115 test loss: 0.0211\n",
      "Epoch 15 batch 800 train loss: 0.0154 test loss: 0.0214\n",
      "Epoch 15 batch 900 train loss: 0.0121 test loss: 0.0213\n",
      "Epoch 15 batch 1000 train loss: 0.0137 test loss: 0.0215\n",
      "Epoch 15 batch 1100 train loss: 0.0070 test loss: 0.0212\n",
      "Epoch 15 batch 1200 train loss: 0.0122 test loss: 0.0211\n",
      "Epoch 15 batch 1300 train loss: 0.0140 test loss: 0.0213\n",
      "Epoch 15 batch 1400 train loss: 0.0097 test loss: 0.0211\n",
      "Epoch 15 batch 1500 train loss: 0.0143 test loss: 0.0213\n",
      "Epoch 15 batch 1600 train loss: 0.0119 test loss: 0.0213\n",
      "Epoch 15 batch 1700 train loss: 0.0090 test loss: 0.0213\n",
      "Epoch 15 batch 1800 train loss: 0.0114 test loss: 0.0210\n",
      "Epoch 15 batch 1900 train loss: 0.0117 test loss: 0.0211\n",
      "Epoch 15 batch 2000 train loss: 0.0092 test loss: 0.0211\n",
      "Epoch 15 batch 2100 train loss: 0.0089 test loss: 0.0213\n",
      "Epoch 15 batch 2200 train loss: 0.0114 test loss: 0.0212\n",
      "Epoch 15 batch 2300 train loss: 0.0146 test loss: 0.0213\n",
      "Epoch 15 batch 2400 train loss: 0.0091 test loss: 0.0211\n",
      "Epoch 15 batch 2500 train loss: 0.0120 test loss: 0.0210\n",
      "Epoch 15 batch 2600 train loss: 0.0121 test loss: 0.0212\n",
      "Epoch 15 batch 2700 train loss: 0.0079 test loss: 0.0213\n",
      "Epoch 15 batch 2800 train loss: 0.0099 test loss: 0.0210\n",
      "Epoch 15 batch 2900 train loss: 0.0207 test loss: 0.0210\n",
      "Epoch 16 batch 0 train loss: 0.0105 test loss: 0.0214\n",
      "Epoch 16 batch 100 train loss: 0.0127 test loss: 0.0215\n",
      "Epoch 16 batch 200 train loss: 0.0185 test loss: 0.0211\n",
      "Epoch 16 batch 300 train loss: 0.0153 test loss: 0.0211\n",
      "Epoch 16 batch 400 train loss: 0.0117 test loss: 0.0216\n",
      "Epoch 16 batch 500 train loss: 0.0140 test loss: 0.0211\n",
      "Epoch 16 batch 600 train loss: 0.0104 test loss: 0.0211\n",
      "Epoch 16 batch 700 train loss: 0.0072 test loss: 0.0212\n",
      "Epoch 16 batch 800 train loss: 0.0249 test loss: 0.0213\n",
      "Epoch 16 batch 900 train loss: 0.0128 test loss: 0.0213\n",
      "Epoch 16 batch 1000 train loss: 0.0124 test loss: 0.0213\n",
      "Epoch 16 batch 1100 train loss: 0.0154 test loss: 0.0215\n",
      "Epoch 16 batch 1200 train loss: 0.0120 test loss: 0.0212\n",
      "Epoch 16 batch 1300 train loss: 0.0228 test loss: 0.0213\n",
      "Epoch 16 batch 1400 train loss: 0.0041 test loss: 0.0214\n",
      "Epoch 16 batch 1500 train loss: 0.0146 test loss: 0.0212\n",
      "Epoch 16 batch 1600 train loss: 0.0160 test loss: 0.0212\n",
      "Epoch 16 batch 1700 train loss: 0.0026 test loss: 0.0215\n",
      "Epoch 16 batch 1800 train loss: 0.0102 test loss: 0.0212\n",
      "Epoch 16 batch 1900 train loss: 0.0144 test loss: 0.0214\n",
      "Epoch 16 batch 2000 train loss: 0.0185 test loss: 0.0210\n",
      "Epoch 16 batch 2100 train loss: 0.0098 test loss: 0.0213\n",
      "Epoch 16 batch 2200 train loss: 0.0142 test loss: 0.0213\n",
      "Epoch 16 batch 2300 train loss: 0.0114 test loss: 0.0213\n",
      "Epoch 16 batch 2400 train loss: 0.0077 test loss: 0.0212\n",
      "Epoch 16 batch 2500 train loss: 0.0126 test loss: 0.0213\n",
      "Epoch 16 batch 2600 train loss: 0.0156 test loss: 0.0214\n",
      "Epoch 16 batch 2700 train loss: 0.0086 test loss: 0.0212\n",
      "Epoch 16 batch 2800 train loss: 0.0148 test loss: 0.0212\n",
      "Epoch 16 batch 2900 train loss: 0.0079 test loss: 0.0213\n",
      "Epoch 17 batch 0 train loss: 0.0169 test loss: 0.0213\n",
      "Epoch 17 batch 100 train loss: 0.0073 test loss: 0.0210\n",
      "Epoch 17 batch 200 train loss: 0.0041 test loss: 0.0214\n",
      "Epoch 17 batch 300 train loss: 0.0146 test loss: 0.0212\n",
      "early stop.\n",
      "Checkpoint 14 restored!!\n",
      "2446/2446 [==============================] - 18s 7ms/step - loss: 0.0145\n",
      "Training for loss rate 0.70 start.\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.70p/ckpt-1\n",
      "Epoch 0 batch 0 train loss: 0.1234 test loss: 0.1263\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.70p/ckpt-2\n",
      "Epoch 0 batch 100 train loss: 0.0257 test loss: 0.0331\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.70p/ckpt-3\n",
      "Epoch 0 batch 200 train loss: 0.0234 test loss: 0.0319\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.70p/ckpt-4\n",
      "Epoch 0 batch 300 train loss: 0.0141 test loss: 0.0298\n",
      "Epoch 0 batch 400 train loss: 0.0263 test loss: 0.0303\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.70p/ckpt-5\n",
      "Epoch 0 batch 500 train loss: 0.0202 test loss: 0.0279\n",
      "Epoch 0 batch 600 train loss: 0.0313 test loss: 0.0284\n",
      "Epoch 0 batch 700 train loss: 0.0296 test loss: 0.0287\n",
      "Epoch 0 batch 800 train loss: 0.0274 test loss: 0.0280\n",
      "Epoch 0 batch 900 train loss: 0.0304 test loss: 0.0285\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.70p/ckpt-6\n",
      "Epoch 0 batch 1000 train loss: 0.0197 test loss: 0.0271\n",
      "Epoch 0 batch 1100 train loss: 0.0264 test loss: 0.0273\n",
      "Epoch 0 batch 1200 train loss: 0.0207 test loss: 0.0284\n",
      "Epoch 0 batch 1300 train loss: 0.0121 test loss: 0.0287\n",
      "Epoch 0 batch 1400 train loss: 0.0138 test loss: 0.0274\n",
      "Epoch 0 batch 1500 train loss: 0.0190 test loss: 0.0275\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.70p/ckpt-7\n",
      "Epoch 0 batch 1600 train loss: 0.0187 test loss: 0.0269\n",
      "Epoch 0 batch 1700 train loss: 0.0196 test loss: 0.0275\n",
      "Epoch 0 batch 1800 train loss: 0.0230 test loss: 0.0272\n",
      "Epoch 0 batch 1900 train loss: 0.0192 test loss: 0.0274\n",
      "Epoch 0 batch 2000 train loss: 0.0256 test loss: 0.0273\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.70p/ckpt-8\n",
      "Epoch 0 batch 2100 train loss: 0.0228 test loss: 0.0269\n",
      "Epoch 0 batch 2200 train loss: 0.0124 test loss: 0.0276\n",
      "Epoch 0 batch 2300 train loss: 0.0191 test loss: 0.0275\n",
      "Epoch 0 batch 2400 train loss: 0.0176 test loss: 0.0275\n",
      "Epoch 0 batch 2500 train loss: 0.0196 test loss: 0.0280\n",
      "Epoch 0 batch 2600 train loss: 0.0214 test loss: 0.0273\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.70p/ckpt-9\n",
      "Epoch 0 batch 2700 train loss: 0.0191 test loss: 0.0266\n",
      "Epoch 0 batch 2800 train loss: 0.0317 test loss: 0.0273\n",
      "Epoch 0 batch 2900 train loss: 0.0243 test loss: 0.0267\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.70p/ckpt-10\n",
      "Epoch 1 batch 0 train loss: 0.0143 test loss: 0.0264\n",
      "Epoch 1 batch 100 train loss: 0.0295 test loss: 0.0268\n",
      "Epoch 1 batch 200 train loss: 0.0212 test loss: 0.0265\n",
      "Epoch 1 batch 300 train loss: 0.0172 test loss: 0.0269\n",
      "Epoch 1 batch 400 train loss: 0.0222 test loss: 0.0279\n",
      "Epoch 1 batch 500 train loss: 0.0204 test loss: 0.0268\n",
      "Epoch 1 batch 600 train loss: 0.0173 test loss: 0.0274\n",
      "Epoch 1 batch 700 train loss: 0.0161 test loss: 0.0272\n",
      "Epoch 1 batch 800 train loss: 0.0217 test loss: 0.0267\n",
      "Epoch 1 batch 900 train loss: 0.0242 test loss: 0.0270\n",
      "Epoch 1 batch 1000 train loss: 0.0176 test loss: 0.0272\n",
      "Epoch 1 batch 1100 train loss: 0.0240 test loss: 0.0267\n",
      "Epoch 1 batch 1200 train loss: 0.0281 test loss: 0.0266\n",
      "Epoch 1 batch 1300 train loss: 0.0216 test loss: 0.0268\n",
      "Epoch 1 batch 1400 train loss: 0.0121 test loss: 0.0267\n",
      "Epoch 1 batch 1500 train loss: 0.0304 test loss: 0.0270\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.70p/ckpt-11\n",
      "Epoch 1 batch 1600 train loss: 0.0192 test loss: 0.0264\n",
      "Epoch 1 batch 1700 train loss: 0.0194 test loss: 0.0277\n",
      "Epoch 1 batch 1800 train loss: 0.0176 test loss: 0.0266\n",
      "Epoch 1 batch 1900 train loss: 0.0193 test loss: 0.0272\n",
      "Epoch 1 batch 2000 train loss: 0.0169 test loss: 0.0270\n",
      "Epoch 1 batch 2100 train loss: 0.0126 test loss: 0.0268\n",
      "Epoch 1 batch 2200 train loss: 0.0196 test loss: 0.0268\n",
      "Epoch 1 batch 2300 train loss: 0.0167 test loss: 0.0273\n",
      "Epoch 1 batch 2400 train loss: 0.0193 test loss: 0.0266\n",
      "Epoch 1 batch 2500 train loss: 0.0220 test loss: 0.0264\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.70p/ckpt-12\n",
      "Epoch 1 batch 2600 train loss: 0.0279 test loss: 0.0263\n",
      "Epoch 1 batch 2700 train loss: 0.0168 test loss: 0.0270\n",
      "Epoch 1 batch 2800 train loss: 0.0273 test loss: 0.0266\n",
      "Epoch 1 batch 2900 train loss: 0.0223 test loss: 0.0264\n",
      "Epoch 2 batch 0 train loss: 0.0192 test loss: 0.0268\n",
      "Epoch 2 batch 100 train loss: 0.0257 test loss: 0.0264\n",
      "Epoch 2 batch 200 train loss: 0.0240 test loss: 0.0266\n",
      "Epoch 2 batch 300 train loss: 0.0108 test loss: 0.0269\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.70p/ckpt-13\n",
      "Epoch 2 batch 400 train loss: 0.0237 test loss: 0.0263\n",
      "Epoch 2 batch 500 train loss: 0.0193 test loss: 0.0268\n",
      "Epoch 2 batch 600 train loss: 0.0233 test loss: 0.0265\n",
      "Epoch 2 batch 700 train loss: 0.0274 test loss: 0.0272\n",
      "Epoch 2 batch 800 train loss: 0.0161 test loss: 0.0267\n",
      "Epoch 2 batch 900 train loss: 0.0197 test loss: 0.0265\n",
      "Epoch 2 batch 1000 train loss: 0.0140 test loss: 0.0269\n",
      "Epoch 2 batch 1100 train loss: 0.0306 test loss: 0.0267\n",
      "Epoch 2 batch 1200 train loss: 0.0242 test loss: 0.0270\n",
      "Epoch 2 batch 1300 train loss: 0.0191 test loss: 0.0270\n",
      "Epoch 2 batch 1400 train loss: 0.0188 test loss: 0.0268\n",
      "Epoch 2 batch 1500 train loss: 0.0183 test loss: 0.0266\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.70p/ckpt-14\n",
      "Epoch 2 batch 1600 train loss: 0.0229 test loss: 0.0262\n",
      "Epoch 2 batch 1700 train loss: 0.0252 test loss: 0.0265\n",
      "Epoch 2 batch 1800 train loss: 0.0223 test loss: 0.0267\n",
      "Epoch 2 batch 1900 train loss: 0.0273 test loss: 0.0267\n",
      "Epoch 2 batch 2000 train loss: 0.0222 test loss: 0.0270\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.70p/ckpt-15\n",
      "Epoch 2 batch 2100 train loss: 0.0166 test loss: 0.0261\n",
      "Epoch 2 batch 2200 train loss: 0.0265 test loss: 0.0264\n",
      "Epoch 2 batch 2300 train loss: 0.0213 test loss: 0.0268\n",
      "Epoch 2 batch 2400 train loss: 0.0210 test loss: 0.0266\n",
      "Epoch 2 batch 2500 train loss: 0.0300 test loss: 0.0262\n",
      "Epoch 2 batch 2600 train loss: 0.0157 test loss: 0.0269\n",
      "Epoch 2 batch 2700 train loss: 0.0197 test loss: 0.0264\n",
      "Epoch 2 batch 2800 train loss: 0.0182 test loss: 0.0268\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.70p/ckpt-16\n",
      "Epoch 2 batch 2900 train loss: 0.0155 test loss: 0.0257\n",
      "Epoch 3 batch 0 train loss: 0.0167 test loss: 0.0263\n",
      "Epoch 3 batch 100 train loss: 0.0089 test loss: 0.0266\n",
      "Epoch 3 batch 200 train loss: 0.0143 test loss: 0.0269\n",
      "Epoch 3 batch 300 train loss: 0.0204 test loss: 0.0264\n",
      "Epoch 3 batch 400 train loss: 0.0182 test loss: 0.0262\n",
      "Epoch 3 batch 500 train loss: 0.0211 test loss: 0.0263\n",
      "Epoch 3 batch 600 train loss: 0.0250 test loss: 0.0265\n",
      "Epoch 3 batch 700 train loss: 0.0273 test loss: 0.0263\n",
      "Epoch 3 batch 800 train loss: 0.0298 test loss: 0.0267\n",
      "Epoch 3 batch 900 train loss: 0.0203 test loss: 0.0273\n",
      "Epoch 3 batch 1000 train loss: 0.0285 test loss: 0.0260\n",
      "Epoch 3 batch 1100 train loss: 0.0198 test loss: 0.0270\n",
      "Epoch 3 batch 1200 train loss: 0.0202 test loss: 0.0264\n",
      "Epoch 3 batch 1300 train loss: 0.0253 test loss: 0.0264\n",
      "Epoch 3 batch 1400 train loss: 0.0261 test loss: 0.0267\n",
      "Epoch 3 batch 1500 train loss: 0.0191 test loss: 0.0266\n",
      "Epoch 3 batch 1600 train loss: 0.0261 test loss: 0.0267\n",
      "Epoch 3 batch 1700 train loss: 0.0177 test loss: 0.0263\n",
      "Epoch 3 batch 1800 train loss: 0.0278 test loss: 0.0267\n",
      "Epoch 3 batch 1900 train loss: 0.0300 test loss: 0.0265\n",
      "Epoch 3 batch 2000 train loss: 0.0146 test loss: 0.0267\n",
      "Epoch 3 batch 2100 train loss: 0.0203 test loss: 0.0262\n",
      "Epoch 3 batch 2200 train loss: 0.0131 test loss: 0.0269\n",
      "Epoch 3 batch 2300 train loss: 0.0238 test loss: 0.0263\n",
      "Epoch 3 batch 2400 train loss: 0.0150 test loss: 0.0263\n",
      "Epoch 3 batch 2500 train loss: 0.0257 test loss: 0.0265\n",
      "Epoch 3 batch 2600 train loss: 0.0165 test loss: 0.0261\n",
      "Epoch 3 batch 2700 train loss: 0.0214 test loss: 0.0271\n",
      "Epoch 3 batch 2800 train loss: 0.0180 test loss: 0.0267\n",
      "Epoch 3 batch 2900 train loss: 0.0236 test loss: 0.0262\n",
      "Epoch 4 batch 0 train loss: 0.0227 test loss: 0.0262\n",
      "Epoch 4 batch 100 train loss: 0.0181 test loss: 0.0264\n",
      "Epoch 4 batch 200 train loss: 0.0237 test loss: 0.0271\n",
      "Epoch 4 batch 300 train loss: 0.0192 test loss: 0.0269\n",
      "Epoch 4 batch 400 train loss: 0.0225 test loss: 0.0268\n",
      "Epoch 4 batch 500 train loss: 0.0194 test loss: 0.0266\n",
      "Epoch 4 batch 600 train loss: 0.0241 test loss: 0.0263\n",
      "Epoch 4 batch 700 train loss: 0.0193 test loss: 0.0268\n",
      "Epoch 4 batch 800 train loss: 0.0272 test loss: 0.0270\n",
      "Epoch 4 batch 900 train loss: 0.0180 test loss: 0.0265\n",
      "Epoch 4 batch 1000 train loss: 0.0211 test loss: 0.0266\n",
      "Epoch 4 batch 1100 train loss: 0.0207 test loss: 0.0263\n",
      "Epoch 4 batch 1200 train loss: 0.0185 test loss: 0.0267\n",
      "Epoch 4 batch 1300 train loss: 0.0197 test loss: 0.0265\n",
      "Epoch 4 batch 1400 train loss: 0.0181 test loss: 0.0263\n",
      "Epoch 4 batch 1500 train loss: 0.0215 test loss: 0.0268\n",
      "Epoch 4 batch 1600 train loss: 0.0252 test loss: 0.0263\n",
      "Epoch 4 batch 1700 train loss: 0.0175 test loss: 0.0265\n",
      "Epoch 4 batch 1800 train loss: 0.0175 test loss: 0.0264\n",
      "Epoch 4 batch 1900 train loss: 0.0276 test loss: 0.0265\n",
      "Epoch 4 batch 2000 train loss: 0.0192 test loss: 0.0263\n",
      "Epoch 4 batch 2100 train loss: 0.0225 test loss: 0.0263\n",
      "Epoch 4 batch 2200 train loss: 0.0210 test loss: 0.0268\n",
      "Epoch 4 batch 2300 train loss: 0.0336 test loss: 0.0268\n",
      "Epoch 4 batch 2400 train loss: 0.0198 test loss: 0.0263\n",
      "Epoch 4 batch 2500 train loss: 0.0119 test loss: 0.0265\n",
      "Epoch 4 batch 2600 train loss: 0.0197 test loss: 0.0263\n",
      "Epoch 4 batch 2700 train loss: 0.0147 test loss: 0.0269\n",
      "Epoch 4 batch 2800 train loss: 0.0294 test loss: 0.0268\n",
      "Epoch 4 batch 2900 train loss: 0.0187 test loss: 0.0267\n",
      "Epoch 5 batch 0 train loss: 0.0192 test loss: 0.0263\n",
      "Epoch 5 batch 100 train loss: 0.0165 test loss: 0.0262\n",
      "Epoch 5 batch 200 train loss: 0.0211 test loss: 0.0267\n",
      "Epoch 5 batch 300 train loss: 0.0228 test loss: 0.0268\n",
      "Epoch 5 batch 400 train loss: 0.0217 test loss: 0.0267\n",
      "Epoch 5 batch 500 train loss: 0.0206 test loss: 0.0265\n",
      "Epoch 5 batch 600 train loss: 0.0212 test loss: 0.0268\n",
      "Epoch 5 batch 700 train loss: 0.0158 test loss: 0.0266\n",
      "Epoch 5 batch 800 train loss: 0.0164 test loss: 0.0267\n",
      "Epoch 5 batch 900 train loss: 0.0213 test loss: 0.0267\n",
      "Epoch 5 batch 1000 train loss: 0.0164 test loss: 0.0270\n",
      "Epoch 5 batch 1100 train loss: 0.0173 test loss: 0.0268\n",
      "Epoch 5 batch 1200 train loss: 0.0142 test loss: 0.0263\n",
      "Epoch 5 batch 1300 train loss: 0.0200 test loss: 0.0271\n",
      "Epoch 5 batch 1400 train loss: 0.0152 test loss: 0.0266\n",
      "Epoch 5 batch 1500 train loss: 0.0150 test loss: 0.0269\n",
      "Epoch 5 batch 1600 train loss: 0.0253 test loss: 0.0262\n",
      "Epoch 5 batch 1700 train loss: 0.0196 test loss: 0.0267\n",
      "Epoch 5 batch 1800 train loss: 0.0182 test loss: 0.0262\n",
      "Epoch 5 batch 1900 train loss: 0.0231 test loss: 0.0269\n",
      "Epoch 5 batch 2000 train loss: 0.0228 test loss: 0.0265\n",
      "Epoch 5 batch 2100 train loss: 0.0165 test loss: 0.0265\n",
      "Epoch 5 batch 2200 train loss: 0.0146 test loss: 0.0270\n",
      "Epoch 5 batch 2300 train loss: 0.0271 test loss: 0.0269\n",
      "Epoch 5 batch 2400 train loss: 0.0222 test loss: 0.0267\n",
      "Epoch 5 batch 2500 train loss: 0.0252 test loss: 0.0262\n",
      "Epoch 5 batch 2600 train loss: 0.0206 test loss: 0.0267\n",
      "Epoch 5 batch 2700 train loss: 0.0200 test loss: 0.0269\n",
      "Epoch 5 batch 2800 train loss: 0.0131 test loss: 0.0267\n",
      "Epoch 5 batch 2900 train loss: 0.0237 test loss: 0.0263\n",
      "Epoch 6 batch 0 train loss: 0.0203 test loss: 0.0265\n",
      "Epoch 6 batch 100 train loss: 0.0179 test loss: 0.0267\n",
      "Epoch 6 batch 200 train loss: 0.0141 test loss: 0.0267\n",
      "Epoch 6 batch 300 train loss: 0.0144 test loss: 0.0267\n",
      "Epoch 6 batch 400 train loss: 0.0213 test loss: 0.0271\n",
      "Epoch 6 batch 500 train loss: 0.0252 test loss: 0.0267\n",
      "Epoch 6 batch 600 train loss: 0.0160 test loss: 0.0266\n",
      "Epoch 6 batch 700 train loss: 0.0237 test loss: 0.0269\n",
      "Epoch 6 batch 800 train loss: 0.0153 test loss: 0.0270\n",
      "Epoch 6 batch 900 train loss: 0.0207 test loss: 0.0267\n",
      "Epoch 6 batch 1000 train loss: 0.0138 test loss: 0.0265\n",
      "Epoch 6 batch 1100 train loss: 0.0182 test loss: 0.0267\n",
      "Epoch 6 batch 1200 train loss: 0.0196 test loss: 0.0269\n",
      "Epoch 6 batch 1300 train loss: 0.0176 test loss: 0.0266\n",
      "Epoch 6 batch 1400 train loss: 0.0208 test loss: 0.0267\n",
      "Epoch 6 batch 1500 train loss: 0.0272 test loss: 0.0265\n",
      "Epoch 6 batch 1600 train loss: 0.0213 test loss: 0.0263\n",
      "Epoch 6 batch 1700 train loss: 0.0303 test loss: 0.0267\n",
      "Epoch 6 batch 1800 train loss: 0.0184 test loss: 0.0268\n",
      "Epoch 6 batch 1900 train loss: 0.0202 test loss: 0.0269\n",
      "Epoch 6 batch 2000 train loss: 0.0196 test loss: 0.0266\n",
      "Epoch 6 batch 2100 train loss: 0.0279 test loss: 0.0268\n",
      "Epoch 6 batch 2200 train loss: 0.0199 test loss: 0.0269\n",
      "Epoch 6 batch 2300 train loss: 0.0179 test loss: 0.0269\n",
      "Epoch 6 batch 2400 train loss: 0.0124 test loss: 0.0266\n",
      "Epoch 6 batch 2500 train loss: 0.0227 test loss: 0.0267\n",
      "Epoch 6 batch 2600 train loss: 0.0138 test loss: 0.0263\n",
      "Epoch 6 batch 2700 train loss: 0.0241 test loss: 0.0267\n",
      "Epoch 6 batch 2800 train loss: 0.0171 test loss: 0.0270\n",
      "Epoch 6 batch 2900 train loss: 0.0152 test loss: 0.0266\n",
      "Epoch 7 batch 0 train loss: 0.0181 test loss: 0.0264\n",
      "Epoch 7 batch 100 train loss: 0.0191 test loss: 0.0266\n",
      "Epoch 7 batch 200 train loss: 0.0193 test loss: 0.0273\n",
      "Epoch 7 batch 300 train loss: 0.0274 test loss: 0.0263\n",
      "Epoch 7 batch 400 train loss: 0.0215 test loss: 0.0271\n",
      "Epoch 7 batch 500 train loss: 0.0252 test loss: 0.0264\n",
      "Epoch 7 batch 600 train loss: 0.0226 test loss: 0.0265\n",
      "Epoch 7 batch 700 train loss: 0.0213 test loss: 0.0267\n",
      "Epoch 7 batch 800 train loss: 0.0248 test loss: 0.0271\n",
      "Epoch 7 batch 900 train loss: 0.0169 test loss: 0.0264\n",
      "Epoch 7 batch 1000 train loss: 0.0123 test loss: 0.0270\n",
      "Epoch 7 batch 1100 train loss: 0.0216 test loss: 0.0265\n",
      "Epoch 7 batch 1200 train loss: 0.0099 test loss: 0.0264\n",
      "Epoch 7 batch 1300 train loss: 0.0174 test loss: 0.0269\n",
      "Epoch 7 batch 1400 train loss: 0.0221 test loss: 0.0266\n",
      "Epoch 7 batch 1500 train loss: 0.0284 test loss: 0.0268\n",
      "Epoch 7 batch 1600 train loss: 0.0170 test loss: 0.0268\n",
      "Epoch 7 batch 1700 train loss: 0.0223 test loss: 0.0270\n",
      "Epoch 7 batch 1800 train loss: 0.0219 test loss: 0.0266\n",
      "Epoch 7 batch 1900 train loss: 0.0190 test loss: 0.0268\n",
      "Epoch 7 batch 2000 train loss: 0.0227 test loss: 0.0271\n",
      "Epoch 7 batch 2100 train loss: 0.0278 test loss: 0.0271\n",
      "Epoch 7 batch 2200 train loss: 0.0185 test loss: 0.0264\n",
      "Epoch 7 batch 2300 train loss: 0.0282 test loss: 0.0265\n",
      "Epoch 7 batch 2400 train loss: 0.0248 test loss: 0.0270\n",
      "Epoch 7 batch 2500 train loss: 0.0211 test loss: 0.0264\n",
      "Epoch 7 batch 2600 train loss: 0.0139 test loss: 0.0268\n",
      "Epoch 7 batch 2700 train loss: 0.0206 test loss: 0.0266\n",
      "Epoch 7 batch 2800 train loss: 0.0135 test loss: 0.0263\n",
      "Epoch 7 batch 2900 train loss: 0.0212 test loss: 0.0268\n",
      "Epoch 8 batch 0 train loss: 0.0296 test loss: 0.0262\n",
      "Epoch 8 batch 100 train loss: 0.0152 test loss: 0.0269\n",
      "Epoch 8 batch 200 train loss: 0.0187 test loss: 0.0266\n",
      "Epoch 8 batch 300 train loss: 0.0222 test loss: 0.0266\n",
      "Epoch 8 batch 400 train loss: 0.0215 test loss: 0.0268\n",
      "Epoch 8 batch 500 train loss: 0.0269 test loss: 0.0266\n",
      "Epoch 8 batch 600 train loss: 0.0174 test loss: 0.0269\n",
      "Epoch 8 batch 700 train loss: 0.0211 test loss: 0.0272\n",
      "Epoch 8 batch 800 train loss: 0.0254 test loss: 0.0267\n",
      "Epoch 8 batch 900 train loss: 0.0246 test loss: 0.0268\n",
      "Epoch 8 batch 1000 train loss: 0.0188 test loss: 0.0271\n",
      "Epoch 8 batch 1100 train loss: 0.0189 test loss: 0.0262\n",
      "Epoch 8 batch 1200 train loss: 0.0174 test loss: 0.0266\n",
      "Epoch 8 batch 1300 train loss: 0.0167 test loss: 0.0268\n",
      "Epoch 8 batch 1400 train loss: 0.0208 test loss: 0.0270\n",
      "Epoch 8 batch 1500 train loss: 0.0230 test loss: 0.0268\n",
      "Epoch 8 batch 1600 train loss: 0.0229 test loss: 0.0265\n",
      "Epoch 8 batch 1700 train loss: 0.0261 test loss: 0.0267\n",
      "Epoch 8 batch 1800 train loss: 0.0147 test loss: 0.0267\n",
      "Epoch 8 batch 1900 train loss: 0.0181 test loss: 0.0268\n",
      "Epoch 8 batch 2000 train loss: 0.0337 test loss: 0.0268\n",
      "Epoch 8 batch 2100 train loss: 0.0177 test loss: 0.0267\n",
      "Epoch 8 batch 2200 train loss: 0.0252 test loss: 0.0268\n",
      "Epoch 8 batch 2300 train loss: 0.0281 test loss: 0.0266\n",
      "Epoch 8 batch 2400 train loss: 0.0198 test loss: 0.0264\n",
      "Epoch 8 batch 2500 train loss: 0.0171 test loss: 0.0270\n",
      "Epoch 8 batch 2600 train loss: 0.0196 test loss: 0.0270\n",
      "Epoch 8 batch 2700 train loss: 0.0164 test loss: 0.0272\n",
      "Epoch 8 batch 2800 train loss: 0.0168 test loss: 0.0265\n",
      "Epoch 8 batch 2900 train loss: 0.0140 test loss: 0.0267\n",
      "Epoch 9 batch 0 train loss: 0.0174 test loss: 0.0267\n",
      "Epoch 9 batch 100 train loss: 0.0272 test loss: 0.0264\n",
      "Epoch 9 batch 200 train loss: 0.0163 test loss: 0.0268\n",
      "Epoch 9 batch 300 train loss: 0.0155 test loss: 0.0265\n",
      "Epoch 9 batch 400 train loss: 0.0254 test loss: 0.0267\n",
      "Epoch 9 batch 500 train loss: 0.0118 test loss: 0.0267\n",
      "Epoch 9 batch 600 train loss: 0.0161 test loss: 0.0265\n",
      "Epoch 9 batch 700 train loss: 0.0087 test loss: 0.0269\n",
      "Epoch 9 batch 800 train loss: 0.0149 test loss: 0.0273\n",
      "Epoch 9 batch 900 train loss: 0.0160 test loss: 0.0270\n",
      "Epoch 9 batch 1000 train loss: 0.0167 test loss: 0.0268\n",
      "Epoch 9 batch 1100 train loss: 0.0144 test loss: 0.0265\n",
      "Epoch 9 batch 1200 train loss: 0.0236 test loss: 0.0264\n",
      "Epoch 9 batch 1300 train loss: 0.0206 test loss: 0.0270\n",
      "Epoch 9 batch 1400 train loss: 0.0266 test loss: 0.0266\n",
      "Epoch 9 batch 1500 train loss: 0.0236 test loss: 0.0269\n",
      "Epoch 9 batch 1600 train loss: 0.0271 test loss: 0.0265\n",
      "Epoch 9 batch 1700 train loss: 0.0212 test loss: 0.0267\n",
      "Epoch 9 batch 1800 train loss: 0.0193 test loss: 0.0265\n",
      "Epoch 9 batch 1900 train loss: 0.0196 test loss: 0.0274\n",
      "Epoch 9 batch 2000 train loss: 0.0252 test loss: 0.0268\n",
      "Epoch 9 batch 2100 train loss: 0.0289 test loss: 0.0263\n",
      "Epoch 9 batch 2200 train loss: 0.0329 test loss: 0.0267\n",
      "Epoch 9 batch 2300 train loss: 0.0332 test loss: 0.0267\n",
      "Epoch 9 batch 2400 train loss: 0.0204 test loss: 0.0268\n",
      "Epoch 9 batch 2500 train loss: 0.0197 test loss: 0.0265\n",
      "Epoch 9 batch 2600 train loss: 0.0188 test loss: 0.0268\n",
      "Epoch 9 batch 2700 train loss: 0.0256 test loss: 0.0269\n",
      "Epoch 9 batch 2800 train loss: 0.0166 test loss: 0.0263\n",
      "Epoch 9 batch 2900 train loss: 0.0259 test loss: 0.0267\n",
      "Epoch 10 batch 0 train loss: 0.0197 test loss: 0.0264\n",
      "Epoch 10 batch 100 train loss: 0.0150 test loss: 0.0263\n",
      "Epoch 10 batch 200 train loss: 0.0160 test loss: 0.0271\n",
      "Epoch 10 batch 300 train loss: 0.0191 test loss: 0.0267\n",
      "Epoch 10 batch 400 train loss: 0.0173 test loss: 0.0263\n",
      "Epoch 10 batch 500 train loss: 0.0206 test loss: 0.0266\n",
      "Epoch 10 batch 600 train loss: 0.0136 test loss: 0.0265\n",
      "Epoch 10 batch 700 train loss: 0.0159 test loss: 0.0267\n",
      "Epoch 10 batch 800 train loss: 0.0208 test loss: 0.0271\n",
      "Epoch 10 batch 900 train loss: 0.0179 test loss: 0.0267\n",
      "Epoch 10 batch 1000 train loss: 0.0142 test loss: 0.0270\n",
      "Epoch 10 batch 1100 train loss: 0.0167 test loss: 0.0266\n",
      "Epoch 10 batch 1200 train loss: 0.0271 test loss: 0.0262\n",
      "Epoch 10 batch 1300 train loss: 0.0247 test loss: 0.0268\n",
      "Epoch 10 batch 1400 train loss: 0.0221 test loss: 0.0267\n",
      "Epoch 10 batch 1500 train loss: 0.0198 test loss: 0.0270\n",
      "Epoch 10 batch 1600 train loss: 0.0231 test loss: 0.0265\n",
      "Epoch 10 batch 1700 train loss: 0.0233 test loss: 0.0270\n",
      "Epoch 10 batch 1800 train loss: 0.0209 test loss: 0.0269\n",
      "Epoch 10 batch 1900 train loss: 0.0212 test loss: 0.0269\n",
      "Epoch 10 batch 2000 train loss: 0.0251 test loss: 0.0268\n",
      "Epoch 10 batch 2100 train loss: 0.0135 test loss: 0.0268\n",
      "Epoch 10 batch 2200 train loss: 0.0213 test loss: 0.0265\n",
      "Epoch 10 batch 2300 train loss: 0.0230 test loss: 0.0268\n",
      "Epoch 10 batch 2400 train loss: 0.0237 test loss: 0.0271\n",
      "Epoch 10 batch 2500 train loss: 0.0145 test loss: 0.0263\n",
      "Epoch 10 batch 2600 train loss: 0.0134 test loss: 0.0267\n",
      "Epoch 10 batch 2700 train loss: 0.0226 test loss: 0.0268\n",
      "Epoch 10 batch 2800 train loss: 0.0240 test loss: 0.0267\n",
      "Epoch 10 batch 2900 train loss: 0.0159 test loss: 0.0268\n",
      "Epoch 11 batch 0 train loss: 0.0188 test loss: 0.0262\n",
      "Epoch 11 batch 100 train loss: 0.0123 test loss: 0.0265\n",
      "Epoch 11 batch 200 train loss: 0.0174 test loss: 0.0267\n",
      "Epoch 11 batch 300 train loss: 0.0100 test loss: 0.0268\n",
      "Epoch 11 batch 400 train loss: 0.0210 test loss: 0.0267\n",
      "Epoch 11 batch 500 train loss: 0.0171 test loss: 0.0263\n",
      "Epoch 11 batch 600 train loss: 0.0222 test loss: 0.0267\n",
      "Epoch 11 batch 700 train loss: 0.0301 test loss: 0.0267\n",
      "Epoch 11 batch 800 train loss: 0.0177 test loss: 0.0265\n",
      "Epoch 11 batch 900 train loss: 0.0169 test loss: 0.0268\n",
      "Epoch 11 batch 1000 train loss: 0.0215 test loss: 0.0267\n",
      "Epoch 11 batch 1100 train loss: 0.0157 test loss: 0.0265\n",
      "Epoch 11 batch 1200 train loss: 0.0312 test loss: 0.0266\n",
      "Epoch 11 batch 1300 train loss: 0.0109 test loss: 0.0267\n",
      "Epoch 11 batch 1400 train loss: 0.0178 test loss: 0.0265\n",
      "Epoch 11 batch 1500 train loss: 0.0182 test loss: 0.0272\n",
      "Epoch 11 batch 1600 train loss: 0.0113 test loss: 0.0266\n",
      "Epoch 11 batch 1700 train loss: 0.0147 test loss: 0.0269\n",
      "Epoch 11 batch 1800 train loss: 0.0228 test loss: 0.0269\n",
      "Epoch 11 batch 1900 train loss: 0.0213 test loss: 0.0267\n",
      "Epoch 11 batch 2000 train loss: 0.0107 test loss: 0.0268\n",
      "Epoch 11 batch 2100 train loss: 0.0175 test loss: 0.0268\n",
      "Epoch 11 batch 2200 train loss: 0.0273 test loss: 0.0271\n",
      "Epoch 11 batch 2300 train loss: 0.0230 test loss: 0.0269\n",
      "Epoch 11 batch 2400 train loss: 0.0118 test loss: 0.0268\n",
      "Epoch 11 batch 2500 train loss: 0.0158 test loss: 0.0266\n",
      "Epoch 11 batch 2600 train loss: 0.0161 test loss: 0.0266\n",
      "Epoch 11 batch 2700 train loss: 0.0285 test loss: 0.0267\n",
      "Epoch 11 batch 2800 train loss: 0.0195 test loss: 0.0266\n",
      "Epoch 11 batch 2900 train loss: 0.0262 test loss: 0.0270\n",
      "Epoch 12 batch 0 train loss: 0.0170 test loss: 0.0263\n",
      "Epoch 12 batch 100 train loss: 0.0229 test loss: 0.0268\n",
      "Epoch 12 batch 200 train loss: 0.0119 test loss: 0.0271\n",
      "Epoch 12 batch 300 train loss: 0.0209 test loss: 0.0268\n",
      "Epoch 12 batch 400 train loss: 0.0270 test loss: 0.0266\n",
      "Epoch 12 batch 500 train loss: 0.0187 test loss: 0.0264\n",
      "Epoch 12 batch 600 train loss: 0.0149 test loss: 0.0269\n",
      "Epoch 12 batch 700 train loss: 0.0184 test loss: 0.0267\n",
      "Epoch 12 batch 800 train loss: 0.0234 test loss: 0.0272\n",
      "Epoch 12 batch 900 train loss: 0.0314 test loss: 0.0265\n",
      "Epoch 12 batch 1000 train loss: 0.0198 test loss: 0.0267\n",
      "Epoch 12 batch 1100 train loss: 0.0172 test loss: 0.0264\n",
      "Epoch 12 batch 1200 train loss: 0.0220 test loss: 0.0267\n",
      "Epoch 12 batch 1300 train loss: 0.0208 test loss: 0.0270\n",
      "Epoch 12 batch 1400 train loss: 0.0168 test loss: 0.0267\n",
      "Epoch 12 batch 1500 train loss: 0.0157 test loss: 0.0266\n",
      "Epoch 12 batch 1600 train loss: 0.0183 test loss: 0.0265\n",
      "Epoch 12 batch 1700 train loss: 0.0202 test loss: 0.0268\n",
      "Epoch 12 batch 1800 train loss: 0.0101 test loss: 0.0266\n",
      "Epoch 12 batch 1900 train loss: 0.0224 test loss: 0.0271\n",
      "Epoch 12 batch 2000 train loss: 0.0275 test loss: 0.0262\n",
      "Epoch 12 batch 2100 train loss: 0.0225 test loss: 0.0268\n",
      "Epoch 12 batch 2200 train loss: 0.0125 test loss: 0.0271\n",
      "Epoch 12 batch 2300 train loss: 0.0222 test loss: 0.0267\n",
      "Epoch 12 batch 2400 train loss: 0.0192 test loss: 0.0268\n",
      "Epoch 12 batch 2500 train loss: 0.0154 test loss: 0.0267\n",
      "Epoch 12 batch 2600 train loss: 0.0248 test loss: 0.0270\n",
      "Epoch 12 batch 2700 train loss: 0.0110 test loss: 0.0264\n",
      "Epoch 12 batch 2800 train loss: 0.0173 test loss: 0.0265\n",
      "Epoch 12 batch 2900 train loss: 0.0089 test loss: 0.0263\n",
      "Epoch 13 batch 0 train loss: 0.0168 test loss: 0.0269\n",
      "Epoch 13 batch 100 train loss: 0.0299 test loss: 0.0268\n",
      "Epoch 13 batch 200 train loss: 0.0178 test loss: 0.0268\n",
      "Epoch 13 batch 300 train loss: 0.0075 test loss: 0.0267\n",
      "Epoch 13 batch 400 train loss: 0.0166 test loss: 0.0273\n",
      "Epoch 13 batch 500 train loss: 0.0323 test loss: 0.0267\n",
      "Epoch 13 batch 600 train loss: 0.0189 test loss: 0.0265\n",
      "Epoch 13 batch 700 train loss: 0.0167 test loss: 0.0270\n",
      "Epoch 13 batch 800 train loss: 0.0230 test loss: 0.0267\n",
      "Epoch 13 batch 900 train loss: 0.0174 test loss: 0.0265\n",
      "Epoch 13 batch 1000 train loss: 0.0292 test loss: 0.0268\n",
      "Epoch 13 batch 1100 train loss: 0.0213 test loss: 0.0265\n",
      "Epoch 13 batch 1200 train loss: 0.0247 test loss: 0.0263\n",
      "Epoch 13 batch 1300 train loss: 0.0089 test loss: 0.0271\n",
      "Epoch 13 batch 1400 train loss: 0.0187 test loss: 0.0268\n",
      "Epoch 13 batch 1500 train loss: 0.0125 test loss: 0.0272\n",
      "Epoch 13 batch 1600 train loss: 0.0168 test loss: 0.0266\n",
      "Epoch 13 batch 1700 train loss: 0.0189 test loss: 0.0269\n",
      "Epoch 13 batch 1800 train loss: 0.0191 test loss: 0.0270\n",
      "Epoch 13 batch 1900 train loss: 0.0173 test loss: 0.0269\n",
      "Epoch 13 batch 2000 train loss: 0.0302 test loss: 0.0267\n",
      "Epoch 13 batch 2100 train loss: 0.0181 test loss: 0.0267\n",
      "Epoch 13 batch 2200 train loss: 0.0215 test loss: 0.0269\n",
      "Epoch 13 batch 2300 train loss: 0.0236 test loss: 0.0269\n",
      "Epoch 13 batch 2400 train loss: 0.0198 test loss: 0.0267\n",
      "Epoch 13 batch 2500 train loss: 0.0249 test loss: 0.0266\n",
      "Epoch 13 batch 2600 train loss: 0.0248 test loss: 0.0270\n",
      "Epoch 13 batch 2700 train loss: 0.0144 test loss: 0.0271\n",
      "Epoch 13 batch 2800 train loss: 0.0175 test loss: 0.0264\n",
      "Epoch 13 batch 2900 train loss: 0.0234 test loss: 0.0268\n",
      "Epoch 14 batch 0 train loss: 0.0228 test loss: 0.0266\n",
      "Epoch 14 batch 100 train loss: 0.0186 test loss: 0.0264\n",
      "Epoch 14 batch 200 train loss: 0.0137 test loss: 0.0267\n",
      "Epoch 14 batch 300 train loss: 0.0218 test loss: 0.0269\n",
      "Epoch 14 batch 400 train loss: 0.0181 test loss: 0.0269\n",
      "Epoch 14 batch 500 train loss: 0.0149 test loss: 0.0264\n",
      "Epoch 14 batch 600 train loss: 0.0261 test loss: 0.0266\n",
      "Epoch 14 batch 700 train loss: 0.0151 test loss: 0.0267\n",
      "Epoch 14 batch 800 train loss: 0.0204 test loss: 0.0268\n",
      "Epoch 14 batch 900 train loss: 0.0157 test loss: 0.0263\n",
      "Epoch 14 batch 1000 train loss: 0.0176 test loss: 0.0264\n",
      "Epoch 14 batch 1100 train loss: 0.0281 test loss: 0.0270\n",
      "Epoch 14 batch 1200 train loss: 0.0212 test loss: 0.0269\n",
      "Epoch 14 batch 1300 train loss: 0.0254 test loss: 0.0269\n",
      "Epoch 14 batch 1400 train loss: 0.0114 test loss: 0.0265\n",
      "Epoch 14 batch 1500 train loss: 0.0190 test loss: 0.0269\n",
      "Epoch 14 batch 1600 train loss: 0.0226 test loss: 0.0268\n",
      "Epoch 14 batch 1700 train loss: 0.0212 test loss: 0.0272\n",
      "Epoch 14 batch 1800 train loss: 0.0265 test loss: 0.0267\n",
      "Epoch 14 batch 1900 train loss: 0.0235 test loss: 0.0264\n",
      "Epoch 14 batch 2000 train loss: 0.0250 test loss: 0.0267\n",
      "Epoch 14 batch 2100 train loss: 0.0212 test loss: 0.0265\n",
      "Epoch 14 batch 2200 train loss: 0.0149 test loss: 0.0270\n",
      "Epoch 14 batch 2300 train loss: 0.0292 test loss: 0.0266\n",
      "Epoch 14 batch 2400 train loss: 0.0202 test loss: 0.0266\n",
      "Epoch 14 batch 2500 train loss: 0.0151 test loss: 0.0265\n",
      "Epoch 14 batch 2600 train loss: 0.0175 test loss: 0.0265\n",
      "Epoch 14 batch 2700 train loss: 0.0126 test loss: 0.0269\n",
      "Epoch 14 batch 2800 train loss: 0.0197 test loss: 0.0264\n",
      "Epoch 14 batch 2900 train loss: 0.0231 test loss: 0.0264\n",
      "Epoch 15 batch 0 train loss: 0.0146 test loss: 0.0271\n",
      "Epoch 15 batch 100 train loss: 0.0203 test loss: 0.0265\n",
      "Epoch 15 batch 200 train loss: 0.0107 test loss: 0.0268\n",
      "Epoch 15 batch 300 train loss: 0.0153 test loss: 0.0266\n",
      "Epoch 15 batch 400 train loss: 0.0187 test loss: 0.0270\n",
      "Epoch 15 batch 500 train loss: 0.0224 test loss: 0.0265\n",
      "Epoch 15 batch 600 train loss: 0.0222 test loss: 0.0267\n",
      "Epoch 15 batch 700 train loss: 0.0191 test loss: 0.0265\n",
      "Epoch 15 batch 800 train loss: 0.0107 test loss: 0.0266\n",
      "Epoch 15 batch 900 train loss: 0.0206 test loss: 0.0268\n",
      "Epoch 15 batch 1000 train loss: 0.0269 test loss: 0.0269\n",
      "Epoch 15 batch 1100 train loss: 0.0162 test loss: 0.0267\n",
      "Epoch 15 batch 1200 train loss: 0.0127 test loss: 0.0263\n",
      "Epoch 15 batch 1300 train loss: 0.0167 test loss: 0.0267\n",
      "Epoch 15 batch 1400 train loss: 0.0212 test loss: 0.0270\n",
      "Epoch 15 batch 1500 train loss: 0.0177 test loss: 0.0268\n",
      "Epoch 15 batch 1600 train loss: 0.0267 test loss: 0.0266\n",
      "Epoch 15 batch 1700 train loss: 0.0197 test loss: 0.0265\n",
      "Epoch 15 batch 1800 train loss: 0.0141 test loss: 0.0267\n",
      "Epoch 15 batch 1900 train loss: 0.0201 test loss: 0.0267\n",
      "Epoch 15 batch 2000 train loss: 0.0224 test loss: 0.0268\n",
      "Epoch 15 batch 2100 train loss: 0.0303 test loss: 0.0267\n",
      "Epoch 15 batch 2200 train loss: 0.0184 test loss: 0.0267\n",
      "Epoch 15 batch 2300 train loss: 0.0184 test loss: 0.0264\n",
      "Epoch 15 batch 2400 train loss: 0.0165 test loss: 0.0270\n",
      "Epoch 15 batch 2500 train loss: 0.0186 test loss: 0.0268\n",
      "Epoch 15 batch 2600 train loss: 0.0095 test loss: 0.0265\n",
      "Epoch 15 batch 2700 train loss: 0.0248 test loss: 0.0267\n",
      "Epoch 15 batch 2800 train loss: 0.0168 test loss: 0.0265\n",
      "Epoch 15 batch 2900 train loss: 0.0219 test loss: 0.0268\n",
      "Epoch 16 batch 0 train loss: 0.0174 test loss: 0.0265\n",
      "Epoch 16 batch 100 train loss: 0.0277 test loss: 0.0270\n",
      "Epoch 16 batch 200 train loss: 0.0216 test loss: 0.0266\n",
      "Epoch 16 batch 300 train loss: 0.0303 test loss: 0.0264\n",
      "Epoch 16 batch 400 train loss: 0.0378 test loss: 0.0269\n",
      "Epoch 16 batch 500 train loss: 0.0228 test loss: 0.0265\n",
      "Epoch 16 batch 600 train loss: 0.0243 test loss: 0.0270\n",
      "Epoch 16 batch 700 train loss: 0.0201 test loss: 0.0269\n",
      "Epoch 16 batch 800 train loss: 0.0205 test loss: 0.0269\n",
      "Epoch 16 batch 900 train loss: 0.0184 test loss: 0.0267\n",
      "Epoch 16 batch 1000 train loss: 0.0211 test loss: 0.0266\n",
      "Epoch 16 batch 1100 train loss: 0.0143 test loss: 0.0270\n",
      "Epoch 16 batch 1200 train loss: 0.0216 test loss: 0.0267\n",
      "Epoch 16 batch 1300 train loss: 0.0208 test loss: 0.0265\n",
      "Epoch 16 batch 1400 train loss: 0.0148 test loss: 0.0264\n",
      "Epoch 16 batch 1500 train loss: 0.0156 test loss: 0.0270\n",
      "Epoch 16 batch 1600 train loss: 0.0175 test loss: 0.0264\n",
      "Epoch 16 batch 1700 train loss: 0.0279 test loss: 0.0268\n",
      "Epoch 16 batch 1800 train loss: 0.0090 test loss: 0.0265\n",
      "Epoch 16 batch 1900 train loss: 0.0196 test loss: 0.0271\n",
      "Epoch 16 batch 2000 train loss: 0.0177 test loss: 0.0268\n",
      "Epoch 16 batch 2100 train loss: 0.0170 test loss: 0.0265\n",
      "Epoch 16 batch 2200 train loss: 0.0270 test loss: 0.0268\n",
      "Epoch 16 batch 2300 train loss: 0.0185 test loss: 0.0265\n",
      "Epoch 16 batch 2400 train loss: 0.0208 test loss: 0.0267\n",
      "Epoch 16 batch 2500 train loss: 0.0202 test loss: 0.0267\n",
      "Epoch 16 batch 2600 train loss: 0.0195 test loss: 0.0270\n",
      "Epoch 16 batch 2700 train loss: 0.0155 test loss: 0.0266\n",
      "Epoch 16 batch 2800 train loss: 0.0200 test loss: 0.0270\n",
      "Epoch 16 batch 2900 train loss: 0.0147 test loss: 0.0263\n",
      "Epoch 17 batch 0 train loss: 0.0196 test loss: 0.0267\n",
      "Epoch 17 batch 100 train loss: 0.0141 test loss: 0.0266\n",
      "Epoch 17 batch 200 train loss: 0.0146 test loss: 0.0271\n",
      "Epoch 17 batch 300 train loss: 0.0230 test loss: 0.0265\n",
      "Epoch 17 batch 400 train loss: 0.0247 test loss: 0.0267\n",
      "Epoch 17 batch 500 train loss: 0.0213 test loss: 0.0268\n",
      "early stop.\n",
      "Checkpoint 16 restored!!\n",
      "2446/2446 [==============================] - 15s 6ms/step - loss: 0.0223\n",
      "Training for loss rate 0.80 start.\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.80p/ckpt-1\n",
      "Epoch 0 batch 0 train loss: 0.1098 test loss: 0.1421\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.80p/ckpt-2\n",
      "Epoch 0 batch 100 train loss: 0.0406 test loss: 0.0524\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.80p/ckpt-3\n",
      "Epoch 0 batch 200 train loss: 0.0346 test loss: 0.0491\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.80p/ckpt-4\n",
      "Epoch 0 batch 300 train loss: 0.0313 test loss: 0.0489\n",
      "Epoch 0 batch 400 train loss: 0.0370 test loss: 0.0508\n",
      "Epoch 0 batch 500 train loss: 0.0351 test loss: 0.0497\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.80p/ckpt-5\n",
      "Epoch 0 batch 600 train loss: 0.0308 test loss: 0.0476\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.80p/ckpt-6\n",
      "Epoch 0 batch 700 train loss: 0.0250 test loss: 0.0470\n",
      "Epoch 0 batch 800 train loss: 0.0366 test loss: 0.0489\n",
      "Epoch 0 batch 900 train loss: 0.0341 test loss: 0.0471\n",
      "Epoch 0 batch 1000 train loss: 0.0380 test loss: 0.0486\n",
      "Epoch 0 batch 1100 train loss: 0.0261 test loss: 0.0471\n",
      "Epoch 0 batch 1200 train loss: 0.0370 test loss: 0.0481\n",
      "Epoch 0 batch 1300 train loss: 0.0252 test loss: 0.0472\n",
      "Epoch 0 batch 1400 train loss: 0.0331 test loss: 0.0477\n",
      "Epoch 0 batch 1500 train loss: 0.0294 test loss: 0.0474\n",
      "Epoch 0 batch 1600 train loss: 0.0297 test loss: 0.0478\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.80p/ckpt-7\n",
      "Epoch 0 batch 1700 train loss: 0.0230 test loss: 0.0469\n",
      "Epoch 0 batch 1800 train loss: 0.0358 test loss: 0.0472\n",
      "Epoch 0 batch 1900 train loss: 0.0338 test loss: 0.0477\n",
      "Epoch 0 batch 2000 train loss: 0.0377 test loss: 0.0473\n",
      "Epoch 0 batch 2100 train loss: 0.0368 test loss: 0.0496\n",
      "Epoch 0 batch 2200 train loss: 0.0353 test loss: 0.0482\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.80p/ckpt-8\n",
      "Epoch 0 batch 2300 train loss: 0.0335 test loss: 0.0468\n",
      "Epoch 0 batch 2400 train loss: 0.0293 test loss: 0.0476\n",
      "Epoch 0 batch 2500 train loss: 0.0314 test loss: 0.0479\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.80p/ckpt-9\n",
      "Epoch 0 batch 2600 train loss: 0.0285 test loss: 0.0465\n",
      "Epoch 0 batch 2700 train loss: 0.0169 test loss: 0.0468\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.80p/ckpt-10\n",
      "Epoch 0 batch 2800 train loss: 0.0262 test loss: 0.0462\n",
      "Epoch 0 batch 2900 train loss: 0.0299 test loss: 0.0473\n",
      "Epoch 1 batch 0 train loss: 0.0302 test loss: 0.0464\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.80p/ckpt-11\n",
      "Epoch 1 batch 100 train loss: 0.0284 test loss: 0.0462\n",
      "Epoch 1 batch 200 train loss: 0.0292 test loss: 0.0465\n",
      "Epoch 1 batch 300 train loss: 0.0223 test loss: 0.0467\n",
      "Epoch 1 batch 400 train loss: 0.0322 test loss: 0.0472\n",
      "Epoch 1 batch 500 train loss: 0.0331 test loss: 0.0465\n",
      "Epoch 1 batch 600 train loss: 0.0397 test loss: 0.0466\n",
      "Epoch 1 batch 700 train loss: 0.0252 test loss: 0.0474\n",
      "Epoch 1 batch 800 train loss: 0.0352 test loss: 0.0470\n",
      "Epoch 1 batch 900 train loss: 0.0293 test loss: 0.0464\n",
      "Epoch 1 batch 1000 train loss: 0.0346 test loss: 0.0478\n",
      "Epoch 1 batch 1100 train loss: 0.0265 test loss: 0.0470\n",
      "Epoch 1 batch 1200 train loss: 0.0188 test loss: 0.0462\n",
      "Epoch 1 batch 1300 train loss: 0.0349 test loss: 0.0480\n",
      "Epoch 1 batch 1400 train loss: 0.0297 test loss: 0.0465\n",
      "Epoch 1 batch 1500 train loss: 0.0273 test loss: 0.0471\n",
      "Epoch 1 batch 1600 train loss: 0.0406 test loss: 0.0464\n",
      "Epoch 1 batch 1700 train loss: 0.0244 test loss: 0.0476\n",
      "Epoch 1 batch 1800 train loss: 0.0312 test loss: 0.0470\n",
      "Epoch 1 batch 1900 train loss: 0.0448 test loss: 0.0466\n",
      "Epoch 1 batch 2000 train loss: 0.0375 test loss: 0.0466\n",
      "Epoch 1 batch 2100 train loss: 0.0296 test loss: 0.0469\n",
      "Epoch 1 batch 2200 train loss: 0.0311 test loss: 0.0480\n",
      "Epoch 1 batch 2300 train loss: 0.0316 test loss: 0.0463\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.80p/ckpt-12\n",
      "Epoch 1 batch 2400 train loss: 0.0329 test loss: 0.0461\n",
      "Epoch 1 batch 2500 train loss: 0.0295 test loss: 0.0466\n",
      "Epoch 1 batch 2600 train loss: 0.0309 test loss: 0.0465\n",
      "Epoch 1 batch 2700 train loss: 0.0274 test loss: 0.0469\n",
      "Epoch 1 batch 2800 train loss: 0.0333 test loss: 0.0470\n",
      "Epoch 1 batch 2900 train loss: 0.0291 test loss: 0.0463\n",
      "Epoch 2 batch 0 train loss: 0.0316 test loss: 0.0466\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.80p/ckpt-13\n",
      "Epoch 2 batch 100 train loss: 0.0265 test loss: 0.0459\n",
      "Epoch 2 batch 200 train loss: 0.0426 test loss: 0.0475\n",
      "Epoch 2 batch 300 train loss: 0.0248 test loss: 0.0460\n",
      "Epoch 2 batch 400 train loss: 0.0307 test loss: 0.0465\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.80p/ckpt-14\n",
      "Epoch 2 batch 500 train loss: 0.0280 test loss: 0.0457\n",
      "Epoch 2 batch 600 train loss: 0.0339 test loss: 0.0465\n",
      "Epoch 2 batch 700 train loss: 0.0344 test loss: 0.0469\n",
      "Epoch 2 batch 800 train loss: 0.0283 test loss: 0.0466\n",
      "Epoch 2 batch 900 train loss: 0.0376 test loss: 0.0468\n",
      "Epoch 2 batch 1000 train loss: 0.0357 test loss: 0.0469\n",
      "Epoch 2 batch 1100 train loss: 0.0306 test loss: 0.0466\n",
      "Epoch 2 batch 1200 train loss: 0.0427 test loss: 0.0469\n",
      "Epoch 2 batch 1300 train loss: 0.0281 test loss: 0.0463\n",
      "Epoch 2 batch 1400 train loss: 0.0294 test loss: 0.0469\n",
      "Epoch 2 batch 1500 train loss: 0.0283 test loss: 0.0473\n",
      "Epoch 2 batch 1600 train loss: 0.0342 test loss: 0.0470\n",
      "Epoch 2 batch 1700 train loss: 0.0324 test loss: 0.0469\n",
      "Epoch 2 batch 1800 train loss: 0.0337 test loss: 0.0464\n",
      "Epoch 2 batch 1900 train loss: 0.0359 test loss: 0.0471\n",
      "Epoch 2 batch 2000 train loss: 0.0349 test loss: 0.0470\n",
      "Epoch 2 batch 2100 train loss: 0.0411 test loss: 0.0476\n",
      "Epoch 2 batch 2200 train loss: 0.0255 test loss: 0.0476\n",
      "Epoch 2 batch 2300 train loss: 0.0298 test loss: 0.0471\n",
      "Epoch 2 batch 2400 train loss: 0.0338 test loss: 0.0473\n",
      "Epoch 2 batch 2500 train loss: 0.0269 test loss: 0.0471\n",
      "Epoch 2 batch 2600 train loss: 0.0303 test loss: 0.0470\n",
      "Epoch 2 batch 2700 train loss: 0.0272 test loss: 0.0462\n",
      "Epoch 2 batch 2800 train loss: 0.0245 test loss: 0.0468\n",
      "Epoch 2 batch 2900 train loss: 0.0299 test loss: 0.0466\n",
      "Epoch 3 batch 0 train loss: 0.0416 test loss: 0.0460\n",
      "Epoch 3 batch 100 train loss: 0.0304 test loss: 0.0468\n",
      "Epoch 3 batch 200 train loss: 0.0289 test loss: 0.0463\n",
      "Epoch 3 batch 300 train loss: 0.0257 test loss: 0.0464\n",
      "Epoch 3 batch 400 train loss: 0.0244 test loss: 0.0467\n",
      "Epoch 3 batch 500 train loss: 0.0243 test loss: 0.0467\n",
      "Epoch 3 batch 600 train loss: 0.0294 test loss: 0.0470\n",
      "Epoch 3 batch 700 train loss: 0.0314 test loss: 0.0472\n",
      "Epoch 3 batch 800 train loss: 0.0183 test loss: 0.0473\n",
      "Epoch 3 batch 900 train loss: 0.0305 test loss: 0.0466\n",
      "Epoch 3 batch 1000 train loss: 0.0203 test loss: 0.0475\n",
      "Epoch 3 batch 1100 train loss: 0.0177 test loss: 0.0469\n",
      "Epoch 3 batch 1200 train loss: 0.0283 test loss: 0.0467\n",
      "Epoch 3 batch 1300 train loss: 0.0228 test loss: 0.0474\n",
      "Epoch 3 batch 1400 train loss: 0.0166 test loss: 0.0470\n",
      "Epoch 3 batch 1500 train loss: 0.0290 test loss: 0.0466\n",
      "Epoch 3 batch 1600 train loss: 0.0349 test loss: 0.0466\n",
      "Epoch 3 batch 1700 train loss: 0.0454 test loss: 0.0465\n",
      "Epoch 3 batch 1800 train loss: 0.0253 test loss: 0.0469\n",
      "Epoch 3 batch 1900 train loss: 0.0234 test loss: 0.0467\n",
      "Epoch 3 batch 2000 train loss: 0.0386 test loss: 0.0465\n",
      "Epoch 3 batch 2100 train loss: 0.0289 test loss: 0.0465\n",
      "Epoch 3 batch 2200 train loss: 0.0312 test loss: 0.0468\n",
      "Epoch 3 batch 2300 train loss: 0.0309 test loss: 0.0471\n",
      "Epoch 3 batch 2400 train loss: 0.0292 test loss: 0.0465\n",
      "Epoch 3 batch 2500 train loss: 0.0328 test loss: 0.0470\n",
      "Epoch 3 batch 2600 train loss: 0.0373 test loss: 0.0468\n",
      "Epoch 3 batch 2700 train loss: 0.0295 test loss: 0.0471\n",
      "Epoch 3 batch 2800 train loss: 0.0274 test loss: 0.0468\n",
      "Epoch 3 batch 2900 train loss: 0.0367 test loss: 0.0469\n",
      "Epoch 4 batch 0 train loss: 0.0262 test loss: 0.0464\n",
      "Epoch 4 batch 100 train loss: 0.0368 test loss: 0.0464\n",
      "Epoch 4 batch 200 train loss: 0.0305 test loss: 0.0472\n",
      "Epoch 4 batch 300 train loss: 0.0267 test loss: 0.0465\n",
      "Epoch 4 batch 400 train loss: 0.0278 test loss: 0.0469\n",
      "Epoch 4 batch 500 train loss: 0.0367 test loss: 0.0463\n",
      "Epoch 4 batch 600 train loss: 0.0323 test loss: 0.0468\n",
      "Epoch 4 batch 700 train loss: 0.0406 test loss: 0.0469\n",
      "Epoch 4 batch 800 train loss: 0.0325 test loss: 0.0473\n",
      "Epoch 4 batch 900 train loss: 0.0270 test loss: 0.0468\n",
      "Epoch 4 batch 1000 train loss: 0.0246 test loss: 0.0465\n",
      "Epoch 4 batch 1100 train loss: 0.0311 test loss: 0.0470\n",
      "Epoch 4 batch 1200 train loss: 0.0400 test loss: 0.0465\n",
      "Epoch 4 batch 1300 train loss: 0.0220 test loss: 0.0476\n",
      "Epoch 4 batch 1400 train loss: 0.0313 test loss: 0.0464\n",
      "Epoch 4 batch 1500 train loss: 0.0203 test loss: 0.0469\n",
      "Epoch 4 batch 1600 train loss: 0.0285 test loss: 0.0467\n",
      "Epoch 4 batch 1700 train loss: 0.0295 test loss: 0.0465\n",
      "Epoch 4 batch 1800 train loss: 0.0252 test loss: 0.0472\n",
      "Epoch 4 batch 1900 train loss: 0.0297 test loss: 0.0477\n",
      "Epoch 4 batch 2000 train loss: 0.0349 test loss: 0.0468\n",
      "Epoch 4 batch 2100 train loss: 0.0201 test loss: 0.0464\n",
      "Epoch 4 batch 2200 train loss: 0.0361 test loss: 0.0470\n",
      "Epoch 4 batch 2300 train loss: 0.0359 test loss: 0.0465\n",
      "Epoch 4 batch 2400 train loss: 0.0386 test loss: 0.0471\n",
      "Epoch 4 batch 2500 train loss: 0.0336 test loss: 0.0473\n",
      "Epoch 4 batch 2600 train loss: 0.0290 test loss: 0.0463\n",
      "Epoch 4 batch 2700 train loss: 0.0284 test loss: 0.0474\n",
      "Epoch 4 batch 2800 train loss: 0.0246 test loss: 0.0469\n",
      "Epoch 4 batch 2900 train loss: 0.0372 test loss: 0.0468\n",
      "Epoch 5 batch 0 train loss: 0.0243 test loss: 0.0467\n",
      "Epoch 5 batch 100 train loss: 0.0334 test loss: 0.0465\n",
      "Epoch 5 batch 200 train loss: 0.0268 test loss: 0.0464\n",
      "Epoch 5 batch 300 train loss: 0.0310 test loss: 0.0466\n",
      "Epoch 5 batch 400 train loss: 0.0180 test loss: 0.0473\n",
      "Epoch 5 batch 500 train loss: 0.0365 test loss: 0.0470\n",
      "Epoch 5 batch 600 train loss: 0.0244 test loss: 0.0469\n",
      "Epoch 5 batch 700 train loss: 0.0288 test loss: 0.0465\n",
      "Epoch 5 batch 800 train loss: 0.0338 test loss: 0.0472\n",
      "Epoch 5 batch 900 train loss: 0.0372 test loss: 0.0463\n",
      "Epoch 5 batch 1000 train loss: 0.0215 test loss: 0.0471\n",
      "Epoch 5 batch 1100 train loss: 0.0302 test loss: 0.0470\n",
      "Epoch 5 batch 1200 train loss: 0.0211 test loss: 0.0470\n",
      "Epoch 5 batch 1300 train loss: 0.0184 test loss: 0.0472\n",
      "Epoch 5 batch 1400 train loss: 0.0293 test loss: 0.0466\n",
      "Epoch 5 batch 1500 train loss: 0.0233 test loss: 0.0475\n",
      "Epoch 5 batch 1600 train loss: 0.0400 test loss: 0.0475\n",
      "Epoch 5 batch 1700 train loss: 0.0341 test loss: 0.0470\n",
      "Epoch 5 batch 1800 train loss: 0.0285 test loss: 0.0472\n",
      "Epoch 5 batch 1900 train loss: 0.0427 test loss: 0.0475\n",
      "Epoch 5 batch 2000 train loss: 0.0412 test loss: 0.0469\n",
      "Epoch 5 batch 2100 train loss: 0.0322 test loss: 0.0468\n",
      "Epoch 5 batch 2200 train loss: 0.0304 test loss: 0.0468\n",
      "Epoch 5 batch 2300 train loss: 0.0265 test loss: 0.0472\n",
      "Epoch 5 batch 2400 train loss: 0.0242 test loss: 0.0472\n",
      "Epoch 5 batch 2500 train loss: 0.0242 test loss: 0.0470\n",
      "Epoch 5 batch 2600 train loss: 0.0269 test loss: 0.0472\n",
      "Epoch 5 batch 2700 train loss: 0.0321 test loss: 0.0471\n",
      "Epoch 5 batch 2800 train loss: 0.0391 test loss: 0.0464\n",
      "Epoch 5 batch 2900 train loss: 0.0208 test loss: 0.0468\n",
      "Epoch 6 batch 0 train loss: 0.0331 test loss: 0.0470\n",
      "Epoch 6 batch 100 train loss: 0.0304 test loss: 0.0468\n",
      "Epoch 6 batch 200 train loss: 0.0475 test loss: 0.0472\n",
      "Epoch 6 batch 300 train loss: 0.0328 test loss: 0.0474\n",
      "Epoch 6 batch 400 train loss: 0.0435 test loss: 0.0463\n",
      "Epoch 6 batch 500 train loss: 0.0356 test loss: 0.0469\n",
      "Epoch 6 batch 600 train loss: 0.0259 test loss: 0.0470\n",
      "Epoch 6 batch 700 train loss: 0.0270 test loss: 0.0470\n",
      "Epoch 6 batch 800 train loss: 0.0204 test loss: 0.0472\n",
      "Epoch 6 batch 900 train loss: 0.0310 test loss: 0.0468\n",
      "Epoch 6 batch 1000 train loss: 0.0301 test loss: 0.0475\n",
      "Epoch 6 batch 1100 train loss: 0.0362 test loss: 0.0472\n",
      "Epoch 6 batch 1200 train loss: 0.0371 test loss: 0.0471\n",
      "Epoch 6 batch 1300 train loss: 0.0283 test loss: 0.0471\n",
      "Epoch 6 batch 1400 train loss: 0.0244 test loss: 0.0471\n",
      "Epoch 6 batch 1500 train loss: 0.0320 test loss: 0.0470\n",
      "Epoch 6 batch 1600 train loss: 0.0359 test loss: 0.0472\n",
      "Epoch 6 batch 1700 train loss: 0.0191 test loss: 0.0479\n",
      "Epoch 6 batch 1800 train loss: 0.0324 test loss: 0.0466\n",
      "Epoch 6 batch 1900 train loss: 0.0285 test loss: 0.0468\n",
      "Epoch 6 batch 2000 train loss: 0.0390 test loss: 0.0486\n",
      "Epoch 6 batch 2100 train loss: 0.0342 test loss: 0.0469\n",
      "Epoch 6 batch 2200 train loss: 0.0382 test loss: 0.0470\n",
      "Epoch 6 batch 2300 train loss: 0.0250 test loss: 0.0470\n",
      "Epoch 6 batch 2400 train loss: 0.0292 test loss: 0.0472\n",
      "Epoch 6 batch 2500 train loss: 0.0291 test loss: 0.0465\n",
      "Epoch 6 batch 2600 train loss: 0.0278 test loss: 0.0467\n",
      "Epoch 6 batch 2700 train loss: 0.0336 test loss: 0.0478\n",
      "Epoch 6 batch 2800 train loss: 0.0331 test loss: 0.0470\n",
      "Epoch 6 batch 2900 train loss: 0.0348 test loss: 0.0469\n",
      "Epoch 7 batch 0 train loss: 0.0394 test loss: 0.0468\n",
      "Epoch 7 batch 100 train loss: 0.0324 test loss: 0.0469\n",
      "Epoch 7 batch 200 train loss: 0.0344 test loss: 0.0474\n",
      "Epoch 7 batch 300 train loss: 0.0349 test loss: 0.0470\n",
      "Epoch 7 batch 400 train loss: 0.0342 test loss: 0.0474\n",
      "Epoch 7 batch 500 train loss: 0.0306 test loss: 0.0473\n",
      "Epoch 7 batch 600 train loss: 0.0362 test loss: 0.0469\n",
      "Epoch 7 batch 700 train loss: 0.0265 test loss: 0.0470\n",
      "Epoch 7 batch 800 train loss: 0.0322 test loss: 0.0472\n",
      "Epoch 7 batch 900 train loss: 0.0245 test loss: 0.0467\n",
      "Epoch 7 batch 1000 train loss: 0.0318 test loss: 0.0472\n",
      "Epoch 7 batch 1100 train loss: 0.0212 test loss: 0.0470\n",
      "Epoch 7 batch 1200 train loss: 0.0289 test loss: 0.0469\n",
      "Epoch 7 batch 1300 train loss: 0.0251 test loss: 0.0468\n",
      "Epoch 7 batch 1400 train loss: 0.0322 test loss: 0.0474\n",
      "Epoch 7 batch 1500 train loss: 0.0417 test loss: 0.0468\n",
      "Epoch 7 batch 1600 train loss: 0.0319 test loss: 0.0469\n",
      "Epoch 7 batch 1700 train loss: 0.0277 test loss: 0.0471\n",
      "Epoch 7 batch 1800 train loss: 0.0256 test loss: 0.0474\n",
      "Epoch 7 batch 1900 train loss: 0.0249 test loss: 0.0474\n",
      "Epoch 7 batch 2000 train loss: 0.0448 test loss: 0.0469\n",
      "Epoch 7 batch 2100 train loss: 0.0316 test loss: 0.0468\n",
      "Epoch 7 batch 2200 train loss: 0.0247 test loss: 0.0468\n",
      "Epoch 7 batch 2300 train loss: 0.0285 test loss: 0.0472\n",
      "Epoch 7 batch 2400 train loss: 0.0233 test loss: 0.0466\n",
      "Epoch 7 batch 2500 train loss: 0.0394 test loss: 0.0473\n",
      "Epoch 7 batch 2600 train loss: 0.0273 test loss: 0.0470\n",
      "Epoch 7 batch 2700 train loss: 0.0266 test loss: 0.0474\n",
      "Epoch 7 batch 2800 train loss: 0.0275 test loss: 0.0469\n",
      "Epoch 7 batch 2900 train loss: 0.0269 test loss: 0.0473\n",
      "Epoch 8 batch 0 train loss: 0.0288 test loss: 0.0467\n",
      "Epoch 8 batch 100 train loss: 0.0321 test loss: 0.0469\n",
      "Epoch 8 batch 200 train loss: 0.0320 test loss: 0.0470\n",
      "Epoch 8 batch 300 train loss: 0.0263 test loss: 0.0472\n",
      "Epoch 8 batch 400 train loss: 0.0183 test loss: 0.0469\n",
      "Epoch 8 batch 500 train loss: 0.0303 test loss: 0.0468\n",
      "Epoch 8 batch 600 train loss: 0.0238 test loss: 0.0473\n",
      "Epoch 8 batch 700 train loss: 0.0265 test loss: 0.0475\n",
      "Epoch 8 batch 800 train loss: 0.0258 test loss: 0.0472\n",
      "Epoch 8 batch 900 train loss: 0.0373 test loss: 0.0468\n",
      "Epoch 8 batch 1000 train loss: 0.0305 test loss: 0.0475\n",
      "Epoch 8 batch 1100 train loss: 0.0264 test loss: 0.0467\n",
      "Epoch 8 batch 1200 train loss: 0.0249 test loss: 0.0463\n",
      "Epoch 8 batch 1300 train loss: 0.0247 test loss: 0.0469\n",
      "Epoch 8 batch 1400 train loss: 0.0241 test loss: 0.0469\n",
      "Epoch 8 batch 1500 train loss: 0.0342 test loss: 0.0470\n",
      "Epoch 8 batch 1600 train loss: 0.0263 test loss: 0.0470\n",
      "Epoch 8 batch 1700 train loss: 0.0218 test loss: 0.0475\n",
      "Epoch 8 batch 1800 train loss: 0.0327 test loss: 0.0470\n",
      "Epoch 8 batch 1900 train loss: 0.0254 test loss: 0.0479\n",
      "Epoch 8 batch 2000 train loss: 0.0328 test loss: 0.0469\n",
      "Epoch 8 batch 2100 train loss: 0.0263 test loss: 0.0471\n",
      "Epoch 8 batch 2200 train loss: 0.0222 test loss: 0.0472\n",
      "Epoch 8 batch 2300 train loss: 0.0257 test loss: 0.0467\n",
      "Epoch 8 batch 2400 train loss: 0.0233 test loss: 0.0478\n",
      "Epoch 8 batch 2500 train loss: 0.0251 test loss: 0.0471\n",
      "Epoch 8 batch 2600 train loss: 0.0243 test loss: 0.0467\n",
      "Epoch 8 batch 2700 train loss: 0.0285 test loss: 0.0472\n",
      "Epoch 8 batch 2800 train loss: 0.0348 test loss: 0.0470\n",
      "Epoch 8 batch 2900 train loss: 0.0345 test loss: 0.0472\n",
      "Epoch 9 batch 0 train loss: 0.0351 test loss: 0.0468\n",
      "Epoch 9 batch 100 train loss: 0.0280 test loss: 0.0470\n",
      "Epoch 9 batch 200 train loss: 0.0242 test loss: 0.0468\n",
      "Epoch 9 batch 300 train loss: 0.0309 test loss: 0.0468\n",
      "Epoch 9 batch 400 train loss: 0.0229 test loss: 0.0471\n",
      "Epoch 9 batch 500 train loss: 0.0279 test loss: 0.0470\n",
      "Epoch 9 batch 600 train loss: 0.0352 test loss: 0.0468\n",
      "Epoch 9 batch 700 train loss: 0.0310 test loss: 0.0473\n",
      "Epoch 9 batch 800 train loss: 0.0265 test loss: 0.0473\n",
      "Epoch 9 batch 900 train loss: 0.0272 test loss: 0.0469\n",
      "Epoch 9 batch 1000 train loss: 0.0302 test loss: 0.0473\n",
      "Epoch 9 batch 1100 train loss: 0.0245 test loss: 0.0472\n",
      "Epoch 9 batch 1200 train loss: 0.0244 test loss: 0.0470\n",
      "Epoch 9 batch 1300 train loss: 0.0281 test loss: 0.0472\n",
      "Epoch 9 batch 1400 train loss: 0.0316 test loss: 0.0467\n",
      "Epoch 9 batch 1500 train loss: 0.0308 test loss: 0.0470\n",
      "Epoch 9 batch 1600 train loss: 0.0259 test loss: 0.0474\n",
      "Epoch 9 batch 1700 train loss: 0.0238 test loss: 0.0473\n",
      "Epoch 9 batch 1800 train loss: 0.0391 test loss: 0.0473\n",
      "Epoch 9 batch 1900 train loss: 0.0405 test loss: 0.0474\n",
      "Epoch 9 batch 2000 train loss: 0.0316 test loss: 0.0472\n",
      "Epoch 9 batch 2100 train loss: 0.0243 test loss: 0.0467\n",
      "Epoch 9 batch 2200 train loss: 0.0488 test loss: 0.0472\n",
      "Epoch 9 batch 2300 train loss: 0.0226 test loss: 0.0477\n",
      "Epoch 9 batch 2400 train loss: 0.0305 test loss: 0.0469\n",
      "Epoch 9 batch 2500 train loss: 0.0339 test loss: 0.0468\n",
      "Epoch 9 batch 2600 train loss: 0.0288 test loss: 0.0474\n",
      "Epoch 9 batch 2700 train loss: 0.0339 test loss: 0.0472\n",
      "Epoch 9 batch 2800 train loss: 0.0272 test loss: 0.0470\n",
      "Epoch 9 batch 2900 train loss: 0.0268 test loss: 0.0468\n",
      "Epoch 10 batch 0 train loss: 0.0304 test loss: 0.0474\n",
      "Epoch 10 batch 100 train loss: 0.0343 test loss: 0.0470\n",
      "Epoch 10 batch 200 train loss: 0.0323 test loss: 0.0472\n",
      "Epoch 10 batch 300 train loss: 0.0338 test loss: 0.0466\n",
      "Epoch 10 batch 400 train loss: 0.0343 test loss: 0.0467\n",
      "Epoch 10 batch 500 train loss: 0.0279 test loss: 0.0471\n",
      "Epoch 10 batch 600 train loss: 0.0344 test loss: 0.0470\n",
      "Epoch 10 batch 700 train loss: 0.0376 test loss: 0.0475\n",
      "Epoch 10 batch 800 train loss: 0.0311 test loss: 0.0472\n",
      "Epoch 10 batch 900 train loss: 0.0299 test loss: 0.0471\n",
      "Epoch 10 batch 1000 train loss: 0.0213 test loss: 0.0472\n",
      "Epoch 10 batch 1100 train loss: 0.0303 test loss: 0.0470\n",
      "Epoch 10 batch 1200 train loss: 0.0276 test loss: 0.0468\n",
      "Epoch 10 batch 1300 train loss: 0.0341 test loss: 0.0468\n",
      "Epoch 10 batch 1400 train loss: 0.0275 test loss: 0.0472\n",
      "Epoch 10 batch 1500 train loss: 0.0330 test loss: 0.0472\n",
      "Epoch 10 batch 1600 train loss: 0.0387 test loss: 0.0470\n",
      "Epoch 10 batch 1700 train loss: 0.0273 test loss: 0.0467\n",
      "Epoch 10 batch 1800 train loss: 0.0321 test loss: 0.0467\n",
      "Epoch 10 batch 1900 train loss: 0.0221 test loss: 0.0475\n",
      "Epoch 10 batch 2000 train loss: 0.0327 test loss: 0.0471\n",
      "Epoch 10 batch 2100 train loss: 0.0261 test loss: 0.0474\n",
      "Epoch 10 batch 2200 train loss: 0.0280 test loss: 0.0470\n",
      "Epoch 10 batch 2300 train loss: 0.0363 test loss: 0.0472\n",
      "Epoch 10 batch 2400 train loss: 0.0231 test loss: 0.0470\n",
      "Epoch 10 batch 2500 train loss: 0.0330 test loss: 0.0468\n",
      "Epoch 10 batch 2600 train loss: 0.0259 test loss: 0.0472\n",
      "Epoch 10 batch 2700 train loss: 0.0317 test loss: 0.0469\n",
      "Epoch 10 batch 2800 train loss: 0.0220 test loss: 0.0469\n",
      "Epoch 10 batch 2900 train loss: 0.0338 test loss: 0.0467\n",
      "Epoch 11 batch 0 train loss: 0.0383 test loss: 0.0468\n",
      "Epoch 11 batch 100 train loss: 0.0310 test loss: 0.0472\n",
      "Epoch 11 batch 200 train loss: 0.0333 test loss: 0.0469\n",
      "Epoch 11 batch 300 train loss: 0.0256 test loss: 0.0473\n",
      "Epoch 11 batch 400 train loss: 0.0302 test loss: 0.0469\n",
      "Epoch 11 batch 500 train loss: 0.0363 test loss: 0.0472\n",
      "Epoch 11 batch 600 train loss: 0.0237 test loss: 0.0472\n",
      "Epoch 11 batch 700 train loss: 0.0291 test loss: 0.0469\n",
      "Epoch 11 batch 800 train loss: 0.0330 test loss: 0.0470\n",
      "Epoch 11 batch 900 train loss: 0.0290 test loss: 0.0469\n",
      "Epoch 11 batch 1000 train loss: 0.0300 test loss: 0.0475\n",
      "Epoch 11 batch 1100 train loss: 0.0239 test loss: 0.0471\n",
      "Epoch 11 batch 1200 train loss: 0.0252 test loss: 0.0466\n",
      "Epoch 11 batch 1300 train loss: 0.0304 test loss: 0.0471\n",
      "Epoch 11 batch 1400 train loss: 0.0259 test loss: 0.0471\n",
      "Epoch 11 batch 1500 train loss: 0.0223 test loss: 0.0470\n",
      "Epoch 11 batch 1600 train loss: 0.0376 test loss: 0.0470\n",
      "Epoch 11 batch 1700 train loss: 0.0229 test loss: 0.0467\n",
      "Epoch 11 batch 1800 train loss: 0.0237 test loss: 0.0470\n",
      "Epoch 11 batch 1900 train loss: 0.0303 test loss: 0.0474\n",
      "Epoch 11 batch 2000 train loss: 0.0257 test loss: 0.0469\n",
      "Epoch 11 batch 2100 train loss: 0.0291 test loss: 0.0470\n",
      "Epoch 11 batch 2200 train loss: 0.0305 test loss: 0.0472\n",
      "Epoch 11 batch 2300 train loss: 0.0319 test loss: 0.0473\n",
      "Epoch 11 batch 2400 train loss: 0.0313 test loss: 0.0467\n",
      "Epoch 11 batch 2500 train loss: 0.0347 test loss: 0.0471\n",
      "Epoch 11 batch 2600 train loss: 0.0299 test loss: 0.0467\n",
      "Epoch 11 batch 2700 train loss: 0.0298 test loss: 0.0472\n",
      "Epoch 11 batch 2800 train loss: 0.0295 test loss: 0.0467\n",
      "Epoch 11 batch 2900 train loss: 0.0298 test loss: 0.0471\n",
      "Epoch 12 batch 0 train loss: 0.0316 test loss: 0.0471\n",
      "Epoch 12 batch 100 train loss: 0.0400 test loss: 0.0465\n",
      "Epoch 12 batch 200 train loss: 0.0343 test loss: 0.0477\n",
      "Epoch 12 batch 300 train loss: 0.0252 test loss: 0.0469\n",
      "Epoch 12 batch 400 train loss: 0.0362 test loss: 0.0470\n",
      "Epoch 12 batch 500 train loss: 0.0308 test loss: 0.0464\n",
      "Epoch 12 batch 600 train loss: 0.0284 test loss: 0.0469\n",
      "Epoch 12 batch 700 train loss: 0.0261 test loss: 0.0478\n",
      "Epoch 12 batch 800 train loss: 0.0431 test loss: 0.0470\n",
      "Epoch 12 batch 900 train loss: 0.0294 test loss: 0.0473\n",
      "Epoch 12 batch 1000 train loss: 0.0167 test loss: 0.0474\n",
      "Epoch 12 batch 1100 train loss: 0.0269 test loss: 0.0468\n",
      "Epoch 12 batch 1200 train loss: 0.0380 test loss: 0.0469\n",
      "Epoch 12 batch 1300 train loss: 0.0215 test loss: 0.0470\n",
      "Epoch 12 batch 1400 train loss: 0.0304 test loss: 0.0471\n",
      "Epoch 12 batch 1500 train loss: 0.0223 test loss: 0.0472\n",
      "Epoch 12 batch 1600 train loss: 0.0229 test loss: 0.0468\n",
      "Epoch 12 batch 1700 train loss: 0.0319 test loss: 0.0472\n",
      "Epoch 12 batch 1800 train loss: 0.0255 test loss: 0.0473\n",
      "Epoch 12 batch 1900 train loss: 0.0350 test loss: 0.0472\n",
      "Epoch 12 batch 2000 train loss: 0.0299 test loss: 0.0472\n",
      "Epoch 12 batch 2100 train loss: 0.0354 test loss: 0.0469\n",
      "Epoch 12 batch 2200 train loss: 0.0284 test loss: 0.0470\n",
      "Epoch 12 batch 2300 train loss: 0.0240 test loss: 0.0474\n",
      "Epoch 12 batch 2400 train loss: 0.0379 test loss: 0.0469\n",
      "Epoch 12 batch 2500 train loss: 0.0298 test loss: 0.0467\n",
      "Epoch 12 batch 2600 train loss: 0.0287 test loss: 0.0470\n",
      "Epoch 12 batch 2700 train loss: 0.0345 test loss: 0.0470\n",
      "Epoch 12 batch 2800 train loss: 0.0287 test loss: 0.0471\n",
      "Epoch 12 batch 2900 train loss: 0.0279 test loss: 0.0470\n",
      "Epoch 13 batch 0 train loss: 0.0399 test loss: 0.0468\n",
      "Epoch 13 batch 100 train loss: 0.0181 test loss: 0.0473\n",
      "Epoch 13 batch 200 train loss: 0.0162 test loss: 0.0469\n",
      "Epoch 13 batch 300 train loss: 0.0346 test loss: 0.0470\n",
      "Epoch 13 batch 400 train loss: 0.0348 test loss: 0.0469\n",
      "Epoch 13 batch 500 train loss: 0.0324 test loss: 0.0470\n",
      "Epoch 13 batch 600 train loss: 0.0336 test loss: 0.0469\n",
      "Epoch 13 batch 700 train loss: 0.0365 test loss: 0.0474\n",
      "Epoch 13 batch 800 train loss: 0.0282 test loss: 0.0469\n",
      "Epoch 13 batch 900 train loss: 0.0328 test loss: 0.0470\n",
      "Epoch 13 batch 1000 train loss: 0.0224 test loss: 0.0472\n",
      "Epoch 13 batch 1100 train loss: 0.0275 test loss: 0.0474\n",
      "Epoch 13 batch 1200 train loss: 0.0314 test loss: 0.0466\n",
      "Epoch 13 batch 1300 train loss: 0.0141 test loss: 0.0472\n",
      "Epoch 13 batch 1400 train loss: 0.0308 test loss: 0.0466\n",
      "Epoch 13 batch 1500 train loss: 0.0271 test loss: 0.0470\n",
      "Epoch 13 batch 1600 train loss: 0.0383 test loss: 0.0469\n",
      "Epoch 13 batch 1700 train loss: 0.0262 test loss: 0.0475\n",
      "Epoch 13 batch 1800 train loss: 0.0236 test loss: 0.0469\n",
      "Epoch 13 batch 1900 train loss: 0.0271 test loss: 0.0471\n",
      "Epoch 13 batch 2000 train loss: 0.0399 test loss: 0.0474\n",
      "Epoch 13 batch 2100 train loss: 0.0337 test loss: 0.0471\n",
      "Epoch 13 batch 2200 train loss: 0.0317 test loss: 0.0471\n",
      "Epoch 13 batch 2300 train loss: 0.0327 test loss: 0.0469\n",
      "Epoch 13 batch 2400 train loss: 0.0367 test loss: 0.0470\n",
      "Epoch 13 batch 2500 train loss: 0.0258 test loss: 0.0467\n",
      "Epoch 13 batch 2600 train loss: 0.0308 test loss: 0.0474\n",
      "Epoch 13 batch 2700 train loss: 0.0305 test loss: 0.0471\n",
      "Epoch 13 batch 2800 train loss: 0.0236 test loss: 0.0469\n",
      "Epoch 13 batch 2900 train loss: 0.0231 test loss: 0.0473\n",
      "Epoch 14 batch 0 train loss: 0.0358 test loss: 0.0468\n",
      "Epoch 14 batch 100 train loss: 0.0289 test loss: 0.0469\n",
      "Epoch 14 batch 200 train loss: 0.0339 test loss: 0.0465\n",
      "Epoch 14 batch 300 train loss: 0.0288 test loss: 0.0468\n",
      "Epoch 14 batch 400 train loss: 0.0345 test loss: 0.0474\n",
      "Epoch 14 batch 500 train loss: 0.0315 test loss: 0.0471\n",
      "Epoch 14 batch 600 train loss: 0.0294 test loss: 0.0474\n",
      "Epoch 14 batch 700 train loss: 0.0303 test loss: 0.0473\n",
      "Epoch 14 batch 800 train loss: 0.0292 test loss: 0.0472\n",
      "Epoch 14 batch 900 train loss: 0.0239 test loss: 0.0467\n",
      "Epoch 14 batch 1000 train loss: 0.0294 test loss: 0.0468\n",
      "Epoch 14 batch 1100 train loss: 0.0252 test loss: 0.0469\n",
      "Epoch 14 batch 1200 train loss: 0.0279 test loss: 0.0471\n",
      "Epoch 14 batch 1300 train loss: 0.0348 test loss: 0.0473\n",
      "Epoch 14 batch 1400 train loss: 0.0280 test loss: 0.0471\n",
      "Epoch 14 batch 1500 train loss: 0.0193 test loss: 0.0470\n",
      "Epoch 14 batch 1600 train loss: 0.0302 test loss: 0.0470\n",
      "Epoch 14 batch 1700 train loss: 0.0257 test loss: 0.0469\n",
      "Epoch 14 batch 1800 train loss: 0.0452 test loss: 0.0468\n",
      "Epoch 14 batch 1900 train loss: 0.0344 test loss: 0.0473\n",
      "Epoch 14 batch 2000 train loss: 0.0280 test loss: 0.0472\n",
      "Epoch 14 batch 2100 train loss: 0.0411 test loss: 0.0472\n",
      "Epoch 14 batch 2200 train loss: 0.0275 test loss: 0.0475\n",
      "Epoch 14 batch 2300 train loss: 0.0333 test loss: 0.0472\n",
      "Epoch 14 batch 2400 train loss: 0.0366 test loss: 0.0468\n",
      "Epoch 14 batch 2500 train loss: 0.0334 test loss: 0.0468\n",
      "Epoch 14 batch 2600 train loss: 0.0258 test loss: 0.0473\n",
      "Epoch 14 batch 2700 train loss: 0.0312 test loss: 0.0471\n",
      "Epoch 14 batch 2800 train loss: 0.0410 test loss: 0.0466\n",
      "Epoch 14 batch 2900 train loss: 0.0427 test loss: 0.0470\n",
      "Epoch 15 batch 0 train loss: 0.0302 test loss: 0.0470\n",
      "Epoch 15 batch 100 train loss: 0.0278 test loss: 0.0472\n",
      "Epoch 15 batch 200 train loss: 0.0356 test loss: 0.0468\n",
      "Epoch 15 batch 300 train loss: 0.0312 test loss: 0.0465\n",
      "Epoch 15 batch 400 train loss: 0.0153 test loss: 0.0471\n",
      "Epoch 15 batch 500 train loss: 0.0298 test loss: 0.0468\n",
      "Epoch 15 batch 600 train loss: 0.0343 test loss: 0.0471\n",
      "Epoch 15 batch 700 train loss: 0.0337 test loss: 0.0467\n",
      "Epoch 15 batch 800 train loss: 0.0301 test loss: 0.0475\n",
      "Epoch 15 batch 900 train loss: 0.0363 test loss: 0.0471\n",
      "Epoch 15 batch 1000 train loss: 0.0307 test loss: 0.0467\n",
      "Epoch 15 batch 1100 train loss: 0.0348 test loss: 0.0469\n",
      "Epoch 15 batch 1200 train loss: 0.0240 test loss: 0.0466\n",
      "Epoch 15 batch 1300 train loss: 0.0286 test loss: 0.0473\n",
      "Epoch 15 batch 1400 train loss: 0.0230 test loss: 0.0474\n",
      "Epoch 15 batch 1500 train loss: 0.0263 test loss: 0.0468\n",
      "Epoch 15 batch 1600 train loss: 0.0200 test loss: 0.0465\n",
      "Epoch 15 batch 1700 train loss: 0.0334 test loss: 0.0476\n",
      "Epoch 15 batch 1800 train loss: 0.0233 test loss: 0.0468\n",
      "Epoch 15 batch 1900 train loss: 0.0397 test loss: 0.0471\n",
      "Epoch 15 batch 2000 train loss: 0.0361 test loss: 0.0472\n",
      "Epoch 15 batch 2100 train loss: 0.0224 test loss: 0.0473\n",
      "Epoch 15 batch 2200 train loss: 0.0268 test loss: 0.0471\n",
      "Epoch 15 batch 2300 train loss: 0.0313 test loss: 0.0472\n",
      "Epoch 15 batch 2400 train loss: 0.0298 test loss: 0.0469\n",
      "Epoch 15 batch 2500 train loss: 0.0418 test loss: 0.0470\n",
      "Epoch 15 batch 2600 train loss: 0.0270 test loss: 0.0473\n",
      "Epoch 15 batch 2700 train loss: 0.0315 test loss: 0.0474\n",
      "Epoch 15 batch 2800 train loss: 0.0369 test loss: 0.0465\n",
      "Epoch 15 batch 2900 train loss: 0.0279 test loss: 0.0476\n",
      "Epoch 16 batch 0 train loss: 0.0296 test loss: 0.0466\n",
      "Epoch 16 batch 100 train loss: 0.0244 test loss: 0.0472\n",
      "Epoch 16 batch 200 train loss: 0.0382 test loss: 0.0465\n",
      "Epoch 16 batch 300 train loss: 0.0347 test loss: 0.0470\n",
      "Epoch 16 batch 400 train loss: 0.0384 test loss: 0.0470\n",
      "Epoch 16 batch 500 train loss: 0.0291 test loss: 0.0468\n",
      "Epoch 16 batch 600 train loss: 0.0373 test loss: 0.0466\n",
      "Epoch 16 batch 700 train loss: 0.0431 test loss: 0.0471\n",
      "Epoch 16 batch 800 train loss: 0.0391 test loss: 0.0475\n",
      "Epoch 16 batch 900 train loss: 0.0341 test loss: 0.0473\n",
      "Epoch 16 batch 1000 train loss: 0.0318 test loss: 0.0472\n",
      "Epoch 16 batch 1100 train loss: 0.0277 test loss: 0.0468\n",
      "Epoch 16 batch 1200 train loss: 0.0363 test loss: 0.0466\n",
      "Epoch 16 batch 1300 train loss: 0.0317 test loss: 0.0469\n",
      "Epoch 16 batch 1400 train loss: 0.0249 test loss: 0.0472\n",
      "Epoch 16 batch 1500 train loss: 0.0308 test loss: 0.0474\n",
      "Epoch 16 batch 1600 train loss: 0.0307 test loss: 0.0470\n",
      "Epoch 16 batch 1700 train loss: 0.0250 test loss: 0.0475\n",
      "Epoch 16 batch 1800 train loss: 0.0305 test loss: 0.0468\n",
      "Epoch 16 batch 1900 train loss: 0.0396 test loss: 0.0476\n",
      "Epoch 16 batch 2000 train loss: 0.0278 test loss: 0.0468\n",
      "Epoch 16 batch 2100 train loss: 0.0331 test loss: 0.0470\n",
      "Epoch 16 batch 2200 train loss: 0.0269 test loss: 0.0472\n",
      "Epoch 16 batch 2300 train loss: 0.0298 test loss: 0.0469\n",
      "Epoch 16 batch 2400 train loss: 0.0255 test loss: 0.0475\n",
      "Epoch 16 batch 2500 train loss: 0.0345 test loss: 0.0472\n",
      "Epoch 16 batch 2600 train loss: 0.0322 test loss: 0.0471\n",
      "Epoch 16 batch 2700 train loss: 0.0300 test loss: 0.0468\n",
      "Epoch 16 batch 2800 train loss: 0.0413 test loss: 0.0469\n",
      "Epoch 16 batch 2900 train loss: 0.0214 test loss: 0.0467\n",
      "Epoch 17 batch 0 train loss: 0.0383 test loss: 0.0472\n",
      "Epoch 17 batch 100 train loss: 0.0301 test loss: 0.0469\n",
      "Epoch 17 batch 200 train loss: 0.0246 test loss: 0.0468\n",
      "Epoch 17 batch 300 train loss: 0.0369 test loss: 0.0474\n",
      "early stop.\n",
      "Checkpoint 14 restored!!\n",
      "2446/2446 [==============================] - 15s 6ms/step - loss: 0.0316\n",
      "Training for loss rate 0.90 start.\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.90p/ckpt-1\n",
      "Epoch 0 batch 0 train loss: 0.1281 test loss: 0.1355\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.90p/ckpt-2\n",
      "Epoch 0 batch 100 train loss: 0.0544 test loss: 0.0597\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.90p/ckpt-3\n",
      "Epoch 0 batch 200 train loss: 0.0422 test loss: 0.0596\n",
      "Epoch 0 batch 300 train loss: 0.0360 test loss: 0.0605\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.90p/ckpt-4\n",
      "Epoch 0 batch 400 train loss: 0.0546 test loss: 0.0594\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.90p/ckpt-5\n",
      "Epoch 0 batch 500 train loss: 0.0442 test loss: 0.0578\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.90p/ckpt-6\n",
      "Epoch 0 batch 600 train loss: 0.0433 test loss: 0.0570\n",
      "Epoch 0 batch 700 train loss: 0.0518 test loss: 0.0577\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.90p/ckpt-7\n",
      "Epoch 0 batch 800 train loss: 0.0416 test loss: 0.0561\n",
      "Epoch 0 batch 900 train loss: 0.0518 test loss: 0.0572\n",
      "Epoch 0 batch 1000 train loss: 0.0529 test loss: 0.0575\n",
      "Epoch 0 batch 1100 train loss: 0.0423 test loss: 0.0575\n",
      "Epoch 0 batch 1200 train loss: 0.0502 test loss: 0.0570\n",
      "Epoch 0 batch 1300 train loss: 0.0426 test loss: 0.0574\n",
      "Epoch 0 batch 1400 train loss: 0.0523 test loss: 0.0573\n",
      "Epoch 0 batch 1500 train loss: 0.0473 test loss: 0.0576\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.90p/ckpt-8\n",
      "Epoch 0 batch 1600 train loss: 0.0405 test loss: 0.0560\n",
      "Epoch 0 batch 1700 train loss: 0.0521 test loss: 0.0568\n",
      "Epoch 0 batch 1800 train loss: 0.0473 test loss: 0.0565\n",
      "Epoch 0 batch 1900 train loss: 0.0387 test loss: 0.0568\n",
      "Epoch 0 batch 2000 train loss: 0.0504 test loss: 0.0569\n",
      "Epoch 0 batch 2100 train loss: 0.0406 test loss: 0.0584\n",
      "Epoch 0 batch 2200 train loss: 0.0498 test loss: 0.0570\n",
      "Epoch 0 batch 2300 train loss: 0.0420 test loss: 0.0568\n",
      "Epoch 0 batch 2400 train loss: 0.0481 test loss: 0.0571\n",
      "Epoch 0 batch 2500 train loss: 0.0446 test loss: 0.0562\n",
      "Epoch 0 batch 2600 train loss: 0.0465 test loss: 0.0566\n",
      "Epoch 0 batch 2700 train loss: 0.0476 test loss: 0.0569\n",
      "Epoch 0 batch 2800 train loss: 0.0548 test loss: 0.0565\n",
      "Epoch 0 batch 2900 train loss: 0.0475 test loss: 0.0562\n",
      "Epoch 1 batch 0 train loss: 0.0402 test loss: 0.0563\n",
      "Epoch 1 batch 100 train loss: 0.0314 test loss: 0.0572\n",
      "Epoch 1 batch 200 train loss: 0.0475 test loss: 0.0562\n",
      "Epoch 1 batch 300 train loss: 0.0449 test loss: 0.0563\n",
      "Epoch 1 batch 400 train loss: 0.0462 test loss: 0.0570\n",
      "Epoch 1 batch 500 train loss: 0.0463 test loss: 0.0560\n",
      "Epoch 1 batch 600 train loss: 0.0446 test loss: 0.0568\n",
      "Epoch 1 batch 700 train loss: 0.0435 test loss: 0.0578\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.90p/ckpt-9\n",
      "Epoch 1 batch 800 train loss: 0.0391 test loss: 0.0559\n",
      "Epoch 1 batch 900 train loss: 0.0506 test loss: 0.0563\n",
      "Epoch 1 batch 1000 train loss: 0.0373 test loss: 0.0564\n",
      "Epoch 1 batch 1100 train loss: 0.0398 test loss: 0.0569\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.90p/ckpt-10\n",
      "Epoch 1 batch 1200 train loss: 0.0507 test loss: 0.0559\n",
      "Epoch 1 batch 1300 train loss: 0.0406 test loss: 0.0572\n",
      "Epoch 1 batch 1400 train loss: 0.0452 test loss: 0.0566\n",
      "Epoch 1 batch 1500 train loss: 0.0302 test loss: 0.0582\n",
      "Epoch 1 batch 1600 train loss: 0.0437 test loss: 0.0571\n",
      "Epoch 1 batch 1700 train loss: 0.0407 test loss: 0.0561\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.90p/ckpt-11\n",
      "Epoch 1 batch 1800 train loss: 0.0463 test loss: 0.0557\n",
      "Epoch 1 batch 1900 train loss: 0.0485 test loss: 0.0561\n",
      "Epoch 1 batch 2000 train loss: 0.0457 test loss: 0.0569\n",
      "Epoch 1 batch 2100 train loss: 0.0387 test loss: 0.0562\n",
      "Epoch 1 batch 2200 train loss: 0.0490 test loss: 0.0563\n",
      "Epoch 1 batch 2300 train loss: 0.0459 test loss: 0.0570\n",
      "Epoch 1 batch 2400 train loss: 0.0467 test loss: 0.0570\n",
      "Epoch 1 batch 2500 train loss: 0.0454 test loss: 0.0568\n",
      "Epoch 1 batch 2600 train loss: 0.0436 test loss: 0.0566\n",
      "Epoch 1 batch 2700 train loss: 0.0407 test loss: 0.0569\n",
      "Epoch 1 batch 2800 train loss: 0.0381 test loss: 0.0569\n",
      "Epoch 1 batch 2900 train loss: 0.0518 test loss: 0.0566\n",
      "Epoch 2 batch 0 train loss: 0.0466 test loss: 0.0570\n",
      "Epoch 2 batch 100 train loss: 0.0477 test loss: 0.0577\n",
      "Epoch 2 batch 200 train loss: 0.0454 test loss: 0.0570\n",
      "Epoch 2 batch 300 train loss: 0.0395 test loss: 0.0564\n",
      "Epoch 2 batch 400 train loss: 0.0510 test loss: 0.0571\n",
      "Epoch 2 batch 500 train loss: 0.0448 test loss: 0.0562\n",
      "Epoch 2 batch 600 train loss: 0.0432 test loss: 0.0575\n",
      "Epoch 2 batch 700 train loss: 0.0478 test loss: 0.0580\n",
      "Epoch 2 batch 800 train loss: 0.0469 test loss: 0.0569\n",
      "Epoch 2 batch 900 train loss: 0.0380 test loss: 0.0565\n",
      "Epoch 2 batch 1000 train loss: 0.0466 test loss: 0.0570\n",
      "Epoch 2 batch 1100 train loss: 0.0449 test loss: 0.0564\n",
      "Epoch 2 batch 1200 train loss: 0.0427 test loss: 0.0566\n",
      "Epoch 2 batch 1300 train loss: 0.0498 test loss: 0.0573\n",
      "Epoch 2 batch 1400 train loss: 0.0426 test loss: 0.0573\n",
      "Epoch 2 batch 1500 train loss: 0.0452 test loss: 0.0568\n",
      "Epoch 2 batch 1600 train loss: 0.0466 test loss: 0.0570\n",
      "Epoch 2 batch 1700 train loss: 0.0479 test loss: 0.0564\n",
      "Epoch 2 batch 1800 train loss: 0.0432 test loss: 0.0564\n",
      "Epoch 2 batch 1900 train loss: 0.0495 test loss: 0.0558\n",
      "Epoch 2 batch 2000 train loss: 0.0389 test loss: 0.0563\n",
      "Epoch 2 batch 2100 train loss: 0.0464 test loss: 0.0570\n",
      "Epoch 2 batch 2200 train loss: 0.0439 test loss: 0.0564\n",
      "Epoch 2 batch 2300 train loss: 0.0348 test loss: 0.0565\n",
      "Epoch 2 batch 2400 train loss: 0.0479 test loss: 0.0569\n",
      "Epoch 2 batch 2500 train loss: 0.0500 test loss: 0.0563\n",
      "Epoch 2 batch 2600 train loss: 0.0421 test loss: 0.0575\n",
      "Epoch 2 batch 2700 train loss: 0.0367 test loss: 0.0564\n",
      "Epoch 2 batch 2800 train loss: 0.0418 test loss: 0.0565\n",
      "Epoch 2 batch 2900 train loss: 0.0487 test loss: 0.0567\n",
      "Epoch 3 batch 0 train loss: 0.0457 test loss: 0.0562\n",
      "Epoch 3 batch 100 train loss: 0.0388 test loss: 0.0571\n",
      "Epoch 3 batch 200 train loss: 0.0448 test loss: 0.0568\n",
      "Epoch 3 batch 300 train loss: 0.0359 test loss: 0.0567\n",
      "Epoch 3 batch 400 train loss: 0.0451 test loss: 0.0568\n",
      "Epoch 3 batch 500 train loss: 0.0437 test loss: 0.0565\n",
      "Epoch 3 batch 600 train loss: 0.0400 test loss: 0.0566\n",
      "Epoch 3 batch 700 train loss: 0.0359 test loss: 0.0559\n",
      "Epoch 3 batch 800 train loss: 0.0508 test loss: 0.0567\n",
      "Epoch 3 batch 900 train loss: 0.0524 test loss: 0.0567\n",
      "Epoch 3 batch 1000 train loss: 0.0447 test loss: 0.0563\n",
      "Epoch 3 batch 1100 train loss: 0.0499 test loss: 0.0558\n",
      "Epoch 3 batch 1200 train loss: 0.0400 test loss: 0.0560\n",
      "Epoch 3 batch 1300 train loss: 0.0402 test loss: 0.0567\n",
      "Epoch 3 batch 1400 train loss: 0.0373 test loss: 0.0565\n",
      "Epoch 3 batch 1500 train loss: 0.0431 test loss: 0.0573\n",
      "Epoch 3 batch 1600 train loss: 0.0462 test loss: 0.0570\n",
      "Epoch 3 batch 1700 train loss: 0.0472 test loss: 0.0566\n",
      "Epoch 3 batch 1800 train loss: 0.0388 test loss: 0.0573\n",
      "Epoch 3 batch 1900 train loss: 0.0507 test loss: 0.0559\n",
      "Epoch 3 batch 2000 train loss: 0.0496 test loss: 0.0571\n",
      "Epoch 3 batch 2100 train loss: 0.0422 test loss: 0.0562\n",
      "Epoch 3 batch 2200 train loss: 0.0487 test loss: 0.0569\n",
      "Epoch 3 batch 2300 train loss: 0.0354 test loss: 0.0565\n",
      "Epoch 3 batch 2400 train loss: 0.0379 test loss: 0.0563\n",
      "Epoch 3 batch 2500 train loss: 0.0401 test loss: 0.0564\n",
      "Epoch 3 batch 2600 train loss: 0.0451 test loss: 0.0566\n",
      "Epoch 3 batch 2700 train loss: 0.0379 test loss: 0.0564\n",
      "Epoch 3 batch 2800 train loss: 0.0432 test loss: 0.0560\n",
      "Epoch 3 batch 2900 train loss: 0.0428 test loss: 0.0564\n",
      "Epoch 4 batch 0 train loss: 0.0373 test loss: 0.0562\n",
      "Epoch 4 batch 100 train loss: 0.0479 test loss: 0.0563\n",
      "Epoch 4 batch 200 train loss: 0.0436 test loss: 0.0567\n",
      "Epoch 4 batch 300 train loss: 0.0542 test loss: 0.0559\n",
      "Epoch 4 batch 400 train loss: 0.0489 test loss: 0.0565\n",
      "Epoch 4 batch 500 train loss: 0.0373 test loss: 0.0567\n",
      "Epoch 4 batch 600 train loss: 0.0440 test loss: 0.0563\n",
      "Epoch 4 batch 700 train loss: 0.0455 test loss: 0.0571\n",
      "Epoch 4 batch 800 train loss: 0.0446 test loss: 0.0568\n",
      "Epoch 4 batch 900 train loss: 0.0545 test loss: 0.0565\n",
      "Epoch 4 batch 1000 train loss: 0.0415 test loss: 0.0567\n",
      "Epoch 4 batch 1100 train loss: 0.0435 test loss: 0.0566\n",
      "Epoch 4 batch 1200 train loss: 0.0425 test loss: 0.0562\n",
      "Epoch 4 batch 1300 train loss: 0.0519 test loss: 0.0567\n",
      "Epoch 4 batch 1400 train loss: 0.0442 test loss: 0.0576\n",
      "Epoch 4 batch 1500 train loss: 0.0389 test loss: 0.0563\n",
      "Epoch 4 batch 1600 train loss: 0.0463 test loss: 0.0570\n",
      "Epoch 4 batch 1700 train loss: 0.0423 test loss: 0.0569\n",
      "Epoch 4 batch 1800 train loss: 0.0441 test loss: 0.0569\n",
      "Epoch 4 batch 1900 train loss: 0.0490 test loss: 0.0571\n",
      "Epoch 4 batch 2000 train loss: 0.0507 test loss: 0.0566\n",
      "Epoch 4 batch 2100 train loss: 0.0378 test loss: 0.0562\n",
      "Epoch 4 batch 2200 train loss: 0.0424 test loss: 0.0561\n",
      "Epoch 4 batch 2300 train loss: 0.0422 test loss: 0.0571\n",
      "Epoch 4 batch 2400 train loss: 0.0415 test loss: 0.0566\n",
      "Epoch 4 batch 2500 train loss: 0.0375 test loss: 0.0567\n",
      "Epoch 4 batch 2600 train loss: 0.0415 test loss: 0.0558\n",
      "Epoch 4 batch 2700 train loss: 0.0427 test loss: 0.0559\n",
      "Epoch 4 batch 2800 train loss: 0.0493 test loss: 0.0562\n",
      "Epoch 4 batch 2900 train loss: 0.0474 test loss: 0.0565\n",
      "Epoch 5 batch 0 train loss: 0.0512 test loss: 0.0562\n",
      "Epoch 5 batch 100 train loss: 0.0415 test loss: 0.0565\n",
      "Epoch 5 batch 200 train loss: 0.0385 test loss: 0.0566\n",
      "Epoch 5 batch 300 train loss: 0.0589 test loss: 0.0567\n",
      "Epoch 5 batch 400 train loss: 0.0526 test loss: 0.0565\n",
      "Epoch 5 batch 500 train loss: 0.0479 test loss: 0.0566\n",
      "Epoch 5 batch 600 train loss: 0.0500 test loss: 0.0569\n",
      "Epoch 5 batch 700 train loss: 0.0483 test loss: 0.0570\n",
      "Epoch 5 batch 800 train loss: 0.0382 test loss: 0.0564\n",
      "Epoch 5 batch 900 train loss: 0.0443 test loss: 0.0563\n",
      "Epoch 5 batch 1000 train loss: 0.0449 test loss: 0.0566\n",
      "Epoch 5 batch 1100 train loss: 0.0450 test loss: 0.0565\n",
      "Epoch 5 batch 1200 train loss: 0.0346 test loss: 0.0559\n",
      "Epoch 5 batch 1300 train loss: 0.0426 test loss: 0.0567\n",
      "Epoch 5 batch 1400 train loss: 0.0463 test loss: 0.0568\n",
      "Epoch 5 batch 1500 train loss: 0.0379 test loss: 0.0572\n",
      "Epoch 5 batch 1600 train loss: 0.0327 test loss: 0.0559\n",
      "Epoch 5 batch 1700 train loss: 0.0428 test loss: 0.0571\n",
      "Epoch 5 batch 1800 train loss: 0.0454 test loss: 0.0565\n",
      "Epoch 5 batch 1900 train loss: 0.0424 test loss: 0.0571\n",
      "Epoch 5 batch 2000 train loss: 0.0422 test loss: 0.0561\n",
      "Epoch 5 batch 2100 train loss: 0.0410 test loss: 0.0570\n",
      "Epoch 5 batch 2200 train loss: 0.0448 test loss: 0.0564\n",
      "Epoch 5 batch 2300 train loss: 0.0489 test loss: 0.0563\n",
      "Epoch 5 batch 2400 train loss: 0.0381 test loss: 0.0565\n",
      "Epoch 5 batch 2500 train loss: 0.0381 test loss: 0.0562\n",
      "Epoch 5 batch 2600 train loss: 0.0465 test loss: 0.0561\n",
      "Epoch 5 batch 2700 train loss: 0.0401 test loss: 0.0569\n",
      "Epoch 5 batch 2800 train loss: 0.0510 test loss: 0.0559\n",
      "Epoch 5 batch 2900 train loss: 0.0420 test loss: 0.0566\n",
      "Epoch 6 batch 0 train loss: 0.0590 test loss: 0.0561\n",
      "Epoch 6 batch 100 train loss: 0.0496 test loss: 0.0561\n",
      "Epoch 6 batch 200 train loss: 0.0474 test loss: 0.0566\n",
      "Epoch 6 batch 300 train loss: 0.0494 test loss: 0.0566\n",
      "Epoch 6 batch 400 train loss: 0.0487 test loss: 0.0562\n",
      "Epoch 6 batch 500 train loss: 0.0485 test loss: 0.0568\n",
      "Epoch 6 batch 600 train loss: 0.0430 test loss: 0.0566\n",
      "Epoch 6 batch 700 train loss: 0.0417 test loss: 0.0570\n",
      "Epoch 6 batch 800 train loss: 0.0493 test loss: 0.0563\n",
      "Epoch 6 batch 900 train loss: 0.0400 test loss: 0.0569\n",
      "Epoch 6 batch 1000 train loss: 0.0521 test loss: 0.0566\n",
      "Epoch 6 batch 1100 train loss: 0.0375 test loss: 0.0561\n",
      "Epoch 6 batch 1200 train loss: 0.0489 test loss: 0.0562\n",
      "Epoch 6 batch 1300 train loss: 0.0495 test loss: 0.0567\n",
      "Epoch 6 batch 1400 train loss: 0.0494 test loss: 0.0560\n",
      "Epoch 6 batch 1500 train loss: 0.0454 test loss: 0.0566\n",
      "Epoch 6 batch 1600 train loss: 0.0432 test loss: 0.0562\n",
      "Epoch 6 batch 1700 train loss: 0.0438 test loss: 0.0562\n",
      "Epoch 6 batch 1800 train loss: 0.0336 test loss: 0.0560\n",
      "Epoch 6 batch 1900 train loss: 0.0482 test loss: 0.0568\n",
      "Epoch 6 batch 2000 train loss: 0.0494 test loss: 0.0564\n",
      "Epoch 6 batch 2100 train loss: 0.0460 test loss: 0.0563\n",
      "Epoch 6 batch 2200 train loss: 0.0408 test loss: 0.0564\n",
      "Epoch 6 batch 2300 train loss: 0.0435 test loss: 0.0564\n",
      "Epoch 6 batch 2400 train loss: 0.0497 test loss: 0.0563\n",
      "Epoch 6 batch 2500 train loss: 0.0558 test loss: 0.0561\n",
      "Epoch 6 batch 2600 train loss: 0.0424 test loss: 0.0568\n",
      "Epoch 6 batch 2700 train loss: 0.0447 test loss: 0.0563\n",
      "Epoch 6 batch 2800 train loss: 0.0439 test loss: 0.0559\n",
      "Epoch 6 batch 2900 train loss: 0.0473 test loss: 0.0564\n",
      "Epoch 7 batch 0 train loss: 0.0407 test loss: 0.0563\n",
      "Epoch 7 batch 100 train loss: 0.0480 test loss: 0.0569\n",
      "Epoch 7 batch 200 train loss: 0.0467 test loss: 0.0569\n",
      "Epoch 7 batch 300 train loss: 0.0403 test loss: 0.0564\n",
      "Epoch 7 batch 400 train loss: 0.0533 test loss: 0.0564\n",
      "Epoch 7 batch 500 train loss: 0.0448 test loss: 0.0567\n",
      "Epoch 7 batch 600 train loss: 0.0343 test loss: 0.0569\n",
      "Epoch 7 batch 700 train loss: 0.0532 test loss: 0.0565\n",
      "Epoch 7 batch 800 train loss: 0.0485 test loss: 0.0565\n",
      "Epoch 7 batch 900 train loss: 0.0433 test loss: 0.0563\n",
      "Epoch 7 batch 1000 train loss: 0.0401 test loss: 0.0566\n",
      "Epoch 7 batch 1100 train loss: 0.0464 test loss: 0.0563\n",
      "Epoch 7 batch 1200 train loss: 0.0498 test loss: 0.0564\n",
      "Epoch 7 batch 1300 train loss: 0.0382 test loss: 0.0561\n",
      "Epoch 7 batch 1400 train loss: 0.0409 test loss: 0.0566\n",
      "Epoch 7 batch 1500 train loss: 0.0441 test loss: 0.0565\n",
      "Epoch 7 batch 1600 train loss: 0.0353 test loss: 0.0566\n",
      "Epoch 7 batch 1700 train loss: 0.0473 test loss: 0.0569\n",
      "Epoch 7 batch 1800 train loss: 0.0435 test loss: 0.0563\n",
      "Epoch 7 batch 1900 train loss: 0.0433 test loss: 0.0566\n",
      "Epoch 7 batch 2000 train loss: 0.0507 test loss: 0.0559\n",
      "Epoch 7 batch 2100 train loss: 0.0405 test loss: 0.0561\n",
      "Epoch 7 batch 2200 train loss: 0.0400 test loss: 0.0561\n",
      "Epoch 7 batch 2300 train loss: 0.0445 test loss: 0.0570\n",
      "Epoch 7 batch 2400 train loss: 0.0338 test loss: 0.0566\n",
      "Epoch 7 batch 2500 train loss: 0.0491 test loss: 0.0563\n",
      "Epoch 7 batch 2600 train loss: 0.0466 test loss: 0.0565\n",
      "Epoch 7 batch 2700 train loss: 0.0469 test loss: 0.0567\n",
      "Epoch 7 batch 2800 train loss: 0.0386 test loss: 0.0563\n",
      "Epoch 7 batch 2900 train loss: 0.0499 test loss: 0.0566\n",
      "Epoch 8 batch 0 train loss: 0.0407 test loss: 0.0564\n",
      "Epoch 8 batch 100 train loss: 0.0429 test loss: 0.0565\n",
      "Epoch 8 batch 200 train loss: 0.0471 test loss: 0.0567\n",
      "Epoch 8 batch 300 train loss: 0.0477 test loss: 0.0562\n",
      "Epoch 8 batch 400 train loss: 0.0437 test loss: 0.0560\n",
      "Epoch 8 batch 500 train loss: 0.0452 test loss: 0.0564\n",
      "Epoch 8 batch 600 train loss: 0.0460 test loss: 0.0563\n",
      "Epoch 8 batch 700 train loss: 0.0501 test loss: 0.0560\n",
      "Epoch 8 batch 800 train loss: 0.0492 test loss: 0.0563\n",
      "Epoch 8 batch 900 train loss: 0.0465 test loss: 0.0561\n",
      "Epoch 8 batch 1000 train loss: 0.0392 test loss: 0.0565\n",
      "Epoch 8 batch 1100 train loss: 0.0452 test loss: 0.0559\n",
      "Epoch 8 batch 1200 train loss: 0.0454 test loss: 0.0565\n",
      "Epoch 8 batch 1300 train loss: 0.0459 test loss: 0.0566\n",
      "Epoch 8 batch 1400 train loss: 0.0485 test loss: 0.0568\n",
      "Epoch 8 batch 1500 train loss: 0.0467 test loss: 0.0565\n",
      "Epoch 8 batch 1600 train loss: 0.0429 test loss: 0.0566\n",
      "Epoch 8 batch 1700 train loss: 0.0471 test loss: 0.0559\n",
      "Epoch 8 batch 1800 train loss: 0.0460 test loss: 0.0565\n",
      "Epoch 8 batch 1900 train loss: 0.0477 test loss: 0.0565\n",
      "Epoch 8 batch 2000 train loss: 0.0373 test loss: 0.0562\n",
      "Epoch 8 batch 2100 train loss: 0.0436 test loss: 0.0562\n",
      "Epoch 8 batch 2200 train loss: 0.0460 test loss: 0.0563\n",
      "Epoch 8 batch 2300 train loss: 0.0458 test loss: 0.0566\n",
      "Epoch 8 batch 2400 train loss: 0.0452 test loss: 0.0565\n",
      "Epoch 8 batch 2500 train loss: 0.0519 test loss: 0.0567\n",
      "Epoch 8 batch 2600 train loss: 0.0485 test loss: 0.0564\n",
      "Epoch 8 batch 2700 train loss: 0.0390 test loss: 0.0563\n",
      "Epoch 8 batch 2800 train loss: 0.0478 test loss: 0.0562\n",
      "Epoch 8 batch 2900 train loss: 0.0410 test loss: 0.0563\n",
      "Epoch 9 batch 0 train loss: 0.0414 test loss: 0.0559\n",
      "Epoch 9 batch 100 train loss: 0.0397 test loss: 0.0560\n",
      "Epoch 9 batch 200 train loss: 0.0470 test loss: 0.0566\n",
      "Epoch 9 batch 300 train loss: 0.0425 test loss: 0.0561\n",
      "Epoch 9 batch 400 train loss: 0.0449 test loss: 0.0563\n",
      "Epoch 9 batch 500 train loss: 0.0411 test loss: 0.0565\n",
      "Epoch 9 batch 600 train loss: 0.0461 test loss: 0.0567\n",
      "Epoch 9 batch 700 train loss: 0.0475 test loss: 0.0570\n",
      "Epoch 9 batch 800 train loss: 0.0519 test loss: 0.0562\n",
      "Epoch 9 batch 900 train loss: 0.0441 test loss: 0.0562\n",
      "Epoch 9 batch 1000 train loss: 0.0470 test loss: 0.0564\n",
      "Epoch 9 batch 1100 train loss: 0.0402 test loss: 0.0565\n",
      "Epoch 9 batch 1200 train loss: 0.0441 test loss: 0.0563\n",
      "Epoch 9 batch 1300 train loss: 0.0445 test loss: 0.0561\n",
      "Epoch 9 batch 1400 train loss: 0.0422 test loss: 0.0559\n",
      "Epoch 9 batch 1500 train loss: 0.0476 test loss: 0.0569\n",
      "Epoch 9 batch 1600 train loss: 0.0479 test loss: 0.0568\n",
      "Epoch 9 batch 1700 train loss: 0.0415 test loss: 0.0570\n",
      "Epoch 9 batch 1800 train loss: 0.0294 test loss: 0.0568\n",
      "Epoch 9 batch 1900 train loss: 0.0431 test loss: 0.0567\n",
      "Epoch 9 batch 2000 train loss: 0.0468 test loss: 0.0560\n",
      "Epoch 9 batch 2100 train loss: 0.0393 test loss: 0.0563\n",
      "Epoch 9 batch 2200 train loss: 0.0421 test loss: 0.0562\n",
      "Epoch 9 batch 2300 train loss: 0.0438 test loss: 0.0568\n",
      "Epoch 9 batch 2400 train loss: 0.0433 test loss: 0.0565\n",
      "Epoch 9 batch 2500 train loss: 0.0399 test loss: 0.0562\n",
      "Epoch 9 batch 2600 train loss: 0.0370 test loss: 0.0566\n",
      "Epoch 9 batch 2700 train loss: 0.0352 test loss: 0.0565\n",
      "Epoch 9 batch 2800 train loss: 0.0419 test loss: 0.0565\n",
      "Epoch 9 batch 2900 train loss: 0.0502 test loss: 0.0563\n",
      "Epoch 10 batch 0 train loss: 0.0480 test loss: 0.0560\n",
      "Epoch 10 batch 100 train loss: 0.0415 test loss: 0.0562\n",
      "Epoch 10 batch 200 train loss: 0.0436 test loss: 0.0561\n",
      "Epoch 10 batch 300 train loss: 0.0277 test loss: 0.0561\n",
      "Epoch 10 batch 400 train loss: 0.0429 test loss: 0.0563\n",
      "Epoch 10 batch 500 train loss: 0.0515 test loss: 0.0563\n",
      "Epoch 10 batch 600 train loss: 0.0501 test loss: 0.0568\n",
      "Epoch 10 batch 700 train loss: 0.0473 test loss: 0.0564\n",
      "Epoch 10 batch 800 train loss: 0.0383 test loss: 0.0562\n",
      "Epoch 10 batch 900 train loss: 0.0502 test loss: 0.0564\n",
      "Epoch 10 batch 1000 train loss: 0.0477 test loss: 0.0563\n",
      "Epoch 10 batch 1100 train loss: 0.0398 test loss: 0.0565\n",
      "Epoch 10 batch 1200 train loss: 0.0558 test loss: 0.0565\n",
      "Epoch 10 batch 1300 train loss: 0.0459 test loss: 0.0562\n",
      "Epoch 10 batch 1400 train loss: 0.0444 test loss: 0.0566\n",
      "Epoch 10 batch 1500 train loss: 0.0520 test loss: 0.0568\n",
      "Epoch 10 batch 1600 train loss: 0.0548 test loss: 0.0565\n",
      "Epoch 10 batch 1700 train loss: 0.0480 test loss: 0.0563\n",
      "Epoch 10 batch 1800 train loss: 0.0411 test loss: 0.0562\n",
      "Epoch 10 batch 1900 train loss: 0.0460 test loss: 0.0565\n",
      "Epoch 10 batch 2000 train loss: 0.0462 test loss: 0.0563\n",
      "Epoch 10 batch 2100 train loss: 0.0458 test loss: 0.0564\n",
      "Epoch 10 batch 2200 train loss: 0.0410 test loss: 0.0564\n",
      "Epoch 10 batch 2300 train loss: 0.0468 test loss: 0.0567\n",
      "Epoch 10 batch 2400 train loss: 0.0371 test loss: 0.0566\n",
      "Epoch 10 batch 2500 train loss: 0.0452 test loss: 0.0561\n",
      "Epoch 10 batch 2600 train loss: 0.0493 test loss: 0.0565\n",
      "Epoch 10 batch 2700 train loss: 0.0442 test loss: 0.0563\n",
      "Epoch 10 batch 2800 train loss: 0.0338 test loss: 0.0562\n",
      "Epoch 10 batch 2900 train loss: 0.0375 test loss: 0.0565\n",
      "Epoch 11 batch 0 train loss: 0.0469 test loss: 0.0564\n",
      "Epoch 11 batch 100 train loss: 0.0447 test loss: 0.0564\n",
      "Epoch 11 batch 200 train loss: 0.0453 test loss: 0.0562\n",
      "Epoch 11 batch 300 train loss: 0.0518 test loss: 0.0565\n",
      "Epoch 11 batch 400 train loss: 0.0504 test loss: 0.0569\n",
      "Epoch 11 batch 500 train loss: 0.0464 test loss: 0.0568\n",
      "Epoch 11 batch 600 train loss: 0.0405 test loss: 0.0564\n",
      "Epoch 11 batch 700 train loss: 0.0414 test loss: 0.0570\n",
      "Epoch 11 batch 800 train loss: 0.0396 test loss: 0.0564\n",
      "Epoch 11 batch 900 train loss: 0.0503 test loss: 0.0566\n",
      "Epoch 11 batch 1000 train loss: 0.0432 test loss: 0.0566\n",
      "Epoch 11 batch 1100 train loss: 0.0442 test loss: 0.0563\n",
      "Epoch 11 batch 1200 train loss: 0.0440 test loss: 0.0567\n",
      "Epoch 11 batch 1300 train loss: 0.0340 test loss: 0.0566\n",
      "Epoch 11 batch 1400 train loss: 0.0427 test loss: 0.0561\n",
      "Epoch 11 batch 1500 train loss: 0.0427 test loss: 0.0565\n",
      "Epoch 11 batch 1600 train loss: 0.0405 test loss: 0.0562\n",
      "Epoch 11 batch 1700 train loss: 0.0408 test loss: 0.0566\n",
      "Epoch 11 batch 1800 train loss: 0.0445 test loss: 0.0562\n",
      "Epoch 11 batch 1900 train loss: 0.0432 test loss: 0.0561\n",
      "Epoch 11 batch 2000 train loss: 0.0423 test loss: 0.0567\n",
      "Epoch 11 batch 2100 train loss: 0.0396 test loss: 0.0568\n",
      "Epoch 11 batch 2200 train loss: 0.0434 test loss: 0.0566\n",
      "Epoch 11 batch 2300 train loss: 0.0453 test loss: 0.0567\n",
      "Epoch 11 batch 2400 train loss: 0.0467 test loss: 0.0563\n",
      "Epoch 11 batch 2500 train loss: 0.0401 test loss: 0.0565\n",
      "Epoch 11 batch 2600 train loss: 0.0516 test loss: 0.0565\n",
      "Epoch 11 batch 2700 train loss: 0.0408 test loss: 0.0564\n",
      "Epoch 11 batch 2800 train loss: 0.0428 test loss: 0.0563\n",
      "Epoch 11 batch 2900 train loss: 0.0485 test loss: 0.0563\n",
      "Epoch 12 batch 0 train loss: 0.0437 test loss: 0.0565\n",
      "Epoch 12 batch 100 train loss: 0.0354 test loss: 0.0565\n",
      "Epoch 12 batch 200 train loss: 0.0414 test loss: 0.0563\n",
      "Epoch 12 batch 300 train loss: 0.0395 test loss: 0.0563\n",
      "Epoch 12 batch 400 train loss: 0.0430 test loss: 0.0567\n",
      "Epoch 12 batch 500 train loss: 0.0417 test loss: 0.0564\n",
      "Epoch 12 batch 600 train loss: 0.0528 test loss: 0.0560\n",
      "Epoch 12 batch 700 train loss: 0.0396 test loss: 0.0565\n",
      "Epoch 12 batch 800 train loss: 0.0500 test loss: 0.0565\n",
      "Epoch 12 batch 900 train loss: 0.0331 test loss: 0.0563\n",
      "Epoch 12 batch 1000 train loss: 0.0413 test loss: 0.0568\n",
      "Epoch 12 batch 1100 train loss: 0.0446 test loss: 0.0563\n",
      "Epoch 12 batch 1200 train loss: 0.0495 test loss: 0.0562\n",
      "Epoch 12 batch 1300 train loss: 0.0472 test loss: 0.0561\n",
      "Epoch 12 batch 1400 train loss: 0.0461 test loss: 0.0561\n",
      "Epoch 12 batch 1500 train loss: 0.0415 test loss: 0.0565\n",
      "Epoch 12 batch 1600 train loss: 0.0469 test loss: 0.0569\n",
      "Epoch 12 batch 1700 train loss: 0.0427 test loss: 0.0572\n",
      "Epoch 12 batch 1800 train loss: 0.0512 test loss: 0.0562\n",
      "Epoch 12 batch 1900 train loss: 0.0404 test loss: 0.0561\n",
      "Epoch 12 batch 2000 train loss: 0.0508 test loss: 0.0565\n",
      "Epoch 12 batch 2100 train loss: 0.0411 test loss: 0.0564\n",
      "Epoch 12 batch 2200 train loss: 0.0502 test loss: 0.0561\n",
      "Epoch 12 batch 2300 train loss: 0.0441 test loss: 0.0565\n",
      "Epoch 12 batch 2400 train loss: 0.0425 test loss: 0.0566\n",
      "Epoch 12 batch 2500 train loss: 0.0499 test loss: 0.0565\n",
      "Epoch 12 batch 2600 train loss: 0.0395 test loss: 0.0563\n",
      "Epoch 12 batch 2700 train loss: 0.0417 test loss: 0.0565\n",
      "Epoch 12 batch 2800 train loss: 0.0558 test loss: 0.0562\n",
      "Epoch 12 batch 2900 train loss: 0.0430 test loss: 0.0563\n",
      "Epoch 13 batch 0 train loss: 0.0521 test loss: 0.0564\n",
      "Epoch 13 batch 100 train loss: 0.0375 test loss: 0.0562\n",
      "Epoch 13 batch 200 train loss: 0.0527 test loss: 0.0565\n",
      "Epoch 13 batch 300 train loss: 0.0412 test loss: 0.0565\n",
      "Epoch 13 batch 400 train loss: 0.0435 test loss: 0.0568\n",
      "Epoch 13 batch 500 train loss: 0.0536 test loss: 0.0563\n",
      "Epoch 13 batch 600 train loss: 0.0415 test loss: 0.0562\n",
      "Epoch 13 batch 700 train loss: 0.0348 test loss: 0.0564\n",
      "Epoch 13 batch 800 train loss: 0.0479 test loss: 0.0566\n",
      "Epoch 13 batch 900 train loss: 0.0457 test loss: 0.0563\n",
      "Epoch 13 batch 1000 train loss: 0.0414 test loss: 0.0565\n",
      "Epoch 13 batch 1100 train loss: 0.0412 test loss: 0.0561\n",
      "Epoch 13 batch 1200 train loss: 0.0453 test loss: 0.0565\n",
      "Epoch 13 batch 1300 train loss: 0.0376 test loss: 0.0561\n",
      "Epoch 13 batch 1400 train loss: 0.0402 test loss: 0.0564\n",
      "Epoch 13 batch 1500 train loss: 0.0470 test loss: 0.0567\n",
      "Epoch 13 batch 1600 train loss: 0.0399 test loss: 0.0561\n",
      "Epoch 13 batch 1700 train loss: 0.0402 test loss: 0.0569\n",
      "Epoch 13 batch 1800 train loss: 0.0484 test loss: 0.0566\n",
      "Epoch 13 batch 1900 train loss: 0.0477 test loss: 0.0569\n",
      "Epoch 13 batch 2000 train loss: 0.0463 test loss: 0.0562\n",
      "Epoch 13 batch 2100 train loss: 0.0384 test loss: 0.0564\n",
      "Epoch 13 batch 2200 train loss: 0.0465 test loss: 0.0565\n",
      "Epoch 13 batch 2300 train loss: 0.0366 test loss: 0.0563\n",
      "Epoch 13 batch 2400 train loss: 0.0464 test loss: 0.0564\n",
      "Epoch 13 batch 2500 train loss: 0.0459 test loss: 0.0561\n",
      "Epoch 13 batch 2600 train loss: 0.0474 test loss: 0.0563\n",
      "Epoch 13 batch 2700 train loss: 0.0399 test loss: 0.0562\n",
      "Epoch 13 batch 2800 train loss: 0.0517 test loss: 0.0563\n",
      "Epoch 13 batch 2900 train loss: 0.0415 test loss: 0.0563\n",
      "Epoch 14 batch 0 train loss: 0.0454 test loss: 0.0562\n",
      "Epoch 14 batch 100 train loss: 0.0433 test loss: 0.0564\n",
      "Epoch 14 batch 200 train loss: 0.0534 test loss: 0.0564\n",
      "Epoch 14 batch 300 train loss: 0.0410 test loss: 0.0559\n",
      "Epoch 14 batch 400 train loss: 0.0424 test loss: 0.0560\n",
      "Epoch 14 batch 500 train loss: 0.0434 test loss: 0.0565\n",
      "Epoch 14 batch 600 train loss: 0.0333 test loss: 0.0567\n",
      "Epoch 14 batch 700 train loss: 0.0429 test loss: 0.0566\n",
      "Epoch 14 batch 800 train loss: 0.0382 test loss: 0.0566\n",
      "Epoch 14 batch 900 train loss: 0.0564 test loss: 0.0561\n",
      "Epoch 14 batch 1000 train loss: 0.0513 test loss: 0.0565\n",
      "Epoch 14 batch 1100 train loss: 0.0457 test loss: 0.0563\n",
      "Epoch 14 batch 1200 train loss: 0.0423 test loss: 0.0565\n",
      "Epoch 14 batch 1300 train loss: 0.0511 test loss: 0.0564\n",
      "Epoch 14 batch 1400 train loss: 0.0480 test loss: 0.0565\n",
      "Epoch 14 batch 1500 train loss: 0.0426 test loss: 0.0567\n",
      "Epoch 14 batch 1600 train loss: 0.0389 test loss: 0.0565\n",
      "Epoch 14 batch 1700 train loss: 0.0432 test loss: 0.0568\n",
      "Epoch 14 batch 1800 train loss: 0.0459 test loss: 0.0562\n",
      "Epoch 14 batch 1900 train loss: 0.0433 test loss: 0.0569\n",
      "Epoch 14 batch 2000 train loss: 0.0515 test loss: 0.0566\n",
      "Epoch 14 batch 2100 train loss: 0.0423 test loss: 0.0563\n",
      "Epoch 14 batch 2200 train loss: 0.0445 test loss: 0.0565\n",
      "Epoch 14 batch 2300 train loss: 0.0540 test loss: 0.0565\n",
      "Epoch 14 batch 2400 train loss: 0.0491 test loss: 0.0567\n",
      "Epoch 14 batch 2500 train loss: 0.0428 test loss: 0.0564\n",
      "Epoch 14 batch 2600 train loss: 0.0521 test loss: 0.0566\n",
      "Epoch 14 batch 2700 train loss: 0.0429 test loss: 0.0569\n",
      "Epoch 14 batch 2800 train loss: 0.0469 test loss: 0.0565\n",
      "Epoch 14 batch 2900 train loss: 0.0362 test loss: 0.0564\n",
      "Epoch 15 batch 0 train loss: 0.0426 test loss: 0.0564\n",
      "Epoch 15 batch 100 train loss: 0.0350 test loss: 0.0562\n",
      "Epoch 15 batch 200 train loss: 0.0412 test loss: 0.0562\n",
      "Epoch 15 batch 300 train loss: 0.0394 test loss: 0.0561\n",
      "Epoch 15 batch 400 train loss: 0.0484 test loss: 0.0563\n",
      "Epoch 15 batch 500 train loss: 0.0390 test loss: 0.0561\n",
      "Epoch 15 batch 600 train loss: 0.0502 test loss: 0.0567\n",
      "Epoch 15 batch 700 train loss: 0.0419 test loss: 0.0566\n",
      "Epoch 15 batch 800 train loss: 0.0504 test loss: 0.0566\n",
      "Epoch 15 batch 900 train loss: 0.0524 test loss: 0.0562\n",
      "Epoch 15 batch 1000 train loss: 0.0484 test loss: 0.0562\n",
      "Epoch 15 batch 1100 train loss: 0.0421 test loss: 0.0564\n",
      "Epoch 15 batch 1200 train loss: 0.0418 test loss: 0.0563\n",
      "Epoch 15 batch 1300 train loss: 0.0528 test loss: 0.0563\n",
      "Epoch 15 batch 1400 train loss: 0.0506 test loss: 0.0563\n",
      "Epoch 15 batch 1500 train loss: 0.0485 test loss: 0.0568\n",
      "Epoch 15 batch 1600 train loss: 0.0441 test loss: 0.0566\n",
      "Epoch 15 batch 1700 train loss: 0.0508 test loss: 0.0563\n",
      "Epoch 15 batch 1800 train loss: 0.0475 test loss: 0.0560\n",
      "Epoch 15 batch 1900 train loss: 0.0378 test loss: 0.0567\n",
      "Epoch 15 batch 2000 train loss: 0.0418 test loss: 0.0564\n",
      "Epoch 15 batch 2100 train loss: 0.0436 test loss: 0.0565\n",
      "Epoch 15 batch 2200 train loss: 0.0350 test loss: 0.0562\n",
      "Epoch 15 batch 2300 train loss: 0.0431 test loss: 0.0560\n",
      "Epoch 15 batch 2400 train loss: 0.0392 test loss: 0.0566\n",
      "Epoch 15 batch 2500 train loss: 0.0380 test loss: 0.0561\n",
      "Epoch 15 batch 2600 train loss: 0.0460 test loss: 0.0563\n",
      "Epoch 15 batch 2700 train loss: 0.0403 test loss: 0.0567\n",
      "Epoch 15 batch 2800 train loss: 0.0481 test loss: 0.0565\n",
      "Epoch 15 batch 2900 train loss: 0.0480 test loss: 0.0563\n",
      "Epoch 16 batch 0 train loss: 0.0497 test loss: 0.0564\n",
      "Epoch 16 batch 100 train loss: 0.0494 test loss: 0.0562\n",
      "Epoch 16 batch 200 train loss: 0.0419 test loss: 0.0561\n",
      "Epoch 16 batch 300 train loss: 0.0402 test loss: 0.0561\n",
      "Epoch 16 batch 400 train loss: 0.0467 test loss: 0.0564\n",
      "Epoch 16 batch 500 train loss: 0.0446 test loss: 0.0566\n",
      "Epoch 16 batch 600 train loss: 0.0457 test loss: 0.0561\n",
      "Epoch 16 batch 700 train loss: 0.0445 test loss: 0.0566\n",
      "Epoch 16 batch 800 train loss: 0.0417 test loss: 0.0564\n",
      "Epoch 16 batch 900 train loss: 0.0406 test loss: 0.0564\n",
      "Epoch 16 batch 1000 train loss: 0.0619 test loss: 0.0564\n",
      "Epoch 16 batch 1100 train loss: 0.0534 test loss: 0.0561\n",
      "Epoch 16 batch 1200 train loss: 0.0536 test loss: 0.0565\n",
      "Epoch 16 batch 1300 train loss: 0.0437 test loss: 0.0566\n",
      "Epoch 16 batch 1400 train loss: 0.0451 test loss: 0.0565\n",
      "Epoch 16 batch 1500 train loss: 0.0424 test loss: 0.0567\n",
      "Epoch 16 batch 1600 train loss: 0.0388 test loss: 0.0565\n",
      "Epoch 16 batch 1700 train loss: 0.0572 test loss: 0.0564\n",
      "Epoch 16 batch 1800 train loss: 0.0466 test loss: 0.0563\n",
      "Epoch 16 batch 1900 train loss: 0.0521 test loss: 0.0567\n",
      "Epoch 16 batch 2000 train loss: 0.0367 test loss: 0.0564\n",
      "Epoch 16 batch 2100 train loss: 0.0368 test loss: 0.0566\n",
      "Epoch 16 batch 2200 train loss: 0.0449 test loss: 0.0565\n",
      "Epoch 16 batch 2300 train loss: 0.0452 test loss: 0.0567\n",
      "Epoch 16 batch 2400 train loss: 0.0450 test loss: 0.0569\n",
      "Epoch 16 batch 2500 train loss: 0.0453 test loss: 0.0563\n",
      "Epoch 16 batch 2600 train loss: 0.0444 test loss: 0.0566\n",
      "Epoch 16 batch 2700 train loss: 0.0358 test loss: 0.0563\n",
      "Epoch 16 batch 2800 train loss: 0.0421 test loss: 0.0562\n",
      "Epoch 16 batch 2900 train loss: 0.0440 test loss: 0.0563\n",
      "Epoch 17 batch 0 train loss: 0.0386 test loss: 0.0559\n",
      "early stop.\n",
      "Checkpoint 11 restored!!\n",
      "2446/2446 [==============================] - 15s 6ms/step - loss: 0.0459\n",
      "Training for loss rate 0.95 start.\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.95p/ckpt-1\n",
      "Epoch 0 batch 0 train loss: 0.2092 test loss: 0.1939\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.95p/ckpt-2\n",
      "Epoch 0 batch 100 train loss: 0.0580 test loss: 0.0714\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.95p/ckpt-3\n",
      "Epoch 0 batch 200 train loss: 0.0571 test loss: 0.0706\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.95p/ckpt-4\n",
      "Epoch 0 batch 300 train loss: 0.0585 test loss: 0.0696\n",
      "Epoch 0 batch 400 train loss: 0.0568 test loss: 0.0706\n",
      "Epoch 0 batch 500 train loss: 0.0556 test loss: 0.0703\n",
      "Epoch 0 batch 600 train loss: 0.0540 test loss: 0.0726\n",
      "Epoch 0 batch 700 train loss: 0.0566 test loss: 0.0717\n",
      "Epoch 0 batch 800 train loss: 0.0549 test loss: 0.0712\n",
      "Epoch 0 batch 900 train loss: 0.0416 test loss: 0.0718\n",
      "Epoch 0 batch 1000 train loss: 0.0538 test loss: 0.0721\n",
      "Epoch 0 batch 1100 train loss: 0.0464 test loss: 0.0703\n",
      "Epoch 0 batch 1200 train loss: 0.0539 test loss: 0.0707\n",
      "Epoch 0 batch 1300 train loss: 0.0553 test loss: 0.0709\n",
      "Epoch 0 batch 1400 train loss: 0.0502 test loss: 0.0702\n",
      "Epoch 0 batch 1500 train loss: 0.0548 test loss: 0.0714\n",
      "Epoch 0 batch 1600 train loss: 0.0544 test loss: 0.0706\n",
      "Epoch 0 batch 1700 train loss: 0.0510 test loss: 0.0713\n",
      "Epoch 0 batch 1800 train loss: 0.0547 test loss: 0.0716\n",
      "Epoch 0 batch 1900 train loss: 0.0542 test loss: 0.0727\n",
      "Epoch 0 batch 2000 train loss: 0.0586 test loss: 0.0716\n",
      "Epoch 0 batch 2100 train loss: 0.0556 test loss: 0.0705\n",
      "Epoch 0 batch 2200 train loss: 0.0548 test loss: 0.0704\n",
      "Epoch 0 batch 2300 train loss: 0.0554 test loss: 0.0727\n",
      "Epoch 0 batch 2400 train loss: 0.0564 test loss: 0.0701\n",
      "Epoch 0 batch 2500 train loss: 0.0543 test loss: 0.0698\n",
      "Epoch 0 batch 2600 train loss: 0.0557 test loss: 0.0716\n",
      "Epoch 0 batch 2700 train loss: 0.0557 test loss: 0.0710\n",
      "Epoch 0 batch 2800 train loss: 0.0417 test loss: 0.0707\n",
      "Epoch 0 batch 2900 train loss: 0.0527 test loss: 0.0715\n",
      "Saving checkpoint at ./checkpoints/UNet_best_loss_0.95p/ckpt-5\n",
      "Epoch 1 batch 0 train loss: 0.0501 test loss: 0.0693\n",
      "Epoch 1 batch 100 train loss: 0.0568 test loss: 0.0711\n",
      "Epoch 1 batch 200 train loss: 0.0593 test loss: 0.0706\n",
      "Epoch 1 batch 300 train loss: 0.0540 test loss: 0.0697\n",
      "Epoch 1 batch 400 train loss: 0.0545 test loss: 0.0717\n",
      "Epoch 1 batch 500 train loss: 0.0491 test loss: 0.0704\n",
      "Epoch 1 batch 600 train loss: 0.0625 test loss: 0.0701\n",
      "Epoch 1 batch 700 train loss: 0.0497 test loss: 0.0714\n",
      "Epoch 1 batch 800 train loss: 0.0455 test loss: 0.0718\n",
      "Epoch 1 batch 900 train loss: 0.0533 test loss: 0.0705\n",
      "Epoch 1 batch 1000 train loss: 0.0466 test loss: 0.0720\n",
      "Epoch 1 batch 1100 train loss: 0.0615 test loss: 0.0700\n",
      "Epoch 1 batch 1200 train loss: 0.0571 test loss: 0.0708\n",
      "Epoch 1 batch 1300 train loss: 0.0452 test loss: 0.0707\n",
      "Epoch 1 batch 1400 train loss: 0.0545 test loss: 0.0704\n",
      "Epoch 1 batch 1500 train loss: 0.0531 test loss: 0.0711\n",
      "Epoch 1 batch 1600 train loss: 0.0515 test loss: 0.0704\n",
      "Epoch 1 batch 1700 train loss: 0.0518 test loss: 0.0699\n",
      "Epoch 1 batch 1800 train loss: 0.0478 test loss: 0.0723\n",
      "Epoch 1 batch 1900 train loss: 0.0550 test loss: 0.0698\n",
      "Epoch 1 batch 2000 train loss: 0.0496 test loss: 0.0713\n",
      "Epoch 1 batch 2100 train loss: 0.0502 test loss: 0.0710\n",
      "Epoch 1 batch 2200 train loss: 0.0505 test loss: 0.0702\n",
      "Epoch 1 batch 2300 train loss: 0.0584 test loss: 0.0705\n",
      "Epoch 1 batch 2400 train loss: 0.0484 test loss: 0.0713\n",
      "Epoch 1 batch 2500 train loss: 0.0485 test loss: 0.0705\n",
      "Epoch 1 batch 2600 train loss: 0.0521 test loss: 0.0702\n",
      "Epoch 1 batch 2700 train loss: 0.0500 test loss: 0.0698\n",
      "Epoch 1 batch 2800 train loss: 0.0532 test loss: 0.0703\n",
      "Epoch 1 batch 2900 train loss: 0.0569 test loss: 0.0699\n",
      "Epoch 2 batch 0 train loss: 0.0488 test loss: 0.0710\n",
      "Epoch 2 batch 100 train loss: 0.0584 test loss: 0.0708\n",
      "Epoch 2 batch 200 train loss: 0.0558 test loss: 0.0697\n",
      "Epoch 2 batch 300 train loss: 0.0551 test loss: 0.0698\n",
      "Epoch 2 batch 400 train loss: 0.0507 test loss: 0.0698\n",
      "Epoch 2 batch 500 train loss: 0.0526 test loss: 0.0706\n",
      "Epoch 2 batch 600 train loss: 0.0423 test loss: 0.0705\n",
      "Epoch 2 batch 700 train loss: 0.0539 test loss: 0.0706\n",
      "Epoch 2 batch 800 train loss: 0.0514 test loss: 0.0719\n",
      "Epoch 2 batch 900 train loss: 0.0558 test loss: 0.0700\n",
      "Epoch 2 batch 1000 train loss: 0.0492 test loss: 0.0709\n",
      "Epoch 2 batch 1100 train loss: 0.0503 test loss: 0.0701\n",
      "Epoch 2 batch 1200 train loss: 0.0466 test loss: 0.0705\n",
      "Epoch 2 batch 1300 train loss: 0.0533 test loss: 0.0701\n",
      "Epoch 2 batch 1400 train loss: 0.0545 test loss: 0.0701\n",
      "Epoch 2 batch 1500 train loss: 0.0602 test loss: 0.0711\n",
      "Epoch 2 batch 1600 train loss: 0.0433 test loss: 0.0708\n",
      "Epoch 2 batch 1700 train loss: 0.0534 test loss: 0.0702\n",
      "Epoch 2 batch 1800 train loss: 0.0590 test loss: 0.0705\n",
      "Epoch 2 batch 1900 train loss: 0.0518 test loss: 0.0705\n",
      "Epoch 2 batch 2000 train loss: 0.0505 test loss: 0.0700\n",
      "Epoch 2 batch 2100 train loss: 0.0537 test loss: 0.0699\n",
      "Epoch 2 batch 2200 train loss: 0.0608 test loss: 0.0697\n",
      "Epoch 2 batch 2300 train loss: 0.0488 test loss: 0.0699\n",
      "Epoch 2 batch 2400 train loss: 0.0533 test loss: 0.0707\n",
      "Epoch 2 batch 2500 train loss: 0.0473 test loss: 0.0719\n",
      "Epoch 2 batch 2600 train loss: 0.0526 test loss: 0.0704\n",
      "Epoch 2 batch 2700 train loss: 0.0592 test loss: 0.0697\n",
      "Epoch 2 batch 2800 train loss: 0.0574 test loss: 0.0703\n",
      "Epoch 2 batch 2900 train loss: 0.0573 test loss: 0.0702\n",
      "Epoch 3 batch 0 train loss: 0.0582 test loss: 0.0703\n",
      "Epoch 3 batch 100 train loss: 0.0544 test loss: 0.0701\n",
      "Epoch 3 batch 200 train loss: 0.0512 test loss: 0.0702\n",
      "Epoch 3 batch 300 train loss: 0.0534 test loss: 0.0711\n",
      "Epoch 3 batch 400 train loss: 0.0489 test loss: 0.0699\n",
      "Epoch 3 batch 500 train loss: 0.0499 test loss: 0.0702\n",
      "Epoch 3 batch 600 train loss: 0.0488 test loss: 0.0703\n",
      "Epoch 3 batch 700 train loss: 0.0508 test loss: 0.0706\n",
      "Epoch 3 batch 800 train loss: 0.0514 test loss: 0.0702\n",
      "Epoch 3 batch 900 train loss: 0.0601 test loss: 0.0704\n",
      "Epoch 3 batch 1000 train loss: 0.0514 test loss: 0.0703\n",
      "Epoch 3 batch 1100 train loss: 0.0505 test loss: 0.0705\n",
      "Epoch 3 batch 1200 train loss: 0.0475 test loss: 0.0710\n",
      "Epoch 3 batch 1300 train loss: 0.0622 test loss: 0.0707\n",
      "Epoch 3 batch 1400 train loss: 0.0462 test loss: 0.0709\n",
      "Epoch 3 batch 1500 train loss: 0.0543 test loss: 0.0709\n",
      "Epoch 3 batch 1600 train loss: 0.0522 test loss: 0.0711\n",
      "Epoch 3 batch 1700 train loss: 0.0479 test loss: 0.0708\n",
      "Epoch 3 batch 1800 train loss: 0.0568 test loss: 0.0702\n",
      "Epoch 3 batch 1900 train loss: 0.0466 test loss: 0.0704\n",
      "Epoch 3 batch 2000 train loss: 0.0492 test loss: 0.0699\n",
      "Epoch 3 batch 2100 train loss: 0.0593 test loss: 0.0715\n",
      "Epoch 3 batch 2200 train loss: 0.0519 test loss: 0.0703\n",
      "Epoch 3 batch 2300 train loss: 0.0566 test loss: 0.0706\n",
      "Epoch 3 batch 2400 train loss: 0.0528 test loss: 0.0713\n",
      "Epoch 3 batch 2500 train loss: 0.0500 test loss: 0.0717\n",
      "Epoch 3 batch 2600 train loss: 0.0548 test loss: 0.0714\n",
      "Epoch 3 batch 2700 train loss: 0.0534 test loss: 0.0720\n",
      "Epoch 3 batch 2800 train loss: 0.0550 test loss: 0.0702\n",
      "Epoch 3 batch 2900 train loss: 0.0605 test loss: 0.0704\n",
      "Epoch 4 batch 0 train loss: 0.0551 test loss: 0.0703\n",
      "Epoch 4 batch 100 train loss: 0.0575 test loss: 0.0701\n",
      "Epoch 4 batch 200 train loss: 0.0600 test loss: 0.0701\n",
      "Epoch 4 batch 300 train loss: 0.0510 test loss: 0.0700\n",
      "Epoch 4 batch 400 train loss: 0.0636 test loss: 0.0703\n",
      "Epoch 4 batch 500 train loss: 0.0589 test loss: 0.0705\n",
      "Epoch 4 batch 600 train loss: 0.0526 test loss: 0.0715\n",
      "Epoch 4 batch 700 train loss: 0.0515 test loss: 0.0703\n",
      "Epoch 4 batch 800 train loss: 0.0465 test loss: 0.0703\n",
      "Epoch 4 batch 900 train loss: 0.0606 test loss: 0.0700\n",
      "Epoch 4 batch 1000 train loss: 0.0607 test loss: 0.0705\n",
      "Epoch 4 batch 1100 train loss: 0.0566 test loss: 0.0706\n",
      "Epoch 4 batch 1200 train loss: 0.0599 test loss: 0.0704\n",
      "Epoch 4 batch 1300 train loss: 0.0524 test loss: 0.0698\n",
      "Epoch 4 batch 1400 train loss: 0.0542 test loss: 0.0705\n",
      "Epoch 4 batch 1500 train loss: 0.0467 test loss: 0.0706\n",
      "Epoch 4 batch 1600 train loss: 0.0550 test loss: 0.0705\n",
      "Epoch 4 batch 1700 train loss: 0.0525 test loss: 0.0704\n",
      "Epoch 4 batch 1800 train loss: 0.0489 test loss: 0.0701\n",
      "Epoch 4 batch 1900 train loss: 0.0490 test loss: 0.0710\n",
      "Epoch 4 batch 2000 train loss: 0.0523 test loss: 0.0700\n",
      "Epoch 4 batch 2100 train loss: 0.0536 test loss: 0.0706\n",
      "Epoch 4 batch 2200 train loss: 0.0557 test loss: 0.0707\n",
      "Epoch 4 batch 2300 train loss: 0.0472 test loss: 0.0712\n",
      "Epoch 4 batch 2400 train loss: 0.0488 test loss: 0.0708\n",
      "Epoch 4 batch 2500 train loss: 0.0617 test loss: 0.0706\n",
      "Epoch 4 batch 2600 train loss: 0.0558 test loss: 0.0698\n",
      "Epoch 4 batch 2700 train loss: 0.0536 test loss: 0.0706\n",
      "Epoch 4 batch 2800 train loss: 0.0533 test loss: 0.0696\n",
      "Epoch 4 batch 2900 train loss: 0.0570 test loss: 0.0705\n",
      "Epoch 5 batch 0 train loss: 0.0532 test loss: 0.0705\n",
      "Epoch 5 batch 100 train loss: 0.0479 test loss: 0.0702\n",
      "Epoch 5 batch 200 train loss: 0.0558 test loss: 0.0702\n",
      "Epoch 5 batch 300 train loss: 0.0502 test loss: 0.0709\n",
      "Epoch 5 batch 400 train loss: 0.0517 test loss: 0.0703\n",
      "Epoch 5 batch 500 train loss: 0.0453 test loss: 0.0707\n",
      "Epoch 5 batch 600 train loss: 0.0532 test loss: 0.0707\n",
      "Epoch 5 batch 700 train loss: 0.0560 test loss: 0.0707\n",
      "Epoch 5 batch 800 train loss: 0.0579 test loss: 0.0710\n",
      "Epoch 5 batch 900 train loss: 0.0519 test loss: 0.0708\n",
      "Epoch 5 batch 1000 train loss: 0.0536 test loss: 0.0697\n",
      "Epoch 5 batch 1100 train loss: 0.0560 test loss: 0.0698\n",
      "Epoch 5 batch 1200 train loss: 0.0516 test loss: 0.0706\n",
      "Epoch 5 batch 1300 train loss: 0.0543 test loss: 0.0701\n",
      "Epoch 5 batch 1400 train loss: 0.0625 test loss: 0.0708\n",
      "Epoch 5 batch 1500 train loss: 0.0556 test loss: 0.0708\n",
      "Epoch 5 batch 1600 train loss: 0.0533 test loss: 0.0706\n",
      "Epoch 5 batch 1700 train loss: 0.0523 test loss: 0.0700\n",
      "Epoch 5 batch 1800 train loss: 0.0522 test loss: 0.0709\n",
      "Epoch 5 batch 1900 train loss: 0.0520 test loss: 0.0704\n",
      "Epoch 5 batch 2000 train loss: 0.0504 test loss: 0.0701\n",
      "Epoch 5 batch 2100 train loss: 0.0492 test loss: 0.0708\n",
      "Epoch 5 batch 2200 train loss: 0.0418 test loss: 0.0707\n",
      "Epoch 5 batch 2300 train loss: 0.0562 test loss: 0.0700\n",
      "Epoch 5 batch 2400 train loss: 0.0451 test loss: 0.0704\n",
      "Epoch 5 batch 2500 train loss: 0.0556 test loss: 0.0703\n",
      "Epoch 5 batch 2600 train loss: 0.0512 test loss: 0.0710\n",
      "Epoch 5 batch 2700 train loss: 0.0542 test loss: 0.0709\n",
      "Epoch 5 batch 2800 train loss: 0.0566 test loss: 0.0708\n",
      "Epoch 5 batch 2900 train loss: 0.0534 test loss: 0.0699\n",
      "Epoch 6 batch 0 train loss: 0.0586 test loss: 0.0710\n",
      "Epoch 6 batch 100 train loss: 0.0514 test loss: 0.0699\n",
      "Epoch 6 batch 200 train loss: 0.0518 test loss: 0.0707\n",
      "Epoch 6 batch 300 train loss: 0.0475 test loss: 0.0701\n",
      "Epoch 6 batch 400 train loss: 0.0525 test loss: 0.0701\n",
      "Epoch 6 batch 500 train loss: 0.0512 test loss: 0.0704\n",
      "Epoch 6 batch 600 train loss: 0.0477 test loss: 0.0702\n",
      "Epoch 6 batch 700 train loss: 0.0522 test loss: 0.0706\n",
      "Epoch 6 batch 800 train loss: 0.0541 test loss: 0.0704\n",
      "Epoch 6 batch 900 train loss: 0.0575 test loss: 0.0703\n",
      "Epoch 6 batch 1000 train loss: 0.0482 test loss: 0.0713\n",
      "Epoch 6 batch 1100 train loss: 0.0515 test loss: 0.0701\n",
      "Epoch 6 batch 1200 train loss: 0.0507 test loss: 0.0699\n",
      "Epoch 6 batch 1300 train loss: 0.0512 test loss: 0.0706\n",
      "Epoch 6 batch 1400 train loss: 0.0604 test loss: 0.0710\n",
      "Epoch 6 batch 1500 train loss: 0.0537 test loss: 0.0700\n",
      "Epoch 6 batch 1600 train loss: 0.0529 test loss: 0.0702\n",
      "Epoch 6 batch 1700 train loss: 0.0530 test loss: 0.0702\n",
      "Epoch 6 batch 1800 train loss: 0.0589 test loss: 0.0702\n",
      "Epoch 6 batch 1900 train loss: 0.0552 test loss: 0.0703\n",
      "Epoch 6 batch 2000 train loss: 0.0501 test loss: 0.0699\n",
      "Epoch 6 batch 2100 train loss: 0.0529 test loss: 0.0700\n",
      "Epoch 6 batch 2200 train loss: 0.0555 test loss: 0.0705\n",
      "Epoch 6 batch 2300 train loss: 0.0475 test loss: 0.0699\n",
      "Epoch 6 batch 2400 train loss: 0.0500 test loss: 0.0707\n",
      "Epoch 6 batch 2500 train loss: 0.0527 test loss: 0.0707\n",
      "Epoch 6 batch 2600 train loss: 0.0583 test loss: 0.0704\n",
      "Epoch 6 batch 2700 train loss: 0.0530 test loss: 0.0701\n",
      "Epoch 6 batch 2800 train loss: 0.0517 test loss: 0.0708\n",
      "Epoch 6 batch 2900 train loss: 0.0551 test loss: 0.0698\n",
      "Epoch 7 batch 0 train loss: 0.0512 test loss: 0.0705\n",
      "Epoch 7 batch 100 train loss: 0.0512 test loss: 0.0711\n",
      "Epoch 7 batch 200 train loss: 0.0558 test loss: 0.0702\n",
      "Epoch 7 batch 300 train loss: 0.0519 test loss: 0.0703\n",
      "Epoch 7 batch 400 train loss: 0.0530 test loss: 0.0704\n",
      "Epoch 7 batch 500 train loss: 0.0567 test loss: 0.0704\n",
      "Epoch 7 batch 600 train loss: 0.0621 test loss: 0.0712\n",
      "Epoch 7 batch 700 train loss: 0.0530 test loss: 0.0706\n",
      "Epoch 7 batch 800 train loss: 0.0533 test loss: 0.0708\n",
      "Epoch 7 batch 900 train loss: 0.0514 test loss: 0.0703\n",
      "Epoch 7 batch 1000 train loss: 0.0444 test loss: 0.0705\n",
      "Epoch 7 batch 1100 train loss: 0.0590 test loss: 0.0705\n",
      "Epoch 7 batch 1200 train loss: 0.0621 test loss: 0.0703\n",
      "Epoch 7 batch 1300 train loss: 0.0509 test loss: 0.0699\n",
      "Epoch 7 batch 1400 train loss: 0.0597 test loss: 0.0708\n",
      "Epoch 7 batch 1500 train loss: 0.0510 test loss: 0.0704\n",
      "Epoch 7 batch 1600 train loss: 0.0527 test loss: 0.0703\n",
      "Epoch 7 batch 1700 train loss: 0.0591 test loss: 0.0712\n",
      "Epoch 7 batch 1800 train loss: 0.0471 test loss: 0.0705\n",
      "Epoch 7 batch 1900 train loss: 0.0449 test loss: 0.0709\n",
      "Epoch 7 batch 2000 train loss: 0.0576 test loss: 0.0704\n",
      "Epoch 7 batch 2100 train loss: 0.0569 test loss: 0.0708\n",
      "Epoch 7 batch 2200 train loss: 0.0546 test loss: 0.0700\n",
      "Epoch 7 batch 2300 train loss: 0.0510 test loss: 0.0704\n",
      "Epoch 7 batch 2400 train loss: 0.0505 test loss: 0.0705\n",
      "Epoch 7 batch 2500 train loss: 0.0568 test loss: 0.0709\n",
      "Epoch 7 batch 2600 train loss: 0.0497 test loss: 0.0703\n",
      "Epoch 7 batch 2700 train loss: 0.0527 test loss: 0.0704\n",
      "Epoch 7 batch 2800 train loss: 0.0594 test loss: 0.0703\n",
      "Epoch 7 batch 2900 train loss: 0.0558 test loss: 0.0698\n",
      "Epoch 8 batch 0 train loss: 0.0560 test loss: 0.0696\n",
      "Epoch 8 batch 100 train loss: 0.0532 test loss: 0.0703\n",
      "Epoch 8 batch 200 train loss: 0.0521 test loss: 0.0703\n",
      "Epoch 8 batch 300 train loss: 0.0527 test loss: 0.0698\n",
      "Epoch 8 batch 400 train loss: 0.0574 test loss: 0.0711\n",
      "Epoch 8 batch 500 train loss: 0.0558 test loss: 0.0708\n",
      "Epoch 8 batch 600 train loss: 0.0538 test loss: 0.0703\n",
      "Epoch 8 batch 700 train loss: 0.0592 test loss: 0.0701\n",
      "Epoch 8 batch 800 train loss: 0.0501 test loss: 0.0708\n",
      "Epoch 8 batch 900 train loss: 0.0519 test loss: 0.0707\n",
      "Epoch 8 batch 1000 train loss: 0.0585 test loss: 0.0708\n",
      "Epoch 8 batch 1100 train loss: 0.0595 test loss: 0.0699\n",
      "Epoch 8 batch 1200 train loss: 0.0505 test loss: 0.0704\n",
      "Epoch 8 batch 1300 train loss: 0.0593 test loss: 0.0712\n",
      "Epoch 8 batch 1400 train loss: 0.0530 test loss: 0.0703\n",
      "Epoch 8 batch 1500 train loss: 0.0571 test loss: 0.0709\n",
      "Epoch 8 batch 1600 train loss: 0.0454 test loss: 0.0708\n",
      "Epoch 8 batch 1700 train loss: 0.0498 test loss: 0.0707\n",
      "Epoch 8 batch 1800 train loss: 0.0537 test loss: 0.0704\n",
      "Epoch 8 batch 1900 train loss: 0.0576 test loss: 0.0706\n",
      "Epoch 8 batch 2000 train loss: 0.0506 test loss: 0.0705\n",
      "Epoch 8 batch 2100 train loss: 0.0585 test loss: 0.0704\n",
      "Epoch 8 batch 2200 train loss: 0.0534 test loss: 0.0710\n",
      "Epoch 8 batch 2300 train loss: 0.0510 test loss: 0.0711\n",
      "Epoch 8 batch 2400 train loss: 0.0448 test loss: 0.0696\n",
      "Epoch 8 batch 2500 train loss: 0.0522 test loss: 0.0703\n",
      "Epoch 8 batch 2600 train loss: 0.0456 test loss: 0.0705\n",
      "Epoch 8 batch 2700 train loss: 0.0521 test loss: 0.0711\n",
      "Epoch 8 batch 2800 train loss: 0.0510 test loss: 0.0700\n",
      "Epoch 8 batch 2900 train loss: 0.0477 test loss: 0.0702\n",
      "Epoch 9 batch 0 train loss: 0.0587 test loss: 0.0702\n",
      "Epoch 9 batch 100 train loss: 0.0487 test loss: 0.0698\n",
      "Epoch 9 batch 200 train loss: 0.0567 test loss: 0.0702\n",
      "Epoch 9 batch 300 train loss: 0.0539 test loss: 0.0707\n",
      "Epoch 9 batch 400 train loss: 0.0585 test loss: 0.0699\n",
      "Epoch 9 batch 500 train loss: 0.0384 test loss: 0.0709\n",
      "Epoch 9 batch 600 train loss: 0.0522 test loss: 0.0707\n",
      "Epoch 9 batch 700 train loss: 0.0538 test loss: 0.0706\n",
      "Epoch 9 batch 800 train loss: 0.0644 test loss: 0.0706\n",
      "Epoch 9 batch 900 train loss: 0.0502 test loss: 0.0699\n",
      "Epoch 9 batch 1000 train loss: 0.0505 test loss: 0.0700\n",
      "Epoch 9 batch 1100 train loss: 0.0500 test loss: 0.0700\n",
      "Epoch 9 batch 1200 train loss: 0.0598 test loss: 0.0705\n",
      "Epoch 9 batch 1300 train loss: 0.0540 test loss: 0.0708\n",
      "Epoch 9 batch 1400 train loss: 0.0565 test loss: 0.0694\n",
      "Epoch 9 batch 1500 train loss: 0.0555 test loss: 0.0711\n",
      "Epoch 9 batch 1600 train loss: 0.0592 test loss: 0.0700\n",
      "Epoch 9 batch 1700 train loss: 0.0500 test loss: 0.0701\n",
      "Epoch 9 batch 1800 train loss: 0.0579 test loss: 0.0702\n",
      "Epoch 9 batch 1900 train loss: 0.0492 test loss: 0.0704\n",
      "Epoch 9 batch 2000 train loss: 0.0529 test loss: 0.0702\n",
      "Epoch 9 batch 2100 train loss: 0.0574 test loss: 0.0698\n",
      "Epoch 9 batch 2200 train loss: 0.0524 test loss: 0.0705\n",
      "Epoch 9 batch 2300 train loss: 0.0486 test loss: 0.0713\n",
      "Epoch 9 batch 2400 train loss: 0.0610 test loss: 0.0705\n",
      "Epoch 9 batch 2500 train loss: 0.0563 test loss: 0.0695\n",
      "Epoch 9 batch 2600 train loss: 0.0440 test loss: 0.0702\n",
      "Epoch 9 batch 2700 train loss: 0.0577 test loss: 0.0700\n",
      "Epoch 9 batch 2800 train loss: 0.0435 test loss: 0.0697\n",
      "Epoch 9 batch 2900 train loss: 0.0540 test loss: 0.0703\n",
      "Epoch 10 batch 0 train loss: 0.0560 test loss: 0.0702\n",
      "Epoch 10 batch 100 train loss: 0.0506 test loss: 0.0706\n",
      "Epoch 10 batch 200 train loss: 0.0432 test loss: 0.0698\n",
      "Epoch 10 batch 300 train loss: 0.0549 test loss: 0.0703\n",
      "Epoch 10 batch 400 train loss: 0.0572 test loss: 0.0703\n",
      "Epoch 10 batch 500 train loss: 0.0597 test loss: 0.0703\n",
      "Epoch 10 batch 600 train loss: 0.0487 test loss: 0.0709\n",
      "Epoch 10 batch 700 train loss: 0.0555 test loss: 0.0707\n",
      "Epoch 10 batch 800 train loss: 0.0504 test loss: 0.0703\n",
      "Epoch 10 batch 900 train loss: 0.0496 test loss: 0.0702\n",
      "Epoch 10 batch 1000 train loss: 0.0536 test loss: 0.0706\n",
      "Epoch 10 batch 1100 train loss: 0.0571 test loss: 0.0699\n",
      "Epoch 10 batch 1200 train loss: 0.0541 test loss: 0.0701\n",
      "Epoch 10 batch 1300 train loss: 0.0505 test loss: 0.0704\n",
      "Epoch 10 batch 1400 train loss: 0.0530 test loss: 0.0701\n",
      "Epoch 10 batch 1500 train loss: 0.0533 test loss: 0.0707\n",
      "Epoch 10 batch 1600 train loss: 0.0634 test loss: 0.0705\n",
      "Epoch 10 batch 1700 train loss: 0.0462 test loss: 0.0709\n",
      "Epoch 10 batch 1800 train loss: 0.0607 test loss: 0.0702\n",
      "Epoch 10 batch 1900 train loss: 0.0490 test loss: 0.0703\n",
      "Epoch 10 batch 2000 train loss: 0.0571 test loss: 0.0704\n",
      "Epoch 10 batch 2100 train loss: 0.0595 test loss: 0.0702\n",
      "Epoch 10 batch 2200 train loss: 0.0515 test loss: 0.0706\n",
      "Epoch 10 batch 2300 train loss: 0.0543 test loss: 0.0704\n",
      "Epoch 10 batch 2400 train loss: 0.0551 test loss: 0.0706\n",
      "Epoch 10 batch 2500 train loss: 0.0493 test loss: 0.0706\n",
      "Epoch 10 batch 2600 train loss: 0.0657 test loss: 0.0702\n",
      "Epoch 10 batch 2700 train loss: 0.0584 test loss: 0.0705\n",
      "Epoch 10 batch 2800 train loss: 0.0558 test loss: 0.0695\n",
      "Epoch 10 batch 2900 train loss: 0.0513 test loss: 0.0698\n",
      "Epoch 11 batch 0 train loss: 0.0471 test loss: 0.0701\n",
      "Epoch 11 batch 100 train loss: 0.0516 test loss: 0.0705\n",
      "Epoch 11 batch 200 train loss: 0.0572 test loss: 0.0702\n",
      "Epoch 11 batch 300 train loss: 0.0619 test loss: 0.0705\n",
      "Epoch 11 batch 400 train loss: 0.0630 test loss: 0.0702\n",
      "Epoch 11 batch 500 train loss: 0.0555 test loss: 0.0705\n",
      "Epoch 11 batch 600 train loss: 0.0518 test loss: 0.0711\n",
      "Epoch 11 batch 700 train loss: 0.0538 test loss: 0.0704\n",
      "Epoch 11 batch 800 train loss: 0.0613 test loss: 0.0712\n",
      "Epoch 11 batch 900 train loss: 0.0583 test loss: 0.0705\n",
      "Epoch 11 batch 1000 train loss: 0.0504 test loss: 0.0708\n",
      "Epoch 11 batch 1100 train loss: 0.0540 test loss: 0.0702\n",
      "Epoch 11 batch 1200 train loss: 0.0526 test loss: 0.0700\n",
      "Epoch 11 batch 1300 train loss: 0.0550 test loss: 0.0701\n",
      "Epoch 11 batch 1400 train loss: 0.0518 test loss: 0.0707\n",
      "Epoch 11 batch 1500 train loss: 0.0552 test loss: 0.0705\n",
      "Epoch 11 batch 1600 train loss: 0.0599 test loss: 0.0704\n",
      "Epoch 11 batch 1700 train loss: 0.0505 test loss: 0.0708\n",
      "Epoch 11 batch 1800 train loss: 0.0525 test loss: 0.0703\n",
      "Epoch 11 batch 1900 train loss: 0.0471 test loss: 0.0701\n",
      "Epoch 11 batch 2000 train loss: 0.0481 test loss: 0.0708\n",
      "Epoch 11 batch 2100 train loss: 0.0498 test loss: 0.0711\n",
      "Epoch 11 batch 2200 train loss: 0.0453 test loss: 0.0705\n",
      "Epoch 11 batch 2300 train loss: 0.0560 test loss: 0.0704\n",
      "Epoch 11 batch 2400 train loss: 0.0532 test loss: 0.0702\n",
      "Epoch 11 batch 2500 train loss: 0.0481 test loss: 0.0703\n",
      "Epoch 11 batch 2600 train loss: 0.0580 test loss: 0.0707\n",
      "Epoch 11 batch 2700 train loss: 0.0483 test loss: 0.0697\n",
      "Epoch 11 batch 2800 train loss: 0.0555 test loss: 0.0699\n",
      "Epoch 11 batch 2900 train loss: 0.0490 test loss: 0.0702\n",
      "Epoch 12 batch 0 train loss: 0.0568 test loss: 0.0702\n",
      "Epoch 12 batch 100 train loss: 0.0563 test loss: 0.0704\n",
      "Epoch 12 batch 200 train loss: 0.0547 test loss: 0.0703\n",
      "Epoch 12 batch 300 train loss: 0.0576 test loss: 0.0697\n",
      "Epoch 12 batch 400 train loss: 0.0618 test loss: 0.0702\n",
      "Epoch 12 batch 500 train loss: 0.0517 test loss: 0.0700\n",
      "Epoch 12 batch 600 train loss: 0.0540 test loss: 0.0699\n",
      "Epoch 12 batch 700 train loss: 0.0473 test loss: 0.0707\n",
      "Epoch 12 batch 800 train loss: 0.0559 test loss: 0.0705\n",
      "Epoch 12 batch 900 train loss: 0.0571 test loss: 0.0697\n",
      "Epoch 12 batch 1000 train loss: 0.0546 test loss: 0.0698\n",
      "Epoch 12 batch 1100 train loss: 0.0559 test loss: 0.0702\n",
      "Epoch 12 batch 1200 train loss: 0.0543 test loss: 0.0706\n",
      "Epoch 12 batch 1300 train loss: 0.0515 test loss: 0.0703\n",
      "Epoch 12 batch 1400 train loss: 0.0508 test loss: 0.0705\n",
      "Epoch 12 batch 1500 train loss: 0.0481 test loss: 0.0702\n",
      "Epoch 12 batch 1600 train loss: 0.0470 test loss: 0.0712\n",
      "Epoch 12 batch 1700 train loss: 0.0495 test loss: 0.0709\n",
      "Epoch 12 batch 1800 train loss: 0.0560 test loss: 0.0707\n",
      "Epoch 12 batch 1900 train loss: 0.0546 test loss: 0.0701\n",
      "Epoch 12 batch 2000 train loss: 0.0510 test loss: 0.0701\n",
      "Epoch 12 batch 2100 train loss: 0.0520 test loss: 0.0705\n",
      "Epoch 12 batch 2200 train loss: 0.0487 test loss: 0.0704\n",
      "Epoch 12 batch 2300 train loss: 0.0557 test loss: 0.0702\n",
      "Epoch 12 batch 2400 train loss: 0.0550 test loss: 0.0699\n",
      "Epoch 12 batch 2500 train loss: 0.0517 test loss: 0.0707\n",
      "Epoch 12 batch 2600 train loss: 0.0517 test loss: 0.0705\n",
      "Epoch 12 batch 2700 train loss: 0.0525 test loss: 0.0705\n",
      "Epoch 12 batch 2800 train loss: 0.0521 test loss: 0.0698\n",
      "Epoch 12 batch 2900 train loss: 0.0498 test loss: 0.0706\n",
      "Epoch 13 batch 0 train loss: 0.0533 test loss: 0.0694\n",
      "Epoch 13 batch 100 train loss: 0.0477 test loss: 0.0704\n",
      "Epoch 13 batch 200 train loss: 0.0476 test loss: 0.0704\n",
      "Epoch 13 batch 300 train loss: 0.0472 test loss: 0.0703\n",
      "Epoch 13 batch 400 train loss: 0.0496 test loss: 0.0705\n",
      "Epoch 13 batch 500 train loss: 0.0545 test loss: 0.0704\n",
      "Epoch 13 batch 600 train loss: 0.0541 test loss: 0.0704\n",
      "Epoch 13 batch 700 train loss: 0.0484 test loss: 0.0706\n",
      "Epoch 13 batch 800 train loss: 0.0489 test loss: 0.0702\n",
      "Epoch 13 batch 900 train loss: 0.0533 test loss: 0.0704\n",
      "Epoch 13 batch 1000 train loss: 0.0466 test loss: 0.0703\n",
      "Epoch 13 batch 1100 train loss: 0.0553 test loss: 0.0701\n",
      "Epoch 13 batch 1200 train loss: 0.0525 test loss: 0.0701\n",
      "Epoch 13 batch 1300 train loss: 0.0512 test loss: 0.0706\n",
      "Epoch 13 batch 1400 train loss: 0.0536 test loss: 0.0705\n",
      "Epoch 13 batch 1500 train loss: 0.0457 test loss: 0.0697\n",
      "Epoch 13 batch 1600 train loss: 0.0510 test loss: 0.0704\n",
      "Epoch 13 batch 1700 train loss: 0.0522 test loss: 0.0705\n",
      "Epoch 13 batch 1800 train loss: 0.0558 test loss: 0.0702\n",
      "Epoch 13 batch 1900 train loss: 0.0512 test loss: 0.0704\n",
      "Epoch 13 batch 2000 train loss: 0.0549 test loss: 0.0704\n",
      "Epoch 13 batch 2100 train loss: 0.0532 test loss: 0.0701\n",
      "Epoch 13 batch 2200 train loss: 0.0452 test loss: 0.0707\n",
      "Epoch 13 batch 2300 train loss: 0.0581 test loss: 0.0707\n",
      "Epoch 13 batch 2400 train loss: 0.0526 test loss: 0.0706\n",
      "Epoch 13 batch 2500 train loss: 0.0566 test loss: 0.0702\n",
      "Epoch 13 batch 2600 train loss: 0.0437 test loss: 0.0704\n",
      "Epoch 13 batch 2700 train loss: 0.0578 test loss: 0.0704\n",
      "Epoch 13 batch 2800 train loss: 0.0464 test loss: 0.0697\n",
      "Epoch 13 batch 2900 train loss: 0.0518 test loss: 0.0705\n",
      "Epoch 14 batch 0 train loss: 0.0416 test loss: 0.0708\n",
      "Epoch 14 batch 100 train loss: 0.0494 test loss: 0.0707\n",
      "Epoch 14 batch 200 train loss: 0.0634 test loss: 0.0705\n",
      "Epoch 14 batch 300 train loss: 0.0557 test loss: 0.0704\n",
      "Epoch 14 batch 400 train loss: 0.0538 test loss: 0.0701\n",
      "Epoch 14 batch 500 train loss: 0.0542 test loss: 0.0707\n",
      "Epoch 14 batch 600 train loss: 0.0563 test loss: 0.0703\n",
      "Epoch 14 batch 700 train loss: 0.0521 test loss: 0.0702\n",
      "Epoch 14 batch 800 train loss: 0.0523 test loss: 0.0708\n",
      "Epoch 14 batch 900 train loss: 0.0546 test loss: 0.0703\n",
      "Epoch 14 batch 1000 train loss: 0.0502 test loss: 0.0700\n",
      "Epoch 14 batch 1100 train loss: 0.0507 test loss: 0.0704\n",
      "Epoch 14 batch 1200 train loss: 0.0548 test loss: 0.0700\n",
      "Epoch 14 batch 1300 train loss: 0.0573 test loss: 0.0710\n",
      "Epoch 14 batch 1400 train loss: 0.0546 test loss: 0.0702\n",
      "Epoch 14 batch 1500 train loss: 0.0510 test loss: 0.0707\n",
      "Epoch 14 batch 1600 train loss: 0.0570 test loss: 0.0706\n",
      "Epoch 14 batch 1700 train loss: 0.0569 test loss: 0.0710\n",
      "Epoch 14 batch 1800 train loss: 0.0494 test loss: 0.0707\n",
      "Epoch 14 batch 1900 train loss: 0.0546 test loss: 0.0704\n",
      "Epoch 14 batch 2000 train loss: 0.0581 test loss: 0.0700\n",
      "Epoch 14 batch 2100 train loss: 0.0534 test loss: 0.0700\n",
      "Epoch 14 batch 2200 train loss: 0.0552 test loss: 0.0700\n",
      "Epoch 14 batch 2300 train loss: 0.0572 test loss: 0.0704\n",
      "Epoch 14 batch 2400 train loss: 0.0560 test loss: 0.0702\n",
      "Epoch 14 batch 2500 train loss: 0.0599 test loss: 0.0704\n",
      "Epoch 14 batch 2600 train loss: 0.0608 test loss: 0.0704\n",
      "Epoch 14 batch 2700 train loss: 0.0593 test loss: 0.0707\n",
      "Epoch 14 batch 2800 train loss: 0.0586 test loss: 0.0701\n",
      "Epoch 14 batch 2900 train loss: 0.0545 test loss: 0.0699\n",
      "Epoch 15 batch 0 train loss: 0.0532 test loss: 0.0700\n",
      "Epoch 15 batch 100 train loss: 0.0457 test loss: 0.0705\n",
      "Epoch 15 batch 200 train loss: 0.0489 test loss: 0.0710\n",
      "Epoch 15 batch 300 train loss: 0.0531 test loss: 0.0699\n",
      "Epoch 15 batch 400 train loss: 0.0550 test loss: 0.0702\n",
      "Epoch 15 batch 500 train loss: 0.0522 test loss: 0.0708\n",
      "Epoch 15 batch 600 train loss: 0.0566 test loss: 0.0703\n",
      "Epoch 15 batch 700 train loss: 0.0578 test loss: 0.0701\n",
      "Epoch 15 batch 800 train loss: 0.0576 test loss: 0.0708\n",
      "Epoch 15 batch 900 train loss: 0.0535 test loss: 0.0703\n",
      "Epoch 15 batch 1000 train loss: 0.0541 test loss: 0.0702\n",
      "Epoch 15 batch 1100 train loss: 0.0482 test loss: 0.0700\n",
      "Epoch 15 batch 1200 train loss: 0.0552 test loss: 0.0704\n",
      "Epoch 15 batch 1300 train loss: 0.0532 test loss: 0.0703\n",
      "Epoch 15 batch 1400 train loss: 0.0605 test loss: 0.0702\n",
      "Epoch 15 batch 1500 train loss: 0.0478 test loss: 0.0703\n",
      "Epoch 15 batch 1600 train loss: 0.0457 test loss: 0.0709\n",
      "Epoch 15 batch 1700 train loss: 0.0523 test loss: 0.0708\n",
      "Epoch 15 batch 1800 train loss: 0.0545 test loss: 0.0703\n",
      "Epoch 15 batch 1900 train loss: 0.0534 test loss: 0.0712\n",
      "Epoch 15 batch 2000 train loss: 0.0501 test loss: 0.0705\n",
      "Epoch 15 batch 2100 train loss: 0.0543 test loss: 0.0700\n",
      "Epoch 15 batch 2200 train loss: 0.0640 test loss: 0.0709\n",
      "Epoch 15 batch 2300 train loss: 0.0495 test loss: 0.0707\n",
      "Epoch 15 batch 2400 train loss: 0.0465 test loss: 0.0705\n",
      "Epoch 15 batch 2500 train loss: 0.0544 test loss: 0.0701\n",
      "Epoch 15 batch 2600 train loss: 0.0532 test loss: 0.0700\n",
      "Epoch 15 batch 2700 train loss: 0.0523 test loss: 0.0701\n",
      "Epoch 15 batch 2800 train loss: 0.0543 test loss: 0.0700\n",
      "Epoch 15 batch 2900 train loss: 0.0587 test loss: 0.0702\n",
      "Epoch 16 batch 0 train loss: 0.0519 test loss: 0.0699\n",
      "Epoch 16 batch 100 train loss: 0.0437 test loss: 0.0706\n",
      "Epoch 16 batch 200 train loss: 0.0516 test loss: 0.0704\n",
      "Epoch 16 batch 300 train loss: 0.0582 test loss: 0.0703\n",
      "Epoch 16 batch 400 train loss: 0.0531 test loss: 0.0706\n",
      "Epoch 16 batch 500 train loss: 0.0467 test loss: 0.0704\n",
      "Epoch 16 batch 600 train loss: 0.0592 test loss: 0.0705\n",
      "Epoch 16 batch 700 train loss: 0.0525 test loss: 0.0708\n",
      "Epoch 16 batch 800 train loss: 0.0559 test loss: 0.0706\n",
      "Epoch 16 batch 900 train loss: 0.0461 test loss: 0.0707\n",
      "Epoch 16 batch 1000 train loss: 0.0484 test loss: 0.0704\n",
      "Epoch 16 batch 1100 train loss: 0.0537 test loss: 0.0701\n",
      "Epoch 16 batch 1200 train loss: 0.0544 test loss: 0.0704\n",
      "Epoch 16 batch 1300 train loss: 0.0519 test loss: 0.0702\n",
      "Epoch 16 batch 1400 train loss: 0.0518 test loss: 0.0700\n",
      "Epoch 16 batch 1500 train loss: 0.0488 test loss: 0.0708\n",
      "Epoch 16 batch 1600 train loss: 0.0576 test loss: 0.0701\n",
      "Epoch 16 batch 1700 train loss: 0.0530 test loss: 0.0705\n",
      "Epoch 16 batch 1800 train loss: 0.0553 test loss: 0.0705\n",
      "Epoch 16 batch 1900 train loss: 0.0465 test loss: 0.0706\n",
      "Epoch 16 batch 2000 train loss: 0.0512 test loss: 0.0705\n",
      "Epoch 16 batch 2100 train loss: 0.0600 test loss: 0.0704\n",
      "Epoch 16 batch 2200 train loss: 0.0578 test loss: 0.0705\n",
      "Epoch 16 batch 2300 train loss: 0.0548 test loss: 0.0705\n",
      "Epoch 16 batch 2400 train loss: 0.0543 test loss: 0.0702\n",
      "early stop.\n",
      "Checkpoint -5 restored!!\n",
      "2446/2446 [==============================] - 14s 6ms/step - loss: 0.0544\n"
     ]
    }
   ],
   "source": [
    "for LOSS_RATE in LOSS_RATES:\n",
    "    \n",
    "    l = np.load('./data/tot_dataset_loss_%.2f.npz' % LOSS_RATE)\n",
    "    raw_input = l['raw_input']\n",
    "    raw_label = l['raw_label']\n",
    "    test_input = l['test_input']\n",
    "    test_label = l['test_label']\n",
    "    MAXS = l['MAXS']\n",
    "    MINS = l['MINS']\n",
    "\n",
    "    SCREEN_SIZE = l['SCREEN_SIZE']\n",
    "\n",
    "    raw_input = raw_input.astype(np.float32)\n",
    "    raw_label = raw_label.astype(np.float32)\n",
    "    test_input = test_input.astype(np.float32)\n",
    "    test_label = test_label.astype(np.float32)\n",
    "\n",
    "    num_train = int(raw_input.shape[0]*.7)\n",
    "    raw_input, raw_label = shuffle(raw_input, raw_label, random_state=4574)\n",
    "    train_input, train_label = raw_input[:num_train, ...], raw_label[:num_train, ...]\n",
    "    val_input, val_label = raw_input[num_train:, ...], raw_label[num_train:, ...]\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_input, train_label))\n",
    "    train_dataset = train_dataset.cache().shuffle(BATCH_SIZE*50).batch(BATCH_SIZE)\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((val_input, val_label))\n",
    "    val_dataset = val_dataset.cache().shuffle(BATCH_SIZE*50).batch(BATCH_SIZE)\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((test_input, test_label))\n",
    "    test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "    print('Training for loss rate %.2f start.' % LOSS_RATE)\n",
    "    BEST_PATH = './checkpoints/UNet_best_loss_%.2fp' % LOSS_RATE\n",
    "\n",
    "    @tf.function\n",
    "    def train(loss_function, model, opt, inp, tar):\n",
    "        with tf.GradientTape() as tape:\n",
    "            gradients = tape.gradient(loss_function(model, inp, tar), model.trainable_variables)\n",
    "            gradient_variables = zip(gradients, model.trainable_variables)\n",
    "            opt.apply_gradients(gradient_variables)\n",
    "    \n",
    "    unet_model = UNet()\n",
    "    opt = tf.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    \n",
    "    checkpoint_path = BEST_PATH\n",
    "    ckpt = tf.train.Checkpoint(unet_model=unet_model, opt=opt)\n",
    "    ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=10)\n",
    "    writer = tf.summary.create_file_writer('tmp')\n",
    "    \n",
    "    prev_test_loss = 100.0\n",
    "    early_stop_buffer = 500\n",
    "    with writer.as_default():\n",
    "        with tf.summary.record_if(True):\n",
    "            for epoch in range(TRAINING_EPOCHS):\n",
    "                for step, (inp, tar) in enumerate(train_dataset):\n",
    "                    train(loss_function, unet_model, opt, inp, tar)\n",
    "                    loss_values = loss_function(unet_model, inp, tar)\n",
    "                    tf.summary.scalar('loss', loss_values, step=step)\n",
    "\n",
    "                    if step % DISP_STEPS == 0:\n",
    "                        test_loss = 0\n",
    "                        for step_, (inp_, tar_) in enumerate(test_dataset):\n",
    "                            test_loss += loss_function(unet_model, inp_, tar_)\n",
    "\n",
    "                            if step_ > DISP_STEPS:\n",
    "                                test_loss /= DISP_STEPS\n",
    "                                break\n",
    "                        if test_loss.numpy() < prev_test_loss:\n",
    "                            ckpt_save_path = ckpt_manager.save()\n",
    "                            prev_test_loss = test_loss.numpy()\n",
    "                            print('Saving checkpoint at {}'.format(ckpt_save_path))\n",
    "                        else:\n",
    "                            early_stop_buffer -= 1\n",
    "\n",
    "                        print('Epoch {} batch {} train loss: {:.4f} test loss: {:.4f}'\n",
    "                              .format(epoch, step, loss_values.numpy(), test_loss.numpy()))\n",
    "                    if early_stop_buffer <= 0:\n",
    "                        print('early stop.')\n",
    "                        break\n",
    "                if early_stop_buffer <= 0:\n",
    "                        break\n",
    "            \n",
    "    i = -1\n",
    "    if ckpt_manager.checkpoints:\n",
    "        ckpt.restore(ckpt_manager.checkpoints[i])\n",
    "        print ('Checkpoint ' + ckpt_manager.checkpoints[i][-2:] +' restored!!')\n",
    "        \n",
    "    unet_model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "                       loss = tf.keras.losses.MeanSquaredError())\n",
    "    test_loss = unet_model.evaluate(test_dataset)\n",
    "    pred_result = unet_model.predict(test_dataset)\n",
    "    \n",
    "    avg_pred = []\n",
    "    OUTLIER = 2\n",
    "    for __ in range(5):\n",
    "        temp = []\n",
    "        for _ in range(int(pred_result.shape[1]/5)):\n",
    "            temp.append(pred_result[..., _*5:(_+1)*5, 0][..., __])\n",
    "        temp = np.stack(temp, axis=2)\n",
    "        temp.sort(axis=2)\n",
    "        avg_pred.append(temp[..., OUTLIER:-OUTLIER].mean(axis=2))\n",
    "    avg_pred = np.stack(avg_pred, axis=2)\n",
    "    \n",
    "    masking = test_input[..., 1]\n",
    "    avg_masking = masking[..., :5]\n",
    "    masked_pred = np.ma.array(pred_result[..., 0], mask=masking)\n",
    "    masked_avg_pred = np.ma.array(avg_pred, mask=avg_masking)\n",
    "    masked_label = np.ma.array(test_label[..., 0], mask=masking)\n",
    "    \n",
    "    plot_label = ((MAXS[:5]-MINS[:5])*masked_label[..., :5] + MINS[:5])\n",
    "    plot_label.fill_value = np.nan\n",
    "    plot_avg_pred = ((MAXS[:5]-MINS[:5])*masked_avg_pred[..., :5] + MINS[:5])\n",
    "    plot_avg_pred.fill_value = np.nan\n",
    "    f = open('./results/UNet_best_loss_%.2fp.npz' % LOSS_RATE, 'wb')\n",
    "    np.savez(f,\n",
    "             test_label = plot_label.filled(),\n",
    "             test_pred = plot_avg_pred.filled()\n",
    "            )\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p36)",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

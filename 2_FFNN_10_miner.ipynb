{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import *\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 2 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import Metric\n",
    "class RSquare(Metric):\n",
    "    \"\"\"Compute R^2 score.\n",
    "     This is also called as coefficient of determination.\n",
    "     It tells how close are data to the fitted regression line.\n",
    "     - Highest score can be 1.0 and it indicates that the predictors\n",
    "       perfectly accounts for variation in the target.\n",
    "     - Score 0.0 indicates that the predictors do not\n",
    "       account for variation in the target.\n",
    "     - It can also be negative if the model is worse.\n",
    "     Usage:\n",
    "     ```python\n",
    "     actuals = tf.constant([1, 4, 3], dtype=tf.float32)\n",
    "     preds = tf.constant([2, 4, 4], dtype=tf.float32)\n",
    "     result = tf.keras.metrics.RSquare()\n",
    "     result.update_state(actuals, preds)\n",
    "     print('R^2 score is: ', r1.result().numpy()) # 0.57142866\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name='r_square', dtype=tf.float32):\n",
    "        super(RSquare, self).__init__(name=name, dtype=dtype)\n",
    "        self.squared_sum = self.add_weight(\"squared_sum\", initializer=\"zeros\")\n",
    "        self.sum = self.add_weight(\"sum\", initializer=\"zeros\")\n",
    "        self.res = self.add_weight(\"residual\", initializer=\"zeros\")\n",
    "        self.count = self.add_weight(\"count\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred):\n",
    "        y_true = tf.convert_to_tensor(y_true, tf.float32)\n",
    "        y_pred = tf.convert_to_tensor(y_pred, tf.float32)\n",
    "        self.squared_sum.assign_add(tf.reduce_sum(y_true**2))\n",
    "        self.sum.assign_add(tf.reduce_sum(y_true))\n",
    "        self.res.assign_add(\n",
    "            tf.reduce_sum(tf.square(tf.subtract(y_true, y_pred))))\n",
    "        self.count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        mean = self.sum / self.count\n",
    "        total = self.squared_sum - 2 * self.sum * mean + self.count * mean**2\n",
    "        return 1 - (self.res / total)\n",
    "\n",
    "    def reset_states(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.squared_sum.assign(0.0)\n",
    "        self.sum.assign(0.0)\n",
    "        self.res.assign(0.0)\n",
    "        self.count.assign(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = ((8/2.54), (6/2.54))\n",
    "plt.rcParams[\"font.family\"] = \"Arial\"\n",
    "plt.rcParams[\"mathtext.default\"] = \"rm\"\n",
    "plt.rcParams.update({'font.size': 11})\n",
    "MARKER_SIZE = 15\n",
    "cmap_m = [\"#f4a6ad\", \"#f6957e\", \"#fccfa2\", \"#8de7be\", \"#86d6f2\", \"#24a9e4\", \"#b586e0\", \"#d7f293\"]\n",
    "cmap = [\"#e94d5b\", \"#ef4d28\", \"#f9a54f\", \"#25b575\", \"#1bb1e7\", \"#1477a2\", \"#a662e5\", \"#c2f442\"]\n",
    "\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "# plt.rcParams['axes.edgecolor'] = \n",
    "plt.rcParams['axes.linewidth'] = 1\n",
    "plt.rcParams['lines.linewidth'] = 1.5\n",
    "plt.rcParams['xtick.major.width'] = 1\n",
    "plt.rcParams['xtick.minor.width'] = 1\n",
    "plt.rcParams['ytick.major.width'] = 1\n",
    "plt.rcParams['ytick.minor.width'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 10\n",
    "LOSS_RATES = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]\n",
    "DISP_STEPS = 100\n",
    "TRAINING_EPOCHS = 500\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN(Model):\n",
    "    def __init__(self, out_len):\n",
    "        super(FFNN, self).__init__()\n",
    "        \n",
    "        self.out_len = out_len\n",
    "        self.dense1 = layers.Dense(64)\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.dense2 = layers.Dense(64)\n",
    "        self.out_layer = layers.Dense(self.out_len)\n",
    "        \n",
    "    def call(self, inp):\n",
    "        output = self.dense1(inp)\n",
    "        output = self.bn1(inp)\n",
    "        output = self.dense2(inp)\n",
    "        output = self.out_layer(inp)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss inputs should be masked.\n",
    "loss_object = tf.keras.losses.MeanSquaredError()\n",
    "def loss_function(model, inp, tar): #RNN specialized\n",
    "    \n",
    "    masked_real = tar * (1 - inp[..., 5:10])\n",
    "    masked_pred = model(inp) * (1 - inp[..., 5:10])\n",
    "    \n",
    "    return loss_object(masked_real, masked_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for loss rate 0.10 start.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ffnn/dense/kernel:0', 'ffnn/dense/bias:0', 'ffnn/batch_normalization/gamma:0', 'ffnn/batch_normalization/beta:0', 'ffnn/dense_1/kernel:0', 'ffnn/dense_1/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ffnn/dense/kernel:0', 'ffnn/dense/bias:0', 'ffnn/batch_normalization/gamma:0', 'ffnn/batch_normalization/beta:0', 'ffnn/dense_1/kernel:0', 'ffnn/dense_1/bias:0'] when minimizing the loss.\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-1\n",
      "Epoch 0 batch 0 train loss: 0.3293 test loss: 0.3585\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-2\n",
      "Epoch 0 batch 100 train loss: 0.0500 test loss: 0.1298\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-3\n",
      "Epoch 0 batch 200 train loss: 0.0573 test loss: 0.0746\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-4\n",
      "Epoch 0 batch 300 train loss: 0.0625 test loss: 0.0577\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-5\n",
      "Epoch 0 batch 400 train loss: 0.0229 test loss: 0.0488\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-6\n",
      "Epoch 0 batch 500 train loss: 0.0131 test loss: 0.0418\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-7\n",
      "Epoch 0 batch 600 train loss: 0.0314 test loss: 0.0354\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-8\n",
      "Epoch 0 batch 700 train loss: 0.0110 test loss: 0.0311\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-9\n",
      "Epoch 0 batch 800 train loss: 0.0260 test loss: 0.0269\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-10\n",
      "Epoch 0 batch 900 train loss: 0.0122 test loss: 0.0227\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-11\n",
      "Epoch 0 batch 1000 train loss: 0.0173 test loss: 0.0200\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-12\n",
      "Epoch 0 batch 1100 train loss: 0.0098 test loss: 0.0176\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-13\n",
      "Epoch 0 batch 1200 train loss: 0.0072 test loss: 0.0156\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-14\n",
      "Epoch 0 batch 1300 train loss: 0.0055 test loss: 0.0139\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-15\n",
      "Epoch 0 batch 1400 train loss: 0.0120 test loss: 0.0122\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-16\n",
      "Epoch 0 batch 1500 train loss: 0.0058 test loss: 0.0109\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-17\n",
      "Epoch 0 batch 1600 train loss: 0.0088 test loss: 0.0101\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-18\n",
      "Epoch 0 batch 1700 train loss: 0.0065 test loss: 0.0093\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-19\n",
      "Epoch 0 batch 1800 train loss: 0.0056 test loss: 0.0085\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-20\n",
      "Epoch 0 batch 1900 train loss: 0.0032 test loss: 0.0080\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-21\n",
      "Epoch 0 batch 2000 train loss: 0.0092 test loss: 0.0077\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-22\n",
      "Epoch 0 batch 2100 train loss: 0.0060 test loss: 0.0071\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-23\n",
      "Epoch 0 batch 2200 train loss: 0.0064 test loss: 0.0066\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-24\n",
      "Epoch 0 batch 2300 train loss: 0.0055 test loss: 0.0066\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-25\n",
      "Epoch 0 batch 2400 train loss: 0.0037 test loss: 0.0065\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-26\n",
      "Epoch 0 batch 2500 train loss: 0.0056 test loss: 0.0063\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-27\n",
      "Epoch 0 batch 2600 train loss: 0.0074 test loss: 0.0062\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-28\n",
      "Epoch 0 batch 2700 train loss: 0.0030 test loss: 0.0062\n",
      "Epoch 0 batch 2800 train loss: 0.0028 test loss: 0.0062\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-29\n",
      "Epoch 0 batch 2900 train loss: 0.0040 test loss: 0.0060\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-30\n",
      "Epoch 0 batch 3000 train loss: 0.0053 test loss: 0.0058\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ffnn/dense/kernel:0', 'ffnn/dense/bias:0', 'ffnn/batch_normalization/gamma:0', 'ffnn/batch_normalization/beta:0', 'ffnn/dense_1/kernel:0', 'ffnn/dense_1/bias:0'] when minimizing the loss.\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-31\n",
      "Epoch 1 batch 0 train loss: 0.0046 test loss: 0.0056\n",
      "Epoch 1 batch 100 train loss: 0.0053 test loss: 0.0059\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-32\n",
      "Epoch 1 batch 200 train loss: 0.0025 test loss: 0.0054\n",
      "Epoch 1 batch 300 train loss: 0.0027 test loss: 0.0055\n",
      "Epoch 1 batch 400 train loss: 0.0026 test loss: 0.0055\n",
      "Epoch 1 batch 500 train loss: 0.0030 test loss: 0.0055\n",
      "Epoch 1 batch 600 train loss: 0.0025 test loss: 0.0054\n",
      "Epoch 1 batch 700 train loss: 0.0071 test loss: 0.0056\n",
      "Epoch 1 batch 800 train loss: 0.0045 test loss: 0.0055\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-33\n",
      "Epoch 1 batch 900 train loss: 0.0026 test loss: 0.0053\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-34\n",
      "Epoch 1 batch 1000 train loss: 0.0036 test loss: 0.0052\n",
      "Epoch 1 batch 1100 train loss: 0.0040 test loss: 0.0055\n",
      "Epoch 1 batch 1200 train loss: 0.0039 test loss: 0.0052\n",
      "Epoch 1 batch 1300 train loss: 0.0092 test loss: 0.0055\n",
      "Epoch 1 batch 1400 train loss: 0.0048 test loss: 0.0053\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-35\n",
      "Epoch 1 batch 1500 train loss: 0.0032 test loss: 0.0052\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-36\n",
      "Epoch 1 batch 1600 train loss: 0.0039 test loss: 0.0051\n",
      "Epoch 1 batch 1700 train loss: 0.0032 test loss: 0.0051\n",
      "Epoch 1 batch 1800 train loss: 0.0020 test loss: 0.0053\n",
      "Epoch 1 batch 1900 train loss: 0.0039 test loss: 0.0051\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-37\n",
      "Epoch 1 batch 2000 train loss: 0.0034 test loss: 0.0049\n",
      "Epoch 1 batch 2100 train loss: 0.0040 test loss: 0.0050\n",
      "Epoch 1 batch 2200 train loss: 0.0029 test loss: 0.0051\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-38\n",
      "Epoch 1 batch 2300 train loss: 0.0061 test loss: 0.0048\n",
      "Epoch 1 batch 2400 train loss: 0.0038 test loss: 0.0050\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-39\n",
      "Epoch 1 batch 2500 train loss: 0.0039 test loss: 0.0047\n",
      "Epoch 1 batch 2600 train loss: 0.0045 test loss: 0.0050\n",
      "Epoch 1 batch 2700 train loss: 0.0037 test loss: 0.0052\n",
      "Epoch 1 batch 2800 train loss: 0.0034 test loss: 0.0051\n",
      "Epoch 1 batch 2900 train loss: 0.0036 test loss: 0.0047\n",
      "Epoch 1 batch 3000 train loss: 0.0021 test loss: 0.0050\n",
      "Epoch 2 batch 0 train loss: 0.0035 test loss: 0.0049\n",
      "Epoch 2 batch 100 train loss: 0.0044 test loss: 0.0048\n",
      "Epoch 2 batch 200 train loss: 0.0035 test loss: 0.0052\n",
      "Epoch 2 batch 300 train loss: 0.0068 test loss: 0.0049\n",
      "Epoch 2 batch 400 train loss: 0.0061 test loss: 0.0052\n",
      "Epoch 2 batch 500 train loss: 0.0049 test loss: 0.0052\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-40\n",
      "Epoch 2 batch 600 train loss: 0.0026 test loss: 0.0046\n",
      "Epoch 2 batch 700 train loss: 0.0049 test loss: 0.0049\n",
      "Epoch 2 batch 800 train loss: 0.0017 test loss: 0.0049\n",
      "Epoch 2 batch 900 train loss: 0.0038 test loss: 0.0050\n",
      "Epoch 2 batch 1000 train loss: 0.0044 test loss: 0.0049\n",
      "Epoch 2 batch 1100 train loss: 0.0046 test loss: 0.0049\n",
      "Epoch 2 batch 1200 train loss: 0.0051 test loss: 0.0055\n",
      "Epoch 2 batch 1300 train loss: 0.0033 test loss: 0.0050\n",
      "Epoch 2 batch 1400 train loss: 0.0043 test loss: 0.0051\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-41\n",
      "Epoch 2 batch 1500 train loss: 0.0034 test loss: 0.0044\n",
      "Epoch 2 batch 1600 train loss: 0.0060 test loss: 0.0053\n",
      "Epoch 2 batch 1700 train loss: 0.0053 test loss: 0.0051\n",
      "Epoch 2 batch 1800 train loss: 0.0028 test loss: 0.0046\n",
      "Epoch 2 batch 1900 train loss: 0.0025 test loss: 0.0047\n",
      "Epoch 2 batch 2000 train loss: 0.0020 test loss: 0.0046\n",
      "Epoch 2 batch 2100 train loss: 0.0049 test loss: 0.0044\n",
      "Epoch 2 batch 2200 train loss: 0.0028 test loss: 0.0050\n",
      "Epoch 2 batch 2300 train loss: 0.0012 test loss: 0.0048\n",
      "Epoch 2 batch 2400 train loss: 0.0046 test loss: 0.0049\n",
      "Epoch 2 batch 2500 train loss: 0.0044 test loss: 0.0050\n",
      "Epoch 2 batch 2600 train loss: 0.0025 test loss: 0.0045\n",
      "Epoch 2 batch 2700 train loss: 0.0027 test loss: 0.0048\n",
      "Epoch 2 batch 2800 train loss: 0.0031 test loss: 0.0046\n",
      "Epoch 2 batch 2900 train loss: 0.0042 test loss: 0.0048\n",
      "Epoch 2 batch 3000 train loss: 0.0028 test loss: 0.0050\n",
      "Epoch 3 batch 0 train loss: 0.0035 test loss: 0.0046\n",
      "Epoch 3 batch 100 train loss: 0.0022 test loss: 0.0051\n",
      "Epoch 3 batch 200 train loss: 0.0032 test loss: 0.0048\n",
      "Epoch 3 batch 300 train loss: 0.0024 test loss: 0.0049\n",
      "Epoch 3 batch 400 train loss: 0.0043 test loss: 0.0046\n",
      "Epoch 3 batch 500 train loss: 0.0038 test loss: 0.0048\n",
      "Epoch 3 batch 600 train loss: 0.0038 test loss: 0.0045\n",
      "Epoch 3 batch 700 train loss: 0.0046 test loss: 0.0045\n",
      "Epoch 3 batch 800 train loss: 0.0035 test loss: 0.0044\n",
      "Epoch 3 batch 900 train loss: 0.0023 test loss: 0.0046\n",
      "Epoch 3 batch 1000 train loss: 0.0022 test loss: 0.0045\n",
      "Epoch 3 batch 1100 train loss: 0.0031 test loss: 0.0047\n",
      "Epoch 3 batch 1200 train loss: 0.0048 test loss: 0.0045\n",
      "Epoch 3 batch 1300 train loss: 0.0019 test loss: 0.0050\n",
      "Epoch 3 batch 1400 train loss: 0.0054 test loss: 0.0046\n",
      "Epoch 3 batch 1500 train loss: 0.0031 test loss: 0.0045\n",
      "Epoch 3 batch 1600 train loss: 0.0048 test loss: 0.0046\n",
      "Epoch 3 batch 1700 train loss: 0.0037 test loss: 0.0046\n",
      "Epoch 3 batch 1800 train loss: 0.0040 test loss: 0.0047\n",
      "Epoch 3 batch 1900 train loss: 0.0038 test loss: 0.0044\n",
      "Epoch 3 batch 2000 train loss: 0.0044 test loss: 0.0046\n",
      "Epoch 3 batch 2100 train loss: 0.0042 test loss: 0.0045\n",
      "Epoch 3 batch 2200 train loss: 0.0010 test loss: 0.0049\n",
      "Epoch 3 batch 2300 train loss: 0.0030 test loss: 0.0045\n",
      "Epoch 3 batch 2400 train loss: 0.0046 test loss: 0.0047\n",
      "Epoch 3 batch 2500 train loss: 0.0023 test loss: 0.0046\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-42\n",
      "Epoch 3 batch 2600 train loss: 0.0066 test loss: 0.0043\n",
      "Epoch 3 batch 2700 train loss: 0.0031 test loss: 0.0049\n",
      "Epoch 3 batch 2800 train loss: 0.0047 test loss: 0.0049\n",
      "Epoch 3 batch 2900 train loss: 0.0042 test loss: 0.0043\n",
      "Epoch 3 batch 3000 train loss: 0.0015 test loss: 0.0049\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-43\n",
      "Epoch 4 batch 0 train loss: 0.0034 test loss: 0.0042\n",
      "Epoch 4 batch 100 train loss: 0.0031 test loss: 0.0045\n",
      "Epoch 4 batch 200 train loss: 0.0040 test loss: 0.0049\n",
      "Epoch 4 batch 300 train loss: 0.0031 test loss: 0.0047\n",
      "Epoch 4 batch 400 train loss: 0.0023 test loss: 0.0046\n",
      "Epoch 4 batch 500 train loss: 0.0047 test loss: 0.0050\n",
      "Epoch 4 batch 600 train loss: 0.0028 test loss: 0.0046\n",
      "Epoch 4 batch 700 train loss: 0.0030 test loss: 0.0047\n",
      "Epoch 4 batch 800 train loss: 0.0054 test loss: 0.0047\n",
      "Epoch 4 batch 900 train loss: 0.0044 test loss: 0.0049\n",
      "Epoch 4 batch 1000 train loss: 0.0058 test loss: 0.0042\n",
      "Epoch 4 batch 1100 train loss: 0.0047 test loss: 0.0045\n",
      "Epoch 4 batch 1200 train loss: 0.0035 test loss: 0.0044\n",
      "Epoch 4 batch 1300 train loss: 0.0028 test loss: 0.0044\n",
      "Epoch 4 batch 1400 train loss: 0.0042 test loss: 0.0044\n",
      "Epoch 4 batch 1500 train loss: 0.0044 test loss: 0.0045\n",
      "Epoch 4 batch 1600 train loss: 0.0026 test loss: 0.0049\n",
      "Epoch 4 batch 1700 train loss: 0.0014 test loss: 0.0045\n",
      "Epoch 4 batch 1800 train loss: 0.0060 test loss: 0.0044\n",
      "Epoch 4 batch 1900 train loss: 0.0019 test loss: 0.0046\n",
      "Epoch 4 batch 2000 train loss: 0.0040 test loss: 0.0044\n",
      "Epoch 4 batch 2100 train loss: 0.0036 test loss: 0.0042\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-44\n",
      "Epoch 4 batch 2200 train loss: 0.0020 test loss: 0.0042\n",
      "Epoch 4 batch 2300 train loss: 0.0070 test loss: 0.0045\n",
      "Epoch 4 batch 2400 train loss: 0.0047 test loss: 0.0044\n",
      "Epoch 4 batch 2500 train loss: 0.0031 test loss: 0.0043\n",
      "Epoch 4 batch 2600 train loss: 0.0027 test loss: 0.0045\n",
      "Epoch 4 batch 2700 train loss: 0.0033 test loss: 0.0047\n",
      "Epoch 4 batch 2800 train loss: 0.0017 test loss: 0.0051\n",
      "Epoch 4 batch 2900 train loss: 0.0039 test loss: 0.0046\n",
      "Epoch 4 batch 3000 train loss: 0.0028 test loss: 0.0042\n",
      "Epoch 5 batch 0 train loss: 0.0019 test loss: 0.0044\n",
      "Epoch 5 batch 100 train loss: 0.0073 test loss: 0.0044\n",
      "Epoch 5 batch 200 train loss: 0.0028 test loss: 0.0048\n",
      "Epoch 5 batch 300 train loss: 0.0094 test loss: 0.0045\n",
      "Epoch 5 batch 400 train loss: 0.0050 test loss: 0.0048\n",
      "Epoch 5 batch 500 train loss: 0.0035 test loss: 0.0049\n",
      "Epoch 5 batch 600 train loss: 0.0051 test loss: 0.0046\n",
      "Epoch 5 batch 700 train loss: 0.0027 test loss: 0.0050\n",
      "Epoch 5 batch 800 train loss: 0.0022 test loss: 0.0044\n",
      "Epoch 5 batch 900 train loss: 0.0046 test loss: 0.0043\n",
      "Epoch 5 batch 1000 train loss: 0.0036 test loss: 0.0050\n",
      "Epoch 5 batch 1100 train loss: 0.0014 test loss: 0.0048\n",
      "Epoch 5 batch 1200 train loss: 0.0069 test loss: 0.0045\n",
      "Epoch 5 batch 1300 train loss: 0.0026 test loss: 0.0049\n",
      "Epoch 5 batch 1400 train loss: 0.0041 test loss: 0.0048\n",
      "Epoch 5 batch 1500 train loss: 0.0046 test loss: 0.0044\n",
      "Epoch 5 batch 1600 train loss: 0.0033 test loss: 0.0042\n",
      "Epoch 5 batch 1700 train loss: 0.0019 test loss: 0.0044\n",
      "Epoch 5 batch 1800 train loss: 0.0043 test loss: 0.0044\n",
      "Epoch 5 batch 1900 train loss: 0.0027 test loss: 0.0047\n",
      "Epoch 5 batch 2000 train loss: 0.0025 test loss: 0.0046\n",
      "Epoch 5 batch 2100 train loss: 0.0025 test loss: 0.0044\n",
      "Epoch 5 batch 2200 train loss: 0.0023 test loss: 0.0045\n",
      "Epoch 5 batch 2300 train loss: 0.0025 test loss: 0.0044\n",
      "Epoch 5 batch 2400 train loss: 0.0035 test loss: 0.0047\n",
      "Epoch 5 batch 2500 train loss: 0.0026 test loss: 0.0048\n",
      "Epoch 5 batch 2600 train loss: 0.0041 test loss: 0.0047\n",
      "Epoch 5 batch 2700 train loss: 0.0046 test loss: 0.0052\n",
      "Epoch 5 batch 2800 train loss: 0.0048 test loss: 0.0049\n",
      "Epoch 5 batch 2900 train loss: 0.0024 test loss: 0.0046\n",
      "Epoch 5 batch 3000 train loss: 0.0060 test loss: 0.0045\n",
      "Epoch 6 batch 0 train loss: 0.0070 test loss: 0.0044\n",
      "Epoch 6 batch 100 train loss: 0.0028 test loss: 0.0046\n",
      "Epoch 6 batch 200 train loss: 0.0035 test loss: 0.0045\n",
      "Epoch 6 batch 300 train loss: 0.0062 test loss: 0.0045\n",
      "Epoch 6 batch 400 train loss: 0.0059 test loss: 0.0045\n",
      "Epoch 6 batch 500 train loss: 0.0021 test loss: 0.0048\n",
      "Epoch 6 batch 600 train loss: 0.0028 test loss: 0.0046\n",
      "Epoch 6 batch 700 train loss: 0.0024 test loss: 0.0049\n",
      "Epoch 6 batch 800 train loss: 0.0028 test loss: 0.0046\n",
      "Epoch 6 batch 900 train loss: 0.0038 test loss: 0.0047\n",
      "Epoch 6 batch 1000 train loss: 0.0051 test loss: 0.0046\n",
      "Epoch 6 batch 1100 train loss: 0.0047 test loss: 0.0045\n",
      "Epoch 6 batch 1200 train loss: 0.0042 test loss: 0.0046\n",
      "Epoch 6 batch 1300 train loss: 0.0043 test loss: 0.0046\n",
      "Epoch 6 batch 1400 train loss: 0.0027 test loss: 0.0049\n",
      "Epoch 6 batch 1500 train loss: 0.0033 test loss: 0.0045\n",
      "Epoch 6 batch 1600 train loss: 0.0096 test loss: 0.0047\n",
      "Epoch 6 batch 1700 train loss: 0.0026 test loss: 0.0044\n",
      "Epoch 6 batch 1800 train loss: 0.0056 test loss: 0.0048\n",
      "Epoch 6 batch 1900 train loss: 0.0038 test loss: 0.0046\n",
      "Epoch 6 batch 2000 train loss: 0.0026 test loss: 0.0049\n",
      "Epoch 6 batch 2100 train loss: 0.0030 test loss: 0.0044\n",
      "Epoch 6 batch 2200 train loss: 0.0038 test loss: 0.0047\n",
      "Epoch 6 batch 2300 train loss: 0.0034 test loss: 0.0043\n",
      "Epoch 6 batch 2400 train loss: 0.0054 test loss: 0.0047\n",
      "Epoch 6 batch 2500 train loss: 0.0020 test loss: 0.0043\n",
      "Epoch 6 batch 2600 train loss: 0.0043 test loss: 0.0046\n",
      "Epoch 6 batch 2700 train loss: 0.0025 test loss: 0.0048\n",
      "Epoch 6 batch 2800 train loss: 0.0037 test loss: 0.0048\n",
      "Epoch 6 batch 2900 train loss: 0.0036 test loss: 0.0044\n",
      "Epoch 6 batch 3000 train loss: 0.0043 test loss: 0.0045\n",
      "Epoch 7 batch 0 train loss: 0.0045 test loss: 0.0044\n",
      "Epoch 7 batch 100 train loss: 0.0022 test loss: 0.0044\n",
      "Epoch 7 batch 200 train loss: 0.0049 test loss: 0.0046\n",
      "Epoch 7 batch 300 train loss: 0.0037 test loss: 0.0044\n",
      "Epoch 7 batch 400 train loss: 0.0027 test loss: 0.0046\n",
      "Epoch 7 batch 500 train loss: 0.0020 test loss: 0.0046\n",
      "Epoch 7 batch 600 train loss: 0.0028 test loss: 0.0044\n",
      "Epoch 7 batch 700 train loss: 0.0030 test loss: 0.0046\n",
      "Epoch 7 batch 800 train loss: 0.0093 test loss: 0.0044\n",
      "Epoch 7 batch 900 train loss: 0.0042 test loss: 0.0046\n",
      "Epoch 7 batch 1000 train loss: 0.0028 test loss: 0.0043\n",
      "Epoch 7 batch 1100 train loss: 0.0014 test loss: 0.0047\n",
      "Epoch 7 batch 1200 train loss: 0.0059 test loss: 0.0048\n",
      "Epoch 7 batch 1300 train loss: 0.0054 test loss: 0.0046\n",
      "Epoch 7 batch 1400 train loss: 0.0033 test loss: 0.0048\n",
      "Epoch 7 batch 1500 train loss: 0.0032 test loss: 0.0045\n",
      "Epoch 7 batch 1600 train loss: 0.0027 test loss: 0.0044\n",
      "Epoch 7 batch 1700 train loss: 0.0049 test loss: 0.0045\n",
      "Epoch 7 batch 1800 train loss: 0.0028 test loss: 0.0043\n",
      "Epoch 7 batch 1900 train loss: 0.0054 test loss: 0.0044\n",
      "Epoch 7 batch 2000 train loss: 0.0018 test loss: 0.0046\n",
      "Epoch 7 batch 2100 train loss: 0.0019 test loss: 0.0042\n",
      "Epoch 7 batch 2200 train loss: 0.0031 test loss: 0.0044\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-45\n",
      "Epoch 7 batch 2300 train loss: 0.0022 test loss: 0.0041\n",
      "Epoch 7 batch 2400 train loss: 0.0050 test loss: 0.0047\n",
      "Epoch 7 batch 2500 train loss: 0.0025 test loss: 0.0048\n",
      "Epoch 7 batch 2600 train loss: 0.0077 test loss: 0.0046\n",
      "Epoch 7 batch 2700 train loss: 0.0024 test loss: 0.0048\n",
      "Epoch 7 batch 2800 train loss: 0.0038 test loss: 0.0045\n",
      "Epoch 7 batch 2900 train loss: 0.0058 test loss: 0.0044\n",
      "Epoch 7 batch 3000 train loss: 0.0019 test loss: 0.0045\n",
      "Epoch 8 batch 0 train loss: 0.0030 test loss: 0.0046\n",
      "Epoch 8 batch 100 train loss: 0.0032 test loss: 0.0045\n",
      "Epoch 8 batch 200 train loss: 0.0044 test loss: 0.0043\n",
      "Epoch 8 batch 300 train loss: 0.0046 test loss: 0.0045\n",
      "Epoch 8 batch 400 train loss: 0.0025 test loss: 0.0047\n",
      "Epoch 8 batch 500 train loss: 0.0036 test loss: 0.0047\n",
      "Epoch 8 batch 600 train loss: 0.0042 test loss: 0.0047\n",
      "Epoch 8 batch 700 train loss: 0.0032 test loss: 0.0046\n",
      "Epoch 8 batch 800 train loss: 0.0029 test loss: 0.0045\n",
      "Epoch 8 batch 900 train loss: 0.0070 test loss: 0.0046\n",
      "Epoch 8 batch 1000 train loss: 0.0022 test loss: 0.0044\n",
      "Epoch 8 batch 1100 train loss: 0.0017 test loss: 0.0046\n",
      "Epoch 8 batch 1200 train loss: 0.0068 test loss: 0.0045\n",
      "Epoch 8 batch 1300 train loss: 0.0027 test loss: 0.0044\n",
      "Epoch 8 batch 1400 train loss: 0.0021 test loss: 0.0043\n",
      "Epoch 8 batch 1500 train loss: 0.0043 test loss: 0.0045\n",
      "Epoch 8 batch 1600 train loss: 0.0037 test loss: 0.0045\n",
      "Epoch 8 batch 1700 train loss: 0.0065 test loss: 0.0047\n",
      "Epoch 8 batch 1800 train loss: 0.0028 test loss: 0.0045\n",
      "Epoch 8 batch 1900 train loss: 0.0031 test loss: 0.0044\n",
      "Epoch 8 batch 2000 train loss: 0.0024 test loss: 0.0048\n",
      "Epoch 8 batch 2100 train loss: 0.0039 test loss: 0.0043\n",
      "Epoch 8 batch 2200 train loss: 0.0041 test loss: 0.0046\n",
      "Epoch 8 batch 2300 train loss: 0.0030 test loss: 0.0045\n",
      "Epoch 8 batch 2400 train loss: 0.0033 test loss: 0.0049\n",
      "Epoch 8 batch 2500 train loss: 0.0043 test loss: 0.0045\n",
      "Epoch 8 batch 2600 train loss: 0.0035 test loss: 0.0044\n",
      "Epoch 8 batch 2700 train loss: 0.0036 test loss: 0.0051\n",
      "Epoch 8 batch 2800 train loss: 0.0046 test loss: 0.0050\n",
      "Epoch 8 batch 2900 train loss: 0.0058 test loss: 0.0045\n",
      "Epoch 8 batch 3000 train loss: 0.0040 test loss: 0.0049\n",
      "Epoch 9 batch 0 train loss: 0.0025 test loss: 0.0043\n",
      "Epoch 9 batch 100 train loss: 0.0039 test loss: 0.0049\n",
      "Epoch 9 batch 200 train loss: 0.0030 test loss: 0.0045\n",
      "Epoch 9 batch 300 train loss: 0.0018 test loss: 0.0046\n",
      "Epoch 9 batch 400 train loss: 0.0026 test loss: 0.0047\n",
      "Epoch 9 batch 500 train loss: 0.0044 test loss: 0.0046\n",
      "Epoch 9 batch 600 train loss: 0.0075 test loss: 0.0046\n",
      "Epoch 9 batch 700 train loss: 0.0047 test loss: 0.0048\n",
      "Epoch 9 batch 800 train loss: 0.0026 test loss: 0.0045\n",
      "Epoch 9 batch 900 train loss: 0.0027 test loss: 0.0045\n",
      "Epoch 9 batch 1000 train loss: 0.0023 test loss: 0.0042\n",
      "Epoch 9 batch 1100 train loss: 0.0027 test loss: 0.0049\n",
      "Epoch 9 batch 1200 train loss: 0.0038 test loss: 0.0052\n",
      "Epoch 9 batch 1300 train loss: 0.0028 test loss: 0.0045\n",
      "Epoch 9 batch 1400 train loss: 0.0020 test loss: 0.0045\n",
      "Epoch 9 batch 1500 train loss: 0.0015 test loss: 0.0048\n",
      "Epoch 9 batch 1600 train loss: 0.0036 test loss: 0.0043\n",
      "Epoch 9 batch 1700 train loss: 0.0055 test loss: 0.0044\n",
      "Epoch 9 batch 1800 train loss: 0.0048 test loss: 0.0043\n",
      "Epoch 9 batch 1900 train loss: 0.0034 test loss: 0.0044\n",
      "Epoch 9 batch 2000 train loss: 0.0032 test loss: 0.0046\n",
      "Epoch 9 batch 2100 train loss: 0.0022 test loss: 0.0043\n",
      "Epoch 9 batch 2200 train loss: 0.0054 test loss: 0.0045\n",
      "Epoch 9 batch 2300 train loss: 0.0062 test loss: 0.0046\n",
      "Epoch 9 batch 2400 train loss: 0.0045 test loss: 0.0048\n",
      "Epoch 9 batch 2500 train loss: 0.0032 test loss: 0.0045\n",
      "Epoch 9 batch 2600 train loss: 0.0024 test loss: 0.0046\n",
      "Epoch 9 batch 2700 train loss: 0.0026 test loss: 0.0049\n",
      "Epoch 9 batch 2800 train loss: 0.0036 test loss: 0.0047\n",
      "Epoch 9 batch 2900 train loss: 0.0041 test loss: 0.0046\n",
      "Epoch 9 batch 3000 train loss: 0.0058 test loss: 0.0045\n",
      "Epoch 10 batch 0 train loss: 0.0021 test loss: 0.0044\n",
      "Epoch 10 batch 100 train loss: 0.0034 test loss: 0.0044\n",
      "Epoch 10 batch 200 train loss: 0.0043 test loss: 0.0046\n",
      "Epoch 10 batch 300 train loss: 0.0054 test loss: 0.0047\n",
      "Epoch 10 batch 400 train loss: 0.0023 test loss: 0.0045\n",
      "Epoch 10 batch 500 train loss: 0.0022 test loss: 0.0046\n",
      "Epoch 10 batch 600 train loss: 0.0053 test loss: 0.0044\n",
      "Epoch 10 batch 700 train loss: 0.0034 test loss: 0.0047\n",
      "Epoch 10 batch 800 train loss: 0.0030 test loss: 0.0045\n",
      "Epoch 10 batch 900 train loss: 0.0041 test loss: 0.0044\n",
      "Epoch 10 batch 1000 train loss: 0.0055 test loss: 0.0046\n",
      "Epoch 10 batch 1100 train loss: 0.0048 test loss: 0.0045\n",
      "Epoch 10 batch 1200 train loss: 0.0033 test loss: 0.0047\n",
      "Epoch 10 batch 1300 train loss: 0.0040 test loss: 0.0047\n",
      "Epoch 10 batch 1400 train loss: 0.0016 test loss: 0.0047\n",
      "Epoch 10 batch 1500 train loss: 0.0023 test loss: 0.0047\n",
      "Epoch 10 batch 1600 train loss: 0.0025 test loss: 0.0046\n",
      "Epoch 10 batch 1700 train loss: 0.0090 test loss: 0.0046\n",
      "Epoch 10 batch 1800 train loss: 0.0045 test loss: 0.0045\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.10p/ckpt-46\n",
      "Epoch 10 batch 1900 train loss: 0.0025 test loss: 0.0040\n",
      "Epoch 10 batch 2000 train loss: 0.0034 test loss: 0.0046\n",
      "Epoch 10 batch 2100 train loss: 0.0044 test loss: 0.0045\n",
      "Epoch 10 batch 2200 train loss: 0.0031 test loss: 0.0048\n",
      "Epoch 10 batch 2300 train loss: 0.0058 test loss: 0.0045\n",
      "Epoch 10 batch 2400 train loss: 0.0026 test loss: 0.0047\n",
      "Epoch 10 batch 2500 train loss: 0.0029 test loss: 0.0045\n",
      "Epoch 10 batch 2600 train loss: 0.0020 test loss: 0.0045\n",
      "Epoch 10 batch 2700 train loss: 0.0053 test loss: 0.0050\n",
      "Epoch 10 batch 2800 train loss: 0.0055 test loss: 0.0049\n",
      "Epoch 10 batch 2900 train loss: 0.0036 test loss: 0.0046\n",
      "Epoch 10 batch 3000 train loss: 0.0023 test loss: 0.0043\n",
      "Epoch 11 batch 0 train loss: 0.0053 test loss: 0.0045\n",
      "Epoch 11 batch 100 train loss: 0.0017 test loss: 0.0049\n",
      "Epoch 11 batch 200 train loss: 0.0025 test loss: 0.0046\n",
      "Epoch 11 batch 300 train loss: 0.0052 test loss: 0.0047\n",
      "Epoch 11 batch 400 train loss: 0.0034 test loss: 0.0046\n",
      "Epoch 11 batch 500 train loss: 0.0050 test loss: 0.0048\n",
      "Epoch 11 batch 600 train loss: 0.0043 test loss: 0.0047\n",
      "Epoch 11 batch 700 train loss: 0.0026 test loss: 0.0044\n",
      "Epoch 11 batch 800 train loss: 0.0034 test loss: 0.0044\n",
      "Epoch 11 batch 900 train loss: 0.0067 test loss: 0.0042\n",
      "Epoch 11 batch 1000 train loss: 0.0031 test loss: 0.0047\n",
      "Epoch 11 batch 1100 train loss: 0.0055 test loss: 0.0043\n",
      "Epoch 11 batch 1200 train loss: 0.0049 test loss: 0.0047\n",
      "Epoch 11 batch 1300 train loss: 0.0024 test loss: 0.0047\n",
      "Epoch 11 batch 1400 train loss: 0.0017 test loss: 0.0044\n",
      "Epoch 11 batch 1500 train loss: 0.0048 test loss: 0.0047\n",
      "Epoch 11 batch 1600 train loss: 0.0055 test loss: 0.0045\n",
      "Epoch 11 batch 1700 train loss: 0.0035 test loss: 0.0045\n",
      "Epoch 11 batch 1800 train loss: 0.0028 test loss: 0.0044\n",
      "Epoch 11 batch 1900 train loss: 0.0026 test loss: 0.0047\n",
      "Epoch 11 batch 2000 train loss: 0.0024 test loss: 0.0043\n",
      "Epoch 11 batch 2100 train loss: 0.0054 test loss: 0.0049\n",
      "Epoch 11 batch 2200 train loss: 0.0023 test loss: 0.0047\n",
      "Epoch 11 batch 2300 train loss: 0.0046 test loss: 0.0044\n",
      "Epoch 11 batch 2400 train loss: 0.0040 test loss: 0.0048\n",
      "Epoch 11 batch 2500 train loss: 0.0041 test loss: 0.0045\n",
      "Epoch 11 batch 2600 train loss: 0.0032 test loss: 0.0046\n",
      "Epoch 11 batch 2700 train loss: 0.0036 test loss: 0.0046\n",
      "Epoch 11 batch 2800 train loss: 0.0040 test loss: 0.0044\n",
      "Epoch 11 batch 2900 train loss: 0.0031 test loss: 0.0045\n",
      "Epoch 11 batch 3000 train loss: 0.0047 test loss: 0.0045\n",
      "Epoch 12 batch 0 train loss: 0.0050 test loss: 0.0044\n",
      "Epoch 12 batch 100 train loss: 0.0033 test loss: 0.0046\n",
      "Epoch 12 batch 200 train loss: 0.0042 test loss: 0.0045\n",
      "Epoch 12 batch 300 train loss: 0.0028 test loss: 0.0047\n",
      "Epoch 12 batch 400 train loss: 0.0044 test loss: 0.0044\n",
      "Epoch 12 batch 500 train loss: 0.0050 test loss: 0.0045\n",
      "Epoch 12 batch 600 train loss: 0.0037 test loss: 0.0047\n",
      "Epoch 12 batch 700 train loss: 0.0034 test loss: 0.0048\n",
      "Epoch 12 batch 800 train loss: 0.0037 test loss: 0.0048\n",
      "Epoch 12 batch 900 train loss: 0.0030 test loss: 0.0045\n",
      "Epoch 12 batch 1000 train loss: 0.0056 test loss: 0.0046\n",
      "Epoch 12 batch 1100 train loss: 0.0032 test loss: 0.0046\n",
      "Epoch 12 batch 1200 train loss: 0.0062 test loss: 0.0046\n",
      "Epoch 12 batch 1300 train loss: 0.0022 test loss: 0.0045\n",
      "Epoch 12 batch 1400 train loss: 0.0025 test loss: 0.0047\n",
      "Epoch 12 batch 1500 train loss: 0.0062 test loss: 0.0043\n",
      "Epoch 12 batch 1600 train loss: 0.0020 test loss: 0.0046\n",
      "Epoch 12 batch 1700 train loss: 0.0036 test loss: 0.0047\n",
      "Epoch 12 batch 1800 train loss: 0.0027 test loss: 0.0047\n",
      "Epoch 12 batch 1900 train loss: 0.0038 test loss: 0.0045\n",
      "Epoch 12 batch 2000 train loss: 0.0034 test loss: 0.0044\n",
      "Epoch 12 batch 2100 train loss: 0.0049 test loss: 0.0044\n",
      "Epoch 12 batch 2200 train loss: 0.0031 test loss: 0.0043\n",
      "Epoch 12 batch 2300 train loss: 0.0045 test loss: 0.0048\n",
      "Epoch 12 batch 2400 train loss: 0.0025 test loss: 0.0045\n",
      "Epoch 12 batch 2500 train loss: 0.0030 test loss: 0.0049\n",
      "Epoch 12 batch 2600 train loss: 0.0045 test loss: 0.0044\n",
      "Epoch 12 batch 2700 train loss: 0.0028 test loss: 0.0049\n",
      "Epoch 12 batch 2800 train loss: 0.0042 test loss: 0.0048\n",
      "Epoch 12 batch 2900 train loss: 0.0020 test loss: 0.0044\n",
      "Epoch 12 batch 3000 train loss: 0.0048 test loss: 0.0043\n",
      "Epoch 13 batch 0 train loss: 0.0032 test loss: 0.0042\n",
      "Epoch 13 batch 100 train loss: 0.0029 test loss: 0.0050\n",
      "Epoch 13 batch 200 train loss: 0.0046 test loss: 0.0043\n",
      "Epoch 13 batch 300 train loss: 0.0041 test loss: 0.0048\n",
      "Epoch 13 batch 400 train loss: 0.0029 test loss: 0.0048\n",
      "Epoch 13 batch 500 train loss: 0.0064 test loss: 0.0045\n",
      "Epoch 13 batch 600 train loss: 0.0070 test loss: 0.0046\n",
      "Epoch 13 batch 700 train loss: 0.0031 test loss: 0.0045\n",
      "Epoch 13 batch 800 train loss: 0.0037 test loss: 0.0047\n",
      "Epoch 13 batch 900 train loss: 0.0044 test loss: 0.0046\n",
      "Epoch 13 batch 1000 train loss: 0.0039 test loss: 0.0045\n",
      "Epoch 13 batch 1100 train loss: 0.0029 test loss: 0.0046\n",
      "Epoch 13 batch 1200 train loss: 0.0041 test loss: 0.0049\n",
      "Epoch 13 batch 1300 train loss: 0.0024 test loss: 0.0048\n",
      "Epoch 13 batch 1400 train loss: 0.0046 test loss: 0.0048\n",
      "Epoch 13 batch 1500 train loss: 0.0029 test loss: 0.0042\n",
      "Epoch 13 batch 1600 train loss: 0.0023 test loss: 0.0046\n",
      "Epoch 13 batch 1700 train loss: 0.0042 test loss: 0.0045\n",
      "Epoch 13 batch 1800 train loss: 0.0081 test loss: 0.0045\n",
      "Epoch 13 batch 1900 train loss: 0.0020 test loss: 0.0043\n",
      "Epoch 13 batch 2000 train loss: 0.0013 test loss: 0.0043\n",
      "Epoch 13 batch 2100 train loss: 0.0040 test loss: 0.0045\n",
      "Epoch 13 batch 2200 train loss: 0.0016 test loss: 0.0044\n",
      "Epoch 13 batch 2300 train loss: 0.0028 test loss: 0.0048\n",
      "Epoch 13 batch 2400 train loss: 0.0023 test loss: 0.0046\n",
      "Epoch 13 batch 2500 train loss: 0.0035 test loss: 0.0044\n",
      "Epoch 13 batch 2600 train loss: 0.0027 test loss: 0.0044\n",
      "Epoch 13 batch 2700 train loss: 0.0090 test loss: 0.0046\n",
      "Epoch 13 batch 2800 train loss: 0.0046 test loss: 0.0048\n",
      "Epoch 13 batch 2900 train loss: 0.0043 test loss: 0.0045\n",
      "Epoch 13 batch 3000 train loss: 0.0028 test loss: 0.0043\n",
      "Epoch 14 batch 0 train loss: 0.0022 test loss: 0.0044\n",
      "Epoch 14 batch 100 train loss: 0.0051 test loss: 0.0049\n",
      "Epoch 14 batch 200 train loss: 0.0051 test loss: 0.0047\n",
      "Epoch 14 batch 300 train loss: 0.0038 test loss: 0.0048\n",
      "Epoch 14 batch 400 train loss: 0.0047 test loss: 0.0044\n",
      "Epoch 14 batch 500 train loss: 0.0020 test loss: 0.0047\n",
      "Epoch 14 batch 600 train loss: 0.0046 test loss: 0.0044\n",
      "Epoch 14 batch 700 train loss: 0.0017 test loss: 0.0046\n",
      "Epoch 14 batch 800 train loss: 0.0066 test loss: 0.0045\n",
      "Epoch 14 batch 900 train loss: 0.0061 test loss: 0.0049\n",
      "Epoch 14 batch 1000 train loss: 0.0126 test loss: 0.0044\n",
      "Epoch 14 batch 1100 train loss: 0.0030 test loss: 0.0046\n",
      "Epoch 14 batch 1200 train loss: 0.0036 test loss: 0.0046\n",
      "Epoch 14 batch 1300 train loss: 0.0041 test loss: 0.0048\n",
      "Epoch 14 batch 1400 train loss: 0.0025 test loss: 0.0046\n",
      "Epoch 14 batch 1500 train loss: 0.0051 test loss: 0.0045\n",
      "Epoch 14 batch 1600 train loss: 0.0051 test loss: 0.0047\n",
      "Epoch 14 batch 1700 train loss: 0.0041 test loss: 0.0044\n",
      "Epoch 14 batch 1800 train loss: 0.0043 test loss: 0.0042\n",
      "Epoch 14 batch 1900 train loss: 0.0022 test loss: 0.0045\n",
      "Epoch 14 batch 2000 train loss: 0.0035 test loss: 0.0046\n",
      "Epoch 14 batch 2100 train loss: 0.0021 test loss: 0.0047\n",
      "Epoch 14 batch 2200 train loss: 0.0040 test loss: 0.0045\n",
      "Epoch 14 batch 2300 train loss: 0.0037 test loss: 0.0047\n",
      "Epoch 14 batch 2400 train loss: 0.0063 test loss: 0.0045\n",
      "Epoch 14 batch 2500 train loss: 0.0072 test loss: 0.0043\n",
      "Epoch 14 batch 2600 train loss: 0.0049 test loss: 0.0046\n",
      "Epoch 14 batch 2700 train loss: 0.0046 test loss: 0.0045\n",
      "Epoch 14 batch 2800 train loss: 0.0049 test loss: 0.0045\n",
      "Epoch 14 batch 2900 train loss: 0.0025 test loss: 0.0044\n",
      "Epoch 14 batch 3000 train loss: 0.0044 test loss: 0.0043\n",
      "Epoch 15 batch 0 train loss: 0.0043 test loss: 0.0044\n",
      "Epoch 15 batch 100 train loss: 0.0070 test loss: 0.0046\n",
      "Epoch 15 batch 200 train loss: 0.0042 test loss: 0.0049\n",
      "Epoch 15 batch 300 train loss: 0.0021 test loss: 0.0047\n",
      "Epoch 15 batch 400 train loss: 0.0045 test loss: 0.0043\n",
      "Epoch 15 batch 500 train loss: 0.0032 test loss: 0.0047\n",
      "Epoch 15 batch 600 train loss: 0.0037 test loss: 0.0047\n",
      "Epoch 15 batch 700 train loss: 0.0037 test loss: 0.0045\n",
      "Epoch 15 batch 800 train loss: 0.0028 test loss: 0.0044\n",
      "Epoch 15 batch 900 train loss: 0.0048 test loss: 0.0045\n",
      "Epoch 15 batch 1000 train loss: 0.0046 test loss: 0.0045\n",
      "Epoch 15 batch 1100 train loss: 0.0024 test loss: 0.0047\n",
      "Epoch 15 batch 1200 train loss: 0.0048 test loss: 0.0047\n",
      "Epoch 15 batch 1300 train loss: 0.0043 test loss: 0.0042\n",
      "Epoch 15 batch 1400 train loss: 0.0017 test loss: 0.0042\n",
      "Epoch 15 batch 1500 train loss: 0.0031 test loss: 0.0045\n",
      "Epoch 15 batch 1600 train loss: 0.0023 test loss: 0.0049\n",
      "Epoch 15 batch 1700 train loss: 0.0044 test loss: 0.0047\n",
      "Epoch 15 batch 1800 train loss: 0.0048 test loss: 0.0045\n",
      "Epoch 15 batch 1900 train loss: 0.0043 test loss: 0.0044\n",
      "Epoch 15 batch 2000 train loss: 0.0038 test loss: 0.0046\n",
      "Epoch 15 batch 2100 train loss: 0.0040 test loss: 0.0046\n",
      "Epoch 15 batch 2200 train loss: 0.0043 test loss: 0.0044\n",
      "Epoch 15 batch 2300 train loss: 0.0030 test loss: 0.0043\n",
      "Epoch 15 batch 2400 train loss: 0.0038 test loss: 0.0046\n",
      "Epoch 15 batch 2500 train loss: 0.0040 test loss: 0.0045\n",
      "Epoch 15 batch 2600 train loss: 0.0033 test loss: 0.0047\n",
      "Epoch 15 batch 2700 train loss: 0.0057 test loss: 0.0049\n",
      "Epoch 15 batch 2800 train loss: 0.0039 test loss: 0.0047\n",
      "Epoch 15 batch 2900 train loss: 0.0058 test loss: 0.0046\n",
      "Epoch 15 batch 3000 train loss: 0.0039 test loss: 0.0044\n",
      "Epoch 16 batch 0 train loss: 0.0035 test loss: 0.0043\n",
      "Epoch 16 batch 100 train loss: 0.0019 test loss: 0.0047\n",
      "Epoch 16 batch 200 train loss: 0.0066 test loss: 0.0043\n",
      "Epoch 16 batch 300 train loss: 0.0033 test loss: 0.0047\n",
      "Epoch 16 batch 400 train loss: 0.0035 test loss: 0.0044\n",
      "Epoch 16 batch 500 train loss: 0.0081 test loss: 0.0045\n",
      "Epoch 16 batch 600 train loss: 0.0042 test loss: 0.0044\n",
      "Epoch 16 batch 700 train loss: 0.0022 test loss: 0.0048\n",
      "Epoch 16 batch 800 train loss: 0.0040 test loss: 0.0047\n",
      "Epoch 16 batch 900 train loss: 0.0020 test loss: 0.0048\n",
      "Epoch 16 batch 1000 train loss: 0.0025 test loss: 0.0046\n",
      "Epoch 16 batch 1100 train loss: 0.0014 test loss: 0.0048\n",
      "Epoch 16 batch 1200 train loss: 0.0066 test loss: 0.0044\n",
      "Epoch 16 batch 1300 train loss: 0.0034 test loss: 0.0044\n",
      "Epoch 16 batch 1400 train loss: 0.0029 test loss: 0.0047\n",
      "Epoch 16 batch 1500 train loss: 0.0039 test loss: 0.0043\n",
      "Epoch 16 batch 1600 train loss: 0.0047 test loss: 0.0044\n",
      "Epoch 16 batch 1700 train loss: 0.0043 test loss: 0.0043\n",
      "Epoch 16 batch 1800 train loss: 0.0037 test loss: 0.0048\n",
      "Epoch 16 batch 1900 train loss: 0.0026 test loss: 0.0046\n",
      "Epoch 16 batch 2000 train loss: 0.0061 test loss: 0.0044\n",
      "Epoch 16 batch 2100 train loss: 0.0035 test loss: 0.0045\n",
      "Epoch 16 batch 2200 train loss: 0.0020 test loss: 0.0050\n",
      "Epoch 16 batch 2300 train loss: 0.0047 test loss: 0.0043\n",
      "Epoch 16 batch 2400 train loss: 0.0031 test loss: 0.0049\n",
      "Epoch 16 batch 2500 train loss: 0.0029 test loss: 0.0046\n",
      "Epoch 16 batch 2600 train loss: 0.0029 test loss: 0.0046\n",
      "Epoch 16 batch 2700 train loss: 0.0054 test loss: 0.0045\n",
      "Epoch 16 batch 2800 train loss: 0.0022 test loss: 0.0049\n",
      "Epoch 16 batch 2900 train loss: 0.0054 test loss: 0.0048\n",
      "Epoch 16 batch 3000 train loss: 0.0028 test loss: 0.0043\n",
      "Epoch 17 batch 0 train loss: 0.0032 test loss: 0.0043\n",
      "Epoch 17 batch 100 train loss: 0.0053 test loss: 0.0043\n",
      "Epoch 17 batch 200 train loss: 0.0023 test loss: 0.0048\n",
      "Epoch 17 batch 300 train loss: 0.0029 test loss: 0.0042\n",
      "Epoch 17 batch 400 train loss: 0.0031 test loss: 0.0051\n",
      "Epoch 17 batch 500 train loss: 0.0035 test loss: 0.0048\n",
      "Epoch 17 batch 600 train loss: 0.0017 test loss: 0.0042\n",
      "Epoch 17 batch 700 train loss: 0.0023 test loss: 0.0047\n",
      "Epoch 17 batch 800 train loss: 0.0026 test loss: 0.0047\n",
      "Epoch 17 batch 900 train loss: 0.0038 test loss: 0.0047\n",
      "Epoch 17 batch 1000 train loss: 0.0039 test loss: 0.0044\n",
      "Epoch 17 batch 1100 train loss: 0.0023 test loss: 0.0045\n",
      "Epoch 17 batch 1200 train loss: 0.0029 test loss: 0.0050\n",
      "Epoch 17 batch 1300 train loss: 0.0027 test loss: 0.0048\n",
      "Epoch 17 batch 1400 train loss: 0.0033 test loss: 0.0045\n",
      "Epoch 17 batch 1500 train loss: 0.0020 test loss: 0.0043\n",
      "Epoch 17 batch 1600 train loss: 0.0047 test loss: 0.0045\n",
      "Epoch 17 batch 1700 train loss: 0.0023 test loss: 0.0045\n",
      "Epoch 17 batch 1800 train loss: 0.0020 test loss: 0.0045\n",
      "early stop.\n",
      "Checkpoint 46 restored!!\n",
      "Training for loss rate 0.20 start.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ffnn_1/dense_3/kernel:0', 'ffnn_1/dense_3/bias:0', 'ffnn_1/batch_normalization_1/gamma:0', 'ffnn_1/batch_normalization_1/beta:0', 'ffnn_1/dense_4/kernel:0', 'ffnn_1/dense_4/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ffnn_1/dense_3/kernel:0', 'ffnn_1/dense_3/bias:0', 'ffnn_1/batch_normalization_1/gamma:0', 'ffnn_1/batch_normalization_1/beta:0', 'ffnn_1/dense_4/kernel:0', 'ffnn_1/dense_4/bias:0'] when minimizing the loss.\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.20p/ckpt-1\n",
      "Epoch 0 batch 0 train loss: 0.1958 test loss: 0.1929\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.20p/ckpt-2\n",
      "Epoch 0 batch 100 train loss: 0.0864 test loss: 0.1005\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.20p/ckpt-3\n",
      "Epoch 0 batch 200 train loss: 0.0725 test loss: 0.0766\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.20p/ckpt-4\n",
      "Epoch 0 batch 300 train loss: 0.0486 test loss: 0.0626\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.20p/ckpt-5\n",
      "Epoch 0 batch 400 train loss: 0.0310 test loss: 0.0508\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.20p/ckpt-6\n",
      "Epoch 0 batch 500 train loss: 0.0365 test loss: 0.0408\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.20p/ckpt-7\n",
      "Epoch 0 batch 600 train loss: 0.0222 test loss: 0.0341\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.20p/ckpt-8\n",
      "Epoch 0 batch 700 train loss: 0.0175 test loss: 0.0281\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.20p/ckpt-9\n",
      "Epoch 0 batch 800 train loss: 0.0158 test loss: 0.0232\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.20p/ckpt-10\n",
      "Epoch 0 batch 900 train loss: 0.0208 test loss: 0.0194\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.20p/ckpt-11\n",
      "Epoch 0 batch 1000 train loss: 0.0072 test loss: 0.0162\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.20p/ckpt-12\n",
      "Epoch 0 batch 1100 train loss: 0.0105 test loss: 0.0141\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.20p/ckpt-13\n",
      "Epoch 0 batch 1200 train loss: 0.0083 test loss: 0.0126\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.20p/ckpt-14\n",
      "Epoch 0 batch 1300 train loss: 0.0083 test loss: 0.0112\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.20p/ckpt-15\n",
      "Epoch 0 batch 1400 train loss: 0.0070 test loss: 0.0097\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.20p/ckpt-16\n",
      "Epoch 0 batch 1500 train loss: 0.0055 test loss: 0.0090\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.20p/ckpt-17\n",
      "Epoch 0 batch 1600 train loss: 0.0082 test loss: 0.0084\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.20p/ckpt-18\n",
      "Epoch 0 batch 1700 train loss: 0.0046 test loss: 0.0079\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.20p/ckpt-19\n",
      "Epoch 0 batch 1800 train loss: 0.0033 test loss: 0.0075\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.20p/ckpt-20\n",
      "Epoch 0 batch 1900 train loss: 0.0028 test loss: 0.0073\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.20p/ckpt-21\n",
      "Epoch 0 batch 2000 train loss: 0.0048 test loss: 0.0068\n",
      "Epoch 0 batch 2100 train loss: 0.0063 test loss: 0.0069\n",
      "Epoch 0 batch 2200 train loss: 0.0089 test loss: 0.0071\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.20p/ckpt-22\n",
      "Epoch 0 batch 2300 train loss: 0.0049 test loss: 0.0067\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.20p/ckpt-23\n",
      "Epoch 0 batch 2400 train loss: 0.0054 test loss: 0.0065\n",
      "Epoch 0 batch 2500 train loss: 0.0079 test loss: 0.0067\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.20p/ckpt-24\n",
      "Epoch 0 batch 2600 train loss: 0.0048 test loss: 0.0061\n",
      "Epoch 0 batch 2700 train loss: 0.0051 test loss: 0.0064\n",
      "Epoch 0 batch 2800 train loss: 0.0066 test loss: 0.0065\n",
      "Epoch 0 batch 2900 train loss: 0.0030 test loss: 0.0064\n",
      "Epoch 0 batch 3000 train loss: 0.0084 test loss: 0.0063\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ffnn_1/dense_3/kernel:0', 'ffnn_1/dense_3/bias:0', 'ffnn_1/batch_normalization_1/gamma:0', 'ffnn_1/batch_normalization_1/beta:0', 'ffnn_1/dense_4/kernel:0', 'ffnn_1/dense_4/bias:0'] when minimizing the loss.\n",
      "Epoch 1 batch 0 train loss: 0.0023 test loss: 0.0063\n",
      "Epoch 1 batch 100 train loss: 0.0050 test loss: 0.0063\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.20p/ckpt-25\n",
      "Epoch 1 batch 200 train loss: 0.0078 test loss: 0.0060\n",
      "Epoch 1 batch 300 train loss: 0.0045 test loss: 0.0062\n",
      "Epoch 1 batch 400 train loss: 0.0040 test loss: 0.0063\n",
      "Epoch 1 batch 500 train loss: 0.0049 test loss: 0.0060\n",
      "Epoch 1 batch 600 train loss: 0.0050 test loss: 0.0061\n",
      "Epoch 1 batch 700 train loss: 0.0044 test loss: 0.0064\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.20p/ckpt-26\n",
      "Epoch 1 batch 800 train loss: 0.0040 test loss: 0.0058\n",
      "Epoch 1 batch 900 train loss: 0.0069 test loss: 0.0066\n",
      "Epoch 1 batch 1000 train loss: 0.0036 test loss: 0.0060\n",
      "Epoch 1 batch 1100 train loss: 0.0084 test loss: 0.0062\n",
      "Epoch 1 batch 1200 train loss: 0.0045 test loss: 0.0060\n",
      "Epoch 1 batch 1300 train loss: 0.0051 test loss: 0.0061\n",
      "Epoch 1 batch 1400 train loss: 0.0045 test loss: 0.0060\n",
      "Epoch 1 batch 1500 train loss: 0.0051 test loss: 0.0062\n",
      "Epoch 1 batch 1600 train loss: 0.0061 test loss: 0.0063\n",
      "Epoch 1 batch 1700 train loss: 0.0076 test loss: 0.0058\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.20p/ckpt-27\n",
      "Epoch 1 batch 1800 train loss: 0.0071 test loss: 0.0057\n",
      "Epoch 1 batch 1900 train loss: 0.0021 test loss: 0.0062\n",
      "Epoch 1 batch 2000 train loss: 0.0060 test loss: 0.0058\n",
      "Epoch 1 batch 2100 train loss: 0.0034 test loss: 0.0058\n",
      "Epoch 1 batch 2200 train loss: 0.0046 test loss: 0.0059\n",
      "Epoch 1 batch 2300 train loss: 0.0073 test loss: 0.0062\n",
      "Epoch 1 batch 2400 train loss: 0.0034 test loss: 0.0063\n",
      "Epoch 1 batch 2500 train loss: 0.0030 test loss: 0.0061\n",
      "Epoch 1 batch 2600 train loss: 0.0056 test loss: 0.0060\n",
      "Epoch 1 batch 2700 train loss: 0.0050 test loss: 0.0061\n",
      "Epoch 1 batch 2800 train loss: 0.0035 test loss: 0.0062\n",
      "Epoch 1 batch 2900 train loss: 0.0061 test loss: 0.0060\n",
      "Epoch 1 batch 3000 train loss: 0.0039 test loss: 0.0058\n",
      "Epoch 2 batch 0 train loss: 0.0074 test loss: 0.0060\n",
      "Epoch 2 batch 100 train loss: 0.0067 test loss: 0.0062\n",
      "Epoch 2 batch 200 train loss: 0.0040 test loss: 0.0060\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.20p/ckpt-28\n",
      "Epoch 2 batch 300 train loss: 0.0043 test loss: 0.0057\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.20p/ckpt-29\n",
      "Epoch 2 batch 400 train loss: 0.0040 test loss: 0.0056\n",
      "Epoch 2 batch 500 train loss: 0.0042 test loss: 0.0064\n",
      "Epoch 2 batch 600 train loss: 0.0041 test loss: 0.0061\n",
      "Epoch 2 batch 700 train loss: 0.0056 test loss: 0.0065\n",
      "Epoch 2 batch 800 train loss: 0.0043 test loss: 0.0057\n",
      "Epoch 2 batch 900 train loss: 0.0071 test loss: 0.0061\n",
      "Epoch 2 batch 1000 train loss: 0.0040 test loss: 0.0059\n",
      "Epoch 2 batch 1100 train loss: 0.0090 test loss: 0.0059\n",
      "Epoch 2 batch 1200 train loss: 0.0056 test loss: 0.0060\n",
      "Epoch 2 batch 1300 train loss: 0.0067 test loss: 0.0060\n",
      "Epoch 2 batch 1400 train loss: 0.0022 test loss: 0.0062\n",
      "Epoch 2 batch 1500 train loss: 0.0043 test loss: 0.0061\n",
      "Epoch 2 batch 1600 train loss: 0.0049 test loss: 0.0059\n",
      "Epoch 2 batch 1700 train loss: 0.0036 test loss: 0.0059\n",
      "Epoch 2 batch 1800 train loss: 0.0041 test loss: 0.0060\n",
      "Epoch 2 batch 1900 train loss: 0.0037 test loss: 0.0057\n",
      "Epoch 2 batch 2000 train loss: 0.0040 test loss: 0.0059\n",
      "Epoch 2 batch 2100 train loss: 0.0033 test loss: 0.0059\n",
      "Epoch 2 batch 2200 train loss: 0.0095 test loss: 0.0060\n",
      "Epoch 2 batch 2300 train loss: 0.0042 test loss: 0.0062\n",
      "Epoch 2 batch 2400 train loss: 0.0045 test loss: 0.0057\n",
      "Epoch 2 batch 2500 train loss: 0.0043 test loss: 0.0058\n",
      "Epoch 2 batch 2600 train loss: 0.0081 test loss: 0.0059\n",
      "Epoch 2 batch 2700 train loss: 0.0042 test loss: 0.0059\n",
      "Epoch 2 batch 2800 train loss: 0.0056 test loss: 0.0061\n",
      "Epoch 2 batch 2900 train loss: 0.0080 test loss: 0.0056\n",
      "Epoch 2 batch 3000 train loss: 0.0038 test loss: 0.0059\n",
      "Epoch 3 batch 0 train loss: 0.0084 test loss: 0.0062\n",
      "Epoch 3 batch 100 train loss: 0.0038 test loss: 0.0060\n",
      "Epoch 3 batch 200 train loss: 0.0025 test loss: 0.0058\n",
      "Epoch 3 batch 300 train loss: 0.0053 test loss: 0.0057\n",
      "Epoch 3 batch 400 train loss: 0.0071 test loss: 0.0061\n",
      "Epoch 3 batch 500 train loss: 0.0042 test loss: 0.0065\n",
      "Epoch 3 batch 600 train loss: 0.0030 test loss: 0.0061\n",
      "Epoch 3 batch 700 train loss: 0.0064 test loss: 0.0060\n",
      "Epoch 3 batch 800 train loss: 0.0042 test loss: 0.0059\n",
      "Epoch 3 batch 900 train loss: 0.0052 test loss: 0.0063\n",
      "Epoch 3 batch 1000 train loss: 0.0049 test loss: 0.0060\n",
      "Epoch 3 batch 1100 train loss: 0.0039 test loss: 0.0057\n",
      "Epoch 3 batch 1200 train loss: 0.0047 test loss: 0.0058\n",
      "Epoch 3 batch 1300 train loss: 0.0056 test loss: 0.0060\n",
      "Epoch 3 batch 1400 train loss: 0.0047 test loss: 0.0057\n",
      "Epoch 3 batch 1500 train loss: 0.0029 test loss: 0.0063\n",
      "Epoch 3 batch 1600 train loss: 0.0061 test loss: 0.0058\n",
      "Epoch 3 batch 1700 train loss: 0.0060 test loss: 0.0059\n",
      "Epoch 3 batch 1800 train loss: 0.0034 test loss: 0.0058\n",
      "Epoch 3 batch 1900 train loss: 0.0083 test loss: 0.0058\n",
      "Epoch 3 batch 2000 train loss: 0.0079 test loss: 0.0058\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.20p/ckpt-30\n",
      "Epoch 3 batch 2100 train loss: 0.0046 test loss: 0.0055\n",
      "Epoch 3 batch 2200 train loss: 0.0042 test loss: 0.0059\n",
      "Epoch 3 batch 2300 train loss: 0.0060 test loss: 0.0058\n",
      "Epoch 3 batch 2400 train loss: 0.0061 test loss: 0.0061\n",
      "Epoch 3 batch 2500 train loss: 0.0019 test loss: 0.0058\n",
      "Epoch 3 batch 2600 train loss: 0.0022 test loss: 0.0060\n",
      "Epoch 3 batch 2700 train loss: 0.0062 test loss: 0.0059\n",
      "Epoch 3 batch 2800 train loss: 0.0050 test loss: 0.0059\n",
      "Epoch 3 batch 2900 train loss: 0.0092 test loss: 0.0058\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.20p/ckpt-31\n",
      "Epoch 3 batch 3000 train loss: 0.0036 test loss: 0.0055\n",
      "Epoch 4 batch 0 train loss: 0.0067 test loss: 0.0056\n",
      "Epoch 4 batch 100 train loss: 0.0086 test loss: 0.0058\n",
      "Epoch 4 batch 200 train loss: 0.0053 test loss: 0.0059\n",
      "Epoch 4 batch 300 train loss: 0.0034 test loss: 0.0062\n",
      "Epoch 4 batch 400 train loss: 0.0051 test loss: 0.0060\n",
      "Epoch 4 batch 500 train loss: 0.0051 test loss: 0.0057\n",
      "Epoch 4 batch 600 train loss: 0.0051 test loss: 0.0062\n",
      "Epoch 4 batch 700 train loss: 0.0079 test loss: 0.0061\n",
      "Epoch 4 batch 800 train loss: 0.0045 test loss: 0.0057\n",
      "Epoch 4 batch 900 train loss: 0.0033 test loss: 0.0059\n",
      "Epoch 4 batch 1000 train loss: 0.0043 test loss: 0.0059\n",
      "Epoch 4 batch 1100 train loss: 0.0052 test loss: 0.0061\n",
      "Epoch 4 batch 1200 train loss: 0.0072 test loss: 0.0060\n",
      "Epoch 4 batch 1300 train loss: 0.0033 test loss: 0.0057\n",
      "Epoch 4 batch 1400 train loss: 0.0045 test loss: 0.0056\n",
      "Epoch 4 batch 1500 train loss: 0.0058 test loss: 0.0057\n",
      "Epoch 4 batch 1600 train loss: 0.0038 test loss: 0.0061\n",
      "Epoch 4 batch 1700 train loss: 0.0054 test loss: 0.0058\n",
      "Epoch 4 batch 1800 train loss: 0.0046 test loss: 0.0058\n",
      "Epoch 4 batch 1900 train loss: 0.0031 test loss: 0.0060\n",
      "Epoch 4 batch 2000 train loss: 0.0040 test loss: 0.0057\n",
      "Epoch 4 batch 2100 train loss: 0.0060 test loss: 0.0057\n",
      "Epoch 4 batch 2200 train loss: 0.0088 test loss: 0.0060\n",
      "Epoch 4 batch 2300 train loss: 0.0056 test loss: 0.0055\n",
      "Epoch 4 batch 2400 train loss: 0.0071 test loss: 0.0060\n",
      "Epoch 4 batch 2500 train loss: 0.0025 test loss: 0.0057\n",
      "Epoch 4 batch 2600 train loss: 0.0052 test loss: 0.0057\n",
      "Epoch 4 batch 2700 train loss: 0.0062 test loss: 0.0063\n",
      "Epoch 4 batch 2800 train loss: 0.0058 test loss: 0.0060\n",
      "Epoch 4 batch 2900 train loss: 0.0039 test loss: 0.0055\n",
      "Epoch 4 batch 3000 train loss: 0.0026 test loss: 0.0058\n",
      "Epoch 5 batch 0 train loss: 0.0094 test loss: 0.0061\n",
      "Epoch 5 batch 100 train loss: 0.0085 test loss: 0.0056\n",
      "Epoch 5 batch 200 train loss: 0.0061 test loss: 0.0057\n",
      "Epoch 5 batch 300 train loss: 0.0057 test loss: 0.0060\n",
      "Epoch 5 batch 400 train loss: 0.0075 test loss: 0.0061\n",
      "Epoch 5 batch 500 train loss: 0.0037 test loss: 0.0058\n",
      "Epoch 5 batch 600 train loss: 0.0097 test loss: 0.0058\n",
      "Epoch 5 batch 700 train loss: 0.0042 test loss: 0.0064\n",
      "Epoch 5 batch 800 train loss: 0.0033 test loss: 0.0061\n",
      "Epoch 5 batch 900 train loss: 0.0041 test loss: 0.0055\n",
      "Epoch 5 batch 1000 train loss: 0.0032 test loss: 0.0057\n",
      "Epoch 5 batch 1100 train loss: 0.0042 test loss: 0.0058\n",
      "Epoch 5 batch 1200 train loss: 0.0045 test loss: 0.0060\n",
      "Epoch 5 batch 1300 train loss: 0.0084 test loss: 0.0057\n",
      "Epoch 5 batch 1400 train loss: 0.0050 test loss: 0.0059\n",
      "Epoch 5 batch 1500 train loss: 0.0041 test loss: 0.0057\n",
      "Epoch 5 batch 1600 train loss: 0.0047 test loss: 0.0062\n",
      "Epoch 5 batch 1700 train loss: 0.0049 test loss: 0.0060\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.20p/ckpt-32\n",
      "Epoch 5 batch 1800 train loss: 0.0048 test loss: 0.0054\n",
      "Epoch 5 batch 1900 train loss: 0.0061 test loss: 0.0058\n",
      "Epoch 5 batch 2000 train loss: 0.0035 test loss: 0.0057\n",
      "Epoch 5 batch 2100 train loss: 0.0066 test loss: 0.0055\n",
      "Epoch 5 batch 2200 train loss: 0.0034 test loss: 0.0057\n",
      "Epoch 5 batch 2300 train loss: 0.0048 test loss: 0.0057\n",
      "Epoch 5 batch 2400 train loss: 0.0075 test loss: 0.0057\n",
      "Epoch 5 batch 2500 train loss: 0.0072 test loss: 0.0059\n",
      "Epoch 5 batch 2600 train loss: 0.0037 test loss: 0.0062\n",
      "Epoch 5 batch 2700 train loss: 0.0093 test loss: 0.0059\n",
      "Epoch 5 batch 2800 train loss: 0.0051 test loss: 0.0061\n",
      "Epoch 5 batch 2900 train loss: 0.0062 test loss: 0.0057\n",
      "Epoch 5 batch 3000 train loss: 0.0027 test loss: 0.0057\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.20p/ckpt-33\n",
      "Epoch 6 batch 0 train loss: 0.0065 test loss: 0.0054\n",
      "Epoch 6 batch 100 train loss: 0.0046 test loss: 0.0060\n",
      "Epoch 6 batch 200 train loss: 0.0043 test loss: 0.0060\n",
      "Epoch 6 batch 300 train loss: 0.0058 test loss: 0.0057\n",
      "Epoch 6 batch 400 train loss: 0.0083 test loss: 0.0059\n",
      "Epoch 6 batch 500 train loss: 0.0077 test loss: 0.0064\n",
      "Epoch 6 batch 600 train loss: 0.0055 test loss: 0.0059\n",
      "Epoch 6 batch 700 train loss: 0.0050 test loss: 0.0063\n",
      "Epoch 6 batch 800 train loss: 0.0026 test loss: 0.0057\n",
      "Epoch 6 batch 900 train loss: 0.0071 test loss: 0.0057\n",
      "Epoch 6 batch 1000 train loss: 0.0044 test loss: 0.0058\n",
      "Epoch 6 batch 1100 train loss: 0.0045 test loss: 0.0060\n",
      "Epoch 6 batch 1200 train loss: 0.0059 test loss: 0.0058\n",
      "Epoch 6 batch 1300 train loss: 0.0053 test loss: 0.0059\n",
      "Epoch 6 batch 1400 train loss: 0.0034 test loss: 0.0059\n",
      "Epoch 6 batch 1500 train loss: 0.0048 test loss: 0.0057\n",
      "Epoch 6 batch 1600 train loss: 0.0055 test loss: 0.0057\n",
      "Epoch 6 batch 1700 train loss: 0.0091 test loss: 0.0057\n",
      "Epoch 6 batch 1800 train loss: 0.0054 test loss: 0.0059\n",
      "Epoch 6 batch 1900 train loss: 0.0051 test loss: 0.0057\n",
      "Epoch 6 batch 2000 train loss: 0.0036 test loss: 0.0056\n",
      "Epoch 6 batch 2100 train loss: 0.0060 test loss: 0.0058\n",
      "Epoch 6 batch 2200 train loss: 0.0033 test loss: 0.0060\n",
      "Epoch 6 batch 2300 train loss: 0.0080 test loss: 0.0057\n",
      "Epoch 6 batch 2400 train loss: 0.0032 test loss: 0.0056\n",
      "Epoch 6 batch 2500 train loss: 0.0086 test loss: 0.0057\n",
      "Epoch 6 batch 2600 train loss: 0.0054 test loss: 0.0059\n",
      "Epoch 6 batch 2700 train loss: 0.0041 test loss: 0.0064\n",
      "Epoch 6 batch 2800 train loss: 0.0029 test loss: 0.0063\n",
      "Epoch 6 batch 2900 train loss: 0.0060 test loss: 0.0054\n",
      "Epoch 6 batch 3000 train loss: 0.0023 test loss: 0.0062\n",
      "Epoch 7 batch 0 train loss: 0.0034 test loss: 0.0059\n",
      "Epoch 7 batch 100 train loss: 0.0044 test loss: 0.0062\n",
      "Epoch 7 batch 200 train loss: 0.0055 test loss: 0.0060\n",
      "Epoch 7 batch 300 train loss: 0.0041 test loss: 0.0057\n",
      "Epoch 7 batch 400 train loss: 0.0058 test loss: 0.0060\n",
      "Epoch 7 batch 500 train loss: 0.0102 test loss: 0.0063\n",
      "Epoch 7 batch 600 train loss: 0.0051 test loss: 0.0060\n",
      "Epoch 7 batch 700 train loss: 0.0030 test loss: 0.0060\n",
      "Epoch 7 batch 800 train loss: 0.0069 test loss: 0.0061\n",
      "Epoch 7 batch 900 train loss: 0.0047 test loss: 0.0059\n",
      "Epoch 7 batch 1000 train loss: 0.0065 test loss: 0.0057\n",
      "Epoch 7 batch 1100 train loss: 0.0091 test loss: 0.0062\n",
      "Epoch 7 batch 1200 train loss: 0.0045 test loss: 0.0058\n",
      "Epoch 7 batch 1300 train loss: 0.0047 test loss: 0.0060\n",
      "Epoch 7 batch 1400 train loss: 0.0080 test loss: 0.0057\n",
      "Epoch 7 batch 1500 train loss: 0.0026 test loss: 0.0056\n",
      "Epoch 7 batch 1600 train loss: 0.0064 test loss: 0.0059\n",
      "Epoch 7 batch 1700 train loss: 0.0028 test loss: 0.0059\n",
      "Epoch 7 batch 1800 train loss: 0.0037 test loss: 0.0056\n",
      "Epoch 7 batch 1900 train loss: 0.0057 test loss: 0.0057\n",
      "Epoch 7 batch 2000 train loss: 0.0073 test loss: 0.0056\n",
      "Epoch 7 batch 2100 train loss: 0.0067 test loss: 0.0058\n",
      "Epoch 7 batch 2200 train loss: 0.0030 test loss: 0.0059\n",
      "Epoch 7 batch 2300 train loss: 0.0047 test loss: 0.0058\n",
      "Epoch 7 batch 2400 train loss: 0.0037 test loss: 0.0060\n",
      "Epoch 7 batch 2500 train loss: 0.0057 test loss: 0.0061\n",
      "Epoch 7 batch 2600 train loss: 0.0083 test loss: 0.0057\n",
      "Epoch 7 batch 2700 train loss: 0.0061 test loss: 0.0059\n",
      "Epoch 7 batch 2800 train loss: 0.0036 test loss: 0.0063\n",
      "Epoch 7 batch 2900 train loss: 0.0030 test loss: 0.0057\n",
      "Epoch 7 batch 3000 train loss: 0.0037 test loss: 0.0056\n",
      "Epoch 8 batch 0 train loss: 0.0040 test loss: 0.0057\n",
      "Epoch 8 batch 100 train loss: 0.0087 test loss: 0.0062\n",
      "Epoch 8 batch 200 train loss: 0.0044 test loss: 0.0057\n",
      "Epoch 8 batch 300 train loss: 0.0046 test loss: 0.0059\n",
      "Epoch 8 batch 400 train loss: 0.0062 test loss: 0.0060\n",
      "Epoch 8 batch 500 train loss: 0.0058 test loss: 0.0063\n",
      "Epoch 8 batch 600 train loss: 0.0045 test loss: 0.0060\n",
      "Epoch 8 batch 700 train loss: 0.0044 test loss: 0.0060\n",
      "Epoch 8 batch 800 train loss: 0.0037 test loss: 0.0058\n",
      "Epoch 8 batch 900 train loss: 0.0052 test loss: 0.0057\n",
      "Epoch 8 batch 1000 train loss: 0.0061 test loss: 0.0057\n",
      "Epoch 8 batch 1100 train loss: 0.0050 test loss: 0.0055\n",
      "Epoch 8 batch 1200 train loss: 0.0085 test loss: 0.0054\n",
      "Epoch 8 batch 1300 train loss: 0.0040 test loss: 0.0060\n",
      "Epoch 8 batch 1400 train loss: 0.0042 test loss: 0.0060\n",
      "Epoch 8 batch 1500 train loss: 0.0025 test loss: 0.0055\n",
      "Epoch 8 batch 1600 train loss: 0.0032 test loss: 0.0058\n",
      "Epoch 8 batch 1700 train loss: 0.0049 test loss: 0.0060\n",
      "Epoch 8 batch 1800 train loss: 0.0055 test loss: 0.0057\n",
      "Epoch 8 batch 1900 train loss: 0.0072 test loss: 0.0058\n",
      "Epoch 8 batch 2000 train loss: 0.0033 test loss: 0.0056\n",
      "Epoch 8 batch 2100 train loss: 0.0037 test loss: 0.0055\n",
      "Epoch 8 batch 2200 train loss: 0.0075 test loss: 0.0057\n",
      "Epoch 8 batch 2300 train loss: 0.0057 test loss: 0.0057\n",
      "Epoch 8 batch 2400 train loss: 0.0070 test loss: 0.0059\n",
      "Epoch 8 batch 2500 train loss: 0.0069 test loss: 0.0057\n",
      "Epoch 8 batch 2600 train loss: 0.0061 test loss: 0.0059\n",
      "Epoch 8 batch 2700 train loss: 0.0094 test loss: 0.0059\n",
      "Epoch 8 batch 2800 train loss: 0.0044 test loss: 0.0062\n",
      "Epoch 8 batch 2900 train loss: 0.0073 test loss: 0.0058\n",
      "Epoch 8 batch 3000 train loss: 0.0045 test loss: 0.0055\n",
      "Epoch 9 batch 0 train loss: 0.0054 test loss: 0.0056\n",
      "Epoch 9 batch 100 train loss: 0.0034 test loss: 0.0061\n",
      "Epoch 9 batch 200 train loss: 0.0039 test loss: 0.0057\n",
      "Epoch 9 batch 300 train loss: 0.0052 test loss: 0.0057\n",
      "Epoch 9 batch 400 train loss: 0.0055 test loss: 0.0062\n",
      "Epoch 9 batch 500 train loss: 0.0084 test loss: 0.0059\n",
      "Epoch 9 batch 600 train loss: 0.0047 test loss: 0.0061\n",
      "Epoch 9 batch 700 train loss: 0.0050 test loss: 0.0059\n",
      "Epoch 9 batch 800 train loss: 0.0017 test loss: 0.0059\n",
      "Epoch 9 batch 900 train loss: 0.0052 test loss: 0.0057\n",
      "Epoch 9 batch 1000 train loss: 0.0047 test loss: 0.0059\n",
      "Epoch 9 batch 1100 train loss: 0.0093 test loss: 0.0062\n",
      "Epoch 9 batch 1200 train loss: 0.0016 test loss: 0.0059\n",
      "Epoch 9 batch 1300 train loss: 0.0027 test loss: 0.0061\n",
      "Epoch 9 batch 1400 train loss: 0.0057 test loss: 0.0057\n",
      "Epoch 9 batch 1500 train loss: 0.0032 test loss: 0.0061\n",
      "Epoch 9 batch 1600 train loss: 0.0067 test loss: 0.0057\n",
      "Epoch 9 batch 1700 train loss: 0.0083 test loss: 0.0059\n",
      "Epoch 9 batch 1800 train loss: 0.0042 test loss: 0.0058\n",
      "Epoch 9 batch 1900 train loss: 0.0020 test loss: 0.0060\n",
      "Epoch 9 batch 2000 train loss: 0.0036 test loss: 0.0056\n",
      "Epoch 9 batch 2100 train loss: 0.0051 test loss: 0.0057\n",
      "Epoch 9 batch 2200 train loss: 0.0067 test loss: 0.0058\n",
      "Epoch 9 batch 2300 train loss: 0.0034 test loss: 0.0059\n",
      "Epoch 9 batch 2400 train loss: 0.0042 test loss: 0.0060\n",
      "Epoch 9 batch 2500 train loss: 0.0020 test loss: 0.0060\n",
      "Epoch 9 batch 2600 train loss: 0.0039 test loss: 0.0059\n",
      "Epoch 9 batch 2700 train loss: 0.0074 test loss: 0.0059\n",
      "Epoch 9 batch 2800 train loss: 0.0035 test loss: 0.0057\n",
      "Epoch 9 batch 2900 train loss: 0.0061 test loss: 0.0055\n",
      "Epoch 9 batch 3000 train loss: 0.0103 test loss: 0.0059\n",
      "Epoch 10 batch 0 train loss: 0.0067 test loss: 0.0056\n",
      "Epoch 10 batch 100 train loss: 0.0071 test loss: 0.0058\n",
      "Epoch 10 batch 200 train loss: 0.0076 test loss: 0.0057\n",
      "Epoch 10 batch 300 train loss: 0.0083 test loss: 0.0061\n",
      "Epoch 10 batch 400 train loss: 0.0044 test loss: 0.0059\n",
      "Epoch 10 batch 500 train loss: 0.0113 test loss: 0.0060\n",
      "Epoch 10 batch 600 train loss: 0.0087 test loss: 0.0058\n",
      "Epoch 10 batch 700 train loss: 0.0027 test loss: 0.0059\n",
      "Epoch 10 batch 800 train loss: 0.0049 test loss: 0.0057\n",
      "Epoch 10 batch 900 train loss: 0.0038 test loss: 0.0061\n",
      "Epoch 10 batch 1000 train loss: 0.0061 test loss: 0.0059\n",
      "Epoch 10 batch 1100 train loss: 0.0049 test loss: 0.0061\n",
      "Epoch 10 batch 1200 train loss: 0.0032 test loss: 0.0056\n",
      "Epoch 10 batch 1300 train loss: 0.0078 test loss: 0.0056\n",
      "Epoch 10 batch 1400 train loss: 0.0036 test loss: 0.0057\n",
      "Epoch 10 batch 1500 train loss: 0.0046 test loss: 0.0058\n",
      "Epoch 10 batch 1600 train loss: 0.0079 test loss: 0.0061\n",
      "Epoch 10 batch 1700 train loss: 0.0043 test loss: 0.0057\n",
      "Epoch 10 batch 1800 train loss: 0.0049 test loss: 0.0057\n",
      "Epoch 10 batch 1900 train loss: 0.0056 test loss: 0.0057\n",
      "Epoch 10 batch 2000 train loss: 0.0029 test loss: 0.0056\n",
      "Epoch 10 batch 2100 train loss: 0.0032 test loss: 0.0060\n",
      "Epoch 10 batch 2200 train loss: 0.0051 test loss: 0.0059\n",
      "Epoch 10 batch 2300 train loss: 0.0038 test loss: 0.0056\n",
      "Epoch 10 batch 2400 train loss: 0.0035 test loss: 0.0057\n",
      "Epoch 10 batch 2500 train loss: 0.0069 test loss: 0.0058\n",
      "Epoch 10 batch 2600 train loss: 0.0062 test loss: 0.0057\n",
      "Epoch 10 batch 2700 train loss: 0.0043 test loss: 0.0059\n",
      "Epoch 10 batch 2800 train loss: 0.0027 test loss: 0.0062\n",
      "Epoch 10 batch 2900 train loss: 0.0078 test loss: 0.0056\n",
      "Epoch 10 batch 3000 train loss: 0.0034 test loss: 0.0056\n",
      "Epoch 11 batch 0 train loss: 0.0041 test loss: 0.0056\n",
      "Epoch 11 batch 100 train loss: 0.0040 test loss: 0.0061\n",
      "Epoch 11 batch 200 train loss: 0.0060 test loss: 0.0057\n",
      "Epoch 11 batch 300 train loss: 0.0072 test loss: 0.0061\n",
      "Epoch 11 batch 400 train loss: 0.0030 test loss: 0.0058\n",
      "Epoch 11 batch 500 train loss: 0.0062 test loss: 0.0059\n",
      "Epoch 11 batch 600 train loss: 0.0062 test loss: 0.0056\n",
      "Epoch 11 batch 700 train loss: 0.0038 test loss: 0.0062\n",
      "Epoch 11 batch 800 train loss: 0.0041 test loss: 0.0057\n",
      "Epoch 11 batch 900 train loss: 0.0083 test loss: 0.0059\n",
      "Epoch 11 batch 1000 train loss: 0.0060 test loss: 0.0058\n",
      "Epoch 11 batch 1100 train loss: 0.0061 test loss: 0.0061\n",
      "Epoch 11 batch 1200 train loss: 0.0027 test loss: 0.0056\n",
      "Epoch 11 batch 1300 train loss: 0.0079 test loss: 0.0059\n",
      "Epoch 11 batch 1400 train loss: 0.0057 test loss: 0.0057\n",
      "Epoch 11 batch 1500 train loss: 0.0054 test loss: 0.0060\n",
      "Epoch 11 batch 1600 train loss: 0.0070 test loss: 0.0059\n",
      "Epoch 11 batch 1700 train loss: 0.0062 test loss: 0.0056\n",
      "Epoch 11 batch 1800 train loss: 0.0037 test loss: 0.0057\n",
      "Epoch 11 batch 1900 train loss: 0.0074 test loss: 0.0061\n",
      "Epoch 11 batch 2000 train loss: 0.0021 test loss: 0.0055\n",
      "Epoch 11 batch 2100 train loss: 0.0044 test loss: 0.0058\n",
      "Epoch 11 batch 2200 train loss: 0.0051 test loss: 0.0060\n",
      "Epoch 11 batch 2300 train loss: 0.0081 test loss: 0.0055\n",
      "Epoch 11 batch 2400 train loss: 0.0071 test loss: 0.0062\n",
      "Epoch 11 batch 2500 train loss: 0.0077 test loss: 0.0057\n",
      "Epoch 11 batch 2600 train loss: 0.0036 test loss: 0.0057\n",
      "Epoch 11 batch 2700 train loss: 0.0039 test loss: 0.0061\n",
      "Epoch 11 batch 2800 train loss: 0.0043 test loss: 0.0056\n",
      "Epoch 11 batch 2900 train loss: 0.0050 test loss: 0.0057\n",
      "Epoch 11 batch 3000 train loss: 0.0046 test loss: 0.0057\n",
      "Epoch 12 batch 0 train loss: 0.0045 test loss: 0.0055\n",
      "Epoch 12 batch 100 train loss: 0.0072 test loss: 0.0058\n",
      "Epoch 12 batch 200 train loss: 0.0047 test loss: 0.0058\n",
      "Epoch 12 batch 300 train loss: 0.0053 test loss: 0.0059\n",
      "Epoch 12 batch 400 train loss: 0.0050 test loss: 0.0060\n",
      "Epoch 12 batch 500 train loss: 0.0045 test loss: 0.0061\n",
      "Epoch 12 batch 600 train loss: 0.0028 test loss: 0.0062\n",
      "Epoch 12 batch 700 train loss: 0.0074 test loss: 0.0061\n",
      "Epoch 12 batch 800 train loss: 0.0053 test loss: 0.0058\n",
      "Epoch 12 batch 900 train loss: 0.0058 test loss: 0.0058\n",
      "Epoch 12 batch 1000 train loss: 0.0068 test loss: 0.0057\n",
      "Epoch 12 batch 1100 train loss: 0.0029 test loss: 0.0056\n",
      "Epoch 12 batch 1200 train loss: 0.0027 test loss: 0.0057\n",
      "Epoch 12 batch 1300 train loss: 0.0057 test loss: 0.0059\n",
      "Epoch 12 batch 1400 train loss: 0.0069 test loss: 0.0059\n",
      "Epoch 12 batch 1500 train loss: 0.0040 test loss: 0.0059\n",
      "Epoch 12 batch 1600 train loss: 0.0041 test loss: 0.0058\n",
      "Epoch 12 batch 1700 train loss: 0.0029 test loss: 0.0060\n",
      "Epoch 12 batch 1800 train loss: 0.0049 test loss: 0.0057\n",
      "Epoch 12 batch 1900 train loss: 0.0039 test loss: 0.0059\n",
      "Epoch 12 batch 2000 train loss: 0.0036 test loss: 0.0054\n",
      "Epoch 12 batch 2100 train loss: 0.0061 test loss: 0.0058\n",
      "Epoch 12 batch 2200 train loss: 0.0060 test loss: 0.0059\n",
      "Epoch 12 batch 2300 train loss: 0.0060 test loss: 0.0061\n",
      "Epoch 12 batch 2400 train loss: 0.0057 test loss: 0.0057\n",
      "Epoch 12 batch 2500 train loss: 0.0027 test loss: 0.0058\n",
      "Epoch 12 batch 2600 train loss: 0.0054 test loss: 0.0058\n",
      "Epoch 12 batch 2700 train loss: 0.0028 test loss: 0.0058\n",
      "Epoch 12 batch 2800 train loss: 0.0065 test loss: 0.0062\n",
      "Epoch 12 batch 2900 train loss: 0.0090 test loss: 0.0060\n",
      "Epoch 12 batch 3000 train loss: 0.0040 test loss: 0.0060\n",
      "Epoch 13 batch 0 train loss: 0.0032 test loss: 0.0060\n",
      "Epoch 13 batch 100 train loss: 0.0036 test loss: 0.0062\n",
      "Epoch 13 batch 200 train loss: 0.0045 test loss: 0.0057\n",
      "Epoch 13 batch 300 train loss: 0.0064 test loss: 0.0059\n",
      "Epoch 13 batch 400 train loss: 0.0049 test loss: 0.0060\n",
      "Epoch 13 batch 500 train loss: 0.0039 test loss: 0.0058\n",
      "Epoch 13 batch 600 train loss: 0.0067 test loss: 0.0061\n",
      "Epoch 13 batch 700 train loss: 0.0056 test loss: 0.0062\n",
      "Epoch 13 batch 800 train loss: 0.0063 test loss: 0.0058\n",
      "Epoch 13 batch 900 train loss: 0.0042 test loss: 0.0061\n",
      "Epoch 13 batch 1000 train loss: 0.0092 test loss: 0.0057\n",
      "Epoch 13 batch 1100 train loss: 0.0037 test loss: 0.0059\n",
      "Epoch 13 batch 1200 train loss: 0.0082 test loss: 0.0058\n",
      "Epoch 13 batch 1300 train loss: 0.0042 test loss: 0.0057\n",
      "Epoch 13 batch 1400 train loss: 0.0055 test loss: 0.0059\n",
      "Epoch 13 batch 1500 train loss: 0.0074 test loss: 0.0060\n",
      "Epoch 13 batch 1600 train loss: 0.0066 test loss: 0.0060\n",
      "Epoch 13 batch 1700 train loss: 0.0077 test loss: 0.0061\n",
      "Epoch 13 batch 1800 train loss: 0.0060 test loss: 0.0057\n",
      "Epoch 13 batch 1900 train loss: 0.0034 test loss: 0.0059\n",
      "Epoch 13 batch 2000 train loss: 0.0059 test loss: 0.0059\n",
      "Epoch 13 batch 2100 train loss: 0.0069 test loss: 0.0060\n",
      "Epoch 13 batch 2200 train loss: 0.0029 test loss: 0.0056\n",
      "Epoch 13 batch 2300 train loss: 0.0030 test loss: 0.0056\n",
      "Epoch 13 batch 2400 train loss: 0.0064 test loss: 0.0057\n",
      "Epoch 13 batch 2500 train loss: 0.0060 test loss: 0.0060\n",
      "Epoch 13 batch 2600 train loss: 0.0075 test loss: 0.0058\n",
      "Epoch 13 batch 2700 train loss: 0.0032 test loss: 0.0061\n",
      "Epoch 13 batch 2800 train loss: 0.0024 test loss: 0.0057\n",
      "Epoch 13 batch 2900 train loss: 0.0071 test loss: 0.0058\n",
      "Epoch 13 batch 3000 train loss: 0.0048 test loss: 0.0058\n",
      "Epoch 14 batch 0 train loss: 0.0036 test loss: 0.0056\n",
      "Epoch 14 batch 100 train loss: 0.0033 test loss: 0.0061\n",
      "Epoch 14 batch 200 train loss: 0.0053 test loss: 0.0057\n",
      "Epoch 14 batch 300 train loss: 0.0066 test loss: 0.0058\n",
      "Epoch 14 batch 400 train loss: 0.0042 test loss: 0.0060\n",
      "Epoch 14 batch 500 train loss: 0.0041 test loss: 0.0063\n",
      "Epoch 14 batch 600 train loss: 0.0070 test loss: 0.0062\n",
      "Epoch 14 batch 700 train loss: 0.0036 test loss: 0.0058\n",
      "Epoch 14 batch 800 train loss: 0.0070 test loss: 0.0058\n",
      "Epoch 14 batch 900 train loss: 0.0089 test loss: 0.0061\n",
      "Epoch 14 batch 1000 train loss: 0.0064 test loss: 0.0057\n",
      "Epoch 14 batch 1100 train loss: 0.0058 test loss: 0.0059\n",
      "Epoch 14 batch 1200 train loss: 0.0026 test loss: 0.0058\n",
      "Epoch 14 batch 1300 train loss: 0.0066 test loss: 0.0058\n",
      "Epoch 14 batch 1400 train loss: 0.0064 test loss: 0.0055\n",
      "Epoch 14 batch 1500 train loss: 0.0058 test loss: 0.0060\n",
      "Epoch 14 batch 1600 train loss: 0.0065 test loss: 0.0060\n",
      "Epoch 14 batch 1700 train loss: 0.0037 test loss: 0.0061\n",
      "Epoch 14 batch 1800 train loss: 0.0063 test loss: 0.0057\n",
      "Epoch 14 batch 1900 train loss: 0.0069 test loss: 0.0058\n",
      "Epoch 14 batch 2000 train loss: 0.0052 test loss: 0.0059\n",
      "Epoch 14 batch 2100 train loss: 0.0039 test loss: 0.0058\n",
      "Epoch 14 batch 2200 train loss: 0.0091 test loss: 0.0059\n",
      "Epoch 14 batch 2300 train loss: 0.0064 test loss: 0.0055\n",
      "Epoch 14 batch 2400 train loss: 0.0057 test loss: 0.0059\n",
      "Epoch 14 batch 2500 train loss: 0.0040 test loss: 0.0059\n",
      "Epoch 14 batch 2600 train loss: 0.0047 test loss: 0.0060\n",
      "Epoch 14 batch 2700 train loss: 0.0050 test loss: 0.0062\n",
      "Epoch 14 batch 2800 train loss: 0.0071 test loss: 0.0063\n",
      "Epoch 14 batch 2900 train loss: 0.0036 test loss: 0.0058\n",
      "Epoch 14 batch 3000 train loss: 0.0046 test loss: 0.0058\n",
      "Epoch 15 batch 0 train loss: 0.0032 test loss: 0.0057\n",
      "Epoch 15 batch 100 train loss: 0.0074 test loss: 0.0057\n",
      "Epoch 15 batch 200 train loss: 0.0059 test loss: 0.0057\n",
      "Epoch 15 batch 300 train loss: 0.0051 test loss: 0.0058\n",
      "Epoch 15 batch 400 train loss: 0.0054 test loss: 0.0060\n",
      "Epoch 15 batch 500 train loss: 0.0053 test loss: 0.0061\n",
      "Epoch 15 batch 600 train loss: 0.0037 test loss: 0.0060\n",
      "Epoch 15 batch 700 train loss: 0.0034 test loss: 0.0058\n",
      "Epoch 15 batch 800 train loss: 0.0055 test loss: 0.0059\n",
      "Epoch 15 batch 900 train loss: 0.0063 test loss: 0.0059\n",
      "Epoch 15 batch 1000 train loss: 0.0064 test loss: 0.0056\n",
      "Epoch 15 batch 1100 train loss: 0.0048 test loss: 0.0057\n",
      "Epoch 15 batch 1200 train loss: 0.0049 test loss: 0.0058\n",
      "Epoch 15 batch 1300 train loss: 0.0038 test loss: 0.0059\n",
      "Epoch 15 batch 1400 train loss: 0.0073 test loss: 0.0057\n",
      "Epoch 15 batch 1500 train loss: 0.0070 test loss: 0.0058\n",
      "Epoch 15 batch 1600 train loss: 0.0024 test loss: 0.0064\n",
      "Epoch 15 batch 1700 train loss: 0.0090 test loss: 0.0058\n",
      "Epoch 15 batch 1800 train loss: 0.0056 test loss: 0.0059\n",
      "Epoch 15 batch 1900 train loss: 0.0048 test loss: 0.0059\n",
      "Epoch 15 batch 2000 train loss: 0.0021 test loss: 0.0059\n",
      "Epoch 15 batch 2100 train loss: 0.0053 test loss: 0.0061\n",
      "Epoch 15 batch 2200 train loss: 0.0105 test loss: 0.0055\n",
      "Epoch 15 batch 2300 train loss: 0.0059 test loss: 0.0058\n",
      "Epoch 15 batch 2400 train loss: 0.0051 test loss: 0.0056\n",
      "Epoch 15 batch 2500 train loss: 0.0055 test loss: 0.0057\n",
      "Epoch 15 batch 2600 train loss: 0.0038 test loss: 0.0058\n",
      "Epoch 15 batch 2700 train loss: 0.0066 test loss: 0.0060\n",
      "Epoch 15 batch 2800 train loss: 0.0058 test loss: 0.0061\n",
      "Epoch 15 batch 2900 train loss: 0.0050 test loss: 0.0056\n",
      "Epoch 15 batch 3000 train loss: 0.0041 test loss: 0.0056\n",
      "Epoch 16 batch 0 train loss: 0.0045 test loss: 0.0057\n",
      "Epoch 16 batch 100 train loss: 0.0093 test loss: 0.0056\n",
      "Epoch 16 batch 200 train loss: 0.0038 test loss: 0.0059\n",
      "Epoch 16 batch 300 train loss: 0.0087 test loss: 0.0058\n",
      "Epoch 16 batch 400 train loss: 0.0085 test loss: 0.0058\n",
      "Epoch 16 batch 500 train loss: 0.0034 test loss: 0.0058\n",
      "Epoch 16 batch 600 train loss: 0.0053 test loss: 0.0057\n",
      "Epoch 16 batch 700 train loss: 0.0029 test loss: 0.0059\n",
      "Epoch 16 batch 800 train loss: 0.0038 test loss: 0.0061\n",
      "Epoch 16 batch 900 train loss: 0.0033 test loss: 0.0059\n",
      "Epoch 16 batch 1000 train loss: 0.0040 test loss: 0.0056\n",
      "Epoch 16 batch 1100 train loss: 0.0083 test loss: 0.0060\n",
      "Epoch 16 batch 1200 train loss: 0.0086 test loss: 0.0058\n",
      "Epoch 16 batch 1300 train loss: 0.0058 test loss: 0.0060\n",
      "Epoch 16 batch 1400 train loss: 0.0080 test loss: 0.0059\n",
      "Epoch 16 batch 1500 train loss: 0.0046 test loss: 0.0058\n",
      "Epoch 16 batch 1600 train loss: 0.0041 test loss: 0.0056\n",
      "Epoch 16 batch 1700 train loss: 0.0032 test loss: 0.0058\n",
      "Epoch 16 batch 1800 train loss: 0.0031 test loss: 0.0057\n",
      "Epoch 16 batch 1900 train loss: 0.0043 test loss: 0.0061\n",
      "Epoch 16 batch 2000 train loss: 0.0034 test loss: 0.0055\n",
      "Epoch 16 batch 2100 train loss: 0.0053 test loss: 0.0057\n",
      "Epoch 16 batch 2200 train loss: 0.0042 test loss: 0.0060\n",
      "Epoch 16 batch 2300 train loss: 0.0036 test loss: 0.0059\n",
      "Epoch 16 batch 2400 train loss: 0.0051 test loss: 0.0059\n",
      "Epoch 16 batch 2500 train loss: 0.0037 test loss: 0.0058\n",
      "Epoch 16 batch 2600 train loss: 0.0101 test loss: 0.0058\n",
      "Epoch 16 batch 2700 train loss: 0.0058 test loss: 0.0060\n",
      "Epoch 16 batch 2800 train loss: 0.0062 test loss: 0.0062\n",
      "Epoch 16 batch 2900 train loss: 0.0061 test loss: 0.0062\n",
      "Epoch 16 batch 3000 train loss: 0.0060 test loss: 0.0057\n",
      "Epoch 17 batch 0 train loss: 0.0073 test loss: 0.0058\n",
      "Epoch 17 batch 100 train loss: 0.0093 test loss: 0.0059\n",
      "Epoch 17 batch 200 train loss: 0.0059 test loss: 0.0057\n",
      "Epoch 17 batch 300 train loss: 0.0062 test loss: 0.0059\n",
      "Epoch 17 batch 400 train loss: 0.0064 test loss: 0.0059\n",
      "Epoch 17 batch 500 train loss: 0.0047 test loss: 0.0064\n",
      "early stop.\n",
      "Checkpoint 33 restored!!\n",
      "Training for loss rate 0.30 start.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ffnn_2/dense_6/kernel:0', 'ffnn_2/dense_6/bias:0', 'ffnn_2/batch_normalization_2/gamma:0', 'ffnn_2/batch_normalization_2/beta:0', 'ffnn_2/dense_7/kernel:0', 'ffnn_2/dense_7/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ffnn_2/dense_6/kernel:0', 'ffnn_2/dense_6/bias:0', 'ffnn_2/batch_normalization_2/gamma:0', 'ffnn_2/batch_normalization_2/beta:0', 'ffnn_2/dense_7/kernel:0', 'ffnn_2/dense_7/bias:0'] when minimizing the loss.\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.30p/ckpt-1\n",
      "Epoch 0 batch 0 train loss: 0.1851 test loss: 0.2534\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.30p/ckpt-2\n",
      "Epoch 0 batch 100 train loss: 0.0954 test loss: 0.1212\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.30p/ckpt-3\n",
      "Epoch 0 batch 200 train loss: 0.0634 test loss: 0.0869\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.30p/ckpt-4\n",
      "Epoch 0 batch 300 train loss: 0.0502 test loss: 0.0691\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.30p/ckpt-5\n",
      "Epoch 0 batch 400 train loss: 0.0426 test loss: 0.0558\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.30p/ckpt-6\n",
      "Epoch 0 batch 500 train loss: 0.0288 test loss: 0.0441\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.30p/ckpt-7\n",
      "Epoch 0 batch 600 train loss: 0.0297 test loss: 0.0357\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.30p/ckpt-8\n",
      "Epoch 0 batch 700 train loss: 0.0240 test loss: 0.0290\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.30p/ckpt-9\n",
      "Epoch 0 batch 800 train loss: 0.0168 test loss: 0.0232\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.30p/ckpt-10\n",
      "Epoch 0 batch 900 train loss: 0.0097 test loss: 0.0197\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.30p/ckpt-11\n",
      "Epoch 0 batch 1000 train loss: 0.0124 test loss: 0.0165\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.30p/ckpt-12\n",
      "Epoch 0 batch 1100 train loss: 0.0078 test loss: 0.0142\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.30p/ckpt-13\n",
      "Epoch 0 batch 1200 train loss: 0.0091 test loss: 0.0125\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.30p/ckpt-14\n",
      "Epoch 0 batch 1300 train loss: 0.0092 test loss: 0.0112\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.30p/ckpt-15\n",
      "Epoch 0 batch 1400 train loss: 0.0053 test loss: 0.0101\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.30p/ckpt-16\n",
      "Epoch 0 batch 1500 train loss: 0.0074 test loss: 0.0098\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.30p/ckpt-17\n",
      "Epoch 0 batch 1600 train loss: 0.0071 test loss: 0.0092\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.30p/ckpt-18\n",
      "Epoch 0 batch 1700 train loss: 0.0056 test loss: 0.0089\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.30p/ckpt-19\n",
      "Epoch 0 batch 1800 train loss: 0.0157 test loss: 0.0084\n",
      "Epoch 0 batch 1900 train loss: 0.0035 test loss: 0.0084\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.30p/ckpt-20\n",
      "Epoch 0 batch 2000 train loss: 0.0061 test loss: 0.0083\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.30p/ckpt-21\n",
      "Epoch 0 batch 2100 train loss: 0.0075 test loss: 0.0081\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.30p/ckpt-22\n",
      "Epoch 0 batch 2200 train loss: 0.0054 test loss: 0.0078\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.30p/ckpt-23\n",
      "Epoch 0 batch 2300 train loss: 0.0044 test loss: 0.0077\n",
      "Epoch 0 batch 2400 train loss: 0.0096 test loss: 0.0078\n",
      "Epoch 0 batch 2500 train loss: 0.0084 test loss: 0.0078\n",
      "Epoch 0 batch 2600 train loss: 0.0085 test loss: 0.0079\n",
      "Epoch 0 batch 2700 train loss: 0.0063 test loss: 0.0082\n",
      "Epoch 0 batch 2800 train loss: 0.0054 test loss: 0.0078\n",
      "Epoch 0 batch 2900 train loss: 0.0091 test loss: 0.0078\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.30p/ckpt-24\n",
      "Epoch 0 batch 3000 train loss: 0.0094 test loss: 0.0071\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ffnn_2/dense_6/kernel:0', 'ffnn_2/dense_6/bias:0', 'ffnn_2/batch_normalization_2/gamma:0', 'ffnn_2/batch_normalization_2/beta:0', 'ffnn_2/dense_7/kernel:0', 'ffnn_2/dense_7/bias:0'] when minimizing the loss.\n",
      "Epoch 1 batch 0 train loss: 0.0056 test loss: 0.0076\n",
      "Epoch 1 batch 100 train loss: 0.0068 test loss: 0.0078\n",
      "Epoch 1 batch 200 train loss: 0.0113 test loss: 0.0075\n",
      "Epoch 1 batch 300 train loss: 0.0040 test loss: 0.0077\n",
      "Epoch 1 batch 400 train loss: 0.0049 test loss: 0.0075\n",
      "Epoch 1 batch 500 train loss: 0.0064 test loss: 0.0077\n",
      "Epoch 1 batch 600 train loss: 0.0047 test loss: 0.0078\n",
      "Epoch 1 batch 700 train loss: 0.0054 test loss: 0.0080\n",
      "Epoch 1 batch 800 train loss: 0.0039 test loss: 0.0075\n",
      "Epoch 1 batch 900 train loss: 0.0052 test loss: 0.0078\n",
      "Epoch 1 batch 1000 train loss: 0.0047 test loss: 0.0078\n",
      "Epoch 1 batch 1100 train loss: 0.0092 test loss: 0.0076\n",
      "Epoch 1 batch 1200 train loss: 0.0073 test loss: 0.0073\n",
      "Epoch 1 batch 1300 train loss: 0.0086 test loss: 0.0079\n",
      "Epoch 1 batch 1400 train loss: 0.0049 test loss: 0.0073\n",
      "Epoch 1 batch 1500 train loss: 0.0055 test loss: 0.0073\n",
      "Epoch 1 batch 1600 train loss: 0.0052 test loss: 0.0074\n",
      "Epoch 1 batch 1700 train loss: 0.0060 test loss: 0.0078\n",
      "Epoch 1 batch 1800 train loss: 0.0052 test loss: 0.0074\n",
      "Epoch 1 batch 1900 train loss: 0.0073 test loss: 0.0077\n",
      "Epoch 1 batch 2000 train loss: 0.0046 test loss: 0.0075\n",
      "Epoch 1 batch 2100 train loss: 0.0053 test loss: 0.0073\n",
      "Epoch 1 batch 2200 train loss: 0.0068 test loss: 0.0075\n",
      "Epoch 1 batch 2300 train loss: 0.0049 test loss: 0.0075\n",
      "Epoch 1 batch 2400 train loss: 0.0070 test loss: 0.0076\n",
      "Epoch 1 batch 2500 train loss: 0.0071 test loss: 0.0075\n",
      "Epoch 1 batch 2600 train loss: 0.0057 test loss: 0.0076\n",
      "Epoch 1 batch 2700 train loss: 0.0089 test loss: 0.0080\n",
      "Epoch 1 batch 2800 train loss: 0.0061 test loss: 0.0074\n",
      "Epoch 1 batch 2900 train loss: 0.0063 test loss: 0.0073\n",
      "Epoch 1 batch 3000 train loss: 0.0043 test loss: 0.0072\n",
      "Epoch 2 batch 0 train loss: 0.0049 test loss: 0.0072\n",
      "Epoch 2 batch 100 train loss: 0.0100 test loss: 0.0075\n",
      "Epoch 2 batch 200 train loss: 0.0086 test loss: 0.0073\n",
      "Epoch 2 batch 300 train loss: 0.0063 test loss: 0.0073\n",
      "Epoch 2 batch 400 train loss: 0.0081 test loss: 0.0077\n",
      "Epoch 2 batch 500 train loss: 0.0128 test loss: 0.0074\n",
      "Epoch 2 batch 600 train loss: 0.0073 test loss: 0.0078\n",
      "Epoch 2 batch 700 train loss: 0.0111 test loss: 0.0073\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.30p/ckpt-25\n",
      "Epoch 2 batch 800 train loss: 0.0056 test loss: 0.0070\n",
      "Epoch 2 batch 900 train loss: 0.0097 test loss: 0.0073\n",
      "Epoch 2 batch 1000 train loss: 0.0047 test loss: 0.0071\n",
      "Epoch 2 batch 1100 train loss: 0.0044 test loss: 0.0072\n",
      "Epoch 2 batch 1200 train loss: 0.0063 test loss: 0.0075\n",
      "Epoch 2 batch 1300 train loss: 0.0080 test loss: 0.0072\n",
      "Epoch 2 batch 1400 train loss: 0.0045 test loss: 0.0074\n",
      "Epoch 2 batch 1500 train loss: 0.0068 test loss: 0.0071\n",
      "Epoch 2 batch 1600 train loss: 0.0078 test loss: 0.0076\n",
      "Epoch 2 batch 1700 train loss: 0.0060 test loss: 0.0071\n",
      "Epoch 2 batch 1800 train loss: 0.0129 test loss: 0.0074\n",
      "Epoch 2 batch 1900 train loss: 0.0038 test loss: 0.0074\n",
      "Epoch 2 batch 2000 train loss: 0.0069 test loss: 0.0074\n",
      "Epoch 2 batch 2100 train loss: 0.0046 test loss: 0.0073\n",
      "Epoch 2 batch 2200 train loss: 0.0066 test loss: 0.0076\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.30p/ckpt-26\n",
      "Epoch 2 batch 2300 train loss: 0.0053 test loss: 0.0070\n",
      "Epoch 2 batch 2400 train loss: 0.0048 test loss: 0.0072\n",
      "Epoch 2 batch 2500 train loss: 0.0053 test loss: 0.0073\n",
      "Epoch 2 batch 2600 train loss: 0.0086 test loss: 0.0072\n",
      "Epoch 2 batch 2700 train loss: 0.0044 test loss: 0.0074\n",
      "Epoch 2 batch 2800 train loss: 0.0110 test loss: 0.0079\n",
      "Epoch 2 batch 2900 train loss: 0.0049 test loss: 0.0077\n",
      "Epoch 2 batch 3000 train loss: 0.0051 test loss: 0.0073\n",
      "Epoch 3 batch 0 train loss: 0.0056 test loss: 0.0072\n",
      "Epoch 3 batch 100 train loss: 0.0053 test loss: 0.0072\n",
      "Epoch 3 batch 200 train loss: 0.0065 test loss: 0.0071\n",
      "Epoch 3 batch 300 train loss: 0.0083 test loss: 0.0070\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.30p/ckpt-27\n",
      "Epoch 3 batch 400 train loss: 0.0059 test loss: 0.0069\n",
      "Epoch 3 batch 500 train loss: 0.0084 test loss: 0.0075\n",
      "Epoch 3 batch 600 train loss: 0.0064 test loss: 0.0077\n",
      "Epoch 3 batch 700 train loss: 0.0082 test loss: 0.0073\n",
      "Epoch 3 batch 800 train loss: 0.0077 test loss: 0.0072\n",
      "Epoch 3 batch 900 train loss: 0.0066 test loss: 0.0073\n",
      "Epoch 3 batch 1000 train loss: 0.0035 test loss: 0.0071\n",
      "Epoch 3 batch 1100 train loss: 0.0081 test loss: 0.0070\n",
      "Epoch 3 batch 1200 train loss: 0.0058 test loss: 0.0076\n",
      "Epoch 3 batch 1300 train loss: 0.0051 test loss: 0.0075\n",
      "Epoch 3 batch 1400 train loss: 0.0055 test loss: 0.0071\n",
      "Epoch 3 batch 1500 train loss: 0.0050 test loss: 0.0075\n",
      "Epoch 3 batch 1600 train loss: 0.0093 test loss: 0.0071\n",
      "Epoch 3 batch 1700 train loss: 0.0042 test loss: 0.0072\n",
      "Epoch 3 batch 1800 train loss: 0.0065 test loss: 0.0071\n",
      "Epoch 3 batch 1900 train loss: 0.0083 test loss: 0.0071\n",
      "Epoch 3 batch 2000 train loss: 0.0042 test loss: 0.0071\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.30p/ckpt-28\n",
      "Epoch 3 batch 2100 train loss: 0.0050 test loss: 0.0069\n",
      "Epoch 3 batch 2200 train loss: 0.0052 test loss: 0.0073\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.30p/ckpt-29\n",
      "Epoch 3 batch 2300 train loss: 0.0097 test loss: 0.0068\n",
      "Epoch 3 batch 2400 train loss: 0.0060 test loss: 0.0077\n",
      "Epoch 3 batch 2500 train loss: 0.0092 test loss: 0.0071\n",
      "Epoch 3 batch 2600 train loss: 0.0091 test loss: 0.0071\n",
      "Epoch 3 batch 2700 train loss: 0.0069 test loss: 0.0072\n",
      "Epoch 3 batch 2800 train loss: 0.0066 test loss: 0.0079\n",
      "Epoch 3 batch 2900 train loss: 0.0040 test loss: 0.0071\n",
      "Epoch 3 batch 3000 train loss: 0.0055 test loss: 0.0072\n",
      "Epoch 4 batch 0 train loss: 0.0054 test loss: 0.0070\n",
      "Epoch 4 batch 100 train loss: 0.0080 test loss: 0.0073\n",
      "Epoch 4 batch 200 train loss: 0.0123 test loss: 0.0073\n",
      "Epoch 4 batch 300 train loss: 0.0090 test loss: 0.0074\n",
      "Epoch 4 batch 400 train loss: 0.0044 test loss: 0.0071\n",
      "Epoch 4 batch 500 train loss: 0.0045 test loss: 0.0079\n",
      "Epoch 4 batch 600 train loss: 0.0046 test loss: 0.0071\n",
      "Epoch 4 batch 700 train loss: 0.0076 test loss: 0.0075\n",
      "Epoch 4 batch 800 train loss: 0.0062 test loss: 0.0070\n",
      "Epoch 4 batch 900 train loss: 0.0109 test loss: 0.0074\n",
      "Epoch 4 batch 1000 train loss: 0.0092 test loss: 0.0071\n",
      "Epoch 4 batch 1100 train loss: 0.0079 test loss: 0.0073\n",
      "Epoch 4 batch 1200 train loss: 0.0042 test loss: 0.0073\n",
      "Epoch 4 batch 1300 train loss: 0.0095 test loss: 0.0075\n",
      "Epoch 4 batch 1400 train loss: 0.0063 test loss: 0.0071\n",
      "Epoch 4 batch 1500 train loss: 0.0054 test loss: 0.0071\n",
      "Epoch 4 batch 1600 train loss: 0.0101 test loss: 0.0074\n",
      "Epoch 4 batch 1700 train loss: 0.0041 test loss: 0.0072\n",
      "Epoch 4 batch 1800 train loss: 0.0082 test loss: 0.0070\n",
      "Epoch 4 batch 1900 train loss: 0.0046 test loss: 0.0070\n",
      "Epoch 4 batch 2000 train loss: 0.0035 test loss: 0.0071\n",
      "Epoch 4 batch 2100 train loss: 0.0056 test loss: 0.0072\n",
      "Epoch 4 batch 2200 train loss: 0.0048 test loss: 0.0071\n",
      "Epoch 4 batch 2300 train loss: 0.0126 test loss: 0.0072\n",
      "Epoch 4 batch 2400 train loss: 0.0096 test loss: 0.0075\n",
      "Epoch 4 batch 2500 train loss: 0.0076 test loss: 0.0072\n",
      "Epoch 4 batch 2600 train loss: 0.0071 test loss: 0.0070\n",
      "Epoch 4 batch 2700 train loss: 0.0103 test loss: 0.0071\n",
      "Epoch 4 batch 2800 train loss: 0.0031 test loss: 0.0074\n",
      "Epoch 4 batch 2900 train loss: 0.0062 test loss: 0.0073\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.30p/ckpt-30\n",
      "Epoch 4 batch 3000 train loss: 0.0066 test loss: 0.0068\n",
      "Epoch 5 batch 0 train loss: 0.0099 test loss: 0.0071\n",
      "Epoch 5 batch 100 train loss: 0.0121 test loss: 0.0073\n",
      "Epoch 5 batch 200 train loss: 0.0054 test loss: 0.0072\n",
      "Epoch 5 batch 300 train loss: 0.0051 test loss: 0.0072\n",
      "Epoch 5 batch 400 train loss: 0.0048 test loss: 0.0072\n",
      "Epoch 5 batch 500 train loss: 0.0058 test loss: 0.0076\n",
      "Epoch 5 batch 600 train loss: 0.0058 test loss: 0.0073\n",
      "Epoch 5 batch 700 train loss: 0.0091 test loss: 0.0075\n",
      "Epoch 5 batch 800 train loss: 0.0047 test loss: 0.0072\n",
      "Epoch 5 batch 900 train loss: 0.0063 test loss: 0.0073\n",
      "Epoch 5 batch 1000 train loss: 0.0067 test loss: 0.0075\n",
      "Epoch 5 batch 1100 train loss: 0.0083 test loss: 0.0074\n",
      "Epoch 5 batch 1200 train loss: 0.0091 test loss: 0.0073\n",
      "Epoch 5 batch 1300 train loss: 0.0086 test loss: 0.0072\n",
      "Epoch 5 batch 1400 train loss: 0.0063 test loss: 0.0068\n",
      "Epoch 5 batch 1500 train loss: 0.0076 test loss: 0.0072\n",
      "Epoch 5 batch 1600 train loss: 0.0041 test loss: 0.0072\n",
      "Epoch 5 batch 1700 train loss: 0.0063 test loss: 0.0072\n",
      "Epoch 5 batch 1800 train loss: 0.0042 test loss: 0.0069\n",
      "Epoch 5 batch 1900 train loss: 0.0052 test loss: 0.0076\n",
      "Epoch 5 batch 2000 train loss: 0.0084 test loss: 0.0072\n",
      "Epoch 5 batch 2100 train loss: 0.0047 test loss: 0.0074\n",
      "Epoch 5 batch 2200 train loss: 0.0063 test loss: 0.0069\n",
      "Epoch 5 batch 2300 train loss: 0.0053 test loss: 0.0070\n",
      "Epoch 5 batch 2400 train loss: 0.0049 test loss: 0.0077\n",
      "Epoch 5 batch 2500 train loss: 0.0100 test loss: 0.0073\n",
      "Epoch 5 batch 2600 train loss: 0.0088 test loss: 0.0074\n",
      "Epoch 5 batch 2700 train loss: 0.0074 test loss: 0.0074\n",
      "Epoch 5 batch 2800 train loss: 0.0075 test loss: 0.0073\n",
      "Epoch 5 batch 2900 train loss: 0.0053 test loss: 0.0071\n",
      "Epoch 5 batch 3000 train loss: 0.0049 test loss: 0.0070\n",
      "Epoch 6 batch 0 train loss: 0.0060 test loss: 0.0073\n",
      "Epoch 6 batch 100 train loss: 0.0047 test loss: 0.0071\n",
      "Epoch 6 batch 200 train loss: 0.0067 test loss: 0.0074\n",
      "Epoch 6 batch 300 train loss: 0.0080 test loss: 0.0075\n",
      "Epoch 6 batch 400 train loss: 0.0069 test loss: 0.0074\n",
      "Epoch 6 batch 500 train loss: 0.0102 test loss: 0.0069\n",
      "Epoch 6 batch 600 train loss: 0.0092 test loss: 0.0068\n",
      "Epoch 6 batch 700 train loss: 0.0041 test loss: 0.0072\n",
      "Epoch 6 batch 800 train loss: 0.0075 test loss: 0.0069\n",
      "Epoch 6 batch 900 train loss: 0.0079 test loss: 0.0074\n",
      "Epoch 6 batch 1000 train loss: 0.0071 test loss: 0.0070\n",
      "Epoch 6 batch 1100 train loss: 0.0068 test loss: 0.0070\n",
      "Epoch 6 batch 1200 train loss: 0.0043 test loss: 0.0077\n",
      "Epoch 6 batch 1300 train loss: 0.0101 test loss: 0.0073\n",
      "Epoch 6 batch 1400 train loss: 0.0046 test loss: 0.0068\n",
      "Epoch 6 batch 1500 train loss: 0.0067 test loss: 0.0072\n",
      "Epoch 6 batch 1600 train loss: 0.0071 test loss: 0.0072\n",
      "Epoch 6 batch 1700 train loss: 0.0036 test loss: 0.0075\n",
      "Epoch 6 batch 1800 train loss: 0.0067 test loss: 0.0069\n",
      "Epoch 6 batch 1900 train loss: 0.0064 test loss: 0.0072\n",
      "Epoch 6 batch 2000 train loss: 0.0070 test loss: 0.0068\n",
      "Epoch 6 batch 2100 train loss: 0.0061 test loss: 0.0069\n",
      "Epoch 6 batch 2200 train loss: 0.0070 test loss: 0.0071\n",
      "Epoch 6 batch 2300 train loss: 0.0065 test loss: 0.0071\n",
      "Epoch 6 batch 2400 train loss: 0.0119 test loss: 0.0074\n",
      "Epoch 6 batch 2500 train loss: 0.0037 test loss: 0.0070\n",
      "Epoch 6 batch 2600 train loss: 0.0060 test loss: 0.0071\n",
      "Epoch 6 batch 2700 train loss: 0.0070 test loss: 0.0073\n",
      "Epoch 6 batch 2800 train loss: 0.0086 test loss: 0.0077\n",
      "Epoch 6 batch 2900 train loss: 0.0058 test loss: 0.0071\n",
      "Epoch 6 batch 3000 train loss: 0.0079 test loss: 0.0070\n",
      "Epoch 7 batch 0 train loss: 0.0075 test loss: 0.0071\n",
      "Epoch 7 batch 100 train loss: 0.0055 test loss: 0.0073\n",
      "Epoch 7 batch 200 train loss: 0.0083 test loss: 0.0070\n",
      "Epoch 7 batch 300 train loss: 0.0072 test loss: 0.0073\n",
      "Epoch 7 batch 400 train loss: 0.0065 test loss: 0.0072\n",
      "Epoch 7 batch 500 train loss: 0.0030 test loss: 0.0072\n",
      "Epoch 7 batch 600 train loss: 0.0097 test loss: 0.0073\n",
      "Epoch 7 batch 700 train loss: 0.0049 test loss: 0.0071\n",
      "Epoch 7 batch 800 train loss: 0.0078 test loss: 0.0072\n",
      "Epoch 7 batch 900 train loss: 0.0057 test loss: 0.0072\n",
      "Epoch 7 batch 1000 train loss: 0.0040 test loss: 0.0075\n",
      "Epoch 7 batch 1100 train loss: 0.0057 test loss: 0.0073\n",
      "Epoch 7 batch 1200 train loss: 0.0124 test loss: 0.0071\n",
      "Epoch 7 batch 1300 train loss: 0.0067 test loss: 0.0073\n",
      "Epoch 7 batch 1400 train loss: 0.0066 test loss: 0.0072\n",
      "Epoch 7 batch 1500 train loss: 0.0059 test loss: 0.0074\n",
      "Epoch 7 batch 1600 train loss: 0.0065 test loss: 0.0072\n",
      "Epoch 7 batch 1700 train loss: 0.0089 test loss: 0.0074\n",
      "Epoch 7 batch 1800 train loss: 0.0081 test loss: 0.0070\n",
      "Epoch 7 batch 1900 train loss: 0.0060 test loss: 0.0073\n",
      "Epoch 7 batch 2000 train loss: 0.0082 test loss: 0.0070\n",
      "Epoch 7 batch 2100 train loss: 0.0063 test loss: 0.0073\n",
      "Epoch 7 batch 2200 train loss: 0.0123 test loss: 0.0076\n",
      "Epoch 7 batch 2300 train loss: 0.0064 test loss: 0.0071\n",
      "Epoch 7 batch 2400 train loss: 0.0094 test loss: 0.0071\n",
      "Epoch 7 batch 2500 train loss: 0.0077 test loss: 0.0070\n",
      "Epoch 7 batch 2600 train loss: 0.0043 test loss: 0.0075\n",
      "Epoch 7 batch 2700 train loss: 0.0044 test loss: 0.0074\n",
      "Epoch 7 batch 2800 train loss: 0.0068 test loss: 0.0074\n",
      "Epoch 7 batch 2900 train loss: 0.0071 test loss: 0.0071\n",
      "Epoch 7 batch 3000 train loss: 0.0098 test loss: 0.0071\n",
      "Epoch 8 batch 0 train loss: 0.0064 test loss: 0.0071\n",
      "Epoch 8 batch 100 train loss: 0.0046 test loss: 0.0075\n",
      "Epoch 8 batch 200 train loss: 0.0074 test loss: 0.0071\n",
      "Epoch 8 batch 300 train loss: 0.0112 test loss: 0.0072\n",
      "Epoch 8 batch 400 train loss: 0.0062 test loss: 0.0072\n",
      "Epoch 8 batch 500 train loss: 0.0088 test loss: 0.0073\n",
      "Epoch 8 batch 600 train loss: 0.0072 test loss: 0.0072\n",
      "Epoch 8 batch 700 train loss: 0.0064 test loss: 0.0071\n",
      "Epoch 8 batch 800 train loss: 0.0084 test loss: 0.0073\n",
      "Epoch 8 batch 900 train loss: 0.0079 test loss: 0.0072\n",
      "Epoch 8 batch 1000 train loss: 0.0054 test loss: 0.0073\n",
      "Epoch 8 batch 1100 train loss: 0.0073 test loss: 0.0070\n",
      "Epoch 8 batch 1200 train loss: 0.0071 test loss: 0.0069\n",
      "Epoch 8 batch 1300 train loss: 0.0065 test loss: 0.0074\n",
      "Epoch 8 batch 1400 train loss: 0.0064 test loss: 0.0073\n",
      "Epoch 8 batch 1500 train loss: 0.0035 test loss: 0.0070\n",
      "Epoch 8 batch 1600 train loss: 0.0049 test loss: 0.0075\n",
      "Epoch 8 batch 1700 train loss: 0.0109 test loss: 0.0070\n",
      "Epoch 8 batch 1800 train loss: 0.0059 test loss: 0.0069\n",
      "Epoch 8 batch 1900 train loss: 0.0107 test loss: 0.0073\n",
      "Epoch 8 batch 2000 train loss: 0.0099 test loss: 0.0068\n",
      "Epoch 8 batch 2100 train loss: 0.0061 test loss: 0.0071\n",
      "Epoch 8 batch 2200 train loss: 0.0039 test loss: 0.0074\n",
      "Epoch 8 batch 2300 train loss: 0.0068 test loss: 0.0072\n",
      "Epoch 8 batch 2400 train loss: 0.0050 test loss: 0.0072\n",
      "Epoch 8 batch 2500 train loss: 0.0059 test loss: 0.0070\n",
      "Epoch 8 batch 2600 train loss: 0.0089 test loss: 0.0073\n",
      "Epoch 8 batch 2700 train loss: 0.0100 test loss: 0.0075\n",
      "Epoch 8 batch 2800 train loss: 0.0057 test loss: 0.0076\n",
      "Epoch 8 batch 2900 train loss: 0.0050 test loss: 0.0069\n",
      "Epoch 8 batch 3000 train loss: 0.0048 test loss: 0.0069\n",
      "Epoch 9 batch 0 train loss: 0.0037 test loss: 0.0072\n",
      "Epoch 9 batch 100 train loss: 0.0078 test loss: 0.0074\n",
      "Epoch 9 batch 200 train loss: 0.0046 test loss: 0.0071\n",
      "Epoch 9 batch 300 train loss: 0.0073 test loss: 0.0071\n",
      "Epoch 9 batch 400 train loss: 0.0068 test loss: 0.0071\n",
      "Epoch 9 batch 500 train loss: 0.0044 test loss: 0.0072\n",
      "Epoch 9 batch 600 train loss: 0.0064 test loss: 0.0074\n",
      "Epoch 9 batch 700 train loss: 0.0059 test loss: 0.0072\n",
      "Epoch 9 batch 800 train loss: 0.0041 test loss: 0.0073\n",
      "Epoch 9 batch 900 train loss: 0.0078 test loss: 0.0072\n",
      "Epoch 9 batch 1000 train loss: 0.0116 test loss: 0.0071\n",
      "Epoch 9 batch 1100 train loss: 0.0099 test loss: 0.0071\n",
      "Epoch 9 batch 1200 train loss: 0.0102 test loss: 0.0072\n",
      "Epoch 9 batch 1300 train loss: 0.0072 test loss: 0.0073\n",
      "Epoch 9 batch 1400 train loss: 0.0101 test loss: 0.0071\n",
      "Epoch 9 batch 1500 train loss: 0.0100 test loss: 0.0073\n",
      "Epoch 9 batch 1600 train loss: 0.0053 test loss: 0.0073\n",
      "Epoch 9 batch 1700 train loss: 0.0070 test loss: 0.0074\n",
      "Epoch 9 batch 1800 train loss: 0.0062 test loss: 0.0071\n",
      "Epoch 9 batch 1900 train loss: 0.0062 test loss: 0.0071\n",
      "Epoch 9 batch 2000 train loss: 0.0057 test loss: 0.0071\n",
      "Epoch 9 batch 2100 train loss: 0.0103 test loss: 0.0072\n",
      "Epoch 9 batch 2200 train loss: 0.0051 test loss: 0.0074\n",
      "Epoch 9 batch 2300 train loss: 0.0062 test loss: 0.0068\n",
      "Epoch 9 batch 2400 train loss: 0.0066 test loss: 0.0073\n",
      "Epoch 9 batch 2500 train loss: 0.0135 test loss: 0.0070\n",
      "Epoch 9 batch 2600 train loss: 0.0032 test loss: 0.0069\n",
      "Epoch 9 batch 2700 train loss: 0.0060 test loss: 0.0076\n",
      "Epoch 9 batch 2800 train loss: 0.0067 test loss: 0.0074\n",
      "Epoch 9 batch 2900 train loss: 0.0038 test loss: 0.0069\n",
      "Epoch 9 batch 3000 train loss: 0.0059 test loss: 0.0073\n",
      "Epoch 10 batch 0 train loss: 0.0046 test loss: 0.0070\n",
      "Epoch 10 batch 100 train loss: 0.0047 test loss: 0.0072\n",
      "Epoch 10 batch 200 train loss: 0.0073 test loss: 0.0071\n",
      "Epoch 10 batch 300 train loss: 0.0060 test loss: 0.0075\n",
      "Epoch 10 batch 400 train loss: 0.0038 test loss: 0.0070\n",
      "Epoch 10 batch 500 train loss: 0.0080 test loss: 0.0071\n",
      "Epoch 10 batch 600 train loss: 0.0046 test loss: 0.0075\n",
      "Epoch 10 batch 700 train loss: 0.0061 test loss: 0.0076\n",
      "Epoch 10 batch 800 train loss: 0.0037 test loss: 0.0074\n",
      "Epoch 10 batch 900 train loss: 0.0078 test loss: 0.0073\n",
      "Epoch 10 batch 1000 train loss: 0.0083 test loss: 0.0071\n",
      "Epoch 10 batch 1100 train loss: 0.0070 test loss: 0.0075\n",
      "Epoch 10 batch 1200 train loss: 0.0067 test loss: 0.0072\n",
      "Epoch 10 batch 1300 train loss: 0.0082 test loss: 0.0070\n",
      "Epoch 10 batch 1400 train loss: 0.0082 test loss: 0.0071\n",
      "Epoch 10 batch 1500 train loss: 0.0087 test loss: 0.0069\n",
      "Epoch 10 batch 1600 train loss: 0.0074 test loss: 0.0073\n",
      "Epoch 10 batch 1700 train loss: 0.0092 test loss: 0.0073\n",
      "Epoch 10 batch 1800 train loss: 0.0115 test loss: 0.0069\n",
      "Epoch 10 batch 1900 train loss: 0.0072 test loss: 0.0073\n",
      "Epoch 10 batch 2000 train loss: 0.0048 test loss: 0.0070\n",
      "Epoch 10 batch 2100 train loss: 0.0056 test loss: 0.0071\n",
      "Epoch 10 batch 2200 train loss: 0.0071 test loss: 0.0070\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.30p/ckpt-31\n",
      "Epoch 10 batch 2300 train loss: 0.0094 test loss: 0.0068\n",
      "Epoch 10 batch 2400 train loss: 0.0051 test loss: 0.0075\n",
      "Epoch 10 batch 2500 train loss: 0.0076 test loss: 0.0069\n",
      "Epoch 10 batch 2600 train loss: 0.0067 test loss: 0.0071\n",
      "Epoch 10 batch 2700 train loss: 0.0075 test loss: 0.0072\n",
      "Epoch 10 batch 2800 train loss: 0.0062 test loss: 0.0077\n",
      "Epoch 10 batch 2900 train loss: 0.0074 test loss: 0.0069\n",
      "Epoch 10 batch 3000 train loss: 0.0043 test loss: 0.0071\n",
      "Epoch 11 batch 0 train loss: 0.0080 test loss: 0.0069\n",
      "Epoch 11 batch 100 train loss: 0.0064 test loss: 0.0074\n",
      "Epoch 11 batch 200 train loss: 0.0067 test loss: 0.0069\n",
      "Epoch 11 batch 300 train loss: 0.0054 test loss: 0.0070\n",
      "Epoch 11 batch 400 train loss: 0.0035 test loss: 0.0072\n",
      "Epoch 11 batch 500 train loss: 0.0055 test loss: 0.0072\n",
      "Epoch 11 batch 600 train loss: 0.0073 test loss: 0.0072\n",
      "Epoch 11 batch 700 train loss: 0.0038 test loss: 0.0076\n",
      "Epoch 11 batch 800 train loss: 0.0050 test loss: 0.0071\n",
      "Epoch 11 batch 900 train loss: 0.0043 test loss: 0.0072\n",
      "Epoch 11 batch 1000 train loss: 0.0051 test loss: 0.0072\n",
      "Epoch 11 batch 1100 train loss: 0.0085 test loss: 0.0074\n",
      "Epoch 11 batch 1200 train loss: 0.0075 test loss: 0.0072\n",
      "Epoch 11 batch 1300 train loss: 0.0058 test loss: 0.0072\n",
      "Epoch 11 batch 1400 train loss: 0.0065 test loss: 0.0072\n",
      "Epoch 11 batch 1500 train loss: 0.0076 test loss: 0.0070\n",
      "Epoch 11 batch 1600 train loss: 0.0067 test loss: 0.0073\n",
      "Epoch 11 batch 1700 train loss: 0.0081 test loss: 0.0074\n",
      "Epoch 11 batch 1800 train loss: 0.0092 test loss: 0.0072\n",
      "Epoch 11 batch 1900 train loss: 0.0052 test loss: 0.0071\n",
      "Epoch 11 batch 2000 train loss: 0.0051 test loss: 0.0071\n",
      "Epoch 11 batch 2100 train loss: 0.0074 test loss: 0.0071\n",
      "Epoch 11 batch 2200 train loss: 0.0061 test loss: 0.0073\n",
      "Epoch 11 batch 2300 train loss: 0.0120 test loss: 0.0070\n",
      "Epoch 11 batch 2400 train loss: 0.0043 test loss: 0.0075\n",
      "Epoch 11 batch 2500 train loss: 0.0135 test loss: 0.0072\n",
      "Epoch 11 batch 2600 train loss: 0.0060 test loss: 0.0073\n",
      "Epoch 11 batch 2700 train loss: 0.0079 test loss: 0.0072\n",
      "Epoch 11 batch 2800 train loss: 0.0063 test loss: 0.0074\n",
      "Epoch 11 batch 2900 train loss: 0.0072 test loss: 0.0069\n",
      "Epoch 11 batch 3000 train loss: 0.0090 test loss: 0.0074\n",
      "Epoch 12 batch 0 train loss: 0.0044 test loss: 0.0070\n",
      "Epoch 12 batch 100 train loss: 0.0092 test loss: 0.0072\n",
      "Epoch 12 batch 200 train loss: 0.0096 test loss: 0.0072\n",
      "Epoch 12 batch 300 train loss: 0.0083 test loss: 0.0070\n",
      "Epoch 12 batch 400 train loss: 0.0068 test loss: 0.0072\n",
      "Epoch 12 batch 500 train loss: 0.0097 test loss: 0.0072\n",
      "Epoch 12 batch 600 train loss: 0.0048 test loss: 0.0072\n",
      "Epoch 12 batch 700 train loss: 0.0058 test loss: 0.0069\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.30p/ckpt-32\n",
      "Epoch 12 batch 800 train loss: 0.0044 test loss: 0.0067\n",
      "Epoch 12 batch 900 train loss: 0.0070 test loss: 0.0073\n",
      "Epoch 12 batch 1000 train loss: 0.0084 test loss: 0.0073\n",
      "Epoch 12 batch 1100 train loss: 0.0077 test loss: 0.0072\n",
      "Epoch 12 batch 1200 train loss: 0.0043 test loss: 0.0071\n",
      "Epoch 12 batch 1300 train loss: 0.0054 test loss: 0.0070\n",
      "Epoch 12 batch 1400 train loss: 0.0106 test loss: 0.0073\n",
      "Epoch 12 batch 1500 train loss: 0.0050 test loss: 0.0069\n",
      "Epoch 12 batch 1600 train loss: 0.0089 test loss: 0.0074\n",
      "Epoch 12 batch 1700 train loss: 0.0078 test loss: 0.0071\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.30p/ckpt-33\n",
      "Epoch 12 batch 1800 train loss: 0.0110 test loss: 0.0067\n",
      "Epoch 12 batch 1900 train loss: 0.0068 test loss: 0.0070\n",
      "Epoch 12 batch 2000 train loss: 0.0027 test loss: 0.0070\n",
      "Epoch 12 batch 2100 train loss: 0.0123 test loss: 0.0070\n",
      "Epoch 12 batch 2200 train loss: 0.0060 test loss: 0.0072\n",
      "Epoch 12 batch 2300 train loss: 0.0042 test loss: 0.0073\n",
      "Epoch 12 batch 2400 train loss: 0.0062 test loss: 0.0074\n",
      "Epoch 12 batch 2500 train loss: 0.0112 test loss: 0.0072\n",
      "Epoch 12 batch 2600 train loss: 0.0076 test loss: 0.0068\n",
      "Epoch 12 batch 2700 train loss: 0.0044 test loss: 0.0070\n",
      "Epoch 12 batch 2800 train loss: 0.0068 test loss: 0.0075\n",
      "Epoch 12 batch 2900 train loss: 0.0081 test loss: 0.0072\n",
      "Epoch 12 batch 3000 train loss: 0.0055 test loss: 0.0075\n",
      "Epoch 13 batch 0 train loss: 0.0047 test loss: 0.0071\n",
      "Epoch 13 batch 100 train loss: 0.0049 test loss: 0.0072\n",
      "Epoch 13 batch 200 train loss: 0.0062 test loss: 0.0074\n",
      "Epoch 13 batch 300 train loss: 0.0034 test loss: 0.0075\n",
      "Epoch 13 batch 400 train loss: 0.0103 test loss: 0.0073\n",
      "Epoch 13 batch 500 train loss: 0.0046 test loss: 0.0071\n",
      "Epoch 13 batch 600 train loss: 0.0050 test loss: 0.0072\n",
      "Epoch 13 batch 700 train loss: 0.0059 test loss: 0.0073\n",
      "Epoch 13 batch 800 train loss: 0.0076 test loss: 0.0072\n",
      "Epoch 13 batch 900 train loss: 0.0064 test loss: 0.0073\n",
      "Epoch 13 batch 1000 train loss: 0.0025 test loss: 0.0072\n",
      "Epoch 13 batch 1100 train loss: 0.0124 test loss: 0.0069\n",
      "Epoch 13 batch 1200 train loss: 0.0080 test loss: 0.0074\n",
      "Epoch 13 batch 1300 train loss: 0.0078 test loss: 0.0071\n",
      "Epoch 13 batch 1400 train loss: 0.0087 test loss: 0.0070\n",
      "Epoch 13 batch 1500 train loss: 0.0044 test loss: 0.0075\n",
      "Epoch 13 batch 1600 train loss: 0.0081 test loss: 0.0073\n",
      "Epoch 13 batch 1700 train loss: 0.0062 test loss: 0.0072\n",
      "Epoch 13 batch 1800 train loss: 0.0059 test loss: 0.0071\n",
      "Epoch 13 batch 1900 train loss: 0.0077 test loss: 0.0073\n",
      "Epoch 13 batch 2000 train loss: 0.0051 test loss: 0.0070\n",
      "Epoch 13 batch 2100 train loss: 0.0035 test loss: 0.0075\n",
      "Epoch 13 batch 2200 train loss: 0.0092 test loss: 0.0077\n",
      "Epoch 13 batch 2300 train loss: 0.0073 test loss: 0.0067\n",
      "Epoch 13 batch 2400 train loss: 0.0076 test loss: 0.0071\n",
      "Epoch 13 batch 2500 train loss: 0.0063 test loss: 0.0070\n",
      "Epoch 13 batch 2600 train loss: 0.0092 test loss: 0.0070\n",
      "Epoch 13 batch 2700 train loss: 0.0054 test loss: 0.0075\n",
      "Epoch 13 batch 2800 train loss: 0.0066 test loss: 0.0073\n",
      "Epoch 13 batch 2900 train loss: 0.0062 test loss: 0.0071\n",
      "Epoch 13 batch 3000 train loss: 0.0042 test loss: 0.0071\n",
      "Epoch 14 batch 0 train loss: 0.0052 test loss: 0.0071\n",
      "Epoch 14 batch 100 train loss: 0.0062 test loss: 0.0073\n",
      "Epoch 14 batch 200 train loss: 0.0073 test loss: 0.0072\n",
      "Epoch 14 batch 300 train loss: 0.0082 test loss: 0.0072\n",
      "Epoch 14 batch 400 train loss: 0.0043 test loss: 0.0072\n",
      "Epoch 14 batch 500 train loss: 0.0067 test loss: 0.0073\n",
      "Epoch 14 batch 600 train loss: 0.0113 test loss: 0.0074\n",
      "Epoch 14 batch 700 train loss: 0.0051 test loss: 0.0072\n",
      "Epoch 14 batch 800 train loss: 0.0071 test loss: 0.0071\n",
      "Epoch 14 batch 900 train loss: 0.0114 test loss: 0.0071\n",
      "Epoch 14 batch 1000 train loss: 0.0082 test loss: 0.0072\n",
      "Epoch 14 batch 1100 train loss: 0.0043 test loss: 0.0072\n",
      "Epoch 14 batch 1200 train loss: 0.0096 test loss: 0.0070\n",
      "Epoch 14 batch 1300 train loss: 0.0063 test loss: 0.0074\n",
      "Epoch 14 batch 1400 train loss: 0.0097 test loss: 0.0070\n",
      "Epoch 14 batch 1500 train loss: 0.0048 test loss: 0.0073\n",
      "Epoch 14 batch 1600 train loss: 0.0061 test loss: 0.0071\n",
      "Epoch 14 batch 1700 train loss: 0.0075 test loss: 0.0074\n",
      "Epoch 14 batch 1800 train loss: 0.0061 test loss: 0.0068\n",
      "Epoch 14 batch 1900 train loss: 0.0045 test loss: 0.0071\n",
      "Epoch 14 batch 2000 train loss: 0.0072 test loss: 0.0073\n",
      "Epoch 14 batch 2100 train loss: 0.0049 test loss: 0.0071\n",
      "Epoch 14 batch 2200 train loss: 0.0049 test loss: 0.0070\n",
      "Epoch 14 batch 2300 train loss: 0.0070 test loss: 0.0071\n",
      "Epoch 14 batch 2400 train loss: 0.0131 test loss: 0.0071\n",
      "Epoch 14 batch 2500 train loss: 0.0070 test loss: 0.0069\n",
      "Epoch 14 batch 2600 train loss: 0.0058 test loss: 0.0073\n",
      "Epoch 14 batch 2700 train loss: 0.0069 test loss: 0.0073\n",
      "Epoch 14 batch 2800 train loss: 0.0091 test loss: 0.0074\n",
      "Epoch 14 batch 2900 train loss: 0.0090 test loss: 0.0072\n",
      "Epoch 14 batch 3000 train loss: 0.0100 test loss: 0.0068\n",
      "Epoch 15 batch 0 train loss: 0.0068 test loss: 0.0070\n",
      "Epoch 15 batch 100 train loss: 0.0062 test loss: 0.0072\n",
      "Epoch 15 batch 200 train loss: 0.0074 test loss: 0.0070\n",
      "Epoch 15 batch 300 train loss: 0.0034 test loss: 0.0071\n",
      "Epoch 15 batch 400 train loss: 0.0074 test loss: 0.0075\n",
      "Epoch 15 batch 500 train loss: 0.0046 test loss: 0.0075\n",
      "Epoch 15 batch 600 train loss: 0.0049 test loss: 0.0069\n",
      "Epoch 15 batch 700 train loss: 0.0061 test loss: 0.0072\n",
      "Epoch 15 batch 800 train loss: 0.0069 test loss: 0.0070\n",
      "Epoch 15 batch 900 train loss: 0.0060 test loss: 0.0072\n",
      "Epoch 15 batch 1000 train loss: 0.0065 test loss: 0.0070\n",
      "Epoch 15 batch 1100 train loss: 0.0042 test loss: 0.0074\n",
      "Epoch 15 batch 1200 train loss: 0.0066 test loss: 0.0070\n",
      "Epoch 15 batch 1300 train loss: 0.0071 test loss: 0.0074\n",
      "Epoch 15 batch 1400 train loss: 0.0063 test loss: 0.0070\n",
      "Epoch 15 batch 1500 train loss: 0.0044 test loss: 0.0072\n",
      "Epoch 15 batch 1600 train loss: 0.0043 test loss: 0.0075\n",
      "Epoch 15 batch 1700 train loss: 0.0066 test loss: 0.0068\n",
      "Epoch 15 batch 1800 train loss: 0.0081 test loss: 0.0070\n",
      "Epoch 15 batch 1900 train loss: 0.0069 test loss: 0.0075\n",
      "Epoch 15 batch 2000 train loss: 0.0073 test loss: 0.0072\n",
      "Epoch 15 batch 2100 train loss: 0.0095 test loss: 0.0074\n",
      "Epoch 15 batch 2200 train loss: 0.0054 test loss: 0.0073\n",
      "Epoch 15 batch 2300 train loss: 0.0061 test loss: 0.0069\n",
      "Epoch 15 batch 2400 train loss: 0.0041 test loss: 0.0073\n",
      "Epoch 15 batch 2500 train loss: 0.0068 test loss: 0.0070\n",
      "Epoch 15 batch 2600 train loss: 0.0090 test loss: 0.0073\n",
      "Epoch 15 batch 2700 train loss: 0.0081 test loss: 0.0075\n",
      "Epoch 15 batch 2800 train loss: 0.0050 test loss: 0.0072\n",
      "Epoch 15 batch 2900 train loss: 0.0047 test loss: 0.0073\n",
      "Epoch 15 batch 3000 train loss: 0.0094 test loss: 0.0073\n",
      "Epoch 16 batch 0 train loss: 0.0063 test loss: 0.0071\n",
      "Epoch 16 batch 100 train loss: 0.0060 test loss: 0.0074\n",
      "Epoch 16 batch 200 train loss: 0.0067 test loss: 0.0071\n",
      "Epoch 16 batch 300 train loss: 0.0050 test loss: 0.0072\n",
      "Epoch 16 batch 400 train loss: 0.0080 test loss: 0.0073\n",
      "Epoch 16 batch 500 train loss: 0.0038 test loss: 0.0071\n",
      "Epoch 16 batch 600 train loss: 0.0092 test loss: 0.0073\n",
      "Epoch 16 batch 700 train loss: 0.0052 test loss: 0.0075\n",
      "Epoch 16 batch 800 train loss: 0.0040 test loss: 0.0071\n",
      "Epoch 16 batch 900 train loss: 0.0032 test loss: 0.0075\n",
      "Epoch 16 batch 1000 train loss: 0.0079 test loss: 0.0071\n",
      "Epoch 16 batch 1100 train loss: 0.0046 test loss: 0.0071\n",
      "Epoch 16 batch 1200 train loss: 0.0117 test loss: 0.0070\n",
      "Epoch 16 batch 1300 train loss: 0.0106 test loss: 0.0074\n",
      "Epoch 16 batch 1400 train loss: 0.0027 test loss: 0.0070\n",
      "Epoch 16 batch 1500 train loss: 0.0061 test loss: 0.0072\n",
      "Epoch 16 batch 1600 train loss: 0.0045 test loss: 0.0071\n",
      "Epoch 16 batch 1700 train loss: 0.0118 test loss: 0.0071\n",
      "Epoch 16 batch 1800 train loss: 0.0061 test loss: 0.0070\n",
      "Epoch 16 batch 1900 train loss: 0.0049 test loss: 0.0072\n",
      "Epoch 16 batch 2000 train loss: 0.0087 test loss: 0.0073\n",
      "Epoch 16 batch 2100 train loss: 0.0052 test loss: 0.0069\n",
      "Epoch 16 batch 2200 train loss: 0.0043 test loss: 0.0072\n",
      "Epoch 16 batch 2300 train loss: 0.0060 test loss: 0.0071\n",
      "Epoch 16 batch 2400 train loss: 0.0072 test loss: 0.0074\n",
      "Epoch 16 batch 2500 train loss: 0.0054 test loss: 0.0071\n",
      "Epoch 16 batch 2600 train loss: 0.0068 test loss: 0.0074\n",
      "Epoch 16 batch 2700 train loss: 0.0057 test loss: 0.0075\n",
      "Epoch 16 batch 2800 train loss: 0.0032 test loss: 0.0074\n",
      "Epoch 16 batch 2900 train loss: 0.0099 test loss: 0.0068\n",
      "Epoch 16 batch 3000 train loss: 0.0066 test loss: 0.0070\n",
      "Epoch 17 batch 0 train loss: 0.0059 test loss: 0.0072\n",
      "Epoch 17 batch 100 train loss: 0.0059 test loss: 0.0073\n",
      "Epoch 17 batch 200 train loss: 0.0044 test loss: 0.0072\n",
      "Epoch 17 batch 300 train loss: 0.0052 test loss: 0.0072\n",
      "Epoch 17 batch 400 train loss: 0.0072 test loss: 0.0074\n",
      "Epoch 17 batch 500 train loss: 0.0055 test loss: 0.0073\n",
      "early stop.\n",
      "Checkpoint 33 restored!!\n",
      "Training for loss rate 0.40 start.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ffnn_3/dense_9/kernel:0', 'ffnn_3/dense_9/bias:0', 'ffnn_3/batch_normalization_3/gamma:0', 'ffnn_3/batch_normalization_3/beta:0', 'ffnn_3/dense_10/kernel:0', 'ffnn_3/dense_10/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ffnn_3/dense_9/kernel:0', 'ffnn_3/dense_9/bias:0', 'ffnn_3/batch_normalization_3/gamma:0', 'ffnn_3/batch_normalization_3/beta:0', 'ffnn_3/dense_10/kernel:0', 'ffnn_3/dense_10/bias:0'] when minimizing the loss.\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.40p/ckpt-1\n",
      "Epoch 0 batch 0 train loss: 0.2457 test loss: 0.2800\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.40p/ckpt-2\n",
      "Epoch 0 batch 100 train loss: 0.1096 test loss: 0.1481\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.40p/ckpt-3\n",
      "Epoch 0 batch 200 train loss: 0.0992 test loss: 0.1119\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.40p/ckpt-4\n",
      "Epoch 0 batch 300 train loss: 0.0645 test loss: 0.0896\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.40p/ckpt-5\n",
      "Epoch 0 batch 400 train loss: 0.0350 test loss: 0.0714\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.40p/ckpt-6\n",
      "Epoch 0 batch 500 train loss: 0.0316 test loss: 0.0574\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.40p/ckpt-7\n",
      "Epoch 0 batch 600 train loss: 0.0373 test loss: 0.0456\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.40p/ckpt-8\n",
      "Epoch 0 batch 700 train loss: 0.0269 test loss: 0.0369\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.40p/ckpt-9\n",
      "Epoch 0 batch 800 train loss: 0.0166 test loss: 0.0298\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.40p/ckpt-10\n",
      "Epoch 0 batch 900 train loss: 0.0192 test loss: 0.0244\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.40p/ckpt-11\n",
      "Epoch 0 batch 1000 train loss: 0.0113 test loss: 0.0204\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.40p/ckpt-12\n",
      "Epoch 0 batch 1100 train loss: 0.0152 test loss: 0.0171\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.40p/ckpt-13\n",
      "Epoch 0 batch 1200 train loss: 0.0094 test loss: 0.0152\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.40p/ckpt-14\n",
      "Epoch 0 batch 1300 train loss: 0.0103 test loss: 0.0133\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.40p/ckpt-15\n",
      "Epoch 0 batch 1400 train loss: 0.0100 test loss: 0.0122\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.40p/ckpt-16\n",
      "Epoch 0 batch 1500 train loss: 0.0056 test loss: 0.0112\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.40p/ckpt-17\n",
      "Epoch 0 batch 1600 train loss: 0.0091 test loss: 0.0107\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.40p/ckpt-18\n",
      "Epoch 0 batch 1700 train loss: 0.0093 test loss: 0.0103\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.40p/ckpt-19\n",
      "Epoch 0 batch 1800 train loss: 0.0073 test loss: 0.0097\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.40p/ckpt-20\n",
      "Epoch 0 batch 1900 train loss: 0.0096 test loss: 0.0096\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.40p/ckpt-21\n",
      "Epoch 0 batch 2000 train loss: 0.0101 test loss: 0.0092\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.40p/ckpt-22\n",
      "Epoch 0 batch 2100 train loss: 0.0071 test loss: 0.0091\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.40p/ckpt-23\n",
      "Epoch 0 batch 2200 train loss: 0.0084 test loss: 0.0090\n",
      "Epoch 0 batch 2300 train loss: 0.0072 test loss: 0.0091\n",
      "Epoch 0 batch 2400 train loss: 0.0063 test loss: 0.0090\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.40p/ckpt-24\n",
      "Epoch 0 batch 2500 train loss: 0.0076 test loss: 0.0088\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.40p/ckpt-25\n",
      "Epoch 0 batch 2600 train loss: 0.0069 test loss: 0.0086\n",
      "Epoch 0 batch 2700 train loss: 0.0094 test loss: 0.0088\n",
      "Epoch 0 batch 2800 train loss: 0.0070 test loss: 0.0089\n",
      "Epoch 0 batch 2900 train loss: 0.0068 test loss: 0.0087\n",
      "Epoch 0 batch 3000 train loss: 0.0062 test loss: 0.0086\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ffnn_3/dense_9/kernel:0', 'ffnn_3/dense_9/bias:0', 'ffnn_3/batch_normalization_3/gamma:0', 'ffnn_3/batch_normalization_3/beta:0', 'ffnn_3/dense_10/kernel:0', 'ffnn_3/dense_10/bias:0'] when minimizing the loss.\n",
      "Epoch 1 batch 0 train loss: 0.0077 test loss: 0.0087\n",
      "Epoch 1 batch 100 train loss: 0.0096 test loss: 0.0088\n",
      "Epoch 1 batch 200 train loss: 0.0109 test loss: 0.0089\n",
      "Epoch 1 batch 300 train loss: 0.0060 test loss: 0.0087\n",
      "Epoch 1 batch 400 train loss: 0.0102 test loss: 0.0088\n",
      "Epoch 1 batch 500 train loss: 0.0089 test loss: 0.0088\n",
      "Epoch 1 batch 600 train loss: 0.0081 test loss: 0.0088\n",
      "Epoch 1 batch 700 train loss: 0.0068 test loss: 0.0089\n",
      "Epoch 1 batch 800 train loss: 0.0062 test loss: 0.0086\n",
      "Epoch 1 batch 900 train loss: 0.0126 test loss: 0.0089\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.40p/ckpt-26\n",
      "Epoch 1 batch 1000 train loss: 0.0039 test loss: 0.0086\n",
      "Epoch 1 batch 1100 train loss: 0.0050 test loss: 0.0086\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.40p/ckpt-27\n",
      "Epoch 1 batch 1200 train loss: 0.0082 test loss: 0.0086\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.40p/ckpt-28\n",
      "Epoch 1 batch 1300 train loss: 0.0090 test loss: 0.0086\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.40p/ckpt-29\n",
      "Epoch 1 batch 1400 train loss: 0.0060 test loss: 0.0084\n",
      "Epoch 1 batch 1500 train loss: 0.0051 test loss: 0.0086\n",
      "Epoch 1 batch 1600 train loss: 0.0077 test loss: 0.0092\n",
      "Epoch 1 batch 1700 train loss: 0.0086 test loss: 0.0086\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.40p/ckpt-30\n",
      "Epoch 1 batch 1800 train loss: 0.0055 test loss: 0.0084\n",
      "Epoch 1 batch 1900 train loss: 0.0063 test loss: 0.0087\n",
      "Epoch 1 batch 2000 train loss: 0.0118 test loss: 0.0089\n",
      "Epoch 1 batch 2100 train loss: 0.0050 test loss: 0.0084\n",
      "Epoch 1 batch 2200 train loss: 0.0080 test loss: 0.0085\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.40p/ckpt-31\n",
      "Epoch 1 batch 2300 train loss: 0.0076 test loss: 0.0084\n",
      "Epoch 1 batch 2400 train loss: 0.0059 test loss: 0.0088\n",
      "Epoch 1 batch 2500 train loss: 0.0080 test loss: 0.0086\n",
      "Epoch 1 batch 2600 train loss: 0.0133 test loss: 0.0086\n",
      "Epoch 1 batch 2700 train loss: 0.0079 test loss: 0.0089\n",
      "Epoch 1 batch 2800 train loss: 0.0100 test loss: 0.0090\n",
      "Epoch 1 batch 2900 train loss: 0.0094 test loss: 0.0087\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.40p/ckpt-32\n",
      "Epoch 1 batch 3000 train loss: 0.0063 test loss: 0.0082\n",
      "Epoch 2 batch 0 train loss: 0.0073 test loss: 0.0085\n",
      "Epoch 2 batch 100 train loss: 0.0102 test loss: 0.0086\n",
      "Epoch 2 batch 200 train loss: 0.0057 test loss: 0.0087\n",
      "Epoch 2 batch 300 train loss: 0.0066 test loss: 0.0086\n",
      "Epoch 2 batch 400 train loss: 0.0056 test loss: 0.0087\n",
      "Epoch 2 batch 500 train loss: 0.0098 test loss: 0.0086\n",
      "Epoch 2 batch 600 train loss: 0.0074 test loss: 0.0088\n",
      "Epoch 2 batch 700 train loss: 0.0088 test loss: 0.0087\n",
      "Epoch 2 batch 800 train loss: 0.0055 test loss: 0.0089\n",
      "Epoch 2 batch 900 train loss: 0.0069 test loss: 0.0084\n",
      "Epoch 2 batch 1000 train loss: 0.0123 test loss: 0.0087\n",
      "Epoch 2 batch 1100 train loss: 0.0102 test loss: 0.0086\n",
      "Epoch 2 batch 1200 train loss: 0.0090 test loss: 0.0085\n",
      "Epoch 2 batch 1300 train loss: 0.0092 test loss: 0.0088\n",
      "Epoch 2 batch 1400 train loss: 0.0057 test loss: 0.0086\n",
      "Epoch 2 batch 1500 train loss: 0.0088 test loss: 0.0085\n",
      "Epoch 2 batch 1600 train loss: 0.0064 test loss: 0.0086\n",
      "Epoch 2 batch 1700 train loss: 0.0082 test loss: 0.0088\n",
      "Epoch 2 batch 1800 train loss: 0.0077 test loss: 0.0084\n",
      "Epoch 2 batch 1900 train loss: 0.0086 test loss: 0.0084\n",
      "Epoch 2 batch 2000 train loss: 0.0104 test loss: 0.0086\n",
      "Epoch 2 batch 2100 train loss: 0.0078 test loss: 0.0087\n",
      "Epoch 2 batch 2200 train loss: 0.0056 test loss: 0.0088\n",
      "Epoch 2 batch 2300 train loss: 0.0098 test loss: 0.0086\n",
      "Epoch 2 batch 2400 train loss: 0.0071 test loss: 0.0086\n",
      "Epoch 2 batch 2500 train loss: 0.0069 test loss: 0.0086\n",
      "Epoch 2 batch 2600 train loss: 0.0107 test loss: 0.0087\n",
      "Epoch 2 batch 2700 train loss: 0.0148 test loss: 0.0087\n",
      "Epoch 2 batch 2800 train loss: 0.0104 test loss: 0.0089\n",
      "Epoch 2 batch 2900 train loss: 0.0109 test loss: 0.0083\n",
      "Epoch 2 batch 3000 train loss: 0.0087 test loss: 0.0084\n",
      "Epoch 3 batch 0 train loss: 0.0065 test loss: 0.0086\n",
      "Epoch 3 batch 100 train loss: 0.0172 test loss: 0.0087\n",
      "Epoch 3 batch 200 train loss: 0.0111 test loss: 0.0088\n",
      "Epoch 3 batch 300 train loss: 0.0060 test loss: 0.0088\n",
      "Epoch 3 batch 400 train loss: 0.0068 test loss: 0.0087\n",
      "Epoch 3 batch 500 train loss: 0.0095 test loss: 0.0083\n",
      "Epoch 3 batch 600 train loss: 0.0082 test loss: 0.0088\n",
      "Epoch 3 batch 700 train loss: 0.0103 test loss: 0.0086\n",
      "Epoch 3 batch 800 train loss: 0.0101 test loss: 0.0089\n",
      "Epoch 3 batch 900 train loss: 0.0171 test loss: 0.0086\n",
      "Epoch 3 batch 1000 train loss: 0.0138 test loss: 0.0086\n",
      "Epoch 3 batch 1100 train loss: 0.0074 test loss: 0.0089\n",
      "Epoch 3 batch 1200 train loss: 0.0095 test loss: 0.0086\n",
      "Epoch 3 batch 1300 train loss: 0.0135 test loss: 0.0087\n",
      "Epoch 3 batch 1400 train loss: 0.0107 test loss: 0.0084\n",
      "Epoch 3 batch 1500 train loss: 0.0085 test loss: 0.0083\n",
      "Epoch 3 batch 1600 train loss: 0.0070 test loss: 0.0088\n",
      "Epoch 3 batch 1700 train loss: 0.0106 test loss: 0.0090\n",
      "Epoch 3 batch 1800 train loss: 0.0132 test loss: 0.0085\n",
      "Epoch 3 batch 1900 train loss: 0.0105 test loss: 0.0085\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.40p/ckpt-33\n",
      "Epoch 3 batch 2000 train loss: 0.0061 test loss: 0.0082\n",
      "Epoch 3 batch 2100 train loss: 0.0141 test loss: 0.0087\n",
      "Epoch 3 batch 2200 train loss: 0.0061 test loss: 0.0092\n",
      "Epoch 3 batch 2300 train loss: 0.0081 test loss: 0.0087\n",
      "Epoch 3 batch 2400 train loss: 0.0040 test loss: 0.0086\n",
      "Epoch 3 batch 2500 train loss: 0.0124 test loss: 0.0088\n",
      "Epoch 3 batch 2600 train loss: 0.0066 test loss: 0.0087\n",
      "Epoch 3 batch 2700 train loss: 0.0107 test loss: 0.0087\n",
      "Epoch 3 batch 2800 train loss: 0.0092 test loss: 0.0092\n",
      "Epoch 3 batch 2900 train loss: 0.0079 test loss: 0.0090\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.40p/ckpt-34\n",
      "Epoch 3 batch 3000 train loss: 0.0097 test loss: 0.0081\n",
      "Epoch 4 batch 0 train loss: 0.0084 test loss: 0.0082\n",
      "Epoch 4 batch 100 train loss: 0.0078 test loss: 0.0090\n",
      "Epoch 4 batch 200 train loss: 0.0075 test loss: 0.0085\n",
      "Epoch 4 batch 300 train loss: 0.0049 test loss: 0.0088\n",
      "Epoch 4 batch 400 train loss: 0.0064 test loss: 0.0087\n",
      "Epoch 4 batch 500 train loss: 0.0070 test loss: 0.0087\n",
      "Epoch 4 batch 600 train loss: 0.0097 test loss: 0.0089\n",
      "Epoch 4 batch 700 train loss: 0.0067 test loss: 0.0083\n",
      "Epoch 4 batch 800 train loss: 0.0081 test loss: 0.0086\n",
      "Epoch 4 batch 900 train loss: 0.0107 test loss: 0.0086\n",
      "Epoch 4 batch 1000 train loss: 0.0072 test loss: 0.0088\n",
      "Epoch 4 batch 1100 train loss: 0.0100 test loss: 0.0087\n",
      "Epoch 4 batch 1200 train loss: 0.0069 test loss: 0.0088\n",
      "Epoch 4 batch 1300 train loss: 0.0095 test loss: 0.0089\n",
      "Epoch 4 batch 1400 train loss: 0.0050 test loss: 0.0085\n",
      "Epoch 4 batch 1500 train loss: 0.0075 test loss: 0.0086\n",
      "Epoch 4 batch 1600 train loss: 0.0118 test loss: 0.0084\n",
      "Epoch 4 batch 1700 train loss: 0.0054 test loss: 0.0086\n",
      "Epoch 4 batch 1800 train loss: 0.0082 test loss: 0.0085\n",
      "Epoch 4 batch 1900 train loss: 0.0084 test loss: 0.0086\n",
      "Epoch 4 batch 2000 train loss: 0.0078 test loss: 0.0083\n",
      "Epoch 4 batch 2100 train loss: 0.0058 test loss: 0.0085\n",
      "Epoch 4 batch 2200 train loss: 0.0071 test loss: 0.0086\n",
      "Epoch 4 batch 2300 train loss: 0.0098 test loss: 0.0087\n",
      "Epoch 4 batch 2400 train loss: 0.0077 test loss: 0.0087\n",
      "Epoch 4 batch 2500 train loss: 0.0100 test loss: 0.0087\n",
      "Epoch 4 batch 2600 train loss: 0.0097 test loss: 0.0084\n",
      "Epoch 4 batch 2700 train loss: 0.0082 test loss: 0.0087\n",
      "Epoch 4 batch 2800 train loss: 0.0072 test loss: 0.0089\n",
      "Epoch 4 batch 2900 train loss: 0.0119 test loss: 0.0085\n",
      "Epoch 4 batch 3000 train loss: 0.0071 test loss: 0.0084\n",
      "Epoch 5 batch 0 train loss: 0.0041 test loss: 0.0084\n",
      "Epoch 5 batch 100 train loss: 0.0079 test loss: 0.0083\n",
      "Epoch 5 batch 200 train loss: 0.0074 test loss: 0.0087\n",
      "Epoch 5 batch 300 train loss: 0.0071 test loss: 0.0084\n",
      "Epoch 5 batch 400 train loss: 0.0073 test loss: 0.0088\n",
      "Epoch 5 batch 500 train loss: 0.0128 test loss: 0.0086\n",
      "Epoch 5 batch 600 train loss: 0.0071 test loss: 0.0086\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.40p/ckpt-35\n",
      "Epoch 5 batch 700 train loss: 0.0104 test loss: 0.0081\n",
      "Epoch 5 batch 800 train loss: 0.0169 test loss: 0.0088\n",
      "Epoch 5 batch 900 train loss: 0.0093 test loss: 0.0087\n",
      "Epoch 5 batch 1000 train loss: 0.0078 test loss: 0.0086\n",
      "Epoch 5 batch 1100 train loss: 0.0123 test loss: 0.0086\n",
      "Epoch 5 batch 1200 train loss: 0.0044 test loss: 0.0088\n",
      "Epoch 5 batch 1300 train loss: 0.0067 test loss: 0.0084\n",
      "Epoch 5 batch 1400 train loss: 0.0070 test loss: 0.0089\n",
      "Epoch 5 batch 1500 train loss: 0.0113 test loss: 0.0089\n",
      "Epoch 5 batch 1600 train loss: 0.0072 test loss: 0.0085\n",
      "Epoch 5 batch 1700 train loss: 0.0058 test loss: 0.0089\n",
      "Epoch 5 batch 1800 train loss: 0.0092 test loss: 0.0087\n",
      "Epoch 5 batch 1900 train loss: 0.0104 test loss: 0.0086\n",
      "Epoch 5 batch 2000 train loss: 0.0090 test loss: 0.0082\n",
      "Epoch 5 batch 2100 train loss: 0.0086 test loss: 0.0087\n",
      "Epoch 5 batch 2200 train loss: 0.0102 test loss: 0.0083\n",
      "Epoch 5 batch 2300 train loss: 0.0122 test loss: 0.0085\n",
      "Epoch 5 batch 2400 train loss: 0.0097 test loss: 0.0086\n",
      "Epoch 5 batch 2500 train loss: 0.0047 test loss: 0.0085\n",
      "Epoch 5 batch 2600 train loss: 0.0068 test loss: 0.0085\n",
      "Epoch 5 batch 2700 train loss: 0.0107 test loss: 0.0088\n",
      "Epoch 5 batch 2800 train loss: 0.0109 test loss: 0.0091\n",
      "Epoch 5 batch 2900 train loss: 0.0082 test loss: 0.0086\n",
      "Epoch 5 batch 3000 train loss: 0.0082 test loss: 0.0087\n",
      "Epoch 6 batch 0 train loss: 0.0092 test loss: 0.0081\n",
      "Epoch 6 batch 100 train loss: 0.0054 test loss: 0.0088\n",
      "Epoch 6 batch 200 train loss: 0.0097 test loss: 0.0086\n",
      "Epoch 6 batch 300 train loss: 0.0072 test loss: 0.0085\n",
      "Epoch 6 batch 400 train loss: 0.0063 test loss: 0.0085\n",
      "Epoch 6 batch 500 train loss: 0.0061 test loss: 0.0082\n",
      "Epoch 6 batch 600 train loss: 0.0093 test loss: 0.0086\n",
      "Epoch 6 batch 700 train loss: 0.0098 test loss: 0.0086\n",
      "Epoch 6 batch 800 train loss: 0.0074 test loss: 0.0087\n",
      "Epoch 6 batch 900 train loss: 0.0114 test loss: 0.0089\n",
      "Epoch 6 batch 1000 train loss: 0.0055 test loss: 0.0085\n",
      "Epoch 6 batch 1100 train loss: 0.0070 test loss: 0.0087\n",
      "Epoch 6 batch 1200 train loss: 0.0125 test loss: 0.0084\n",
      "Epoch 6 batch 1300 train loss: 0.0061 test loss: 0.0085\n",
      "Epoch 6 batch 1400 train loss: 0.0070 test loss: 0.0088\n",
      "Epoch 6 batch 1500 train loss: 0.0084 test loss: 0.0089\n",
      "Epoch 6 batch 1600 train loss: 0.0115 test loss: 0.0089\n",
      "Epoch 6 batch 1700 train loss: 0.0072 test loss: 0.0086\n",
      "Epoch 6 batch 1800 train loss: 0.0076 test loss: 0.0086\n",
      "Epoch 6 batch 1900 train loss: 0.0045 test loss: 0.0084\n",
      "Epoch 6 batch 2000 train loss: 0.0047 test loss: 0.0086\n",
      "Epoch 6 batch 2100 train loss: 0.0075 test loss: 0.0084\n",
      "Epoch 6 batch 2200 train loss: 0.0060 test loss: 0.0084\n",
      "Epoch 6 batch 2300 train loss: 0.0076 test loss: 0.0086\n",
      "Epoch 6 batch 2400 train loss: 0.0054 test loss: 0.0089\n",
      "Epoch 6 batch 2500 train loss: 0.0074 test loss: 0.0086\n",
      "Epoch 6 batch 2600 train loss: 0.0074 test loss: 0.0087\n",
      "Epoch 6 batch 2700 train loss: 0.0136 test loss: 0.0090\n",
      "Epoch 6 batch 2800 train loss: 0.0076 test loss: 0.0088\n",
      "Epoch 6 batch 2900 train loss: 0.0065 test loss: 0.0085\n",
      "Epoch 6 batch 3000 train loss: 0.0104 test loss: 0.0086\n",
      "Epoch 7 batch 0 train loss: 0.0047 test loss: 0.0085\n",
      "Epoch 7 batch 100 train loss: 0.0055 test loss: 0.0086\n",
      "Epoch 7 batch 200 train loss: 0.0049 test loss: 0.0087\n",
      "Epoch 7 batch 300 train loss: 0.0094 test loss: 0.0084\n",
      "Epoch 7 batch 400 train loss: 0.0073 test loss: 0.0086\n",
      "Epoch 7 batch 500 train loss: 0.0093 test loss: 0.0086\n",
      "Epoch 7 batch 600 train loss: 0.0099 test loss: 0.0087\n",
      "Epoch 7 batch 700 train loss: 0.0087 test loss: 0.0085\n",
      "Epoch 7 batch 800 train loss: 0.0070 test loss: 0.0086\n",
      "Epoch 7 batch 900 train loss: 0.0147 test loss: 0.0087\n",
      "Epoch 7 batch 1000 train loss: 0.0067 test loss: 0.0087\n",
      "Epoch 7 batch 1100 train loss: 0.0100 test loss: 0.0084\n",
      "Epoch 7 batch 1200 train loss: 0.0124 test loss: 0.0084\n",
      "Epoch 7 batch 1300 train loss: 0.0096 test loss: 0.0088\n",
      "Epoch 7 batch 1400 train loss: 0.0066 test loss: 0.0083\n",
      "Epoch 7 batch 1500 train loss: 0.0075 test loss: 0.0087\n",
      "Epoch 7 batch 1600 train loss: 0.0074 test loss: 0.0089\n",
      "Epoch 7 batch 1700 train loss: 0.0069 test loss: 0.0088\n",
      "Epoch 7 batch 1800 train loss: 0.0064 test loss: 0.0083\n",
      "Epoch 7 batch 1900 train loss: 0.0046 test loss: 0.0086\n",
      "Epoch 7 batch 2000 train loss: 0.0151 test loss: 0.0084\n",
      "Epoch 7 batch 2100 train loss: 0.0039 test loss: 0.0089\n",
      "Epoch 7 batch 2200 train loss: 0.0105 test loss: 0.0086\n",
      "Epoch 7 batch 2300 train loss: 0.0067 test loss: 0.0083\n",
      "Epoch 7 batch 2400 train loss: 0.0074 test loss: 0.0087\n",
      "Epoch 7 batch 2500 train loss: 0.0052 test loss: 0.0086\n",
      "Epoch 7 batch 2600 train loss: 0.0057 test loss: 0.0088\n",
      "Epoch 7 batch 2700 train loss: 0.0111 test loss: 0.0089\n",
      "Epoch 7 batch 2800 train loss: 0.0065 test loss: 0.0089\n",
      "Epoch 7 batch 2900 train loss: 0.0093 test loss: 0.0086\n",
      "Epoch 7 batch 3000 train loss: 0.0083 test loss: 0.0088\n",
      "Epoch 8 batch 0 train loss: 0.0060 test loss: 0.0084\n",
      "Epoch 8 batch 100 train loss: 0.0091 test loss: 0.0087\n",
      "Epoch 8 batch 200 train loss: 0.0059 test loss: 0.0088\n",
      "Epoch 8 batch 300 train loss: 0.0090 test loss: 0.0085\n",
      "Epoch 8 batch 400 train loss: 0.0065 test loss: 0.0084\n",
      "Epoch 8 batch 500 train loss: 0.0065 test loss: 0.0088\n",
      "Epoch 8 batch 600 train loss: 0.0083 test loss: 0.0088\n",
      "Epoch 8 batch 700 train loss: 0.0071 test loss: 0.0084\n",
      "Epoch 8 batch 800 train loss: 0.0085 test loss: 0.0086\n",
      "Epoch 8 batch 900 train loss: 0.0083 test loss: 0.0089\n",
      "Epoch 8 batch 1000 train loss: 0.0084 test loss: 0.0084\n",
      "Epoch 8 batch 1100 train loss: 0.0067 test loss: 0.0086\n",
      "Epoch 8 batch 1200 train loss: 0.0055 test loss: 0.0085\n",
      "Epoch 8 batch 1300 train loss: 0.0096 test loss: 0.0085\n",
      "Epoch 8 batch 1400 train loss: 0.0077 test loss: 0.0085\n",
      "Epoch 8 batch 1500 train loss: 0.0036 test loss: 0.0089\n",
      "Epoch 8 batch 1600 train loss: 0.0049 test loss: 0.0090\n",
      "Epoch 8 batch 1700 train loss: 0.0119 test loss: 0.0087\n",
      "Epoch 8 batch 1800 train loss: 0.0101 test loss: 0.0083\n",
      "Epoch 8 batch 1900 train loss: 0.0111 test loss: 0.0086\n",
      "Epoch 8 batch 2000 train loss: 0.0056 test loss: 0.0084\n",
      "Epoch 8 batch 2100 train loss: 0.0086 test loss: 0.0084\n",
      "Epoch 8 batch 2200 train loss: 0.0075 test loss: 0.0086\n",
      "Epoch 8 batch 2300 train loss: 0.0044 test loss: 0.0082\n",
      "Epoch 8 batch 2400 train loss: 0.0039 test loss: 0.0086\n",
      "Epoch 8 batch 2500 train loss: 0.0081 test loss: 0.0087\n",
      "Epoch 8 batch 2600 train loss: 0.0056 test loss: 0.0089\n",
      "Epoch 8 batch 2700 train loss: 0.0085 test loss: 0.0088\n",
      "Epoch 8 batch 2800 train loss: 0.0051 test loss: 0.0088\n",
      "Epoch 8 batch 2900 train loss: 0.0116 test loss: 0.0087\n",
      "Epoch 8 batch 3000 train loss: 0.0047 test loss: 0.0086\n",
      "Epoch 9 batch 0 train loss: 0.0061 test loss: 0.0083\n",
      "Epoch 9 batch 100 train loss: 0.0078 test loss: 0.0088\n",
      "Epoch 9 batch 200 train loss: 0.0068 test loss: 0.0088\n",
      "Epoch 9 batch 300 train loss: 0.0088 test loss: 0.0084\n",
      "Epoch 9 batch 400 train loss: 0.0062 test loss: 0.0087\n",
      "Epoch 9 batch 500 train loss: 0.0100 test loss: 0.0087\n",
      "Epoch 9 batch 600 train loss: 0.0095 test loss: 0.0082\n",
      "Epoch 9 batch 700 train loss: 0.0068 test loss: 0.0089\n",
      "Epoch 9 batch 800 train loss: 0.0052 test loss: 0.0085\n",
      "Epoch 9 batch 900 train loss: 0.0045 test loss: 0.0084\n",
      "Epoch 9 batch 1000 train loss: 0.0112 test loss: 0.0090\n",
      "Epoch 9 batch 1100 train loss: 0.0092 test loss: 0.0083\n",
      "Epoch 9 batch 1200 train loss: 0.0067 test loss: 0.0086\n",
      "Epoch 9 batch 1300 train loss: 0.0078 test loss: 0.0086\n",
      "Epoch 9 batch 1400 train loss: 0.0074 test loss: 0.0085\n",
      "Epoch 9 batch 1500 train loss: 0.0040 test loss: 0.0090\n",
      "Epoch 9 batch 1600 train loss: 0.0060 test loss: 0.0085\n",
      "Epoch 9 batch 1700 train loss: 0.0067 test loss: 0.0085\n",
      "Epoch 9 batch 1800 train loss: 0.0126 test loss: 0.0085\n",
      "Epoch 9 batch 1900 train loss: 0.0084 test loss: 0.0085\n",
      "Epoch 9 batch 2000 train loss: 0.0061 test loss: 0.0086\n",
      "Epoch 9 batch 2100 train loss: 0.0075 test loss: 0.0082\n",
      "Epoch 9 batch 2200 train loss: 0.0128 test loss: 0.0090\n",
      "Epoch 9 batch 2300 train loss: 0.0086 test loss: 0.0081\n",
      "Epoch 9 batch 2400 train loss: 0.0053 test loss: 0.0087\n",
      "Epoch 9 batch 2500 train loss: 0.0091 test loss: 0.0085\n",
      "Epoch 9 batch 2600 train loss: 0.0076 test loss: 0.0085\n",
      "Epoch 9 batch 2700 train loss: 0.0054 test loss: 0.0087\n",
      "Epoch 9 batch 2800 train loss: 0.0070 test loss: 0.0089\n",
      "Epoch 9 batch 2900 train loss: 0.0128 test loss: 0.0088\n",
      "Epoch 9 batch 3000 train loss: 0.0066 test loss: 0.0083\n",
      "Epoch 10 batch 0 train loss: 0.0083 test loss: 0.0084\n",
      "Epoch 10 batch 100 train loss: 0.0088 test loss: 0.0086\n",
      "Epoch 10 batch 200 train loss: 0.0094 test loss: 0.0084\n",
      "Epoch 10 batch 300 train loss: 0.0047 test loss: 0.0085\n",
      "Epoch 10 batch 400 train loss: 0.0042 test loss: 0.0087\n",
      "Epoch 10 batch 500 train loss: 0.0126 test loss: 0.0090\n",
      "Epoch 10 batch 600 train loss: 0.0085 test loss: 0.0083\n",
      "Epoch 10 batch 700 train loss: 0.0094 test loss: 0.0086\n",
      "Epoch 10 batch 800 train loss: 0.0110 test loss: 0.0089\n",
      "Epoch 10 batch 900 train loss: 0.0072 test loss: 0.0087\n",
      "Epoch 10 batch 1000 train loss: 0.0083 test loss: 0.0087\n",
      "Epoch 10 batch 1100 train loss: 0.0066 test loss: 0.0083\n",
      "Epoch 10 batch 1200 train loss: 0.0058 test loss: 0.0086\n",
      "Epoch 10 batch 1300 train loss: 0.0101 test loss: 0.0085\n",
      "Epoch 10 batch 1400 train loss: 0.0041 test loss: 0.0083\n",
      "Epoch 10 batch 1500 train loss: 0.0120 test loss: 0.0088\n",
      "Epoch 10 batch 1600 train loss: 0.0076 test loss: 0.0090\n",
      "Epoch 10 batch 1700 train loss: 0.0086 test loss: 0.0086\n",
      "Epoch 10 batch 1800 train loss: 0.0086 test loss: 0.0086\n",
      "Epoch 10 batch 1900 train loss: 0.0118 test loss: 0.0084\n",
      "Epoch 10 batch 2000 train loss: 0.0053 test loss: 0.0085\n",
      "Epoch 10 batch 2100 train loss: 0.0068 test loss: 0.0085\n",
      "Epoch 10 batch 2200 train loss: 0.0084 test loss: 0.0087\n",
      "Epoch 10 batch 2300 train loss: 0.0104 test loss: 0.0083\n",
      "Epoch 10 batch 2400 train loss: 0.0086 test loss: 0.0086\n",
      "Epoch 10 batch 2500 train loss: 0.0100 test loss: 0.0084\n",
      "Epoch 10 batch 2600 train loss: 0.0047 test loss: 0.0087\n",
      "Epoch 10 batch 2700 train loss: 0.0050 test loss: 0.0089\n",
      "Epoch 10 batch 2800 train loss: 0.0068 test loss: 0.0088\n",
      "Epoch 10 batch 2900 train loss: 0.0155 test loss: 0.0085\n",
      "Epoch 10 batch 3000 train loss: 0.0099 test loss: 0.0085\n",
      "Epoch 11 batch 0 train loss: 0.0084 test loss: 0.0083\n",
      "Epoch 11 batch 100 train loss: 0.0114 test loss: 0.0086\n",
      "Epoch 11 batch 200 train loss: 0.0098 test loss: 0.0083\n",
      "Epoch 11 batch 300 train loss: 0.0073 test loss: 0.0088\n",
      "Epoch 11 batch 400 train loss: 0.0124 test loss: 0.0087\n",
      "Epoch 11 batch 500 train loss: 0.0090 test loss: 0.0087\n",
      "Epoch 11 batch 600 train loss: 0.0064 test loss: 0.0089\n",
      "Epoch 11 batch 700 train loss: 0.0069 test loss: 0.0085\n",
      "Epoch 11 batch 800 train loss: 0.0084 test loss: 0.0085\n",
      "Epoch 11 batch 900 train loss: 0.0046 test loss: 0.0084\n",
      "Epoch 11 batch 1000 train loss: 0.0078 test loss: 0.0085\n",
      "Epoch 11 batch 1100 train loss: 0.0058 test loss: 0.0087\n",
      "Epoch 11 batch 1200 train loss: 0.0038 test loss: 0.0086\n",
      "Epoch 11 batch 1300 train loss: 0.0101 test loss: 0.0087\n",
      "Epoch 11 batch 1400 train loss: 0.0081 test loss: 0.0081\n",
      "Epoch 11 batch 1500 train loss: 0.0069 test loss: 0.0087\n",
      "Epoch 11 batch 1600 train loss: 0.0044 test loss: 0.0090\n",
      "Epoch 11 batch 1700 train loss: 0.0051 test loss: 0.0090\n",
      "Epoch 11 batch 1800 train loss: 0.0075 test loss: 0.0085\n",
      "Epoch 11 batch 1900 train loss: 0.0066 test loss: 0.0084\n",
      "Epoch 11 batch 2000 train loss: 0.0045 test loss: 0.0086\n",
      "Epoch 11 batch 2100 train loss: 0.0085 test loss: 0.0086\n",
      "Epoch 11 batch 2200 train loss: 0.0065 test loss: 0.0086\n",
      "Epoch 11 batch 2300 train loss: 0.0123 test loss: 0.0085\n",
      "Epoch 11 batch 2400 train loss: 0.0087 test loss: 0.0088\n",
      "Epoch 11 batch 2500 train loss: 0.0079 test loss: 0.0085\n",
      "Epoch 11 batch 2600 train loss: 0.0076 test loss: 0.0085\n",
      "Epoch 11 batch 2700 train loss: 0.0054 test loss: 0.0087\n",
      "Epoch 11 batch 2800 train loss: 0.0049 test loss: 0.0090\n",
      "Epoch 11 batch 2900 train loss: 0.0053 test loss: 0.0083\n",
      "Epoch 11 batch 3000 train loss: 0.0067 test loss: 0.0084\n",
      "Epoch 12 batch 0 train loss: 0.0074 test loss: 0.0084\n",
      "Epoch 12 batch 100 train loss: 0.0070 test loss: 0.0089\n",
      "Epoch 12 batch 200 train loss: 0.0093 test loss: 0.0082\n",
      "Epoch 12 batch 300 train loss: 0.0108 test loss: 0.0086\n",
      "Epoch 12 batch 400 train loss: 0.0092 test loss: 0.0085\n",
      "Epoch 12 batch 500 train loss: 0.0095 test loss: 0.0087\n",
      "Epoch 12 batch 600 train loss: 0.0068 test loss: 0.0088\n",
      "Epoch 12 batch 700 train loss: 0.0066 test loss: 0.0085\n",
      "Epoch 12 batch 800 train loss: 0.0070 test loss: 0.0084\n",
      "Epoch 12 batch 900 train loss: 0.0100 test loss: 0.0085\n",
      "Epoch 12 batch 1000 train loss: 0.0095 test loss: 0.0088\n",
      "Epoch 12 batch 1100 train loss: 0.0099 test loss: 0.0087\n",
      "Epoch 12 batch 1200 train loss: 0.0079 test loss: 0.0084\n",
      "Epoch 12 batch 1300 train loss: 0.0069 test loss: 0.0090\n",
      "Epoch 12 batch 1400 train loss: 0.0067 test loss: 0.0084\n",
      "Epoch 12 batch 1500 train loss: 0.0093 test loss: 0.0088\n",
      "Epoch 12 batch 1600 train loss: 0.0080 test loss: 0.0089\n",
      "Epoch 12 batch 1700 train loss: 0.0130 test loss: 0.0087\n",
      "Epoch 12 batch 1800 train loss: 0.0079 test loss: 0.0088\n",
      "Epoch 12 batch 1900 train loss: 0.0061 test loss: 0.0088\n",
      "Epoch 12 batch 2000 train loss: 0.0079 test loss: 0.0084\n",
      "Epoch 12 batch 2100 train loss: 0.0080 test loss: 0.0085\n",
      "Epoch 12 batch 2200 train loss: 0.0083 test loss: 0.0087\n",
      "Epoch 12 batch 2300 train loss: 0.0064 test loss: 0.0085\n",
      "Epoch 12 batch 2400 train loss: 0.0052 test loss: 0.0085\n",
      "Epoch 12 batch 2500 train loss: 0.0089 test loss: 0.0085\n",
      "Epoch 12 batch 2600 train loss: 0.0080 test loss: 0.0086\n",
      "Epoch 12 batch 2700 train loss: 0.0070 test loss: 0.0090\n",
      "Epoch 12 batch 2800 train loss: 0.0078 test loss: 0.0091\n",
      "Epoch 12 batch 2900 train loss: 0.0105 test loss: 0.0085\n",
      "Epoch 12 batch 3000 train loss: 0.0108 test loss: 0.0083\n",
      "Epoch 13 batch 0 train loss: 0.0032 test loss: 0.0084\n",
      "Epoch 13 batch 100 train loss: 0.0037 test loss: 0.0087\n",
      "Epoch 13 batch 200 train loss: 0.0081 test loss: 0.0083\n",
      "Epoch 13 batch 300 train loss: 0.0102 test loss: 0.0091\n",
      "Epoch 13 batch 400 train loss: 0.0100 test loss: 0.0088\n",
      "Epoch 13 batch 500 train loss: 0.0071 test loss: 0.0090\n",
      "Epoch 13 batch 600 train loss: 0.0112 test loss: 0.0085\n",
      "Epoch 13 batch 700 train loss: 0.0067 test loss: 0.0084\n",
      "Epoch 13 batch 800 train loss: 0.0082 test loss: 0.0084\n",
      "Epoch 13 batch 900 train loss: 0.0094 test loss: 0.0083\n",
      "Epoch 13 batch 1000 train loss: 0.0063 test loss: 0.0089\n",
      "Epoch 13 batch 1100 train loss: 0.0124 test loss: 0.0084\n",
      "Epoch 13 batch 1200 train loss: 0.0078 test loss: 0.0086\n",
      "Epoch 13 batch 1300 train loss: 0.0074 test loss: 0.0087\n",
      "Epoch 13 batch 1400 train loss: 0.0104 test loss: 0.0082\n",
      "Epoch 13 batch 1500 train loss: 0.0102 test loss: 0.0086\n",
      "Epoch 13 batch 1600 train loss: 0.0082 test loss: 0.0088\n",
      "Epoch 13 batch 1700 train loss: 0.0055 test loss: 0.0087\n",
      "Epoch 13 batch 1800 train loss: 0.0077 test loss: 0.0083\n",
      "Epoch 13 batch 1900 train loss: 0.0104 test loss: 0.0086\n",
      "Epoch 13 batch 2000 train loss: 0.0081 test loss: 0.0085\n",
      "Epoch 13 batch 2100 train loss: 0.0060 test loss: 0.0083\n",
      "Epoch 13 batch 2200 train loss: 0.0100 test loss: 0.0086\n",
      "Epoch 13 batch 2300 train loss: 0.0141 test loss: 0.0089\n",
      "Epoch 13 batch 2400 train loss: 0.0087 test loss: 0.0085\n",
      "Epoch 13 batch 2500 train loss: 0.0061 test loss: 0.0087\n",
      "Epoch 13 batch 2600 train loss: 0.0083 test loss: 0.0086\n",
      "Epoch 13 batch 2700 train loss: 0.0063 test loss: 0.0088\n",
      "Epoch 13 batch 2800 train loss: 0.0105 test loss: 0.0087\n",
      "Epoch 13 batch 2900 train loss: 0.0108 test loss: 0.0085\n",
      "Epoch 13 batch 3000 train loss: 0.0057 test loss: 0.0082\n",
      "Epoch 14 batch 0 train loss: 0.0078 test loss: 0.0082\n",
      "Epoch 14 batch 100 train loss: 0.0086 test loss: 0.0088\n",
      "Epoch 14 batch 200 train loss: 0.0086 test loss: 0.0084\n",
      "Epoch 14 batch 300 train loss: 0.0075 test loss: 0.0086\n",
      "Epoch 14 batch 400 train loss: 0.0058 test loss: 0.0086\n",
      "Epoch 14 batch 500 train loss: 0.0081 test loss: 0.0091\n",
      "Epoch 14 batch 600 train loss: 0.0116 test loss: 0.0087\n",
      "Epoch 14 batch 700 train loss: 0.0089 test loss: 0.0085\n",
      "Epoch 14 batch 800 train loss: 0.0117 test loss: 0.0085\n",
      "Epoch 14 batch 900 train loss: 0.0090 test loss: 0.0085\n",
      "Epoch 14 batch 1000 train loss: 0.0065 test loss: 0.0086\n",
      "Epoch 14 batch 1100 train loss: 0.0128 test loss: 0.0088\n",
      "Epoch 14 batch 1200 train loss: 0.0122 test loss: 0.0084\n",
      "Epoch 14 batch 1300 train loss: 0.0086 test loss: 0.0088\n",
      "Epoch 14 batch 1400 train loss: 0.0089 test loss: 0.0083\n",
      "Epoch 14 batch 1500 train loss: 0.0069 test loss: 0.0087\n",
      "Epoch 14 batch 1600 train loss: 0.0103 test loss: 0.0093\n",
      "Epoch 14 batch 1700 train loss: 0.0084 test loss: 0.0089\n",
      "Epoch 14 batch 1800 train loss: 0.0096 test loss: 0.0082\n",
      "Epoch 14 batch 1900 train loss: 0.0100 test loss: 0.0083\n",
      "Epoch 14 batch 2000 train loss: 0.0079 test loss: 0.0085\n",
      "Epoch 14 batch 2100 train loss: 0.0034 test loss: 0.0081\n",
      "Epoch 14 batch 2200 train loss: 0.0118 test loss: 0.0088\n",
      "Epoch 14 batch 2300 train loss: 0.0063 test loss: 0.0091\n",
      "Epoch 14 batch 2400 train loss: 0.0071 test loss: 0.0090\n",
      "Epoch 14 batch 2500 train loss: 0.0110 test loss: 0.0086\n",
      "Epoch 14 batch 2600 train loss: 0.0111 test loss: 0.0088\n",
      "Epoch 14 batch 2700 train loss: 0.0100 test loss: 0.0085\n",
      "Epoch 14 batch 2800 train loss: 0.0065 test loss: 0.0085\n",
      "Epoch 14 batch 2900 train loss: 0.0062 test loss: 0.0086\n",
      "Epoch 14 batch 3000 train loss: 0.0090 test loss: 0.0086\n",
      "Epoch 15 batch 0 train loss: 0.0087 test loss: 0.0086\n",
      "Epoch 15 batch 100 train loss: 0.0056 test loss: 0.0085\n",
      "Epoch 15 batch 200 train loss: 0.0096 test loss: 0.0083\n",
      "Epoch 15 batch 300 train loss: 0.0107 test loss: 0.0089\n",
      "Epoch 15 batch 400 train loss: 0.0082 test loss: 0.0088\n",
      "Epoch 15 batch 500 train loss: 0.0088 test loss: 0.0084\n",
      "Epoch 15 batch 600 train loss: 0.0078 test loss: 0.0087\n",
      "Epoch 15 batch 700 train loss: 0.0109 test loss: 0.0084\n",
      "Epoch 15 batch 800 train loss: 0.0050 test loss: 0.0089\n",
      "Epoch 15 batch 900 train loss: 0.0130 test loss: 0.0088\n",
      "Epoch 15 batch 1000 train loss: 0.0103 test loss: 0.0085\n",
      "Epoch 15 batch 1100 train loss: 0.0075 test loss: 0.0087\n",
      "Epoch 15 batch 1200 train loss: 0.0063 test loss: 0.0088\n",
      "Epoch 15 batch 1300 train loss: 0.0101 test loss: 0.0087\n",
      "Epoch 15 batch 1400 train loss: 0.0071 test loss: 0.0088\n",
      "Epoch 15 batch 1500 train loss: 0.0075 test loss: 0.0088\n",
      "Epoch 15 batch 1600 train loss: 0.0117 test loss: 0.0085\n",
      "Epoch 15 batch 1700 train loss: 0.0066 test loss: 0.0088\n",
      "Epoch 15 batch 1800 train loss: 0.0133 test loss: 0.0086\n",
      "Epoch 15 batch 1900 train loss: 0.0086 test loss: 0.0087\n",
      "Epoch 15 batch 2000 train loss: 0.0084 test loss: 0.0083\n",
      "Epoch 15 batch 2100 train loss: 0.0053 test loss: 0.0084\n",
      "Epoch 15 batch 2200 train loss: 0.0057 test loss: 0.0086\n",
      "Epoch 15 batch 2300 train loss: 0.0067 test loss: 0.0084\n",
      "Epoch 15 batch 2400 train loss: 0.0122 test loss: 0.0089\n",
      "Epoch 15 batch 2500 train loss: 0.0123 test loss: 0.0086\n",
      "Epoch 15 batch 2600 train loss: 0.0040 test loss: 0.0086\n",
      "Epoch 15 batch 2700 train loss: 0.0090 test loss: 0.0089\n",
      "Epoch 15 batch 2800 train loss: 0.0070 test loss: 0.0088\n",
      "Epoch 15 batch 2900 train loss: 0.0100 test loss: 0.0085\n",
      "Epoch 15 batch 3000 train loss: 0.0083 test loss: 0.0084\n",
      "Epoch 16 batch 0 train loss: 0.0109 test loss: 0.0087\n",
      "Epoch 16 batch 100 train loss: 0.0077 test loss: 0.0088\n",
      "Epoch 16 batch 200 train loss: 0.0092 test loss: 0.0087\n",
      "Epoch 16 batch 300 train loss: 0.0106 test loss: 0.0087\n",
      "Epoch 16 batch 400 train loss: 0.0057 test loss: 0.0090\n",
      "Epoch 16 batch 500 train loss: 0.0107 test loss: 0.0086\n",
      "Epoch 16 batch 600 train loss: 0.0096 test loss: 0.0090\n",
      "Epoch 16 batch 700 train loss: 0.0066 test loss: 0.0087\n",
      "Epoch 16 batch 800 train loss: 0.0090 test loss: 0.0087\n",
      "Epoch 16 batch 900 train loss: 0.0093 test loss: 0.0084\n",
      "Epoch 16 batch 1000 train loss: 0.0097 test loss: 0.0087\n",
      "Epoch 16 batch 1100 train loss: 0.0108 test loss: 0.0087\n",
      "Epoch 16 batch 1200 train loss: 0.0055 test loss: 0.0086\n",
      "Epoch 16 batch 1300 train loss: 0.0060 test loss: 0.0088\n",
      "Epoch 16 batch 1400 train loss: 0.0093 test loss: 0.0085\n",
      "Epoch 16 batch 1500 train loss: 0.0081 test loss: 0.0088\n",
      "Epoch 16 batch 1600 train loss: 0.0068 test loss: 0.0083\n",
      "Epoch 16 batch 1700 train loss: 0.0058 test loss: 0.0089\n",
      "Epoch 16 batch 1800 train loss: 0.0086 test loss: 0.0085\n",
      "Epoch 16 batch 1900 train loss: 0.0080 test loss: 0.0083\n",
      "Epoch 16 batch 2000 train loss: 0.0125 test loss: 0.0083\n",
      "Epoch 16 batch 2100 train loss: 0.0086 test loss: 0.0086\n",
      "Epoch 16 batch 2200 train loss: 0.0078 test loss: 0.0088\n",
      "Epoch 16 batch 2300 train loss: 0.0069 test loss: 0.0086\n",
      "Epoch 16 batch 2400 train loss: 0.0085 test loss: 0.0088\n",
      "Epoch 16 batch 2500 train loss: 0.0187 test loss: 0.0092\n",
      "Epoch 16 batch 2600 train loss: 0.0118 test loss: 0.0088\n",
      "Epoch 16 batch 2700 train loss: 0.0090 test loss: 0.0087\n",
      "Epoch 16 batch 2800 train loss: 0.0119 test loss: 0.0088\n",
      "Epoch 16 batch 2900 train loss: 0.0139 test loss: 0.0084\n",
      "Epoch 16 batch 3000 train loss: 0.0105 test loss: 0.0085\n",
      "Epoch 17 batch 0 train loss: 0.0085 test loss: 0.0084\n",
      "Epoch 17 batch 100 train loss: 0.0066 test loss: 0.0086\n",
      "Epoch 17 batch 200 train loss: 0.0133 test loss: 0.0086\n",
      "Epoch 17 batch 300 train loss: 0.0085 test loss: 0.0084\n",
      "Epoch 17 batch 400 train loss: 0.0154 test loss: 0.0087\n",
      "Epoch 17 batch 500 train loss: 0.0111 test loss: 0.0085\n",
      "Epoch 17 batch 600 train loss: 0.0087 test loss: 0.0088\n",
      "Epoch 17 batch 700 train loss: 0.0107 test loss: 0.0083\n",
      "early stop.\n",
      "Checkpoint 35 restored!!\n",
      "Training for loss rate 0.50 start.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ffnn_4/dense_12/kernel:0', 'ffnn_4/dense_12/bias:0', 'ffnn_4/batch_normalization_4/gamma:0', 'ffnn_4/batch_normalization_4/beta:0', 'ffnn_4/dense_13/kernel:0', 'ffnn_4/dense_13/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ffnn_4/dense_12/kernel:0', 'ffnn_4/dense_12/bias:0', 'ffnn_4/batch_normalization_4/gamma:0', 'ffnn_4/batch_normalization_4/beta:0', 'ffnn_4/dense_13/kernel:0', 'ffnn_4/dense_13/bias:0'] when minimizing the loss.\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.50p/ckpt-1\n",
      "Epoch 0 batch 0 train loss: 0.1446 test loss: 0.2127\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.50p/ckpt-2\n",
      "Epoch 0 batch 100 train loss: 0.1576 test loss: 0.1536\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.50p/ckpt-3\n",
      "Epoch 0 batch 200 train loss: 0.0843 test loss: 0.1191\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.50p/ckpt-4\n",
      "Epoch 0 batch 300 train loss: 0.0591 test loss: 0.0930\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.50p/ckpt-5\n",
      "Epoch 0 batch 400 train loss: 0.0532 test loss: 0.0725\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.50p/ckpt-6\n",
      "Epoch 0 batch 500 train loss: 0.0373 test loss: 0.0568\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.50p/ckpt-7\n",
      "Epoch 0 batch 600 train loss: 0.0272 test loss: 0.0449\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.50p/ckpt-8\n",
      "Epoch 0 batch 700 train loss: 0.0240 test loss: 0.0360\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.50p/ckpt-9\n",
      "Epoch 0 batch 800 train loss: 0.0177 test loss: 0.0291\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.50p/ckpt-10\n",
      "Epoch 0 batch 900 train loss: 0.0188 test loss: 0.0240\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.50p/ckpt-11\n",
      "Epoch 0 batch 1000 train loss: 0.0136 test loss: 0.0200\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.50p/ckpt-12\n",
      "Epoch 0 batch 1100 train loss: 0.0099 test loss: 0.0175\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.50p/ckpt-13\n",
      "Epoch 0 batch 1200 train loss: 0.0130 test loss: 0.0153\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.50p/ckpt-14\n",
      "Epoch 0 batch 1300 train loss: 0.0119 test loss: 0.0139\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.50p/ckpt-15\n",
      "Epoch 0 batch 1400 train loss: 0.0143 test loss: 0.0128\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.50p/ckpt-16\n",
      "Epoch 0 batch 1500 train loss: 0.0105 test loss: 0.0120\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.50p/ckpt-17\n",
      "Epoch 0 batch 1600 train loss: 0.0097 test loss: 0.0118\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.50p/ckpt-18\n",
      "Epoch 0 batch 1700 train loss: 0.0111 test loss: 0.0112\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.50p/ckpt-19\n",
      "Epoch 0 batch 1800 train loss: 0.0080 test loss: 0.0106\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.50p/ckpt-20\n",
      "Epoch 0 batch 1900 train loss: 0.0105 test loss: 0.0104\n",
      "Epoch 0 batch 2000 train loss: 0.0136 test loss: 0.0105\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.50p/ckpt-21\n",
      "Epoch 0 batch 2100 train loss: 0.0086 test loss: 0.0103\n",
      "Epoch 0 batch 2200 train loss: 0.0059 test loss: 0.0105\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.50p/ckpt-22\n",
      "Epoch 0 batch 2300 train loss: 0.0108 test loss: 0.0103\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.50p/ckpt-23\n",
      "Epoch 0 batch 2400 train loss: 0.0090 test loss: 0.0102\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.50p/ckpt-24\n",
      "Epoch 0 batch 2500 train loss: 0.0065 test loss: 0.0101\n",
      "Epoch 0 batch 2600 train loss: 0.0119 test loss: 0.0102\n",
      "Epoch 0 batch 2700 train loss: 0.0104 test loss: 0.0102\n",
      "Epoch 0 batch 2800 train loss: 0.0075 test loss: 0.0103\n",
      "Epoch 0 batch 2900 train loss: 0.0086 test loss: 0.0102\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.50p/ckpt-25\n",
      "Epoch 0 batch 3000 train loss: 0.0093 test loss: 0.0099\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ffnn_4/dense_12/kernel:0', 'ffnn_4/dense_12/bias:0', 'ffnn_4/batch_normalization_4/gamma:0', 'ffnn_4/batch_normalization_4/beta:0', 'ffnn_4/dense_13/kernel:0', 'ffnn_4/dense_13/bias:0'] when minimizing the loss.\n",
      "Epoch 1 batch 0 train loss: 0.0120 test loss: 0.0100\n",
      "Epoch 1 batch 100 train loss: 0.0134 test loss: 0.0104\n",
      "Epoch 1 batch 200 train loss: 0.0130 test loss: 0.0103\n",
      "Epoch 1 batch 300 train loss: 0.0116 test loss: 0.0102\n",
      "Epoch 1 batch 400 train loss: 0.0084 test loss: 0.0103\n",
      "Epoch 1 batch 500 train loss: 0.0084 test loss: 0.0100\n",
      "Epoch 1 batch 600 train loss: 0.0117 test loss: 0.0102\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.50p/ckpt-26\n",
      "Epoch 1 batch 700 train loss: 0.0105 test loss: 0.0099\n",
      "Epoch 1 batch 800 train loss: 0.0192 test loss: 0.0099\n",
      "Epoch 1 batch 900 train loss: 0.0159 test loss: 0.0101\n",
      "Epoch 1 batch 1000 train loss: 0.0118 test loss: 0.0100\n",
      "Epoch 1 batch 1100 train loss: 0.0166 test loss: 0.0101\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.50p/ckpt-27\n",
      "Epoch 1 batch 1200 train loss: 0.0072 test loss: 0.0097\n",
      "Epoch 1 batch 1300 train loss: 0.0097 test loss: 0.0101\n",
      "Epoch 1 batch 1400 train loss: 0.0082 test loss: 0.0099\n",
      "Epoch 1 batch 1500 train loss: 0.0121 test loss: 0.0099\n",
      "Epoch 1 batch 1600 train loss: 0.0098 test loss: 0.0101\n",
      "Epoch 1 batch 1700 train loss: 0.0113 test loss: 0.0100\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.50p/ckpt-28\n",
      "Epoch 1 batch 1800 train loss: 0.0118 test loss: 0.0096\n",
      "Epoch 1 batch 1900 train loss: 0.0071 test loss: 0.0100\n",
      "Epoch 1 batch 2000 train loss: 0.0085 test loss: 0.0099\n",
      "Epoch 1 batch 2100 train loss: 0.0107 test loss: 0.0101\n",
      "Epoch 1 batch 2200 train loss: 0.0089 test loss: 0.0099\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.50p/ckpt-29\n",
      "Epoch 1 batch 2300 train loss: 0.0109 test loss: 0.0096\n",
      "Epoch 1 batch 2400 train loss: 0.0087 test loss: 0.0099\n",
      "Epoch 1 batch 2500 train loss: 0.0103 test loss: 0.0099\n",
      "Epoch 1 batch 2600 train loss: 0.0123 test loss: 0.0099\n",
      "Epoch 1 batch 2700 train loss: 0.0100 test loss: 0.0098\n",
      "Epoch 1 batch 2800 train loss: 0.0078 test loss: 0.0101\n",
      "Epoch 1 batch 2900 train loss: 0.0141 test loss: 0.0099\n",
      "Epoch 1 batch 3000 train loss: 0.0131 test loss: 0.0098\n",
      "Epoch 2 batch 0 train loss: 0.0081 test loss: 0.0099\n",
      "Epoch 2 batch 100 train loss: 0.0071 test loss: 0.0100\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.50p/ckpt-30\n",
      "Epoch 2 batch 200 train loss: 0.0075 test loss: 0.0095\n",
      "Epoch 2 batch 300 train loss: 0.0139 test loss: 0.0101\n",
      "Epoch 2 batch 400 train loss: 0.0064 test loss: 0.0102\n",
      "Epoch 2 batch 500 train loss: 0.0155 test loss: 0.0098\n",
      "Epoch 2 batch 600 train loss: 0.0127 test loss: 0.0099\n",
      "Epoch 2 batch 700 train loss: 0.0104 test loss: 0.0098\n",
      "Epoch 2 batch 800 train loss: 0.0107 test loss: 0.0098\n",
      "Epoch 2 batch 900 train loss: 0.0093 test loss: 0.0099\n",
      "Epoch 2 batch 1000 train loss: 0.0102 test loss: 0.0097\n",
      "Epoch 2 batch 1100 train loss: 0.0036 test loss: 0.0100\n",
      "Epoch 2 batch 1200 train loss: 0.0088 test loss: 0.0096\n",
      "Epoch 2 batch 1300 train loss: 0.0102 test loss: 0.0102\n",
      "Epoch 2 batch 1400 train loss: 0.0092 test loss: 0.0098\n",
      "Epoch 2 batch 1500 train loss: 0.0155 test loss: 0.0099\n",
      "Epoch 2 batch 1600 train loss: 0.0087 test loss: 0.0099\n",
      "Epoch 2 batch 1700 train loss: 0.0076 test loss: 0.0101\n",
      "Epoch 2 batch 1800 train loss: 0.0112 test loss: 0.0097\n",
      "Epoch 2 batch 1900 train loss: 0.0075 test loss: 0.0098\n",
      "Epoch 2 batch 2000 train loss: 0.0072 test loss: 0.0098\n",
      "Epoch 2 batch 2100 train loss: 0.0063 test loss: 0.0098\n",
      "Epoch 2 batch 2200 train loss: 0.0077 test loss: 0.0101\n",
      "Epoch 2 batch 2300 train loss: 0.0068 test loss: 0.0100\n",
      "Epoch 2 batch 2400 train loss: 0.0099 test loss: 0.0100\n",
      "Epoch 2 batch 2500 train loss: 0.0100 test loss: 0.0100\n",
      "Epoch 2 batch 2600 train loss: 0.0085 test loss: 0.0099\n",
      "Epoch 2 batch 2700 train loss: 0.0089 test loss: 0.0100\n",
      "Epoch 2 batch 2800 train loss: 0.0050 test loss: 0.0102\n",
      "Epoch 2 batch 2900 train loss: 0.0189 test loss: 0.0099\n",
      "Epoch 2 batch 3000 train loss: 0.0099 test loss: 0.0099\n",
      "Epoch 3 batch 0 train loss: 0.0084 test loss: 0.0100\n",
      "Epoch 3 batch 100 train loss: 0.0066 test loss: 0.0100\n",
      "Epoch 3 batch 200 train loss: 0.0088 test loss: 0.0102\n",
      "Epoch 3 batch 300 train loss: 0.0046 test loss: 0.0096\n",
      "Epoch 3 batch 400 train loss: 0.0113 test loss: 0.0096\n",
      "Epoch 3 batch 500 train loss: 0.0119 test loss: 0.0103\n",
      "Epoch 3 batch 600 train loss: 0.0102 test loss: 0.0099\n",
      "Epoch 3 batch 700 train loss: 0.0097 test loss: 0.0098\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.50p/ckpt-31\n",
      "Epoch 3 batch 800 train loss: 0.0101 test loss: 0.0094\n",
      "Epoch 3 batch 900 train loss: 0.0153 test loss: 0.0100\n",
      "Epoch 3 batch 1000 train loss: 0.0121 test loss: 0.0100\n",
      "Epoch 3 batch 1100 train loss: 0.0089 test loss: 0.0095\n",
      "Epoch 3 batch 1200 train loss: 0.0097 test loss: 0.0100\n",
      "Epoch 3 batch 1300 train loss: 0.0102 test loss: 0.0099\n",
      "Epoch 3 batch 1400 train loss: 0.0068 test loss: 0.0095\n",
      "Epoch 3 batch 1500 train loss: 0.0109 test loss: 0.0099\n",
      "Epoch 3 batch 1600 train loss: 0.0104 test loss: 0.0100\n",
      "Epoch 3 batch 1700 train loss: 0.0064 test loss: 0.0099\n",
      "Epoch 3 batch 1800 train loss: 0.0078 test loss: 0.0098\n",
      "Epoch 3 batch 1900 train loss: 0.0072 test loss: 0.0096\n",
      "Epoch 3 batch 2000 train loss: 0.0123 test loss: 0.0097\n",
      "Epoch 3 batch 2100 train loss: 0.0084 test loss: 0.0100\n",
      "Epoch 3 batch 2200 train loss: 0.0070 test loss: 0.0098\n",
      "Epoch 3 batch 2300 train loss: 0.0102 test loss: 0.0097\n",
      "Epoch 3 batch 2400 train loss: 0.0122 test loss: 0.0097\n",
      "Epoch 3 batch 2500 train loss: 0.0079 test loss: 0.0098\n",
      "Epoch 3 batch 2600 train loss: 0.0085 test loss: 0.0098\n",
      "Epoch 3 batch 2700 train loss: 0.0077 test loss: 0.0100\n",
      "Epoch 3 batch 2800 train loss: 0.0059 test loss: 0.0101\n",
      "Epoch 3 batch 2900 train loss: 0.0079 test loss: 0.0100\n",
      "Epoch 3 batch 3000 train loss: 0.0072 test loss: 0.0096\n",
      "Epoch 4 batch 0 train loss: 0.0065 test loss: 0.0098\n",
      "Epoch 4 batch 100 train loss: 0.0075 test loss: 0.0105\n",
      "Epoch 4 batch 200 train loss: 0.0078 test loss: 0.0096\n",
      "Epoch 4 batch 300 train loss: 0.0141 test loss: 0.0102\n",
      "Epoch 4 batch 400 train loss: 0.0077 test loss: 0.0096\n",
      "Epoch 4 batch 500 train loss: 0.0144 test loss: 0.0100\n",
      "Epoch 4 batch 600 train loss: 0.0070 test loss: 0.0102\n",
      "Epoch 4 batch 700 train loss: 0.0078 test loss: 0.0097\n",
      "Epoch 4 batch 800 train loss: 0.0106 test loss: 0.0099\n",
      "Epoch 4 batch 900 train loss: 0.0114 test loss: 0.0099\n",
      "Epoch 4 batch 1000 train loss: 0.0089 test loss: 0.0097\n",
      "Epoch 4 batch 1100 train loss: 0.0085 test loss: 0.0098\n",
      "Epoch 4 batch 1200 train loss: 0.0055 test loss: 0.0098\n",
      "Epoch 4 batch 1300 train loss: 0.0118 test loss: 0.0103\n",
      "Epoch 4 batch 1400 train loss: 0.0074 test loss: 0.0098\n",
      "Epoch 4 batch 1500 train loss: 0.0091 test loss: 0.0099\n",
      "Epoch 4 batch 1600 train loss: 0.0098 test loss: 0.0102\n",
      "Epoch 4 batch 1700 train loss: 0.0076 test loss: 0.0099\n",
      "Epoch 4 batch 1800 train loss: 0.0084 test loss: 0.0095\n",
      "Epoch 4 batch 1900 train loss: 0.0134 test loss: 0.0101\n",
      "Epoch 4 batch 2000 train loss: 0.0108 test loss: 0.0096\n",
      "Epoch 4 batch 2100 train loss: 0.0127 test loss: 0.0097\n",
      "Epoch 4 batch 2200 train loss: 0.0120 test loss: 0.0102\n",
      "Epoch 4 batch 2300 train loss: 0.0061 test loss: 0.0099\n",
      "Epoch 4 batch 2400 train loss: 0.0080 test loss: 0.0098\n",
      "Epoch 4 batch 2500 train loss: 0.0059 test loss: 0.0102\n",
      "Epoch 4 batch 2600 train loss: 0.0113 test loss: 0.0099\n",
      "Epoch 4 batch 2700 train loss: 0.0073 test loss: 0.0101\n",
      "Epoch 4 batch 2800 train loss: 0.0078 test loss: 0.0101\n",
      "Epoch 4 batch 2900 train loss: 0.0107 test loss: 0.0100\n",
      "Epoch 4 batch 3000 train loss: 0.0072 test loss: 0.0096\n",
      "Epoch 5 batch 0 train loss: 0.0069 test loss: 0.0099\n",
      "Epoch 5 batch 100 train loss: 0.0112 test loss: 0.0100\n",
      "Epoch 5 batch 200 train loss: 0.0112 test loss: 0.0097\n",
      "Epoch 5 batch 300 train loss: 0.0111 test loss: 0.0098\n",
      "Epoch 5 batch 400 train loss: 0.0088 test loss: 0.0098\n",
      "Epoch 5 batch 500 train loss: 0.0117 test loss: 0.0099\n",
      "Epoch 5 batch 600 train loss: 0.0108 test loss: 0.0103\n",
      "Epoch 5 batch 700 train loss: 0.0117 test loss: 0.0096\n",
      "Epoch 5 batch 800 train loss: 0.0089 test loss: 0.0098\n",
      "Epoch 5 batch 900 train loss: 0.0112 test loss: 0.0100\n",
      "Epoch 5 batch 1000 train loss: 0.0053 test loss: 0.0102\n",
      "Epoch 5 batch 1100 train loss: 0.0103 test loss: 0.0098\n",
      "Epoch 5 batch 1200 train loss: 0.0068 test loss: 0.0095\n",
      "Epoch 5 batch 1300 train loss: 0.0059 test loss: 0.0099\n",
      "Epoch 5 batch 1400 train loss: 0.0080 test loss: 0.0098\n",
      "Epoch 5 batch 1500 train loss: 0.0093 test loss: 0.0098\n",
      "Epoch 5 batch 1600 train loss: 0.0076 test loss: 0.0098\n",
      "Epoch 5 batch 1700 train loss: 0.0059 test loss: 0.0102\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.50p/ckpt-32\n",
      "Epoch 5 batch 1800 train loss: 0.0073 test loss: 0.0094\n",
      "Epoch 5 batch 1900 train loss: 0.0073 test loss: 0.0098\n",
      "Epoch 5 batch 2000 train loss: 0.0109 test loss: 0.0097\n",
      "Epoch 5 batch 2100 train loss: 0.0098 test loss: 0.0098\n",
      "Epoch 5 batch 2200 train loss: 0.0111 test loss: 0.0102\n",
      "Epoch 5 batch 2300 train loss: 0.0064 test loss: 0.0096\n",
      "Epoch 5 batch 2400 train loss: 0.0078 test loss: 0.0101\n",
      "Epoch 5 batch 2500 train loss: 0.0062 test loss: 0.0098\n",
      "Epoch 5 batch 2600 train loss: 0.0082 test loss: 0.0099\n",
      "Epoch 5 batch 2700 train loss: 0.0118 test loss: 0.0103\n",
      "Epoch 5 batch 2800 train loss: 0.0052 test loss: 0.0103\n",
      "Epoch 5 batch 2900 train loss: 0.0067 test loss: 0.0095\n",
      "Epoch 5 batch 3000 train loss: 0.0112 test loss: 0.0098\n",
      "Epoch 6 batch 0 train loss: 0.0094 test loss: 0.0101\n",
      "Epoch 6 batch 100 train loss: 0.0106 test loss: 0.0099\n",
      "Epoch 6 batch 200 train loss: 0.0076 test loss: 0.0099\n",
      "Epoch 6 batch 300 train loss: 0.0068 test loss: 0.0100\n",
      "Epoch 6 batch 400 train loss: 0.0057 test loss: 0.0097\n",
      "Epoch 6 batch 500 train loss: 0.0090 test loss: 0.0099\n",
      "Epoch 6 batch 600 train loss: 0.0084 test loss: 0.0100\n",
      "Epoch 6 batch 700 train loss: 0.0090 test loss: 0.0100\n",
      "Epoch 6 batch 800 train loss: 0.0099 test loss: 0.0098\n",
      "Epoch 6 batch 900 train loss: 0.0071 test loss: 0.0100\n",
      "Epoch 6 batch 1000 train loss: 0.0115 test loss: 0.0101\n",
      "Epoch 6 batch 1100 train loss: 0.0081 test loss: 0.0098\n",
      "Epoch 6 batch 1200 train loss: 0.0074 test loss: 0.0099\n",
      "Epoch 6 batch 1300 train loss: 0.0089 test loss: 0.0102\n",
      "Epoch 6 batch 1400 train loss: 0.0087 test loss: 0.0100\n",
      "Epoch 6 batch 1500 train loss: 0.0092 test loss: 0.0102\n",
      "Epoch 6 batch 1600 train loss: 0.0085 test loss: 0.0101\n",
      "Epoch 6 batch 1700 train loss: 0.0089 test loss: 0.0099\n",
      "Epoch 6 batch 1800 train loss: 0.0153 test loss: 0.0095\n",
      "Epoch 6 batch 1900 train loss: 0.0090 test loss: 0.0097\n",
      "Epoch 6 batch 2000 train loss: 0.0079 test loss: 0.0097\n",
      "Epoch 6 batch 2100 train loss: 0.0093 test loss: 0.0101\n",
      "Epoch 6 batch 2200 train loss: 0.0092 test loss: 0.0102\n",
      "Epoch 6 batch 2300 train loss: 0.0109 test loss: 0.0098\n",
      "Epoch 6 batch 2400 train loss: 0.0127 test loss: 0.0100\n",
      "Epoch 6 batch 2500 train loss: 0.0109 test loss: 0.0097\n",
      "Epoch 6 batch 2600 train loss: 0.0103 test loss: 0.0102\n",
      "Epoch 6 batch 2700 train loss: 0.0133 test loss: 0.0102\n",
      "Epoch 6 batch 2800 train loss: 0.0102 test loss: 0.0101\n",
      "Epoch 6 batch 2900 train loss: 0.0137 test loss: 0.0099\n",
      "Epoch 6 batch 3000 train loss: 0.0129 test loss: 0.0101\n",
      "Epoch 7 batch 0 train loss: 0.0079 test loss: 0.0100\n",
      "Epoch 7 batch 100 train loss: 0.0127 test loss: 0.0098\n",
      "Epoch 7 batch 200 train loss: 0.0083 test loss: 0.0099\n",
      "Epoch 7 batch 300 train loss: 0.0083 test loss: 0.0099\n",
      "Epoch 7 batch 400 train loss: 0.0085 test loss: 0.0096\n",
      "Epoch 7 batch 500 train loss: 0.0094 test loss: 0.0100\n",
      "Epoch 7 batch 600 train loss: 0.0106 test loss: 0.0100\n",
      "Epoch 7 batch 700 train loss: 0.0094 test loss: 0.0098\n",
      "Epoch 7 batch 800 train loss: 0.0112 test loss: 0.0100\n",
      "Epoch 7 batch 900 train loss: 0.0173 test loss: 0.0102\n",
      "Epoch 7 batch 1000 train loss: 0.0084 test loss: 0.0100\n",
      "Epoch 7 batch 1100 train loss: 0.0125 test loss: 0.0100\n",
      "Epoch 7 batch 1200 train loss: 0.0115 test loss: 0.0099\n",
      "Epoch 7 batch 1300 train loss: 0.0123 test loss: 0.0100\n",
      "Epoch 7 batch 1400 train loss: 0.0102 test loss: 0.0096\n",
      "Epoch 7 batch 1500 train loss: 0.0050 test loss: 0.0098\n",
      "Epoch 7 batch 1600 train loss: 0.0135 test loss: 0.0100\n",
      "Epoch 7 batch 1700 train loss: 0.0069 test loss: 0.0098\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.50p/ckpt-33\n",
      "Epoch 7 batch 1800 train loss: 0.0064 test loss: 0.0092\n",
      "Epoch 7 batch 1900 train loss: 0.0113 test loss: 0.0098\n",
      "Epoch 7 batch 2000 train loss: 0.0059 test loss: 0.0100\n",
      "Epoch 7 batch 2100 train loss: 0.0111 test loss: 0.0101\n",
      "Epoch 7 batch 2200 train loss: 0.0110 test loss: 0.0100\n",
      "Epoch 7 batch 2300 train loss: 0.0095 test loss: 0.0096\n",
      "Epoch 7 batch 2400 train loss: 0.0068 test loss: 0.0098\n",
      "Epoch 7 batch 2500 train loss: 0.0077 test loss: 0.0102\n",
      "Epoch 7 batch 2600 train loss: 0.0092 test loss: 0.0101\n",
      "Epoch 7 batch 2700 train loss: 0.0107 test loss: 0.0101\n",
      "Epoch 7 batch 2800 train loss: 0.0118 test loss: 0.0100\n",
      "Epoch 7 batch 2900 train loss: 0.0113 test loss: 0.0098\n",
      "Epoch 7 batch 3000 train loss: 0.0100 test loss: 0.0097\n",
      "Epoch 8 batch 0 train loss: 0.0060 test loss: 0.0099\n",
      "Epoch 8 batch 100 train loss: 0.0087 test loss: 0.0099\n",
      "Epoch 8 batch 200 train loss: 0.0091 test loss: 0.0097\n",
      "Epoch 8 batch 300 train loss: 0.0076 test loss: 0.0097\n",
      "Epoch 8 batch 400 train loss: 0.0093 test loss: 0.0101\n",
      "Epoch 8 batch 500 train loss: 0.0074 test loss: 0.0101\n",
      "Epoch 8 batch 600 train loss: 0.0075 test loss: 0.0100\n",
      "Epoch 8 batch 700 train loss: 0.0081 test loss: 0.0097\n",
      "Epoch 8 batch 800 train loss: 0.0075 test loss: 0.0100\n",
      "Epoch 8 batch 900 train loss: 0.0061 test loss: 0.0098\n",
      "Epoch 8 batch 1000 train loss: 0.0078 test loss: 0.0098\n",
      "Epoch 8 batch 1100 train loss: 0.0084 test loss: 0.0099\n",
      "Epoch 8 batch 1200 train loss: 0.0086 test loss: 0.0099\n",
      "Epoch 8 batch 1300 train loss: 0.0061 test loss: 0.0099\n",
      "Epoch 8 batch 1400 train loss: 0.0099 test loss: 0.0099\n",
      "Epoch 8 batch 1500 train loss: 0.0116 test loss: 0.0101\n",
      "Epoch 8 batch 1600 train loss: 0.0131 test loss: 0.0101\n",
      "Epoch 8 batch 1700 train loss: 0.0057 test loss: 0.0099\n",
      "Epoch 8 batch 1800 train loss: 0.0107 test loss: 0.0096\n",
      "Epoch 8 batch 1900 train loss: 0.0118 test loss: 0.0098\n",
      "Epoch 8 batch 2000 train loss: 0.0056 test loss: 0.0097\n",
      "Epoch 8 batch 2100 train loss: 0.0067 test loss: 0.0098\n",
      "Epoch 8 batch 2200 train loss: 0.0067 test loss: 0.0098\n",
      "Epoch 8 batch 2300 train loss: 0.0071 test loss: 0.0099\n",
      "Epoch 8 batch 2400 train loss: 0.0060 test loss: 0.0101\n",
      "Epoch 8 batch 2500 train loss: 0.0108 test loss: 0.0101\n",
      "Epoch 8 batch 2600 train loss: 0.0125 test loss: 0.0098\n",
      "Epoch 8 batch 2700 train loss: 0.0112 test loss: 0.0105\n",
      "Epoch 8 batch 2800 train loss: 0.0136 test loss: 0.0100\n",
      "Epoch 8 batch 2900 train loss: 0.0122 test loss: 0.0098\n",
      "Epoch 8 batch 3000 train loss: 0.0057 test loss: 0.0095\n",
      "Epoch 9 batch 0 train loss: 0.0139 test loss: 0.0097\n",
      "Epoch 9 batch 100 train loss: 0.0114 test loss: 0.0098\n",
      "Epoch 9 batch 200 train loss: 0.0079 test loss: 0.0100\n",
      "Epoch 9 batch 300 train loss: 0.0049 test loss: 0.0103\n",
      "Epoch 9 batch 400 train loss: 0.0102 test loss: 0.0096\n",
      "Epoch 9 batch 500 train loss: 0.0149 test loss: 0.0102\n",
      "Epoch 9 batch 600 train loss: 0.0080 test loss: 0.0102\n",
      "Epoch 9 batch 700 train loss: 0.0191 test loss: 0.0101\n",
      "Epoch 9 batch 800 train loss: 0.0066 test loss: 0.0102\n",
      "Epoch 9 batch 900 train loss: 0.0103 test loss: 0.0098\n",
      "Epoch 9 batch 1000 train loss: 0.0120 test loss: 0.0100\n",
      "Epoch 9 batch 1100 train loss: 0.0081 test loss: 0.0100\n",
      "Epoch 9 batch 1200 train loss: 0.0109 test loss: 0.0097\n",
      "Epoch 9 batch 1300 train loss: 0.0095 test loss: 0.0101\n",
      "Epoch 9 batch 1400 train loss: 0.0081 test loss: 0.0098\n",
      "Epoch 9 batch 1500 train loss: 0.0093 test loss: 0.0098\n",
      "Epoch 9 batch 1600 train loss: 0.0092 test loss: 0.0101\n",
      "Epoch 9 batch 1700 train loss: 0.0097 test loss: 0.0098\n",
      "Epoch 9 batch 1800 train loss: 0.0086 test loss: 0.0095\n",
      "Epoch 9 batch 1900 train loss: 0.0096 test loss: 0.0098\n",
      "Epoch 9 batch 2000 train loss: 0.0078 test loss: 0.0095\n",
      "Epoch 9 batch 2100 train loss: 0.0060 test loss: 0.0099\n",
      "Epoch 9 batch 2200 train loss: 0.0124 test loss: 0.0101\n",
      "Epoch 9 batch 2300 train loss: 0.0075 test loss: 0.0097\n",
      "Epoch 9 batch 2400 train loss: 0.0072 test loss: 0.0101\n",
      "Epoch 9 batch 2500 train loss: 0.0132 test loss: 0.0097\n",
      "Epoch 9 batch 2600 train loss: 0.0066 test loss: 0.0100\n",
      "Epoch 9 batch 2700 train loss: 0.0119 test loss: 0.0102\n",
      "Epoch 9 batch 2800 train loss: 0.0088 test loss: 0.0099\n",
      "Epoch 9 batch 2900 train loss: 0.0104 test loss: 0.0097\n",
      "Epoch 9 batch 3000 train loss: 0.0122 test loss: 0.0102\n",
      "Epoch 10 batch 0 train loss: 0.0123 test loss: 0.0098\n",
      "Epoch 10 batch 100 train loss: 0.0118 test loss: 0.0098\n",
      "Epoch 10 batch 200 train loss: 0.0061 test loss: 0.0101\n",
      "Epoch 10 batch 300 train loss: 0.0095 test loss: 0.0099\n",
      "Epoch 10 batch 400 train loss: 0.0049 test loss: 0.0098\n",
      "Epoch 10 batch 500 train loss: 0.0092 test loss: 0.0100\n",
      "Epoch 10 batch 600 train loss: 0.0088 test loss: 0.0100\n",
      "Epoch 10 batch 700 train loss: 0.0087 test loss: 0.0102\n",
      "Epoch 10 batch 800 train loss: 0.0121 test loss: 0.0099\n",
      "Epoch 10 batch 900 train loss: 0.0116 test loss: 0.0097\n",
      "Epoch 10 batch 1000 train loss: 0.0114 test loss: 0.0100\n",
      "Epoch 10 batch 1100 train loss: 0.0111 test loss: 0.0099\n",
      "Epoch 10 batch 1200 train loss: 0.0042 test loss: 0.0095\n",
      "Epoch 10 batch 1300 train loss: 0.0103 test loss: 0.0099\n",
      "Epoch 10 batch 1400 train loss: 0.0127 test loss: 0.0097\n",
      "Epoch 10 batch 1500 train loss: 0.0105 test loss: 0.0102\n",
      "Epoch 10 batch 1600 train loss: 0.0064 test loss: 0.0102\n",
      "Epoch 10 batch 1700 train loss: 0.0129 test loss: 0.0099\n",
      "Epoch 10 batch 1800 train loss: 0.0137 test loss: 0.0095\n",
      "Epoch 10 batch 1900 train loss: 0.0112 test loss: 0.0100\n",
      "Epoch 10 batch 2000 train loss: 0.0087 test loss: 0.0097\n",
      "Epoch 10 batch 2100 train loss: 0.0072 test loss: 0.0100\n",
      "Epoch 10 batch 2200 train loss: 0.0158 test loss: 0.0102\n",
      "Epoch 10 batch 2300 train loss: 0.0127 test loss: 0.0096\n",
      "Epoch 10 batch 2400 train loss: 0.0089 test loss: 0.0099\n",
      "Epoch 10 batch 2500 train loss: 0.0115 test loss: 0.0100\n",
      "Epoch 10 batch 2600 train loss: 0.0125 test loss: 0.0098\n",
      "Epoch 10 batch 2700 train loss: 0.0080 test loss: 0.0101\n",
      "Epoch 10 batch 2800 train loss: 0.0129 test loss: 0.0102\n",
      "Epoch 10 batch 2900 train loss: 0.0096 test loss: 0.0101\n",
      "Epoch 10 batch 3000 train loss: 0.0096 test loss: 0.0102\n",
      "Epoch 11 batch 0 train loss: 0.0102 test loss: 0.0098\n",
      "Epoch 11 batch 100 train loss: 0.0055 test loss: 0.0098\n",
      "Epoch 11 batch 200 train loss: 0.0106 test loss: 0.0098\n",
      "Epoch 11 batch 300 train loss: 0.0111 test loss: 0.0099\n",
      "Epoch 11 batch 400 train loss: 0.0080 test loss: 0.0101\n",
      "Epoch 11 batch 500 train loss: 0.0087 test loss: 0.0099\n",
      "Epoch 11 batch 600 train loss: 0.0109 test loss: 0.0099\n",
      "Epoch 11 batch 700 train loss: 0.0108 test loss: 0.0101\n",
      "Epoch 11 batch 800 train loss: 0.0115 test loss: 0.0098\n",
      "Epoch 11 batch 900 train loss: 0.0076 test loss: 0.0099\n",
      "Epoch 11 batch 1000 train loss: 0.0056 test loss: 0.0100\n",
      "Epoch 11 batch 1100 train loss: 0.0096 test loss: 0.0100\n",
      "Epoch 11 batch 1200 train loss: 0.0102 test loss: 0.0097\n",
      "Epoch 11 batch 1300 train loss: 0.0114 test loss: 0.0100\n",
      "Epoch 11 batch 1400 train loss: 0.0115 test loss: 0.0097\n",
      "Epoch 11 batch 1500 train loss: 0.0120 test loss: 0.0098\n",
      "Epoch 11 batch 1600 train loss: 0.0108 test loss: 0.0102\n",
      "Epoch 11 batch 1700 train loss: 0.0084 test loss: 0.0099\n",
      "Epoch 11 batch 1800 train loss: 0.0112 test loss: 0.0095\n",
      "Epoch 11 batch 1900 train loss: 0.0102 test loss: 0.0100\n",
      "Epoch 11 batch 2000 train loss: 0.0070 test loss: 0.0099\n",
      "Epoch 11 batch 2100 train loss: 0.0089 test loss: 0.0100\n",
      "Epoch 11 batch 2200 train loss: 0.0079 test loss: 0.0100\n",
      "Epoch 11 batch 2300 train loss: 0.0091 test loss: 0.0099\n",
      "Epoch 11 batch 2400 train loss: 0.0127 test loss: 0.0101\n",
      "Epoch 11 batch 2500 train loss: 0.0079 test loss: 0.0097\n",
      "Epoch 11 batch 2600 train loss: 0.0098 test loss: 0.0097\n",
      "Epoch 11 batch 2700 train loss: 0.0088 test loss: 0.0101\n",
      "Epoch 11 batch 2800 train loss: 0.0067 test loss: 0.0100\n",
      "Epoch 11 batch 2900 train loss: 0.0096 test loss: 0.0098\n",
      "Epoch 11 batch 3000 train loss: 0.0141 test loss: 0.0100\n",
      "Epoch 12 batch 0 train loss: 0.0102 test loss: 0.0098\n",
      "Epoch 12 batch 100 train loss: 0.0091 test loss: 0.0103\n",
      "Epoch 12 batch 200 train loss: 0.0058 test loss: 0.0097\n",
      "Epoch 12 batch 300 train loss: 0.0054 test loss: 0.0100\n",
      "Epoch 12 batch 400 train loss: 0.0069 test loss: 0.0101\n",
      "Epoch 12 batch 500 train loss: 0.0130 test loss: 0.0100\n",
      "Epoch 12 batch 600 train loss: 0.0099 test loss: 0.0102\n",
      "Epoch 12 batch 700 train loss: 0.0078 test loss: 0.0096\n",
      "Epoch 12 batch 800 train loss: 0.0154 test loss: 0.0101\n",
      "Epoch 12 batch 900 train loss: 0.0119 test loss: 0.0099\n",
      "Epoch 12 batch 1000 train loss: 0.0067 test loss: 0.0096\n",
      "Epoch 12 batch 1100 train loss: 0.0089 test loss: 0.0101\n",
      "Epoch 12 batch 1200 train loss: 0.0107 test loss: 0.0098\n",
      "Epoch 12 batch 1300 train loss: 0.0068 test loss: 0.0101\n",
      "Epoch 12 batch 1400 train loss: 0.0070 test loss: 0.0097\n",
      "Epoch 12 batch 1500 train loss: 0.0082 test loss: 0.0099\n",
      "Epoch 12 batch 1600 train loss: 0.0109 test loss: 0.0102\n",
      "Epoch 12 batch 1700 train loss: 0.0127 test loss: 0.0101\n",
      "Epoch 12 batch 1800 train loss: 0.0109 test loss: 0.0095\n",
      "Epoch 12 batch 1900 train loss: 0.0052 test loss: 0.0099\n",
      "Epoch 12 batch 2000 train loss: 0.0077 test loss: 0.0096\n",
      "Epoch 12 batch 2100 train loss: 0.0095 test loss: 0.0096\n",
      "Epoch 12 batch 2200 train loss: 0.0050 test loss: 0.0101\n",
      "Epoch 12 batch 2300 train loss: 0.0090 test loss: 0.0098\n",
      "Epoch 12 batch 2400 train loss: 0.0111 test loss: 0.0097\n",
      "Epoch 12 batch 2500 train loss: 0.0098 test loss: 0.0097\n",
      "Epoch 12 batch 2600 train loss: 0.0074 test loss: 0.0100\n",
      "Epoch 12 batch 2700 train loss: 0.0071 test loss: 0.0101\n",
      "Epoch 12 batch 2800 train loss: 0.0119 test loss: 0.0103\n",
      "Epoch 12 batch 2900 train loss: 0.0109 test loss: 0.0098\n",
      "Epoch 12 batch 3000 train loss: 0.0113 test loss: 0.0098\n",
      "Epoch 13 batch 0 train loss: 0.0178 test loss: 0.0097\n",
      "Epoch 13 batch 100 train loss: 0.0089 test loss: 0.0103\n",
      "Epoch 13 batch 200 train loss: 0.0101 test loss: 0.0099\n",
      "Epoch 13 batch 300 train loss: 0.0074 test loss: 0.0099\n",
      "Epoch 13 batch 400 train loss: 0.0122 test loss: 0.0096\n",
      "Epoch 13 batch 500 train loss: 0.0151 test loss: 0.0101\n",
      "Epoch 13 batch 600 train loss: 0.0065 test loss: 0.0098\n",
      "Epoch 13 batch 700 train loss: 0.0126 test loss: 0.0098\n",
      "Epoch 13 batch 800 train loss: 0.0094 test loss: 0.0096\n",
      "Epoch 13 batch 900 train loss: 0.0125 test loss: 0.0099\n",
      "Epoch 13 batch 1000 train loss: 0.0089 test loss: 0.0100\n",
      "Epoch 13 batch 1100 train loss: 0.0071 test loss: 0.0101\n",
      "Epoch 13 batch 1200 train loss: 0.0126 test loss: 0.0098\n",
      "Epoch 13 batch 1300 train loss: 0.0108 test loss: 0.0100\n",
      "Epoch 13 batch 1400 train loss: 0.0073 test loss: 0.0098\n",
      "Epoch 13 batch 1500 train loss: 0.0115 test loss: 0.0101\n",
      "Epoch 13 batch 1600 train loss: 0.0111 test loss: 0.0098\n",
      "Epoch 13 batch 1700 train loss: 0.0091 test loss: 0.0099\n",
      "Epoch 13 batch 1800 train loss: 0.0077 test loss: 0.0097\n",
      "Epoch 13 batch 1900 train loss: 0.0105 test loss: 0.0098\n",
      "Epoch 13 batch 2000 train loss: 0.0059 test loss: 0.0096\n",
      "Epoch 13 batch 2100 train loss: 0.0152 test loss: 0.0097\n",
      "Epoch 13 batch 2200 train loss: 0.0052 test loss: 0.0102\n",
      "Epoch 13 batch 2300 train loss: 0.0104 test loss: 0.0097\n",
      "Epoch 13 batch 2400 train loss: 0.0078 test loss: 0.0097\n",
      "Epoch 13 batch 2500 train loss: 0.0087 test loss: 0.0100\n",
      "Epoch 13 batch 2600 train loss: 0.0120 test loss: 0.0099\n",
      "Epoch 13 batch 2700 train loss: 0.0124 test loss: 0.0101\n",
      "Epoch 13 batch 2800 train loss: 0.0085 test loss: 0.0105\n",
      "Epoch 13 batch 2900 train loss: 0.0080 test loss: 0.0098\n",
      "Epoch 13 batch 3000 train loss: 0.0086 test loss: 0.0098\n",
      "Epoch 14 batch 0 train loss: 0.0102 test loss: 0.0096\n",
      "Epoch 14 batch 100 train loss: 0.0097 test loss: 0.0100\n",
      "Epoch 14 batch 200 train loss: 0.0117 test loss: 0.0100\n",
      "Epoch 14 batch 300 train loss: 0.0117 test loss: 0.0095\n",
      "Epoch 14 batch 400 train loss: 0.0120 test loss: 0.0099\n",
      "Epoch 14 batch 500 train loss: 0.0083 test loss: 0.0101\n",
      "Epoch 14 batch 600 train loss: 0.0107 test loss: 0.0101\n",
      "Epoch 14 batch 700 train loss: 0.0093 test loss: 0.0097\n",
      "Epoch 14 batch 800 train loss: 0.0089 test loss: 0.0096\n",
      "Epoch 14 batch 900 train loss: 0.0127 test loss: 0.0102\n",
      "Epoch 14 batch 1000 train loss: 0.0076 test loss: 0.0100\n",
      "Epoch 14 batch 1100 train loss: 0.0100 test loss: 0.0098\n",
      "Epoch 14 batch 1200 train loss: 0.0102 test loss: 0.0099\n",
      "Epoch 14 batch 1300 train loss: 0.0131 test loss: 0.0103\n",
      "Epoch 14 batch 1400 train loss: 0.0080 test loss: 0.0099\n",
      "Epoch 14 batch 1500 train loss: 0.0085 test loss: 0.0098\n",
      "Epoch 14 batch 1600 train loss: 0.0067 test loss: 0.0103\n",
      "Epoch 14 batch 1700 train loss: 0.0100 test loss: 0.0099\n",
      "Epoch 14 batch 1800 train loss: 0.0067 test loss: 0.0098\n",
      "Epoch 14 batch 1900 train loss: 0.0097 test loss: 0.0096\n",
      "Epoch 14 batch 2000 train loss: 0.0104 test loss: 0.0097\n",
      "Epoch 14 batch 2100 train loss: 0.0056 test loss: 0.0096\n",
      "Epoch 14 batch 2200 train loss: 0.0088 test loss: 0.0100\n",
      "Epoch 14 batch 2300 train loss: 0.0065 test loss: 0.0098\n",
      "Epoch 14 batch 2400 train loss: 0.0081 test loss: 0.0100\n",
      "Epoch 14 batch 2500 train loss: 0.0127 test loss: 0.0099\n",
      "Epoch 14 batch 2600 train loss: 0.0070 test loss: 0.0099\n",
      "Epoch 14 batch 2700 train loss: 0.0129 test loss: 0.0100\n",
      "Epoch 14 batch 2800 train loss: 0.0076 test loss: 0.0102\n",
      "Epoch 14 batch 2900 train loss: 0.0061 test loss: 0.0100\n",
      "Epoch 14 batch 3000 train loss: 0.0078 test loss: 0.0100\n",
      "Epoch 15 batch 0 train loss: 0.0115 test loss: 0.0099\n",
      "Epoch 15 batch 100 train loss: 0.0089 test loss: 0.0096\n",
      "Epoch 15 batch 200 train loss: 0.0102 test loss: 0.0102\n",
      "Epoch 15 batch 300 train loss: 0.0087 test loss: 0.0099\n",
      "Epoch 15 batch 400 train loss: 0.0090 test loss: 0.0097\n",
      "Epoch 15 batch 500 train loss: 0.0105 test loss: 0.0098\n",
      "Epoch 15 batch 600 train loss: 0.0104 test loss: 0.0102\n",
      "Epoch 15 batch 700 train loss: 0.0065 test loss: 0.0098\n",
      "Epoch 15 batch 800 train loss: 0.0112 test loss: 0.0096\n",
      "Epoch 15 batch 900 train loss: 0.0070 test loss: 0.0100\n",
      "Epoch 15 batch 1000 train loss: 0.0148 test loss: 0.0099\n",
      "Epoch 15 batch 1100 train loss: 0.0122 test loss: 0.0099\n",
      "Epoch 15 batch 1200 train loss: 0.0095 test loss: 0.0100\n",
      "Epoch 15 batch 1300 train loss: 0.0079 test loss: 0.0101\n",
      "Epoch 15 batch 1400 train loss: 0.0117 test loss: 0.0100\n",
      "Epoch 15 batch 1500 train loss: 0.0109 test loss: 0.0096\n",
      "Epoch 15 batch 1600 train loss: 0.0066 test loss: 0.0099\n",
      "Epoch 15 batch 1700 train loss: 0.0081 test loss: 0.0100\n",
      "Epoch 15 batch 1800 train loss: 0.0104 test loss: 0.0093\n",
      "Epoch 15 batch 1900 train loss: 0.0108 test loss: 0.0098\n",
      "Epoch 15 batch 2000 train loss: 0.0098 test loss: 0.0097\n",
      "Epoch 15 batch 2100 train loss: 0.0082 test loss: 0.0098\n",
      "Epoch 15 batch 2200 train loss: 0.0048 test loss: 0.0100\n",
      "Epoch 15 batch 2300 train loss: 0.0108 test loss: 0.0100\n",
      "Epoch 15 batch 2400 train loss: 0.0081 test loss: 0.0095\n",
      "Epoch 15 batch 2500 train loss: 0.0107 test loss: 0.0097\n",
      "Epoch 15 batch 2600 train loss: 0.0111 test loss: 0.0099\n",
      "Epoch 15 batch 2700 train loss: 0.0092 test loss: 0.0105\n",
      "Epoch 15 batch 2800 train loss: 0.0076 test loss: 0.0101\n",
      "Epoch 15 batch 2900 train loss: 0.0100 test loss: 0.0100\n",
      "Epoch 15 batch 3000 train loss: 0.0092 test loss: 0.0094\n",
      "Epoch 16 batch 0 train loss: 0.0075 test loss: 0.0096\n",
      "Epoch 16 batch 100 train loss: 0.0068 test loss: 0.0100\n",
      "Epoch 16 batch 200 train loss: 0.0065 test loss: 0.0101\n",
      "Epoch 16 batch 300 train loss: 0.0100 test loss: 0.0100\n",
      "Epoch 16 batch 400 train loss: 0.0094 test loss: 0.0099\n",
      "Epoch 16 batch 500 train loss: 0.0138 test loss: 0.0102\n",
      "Epoch 16 batch 600 train loss: 0.0154 test loss: 0.0104\n",
      "Epoch 16 batch 700 train loss: 0.0104 test loss: 0.0096\n",
      "Epoch 16 batch 800 train loss: 0.0115 test loss: 0.0102\n",
      "Epoch 16 batch 900 train loss: 0.0118 test loss: 0.0098\n",
      "Epoch 16 batch 1000 train loss: 0.0119 test loss: 0.0098\n",
      "Epoch 16 batch 1100 train loss: 0.0131 test loss: 0.0099\n",
      "Epoch 16 batch 1200 train loss: 0.0115 test loss: 0.0100\n",
      "Epoch 16 batch 1300 train loss: 0.0131 test loss: 0.0102\n",
      "Epoch 16 batch 1400 train loss: 0.0089 test loss: 0.0101\n",
      "Epoch 16 batch 1500 train loss: 0.0138 test loss: 0.0100\n",
      "Epoch 16 batch 1600 train loss: 0.0091 test loss: 0.0103\n",
      "Epoch 16 batch 1700 train loss: 0.0094 test loss: 0.0102\n",
      "Epoch 16 batch 1800 train loss: 0.0109 test loss: 0.0098\n",
      "Epoch 16 batch 1900 train loss: 0.0111 test loss: 0.0098\n",
      "Epoch 16 batch 2000 train loss: 0.0139 test loss: 0.0100\n",
      "Epoch 16 batch 2100 train loss: 0.0134 test loss: 0.0096\n",
      "Epoch 16 batch 2200 train loss: 0.0086 test loss: 0.0097\n",
      "Epoch 16 batch 2300 train loss: 0.0082 test loss: 0.0100\n",
      "Epoch 16 batch 2400 train loss: 0.0103 test loss: 0.0102\n",
      "Epoch 16 batch 2500 train loss: 0.0088 test loss: 0.0097\n",
      "Epoch 16 batch 2600 train loss: 0.0079 test loss: 0.0100\n",
      "Epoch 16 batch 2700 train loss: 0.0142 test loss: 0.0101\n",
      "Epoch 16 batch 2800 train loss: 0.0120 test loss: 0.0101\n",
      "Epoch 16 batch 2900 train loss: 0.0094 test loss: 0.0096\n",
      "Epoch 16 batch 3000 train loss: 0.0059 test loss: 0.0099\n",
      "Epoch 17 batch 0 train loss: 0.0140 test loss: 0.0100\n",
      "Epoch 17 batch 100 train loss: 0.0100 test loss: 0.0097\n",
      "Epoch 17 batch 200 train loss: 0.0143 test loss: 0.0102\n",
      "Epoch 17 batch 300 train loss: 0.0111 test loss: 0.0098\n",
      "Epoch 17 batch 400 train loss: 0.0092 test loss: 0.0098\n",
      "Epoch 17 batch 500 train loss: 0.0102 test loss: 0.0100\n",
      "early stop.\n",
      "Checkpoint 33 restored!!\n",
      "Training for loss rate 0.60 start.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ffnn_5/dense_15/kernel:0', 'ffnn_5/dense_15/bias:0', 'ffnn_5/batch_normalization_5/gamma:0', 'ffnn_5/batch_normalization_5/beta:0', 'ffnn_5/dense_16/kernel:0', 'ffnn_5/dense_16/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ffnn_5/dense_15/kernel:0', 'ffnn_5/dense_15/bias:0', 'ffnn_5/batch_normalization_5/gamma:0', 'ffnn_5/batch_normalization_5/beta:0', 'ffnn_5/dense_16/kernel:0', 'ffnn_5/dense_16/bias:0'] when minimizing the loss.\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.60p/ckpt-1\n",
      "Epoch 0 batch 0 train loss: 0.2863 test loss: 0.3207\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.60p/ckpt-2\n",
      "Epoch 0 batch 100 train loss: 0.1595 test loss: 0.2042\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.60p/ckpt-3\n",
      "Epoch 0 batch 200 train loss: 0.1231 test loss: 0.1465\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.60p/ckpt-4\n",
      "Epoch 0 batch 300 train loss: 0.0759 test loss: 0.1135\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.60p/ckpt-5\n",
      "Epoch 0 batch 400 train loss: 0.0545 test loss: 0.0900\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.60p/ckpt-6\n",
      "Epoch 0 batch 500 train loss: 0.0454 test loss: 0.0715\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.60p/ckpt-7\n",
      "Epoch 0 batch 600 train loss: 0.0394 test loss: 0.0574\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.60p/ckpt-8\n",
      "Epoch 0 batch 700 train loss: 0.0407 test loss: 0.0468\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.60p/ckpt-9\n",
      "Epoch 0 batch 800 train loss: 0.0245 test loss: 0.0383\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.60p/ckpt-10\n",
      "Epoch 0 batch 900 train loss: 0.0226 test loss: 0.0318\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.60p/ckpt-11\n",
      "Epoch 0 batch 1000 train loss: 0.0225 test loss: 0.0266\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.60p/ckpt-12\n",
      "Epoch 0 batch 1100 train loss: 0.0142 test loss: 0.0228\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.60p/ckpt-13\n",
      "Epoch 0 batch 1200 train loss: 0.0225 test loss: 0.0199\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.60p/ckpt-14\n",
      "Epoch 0 batch 1300 train loss: 0.0139 test loss: 0.0179\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.60p/ckpt-15\n",
      "Epoch 0 batch 1400 train loss: 0.0149 test loss: 0.0162\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.60p/ckpt-16\n",
      "Epoch 0 batch 1500 train loss: 0.0136 test loss: 0.0152\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.60p/ckpt-17\n",
      "Epoch 0 batch 1600 train loss: 0.0103 test loss: 0.0145\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.60p/ckpt-18\n",
      "Epoch 0 batch 1700 train loss: 0.0105 test loss: 0.0137\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.60p/ckpt-19\n",
      "Epoch 0 batch 1800 train loss: 0.0084 test loss: 0.0131\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.60p/ckpt-20\n",
      "Epoch 0 batch 1900 train loss: 0.0139 test loss: 0.0126\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.60p/ckpt-21\n",
      "Epoch 0 batch 2000 train loss: 0.0131 test loss: 0.0124\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.60p/ckpt-22\n",
      "Epoch 0 batch 2100 train loss: 0.0088 test loss: 0.0123\n",
      "Epoch 0 batch 2200 train loss: 0.0109 test loss: 0.0123\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.60p/ckpt-23\n",
      "Epoch 0 batch 2300 train loss: 0.0128 test loss: 0.0122\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.60p/ckpt-24\n",
      "Epoch 0 batch 2400 train loss: 0.0120 test loss: 0.0120\n",
      "Epoch 0 batch 2500 train loss: 0.0124 test loss: 0.0121\n",
      "Epoch 0 batch 2600 train loss: 0.0078 test loss: 0.0121\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.60p/ckpt-25\n",
      "Epoch 0 batch 2700 train loss: 0.0089 test loss: 0.0120\n",
      "Epoch 0 batch 2800 train loss: 0.0086 test loss: 0.0121\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.60p/ckpt-26\n",
      "Epoch 0 batch 2900 train loss: 0.0164 test loss: 0.0118\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.60p/ckpt-27\n",
      "Epoch 0 batch 3000 train loss: 0.0114 test loss: 0.0118\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ffnn_5/dense_15/kernel:0', 'ffnn_5/dense_15/bias:0', 'ffnn_5/batch_normalization_5/gamma:0', 'ffnn_5/batch_normalization_5/beta:0', 'ffnn_5/dense_16/kernel:0', 'ffnn_5/dense_16/bias:0'] when minimizing the loss.\n",
      "Epoch 1 batch 0 train loss: 0.0113 test loss: 0.0120\n",
      "Epoch 1 batch 100 train loss: 0.0093 test loss: 0.0120\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.60p/ckpt-28\n",
      "Epoch 1 batch 200 train loss: 0.0144 test loss: 0.0118\n",
      "Epoch 1 batch 300 train loss: 0.0138 test loss: 0.0120\n",
      "Epoch 1 batch 400 train loss: 0.0100 test loss: 0.0118\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.60p/ckpt-29\n",
      "Epoch 1 batch 500 train loss: 0.0102 test loss: 0.0116\n",
      "Epoch 1 batch 600 train loss: 0.0119 test loss: 0.0118\n",
      "Epoch 1 batch 700 train loss: 0.0073 test loss: 0.0118\n",
      "Epoch 1 batch 800 train loss: 0.0108 test loss: 0.0116\n",
      "Epoch 1 batch 900 train loss: 0.0130 test loss: 0.0117\n",
      "Epoch 1 batch 1000 train loss: 0.0180 test loss: 0.0117\n",
      "Epoch 1 batch 1100 train loss: 0.0100 test loss: 0.0120\n",
      "Epoch 1 batch 1200 train loss: 0.0121 test loss: 0.0117\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.60p/ckpt-30\n",
      "Epoch 1 batch 1300 train loss: 0.0094 test loss: 0.0113\n",
      "Epoch 1 batch 1400 train loss: 0.0123 test loss: 0.0116\n",
      "Epoch 1 batch 1500 train loss: 0.0124 test loss: 0.0114\n",
      "Epoch 1 batch 1600 train loss: 0.0073 test loss: 0.0120\n",
      "Epoch 1 batch 1700 train loss: 0.0134 test loss: 0.0119\n",
      "Epoch 1 batch 1800 train loss: 0.0090 test loss: 0.0115\n",
      "Epoch 1 batch 1900 train loss: 0.0086 test loss: 0.0118\n",
      "Epoch 1 batch 2000 train loss: 0.0086 test loss: 0.0115\n",
      "Epoch 1 batch 2100 train loss: 0.0074 test loss: 0.0118\n",
      "Epoch 1 batch 2200 train loss: 0.0070 test loss: 0.0116\n",
      "Epoch 1 batch 2300 train loss: 0.0077 test loss: 0.0118\n",
      "Epoch 1 batch 2400 train loss: 0.0149 test loss: 0.0118\n",
      "Epoch 1 batch 2500 train loss: 0.0082 test loss: 0.0117\n",
      "Epoch 1 batch 2600 train loss: 0.0138 test loss: 0.0116\n",
      "Epoch 1 batch 2700 train loss: 0.0098 test loss: 0.0119\n",
      "Epoch 1 batch 2800 train loss: 0.0117 test loss: 0.0118\n",
      "Epoch 1 batch 2900 train loss: 0.0118 test loss: 0.0119\n",
      "Epoch 1 batch 3000 train loss: 0.0078 test loss: 0.0116\n",
      "Epoch 2 batch 0 train loss: 0.0101 test loss: 0.0116\n",
      "Epoch 2 batch 100 train loss: 0.0145 test loss: 0.0120\n",
      "Epoch 2 batch 200 train loss: 0.0111 test loss: 0.0116\n",
      "Epoch 2 batch 300 train loss: 0.0139 test loss: 0.0117\n",
      "Epoch 2 batch 400 train loss: 0.0127 test loss: 0.0117\n",
      "Epoch 2 batch 500 train loss: 0.0104 test loss: 0.0114\n",
      "Epoch 2 batch 600 train loss: 0.0152 test loss: 0.0118\n",
      "Epoch 2 batch 700 train loss: 0.0065 test loss: 0.0117\n",
      "Epoch 2 batch 800 train loss: 0.0146 test loss: 0.0115\n",
      "Epoch 2 batch 900 train loss: 0.0077 test loss: 0.0115\n",
      "Epoch 2 batch 1000 train loss: 0.0124 test loss: 0.0117\n",
      "Epoch 2 batch 1100 train loss: 0.0131 test loss: 0.0116\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.60p/ckpt-31\n",
      "Epoch 2 batch 1200 train loss: 0.0146 test loss: 0.0113\n",
      "Epoch 2 batch 1300 train loss: 0.0120 test loss: 0.0118\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.60p/ckpt-32\n",
      "Epoch 2 batch 1400 train loss: 0.0103 test loss: 0.0112\n",
      "Epoch 2 batch 1500 train loss: 0.0096 test loss: 0.0117\n",
      "Epoch 2 batch 1600 train loss: 0.0118 test loss: 0.0117\n",
      "Epoch 2 batch 1700 train loss: 0.0130 test loss: 0.0118\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.60p/ckpt-33\n",
      "Epoch 2 batch 1800 train loss: 0.0088 test loss: 0.0112\n",
      "Epoch 2 batch 1900 train loss: 0.0090 test loss: 0.0114\n",
      "Epoch 2 batch 2000 train loss: 0.0118 test loss: 0.0113\n",
      "Epoch 2 batch 2100 train loss: 0.0128 test loss: 0.0119\n",
      "Epoch 2 batch 2200 train loss: 0.0054 test loss: 0.0117\n",
      "Epoch 2 batch 2300 train loss: 0.0089 test loss: 0.0115\n",
      "Epoch 2 batch 2400 train loss: 0.0089 test loss: 0.0115\n",
      "Epoch 2 batch 2500 train loss: 0.0159 test loss: 0.0114\n",
      "Epoch 2 batch 2600 train loss: 0.0182 test loss: 0.0117\n",
      "Epoch 2 batch 2700 train loss: 0.0120 test loss: 0.0120\n",
      "Epoch 2 batch 2800 train loss: 0.0156 test loss: 0.0119\n",
      "Epoch 2 batch 2900 train loss: 0.0123 test loss: 0.0116\n",
      "Epoch 2 batch 3000 train loss: 0.0123 test loss: 0.0114\n",
      "Epoch 3 batch 0 train loss: 0.0076 test loss: 0.0117\n",
      "Epoch 3 batch 100 train loss: 0.0093 test loss: 0.0117\n",
      "Epoch 3 batch 200 train loss: 0.0104 test loss: 0.0114\n",
      "Epoch 3 batch 300 train loss: 0.0094 test loss: 0.0118\n",
      "Epoch 3 batch 400 train loss: 0.0141 test loss: 0.0117\n",
      "Epoch 3 batch 500 train loss: 0.0120 test loss: 0.0114\n",
      "Epoch 3 batch 600 train loss: 0.0162 test loss: 0.0116\n",
      "Epoch 3 batch 700 train loss: 0.0084 test loss: 0.0114\n",
      "Epoch 3 batch 800 train loss: 0.0072 test loss: 0.0116\n",
      "Epoch 3 batch 900 train loss: 0.0116 test loss: 0.0114\n",
      "Epoch 3 batch 1000 train loss: 0.0095 test loss: 0.0118\n",
      "Epoch 3 batch 1100 train loss: 0.0108 test loss: 0.0113\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.60p/ckpt-34\n",
      "Epoch 3 batch 1200 train loss: 0.0094 test loss: 0.0112\n",
      "Epoch 3 batch 1300 train loss: 0.0132 test loss: 0.0118\n",
      "Epoch 3 batch 1400 train loss: 0.0116 test loss: 0.0114\n",
      "Epoch 3 batch 1500 train loss: 0.0169 test loss: 0.0115\n",
      "Epoch 3 batch 1600 train loss: 0.0075 test loss: 0.0118\n",
      "Epoch 3 batch 1700 train loss: 0.0142 test loss: 0.0117\n",
      "Epoch 3 batch 1800 train loss: 0.0105 test loss: 0.0116\n",
      "Epoch 3 batch 1900 train loss: 0.0129 test loss: 0.0113\n",
      "Epoch 3 batch 2000 train loss: 0.0147 test loss: 0.0114\n",
      "Epoch 3 batch 2100 train loss: 0.0151 test loss: 0.0115\n",
      "Epoch 3 batch 2200 train loss: 0.0079 test loss: 0.0118\n",
      "Epoch 3 batch 2300 train loss: 0.0121 test loss: 0.0115\n",
      "Epoch 3 batch 2400 train loss: 0.0125 test loss: 0.0113\n",
      "Epoch 3 batch 2500 train loss: 0.0060 test loss: 0.0118\n",
      "Epoch 3 batch 2600 train loss: 0.0077 test loss: 0.0114\n",
      "Epoch 3 batch 2700 train loss: 0.0141 test loss: 0.0124\n",
      "Epoch 3 batch 2800 train loss: 0.0120 test loss: 0.0117\n",
      "Epoch 3 batch 2900 train loss: 0.0172 test loss: 0.0116\n",
      "Epoch 3 batch 3000 train loss: 0.0075 test loss: 0.0115\n",
      "Epoch 4 batch 0 train loss: 0.0131 test loss: 0.0115\n",
      "Epoch 4 batch 100 train loss: 0.0090 test loss: 0.0116\n",
      "Epoch 4 batch 200 train loss: 0.0082 test loss: 0.0115\n",
      "Epoch 4 batch 300 train loss: 0.0094 test loss: 0.0113\n",
      "Epoch 4 batch 400 train loss: 0.0086 test loss: 0.0116\n",
      "Epoch 4 batch 500 train loss: 0.0087 test loss: 0.0114\n",
      "Epoch 4 batch 600 train loss: 0.0075 test loss: 0.0116\n",
      "Epoch 4 batch 700 train loss: 0.0113 test loss: 0.0113\n",
      "Epoch 4 batch 800 train loss: 0.0101 test loss: 0.0114\n",
      "Epoch 4 batch 900 train loss: 0.0113 test loss: 0.0117\n",
      "Epoch 4 batch 1000 train loss: 0.0107 test loss: 0.0117\n",
      "Epoch 4 batch 1100 train loss: 0.0124 test loss: 0.0114\n",
      "Epoch 4 batch 1200 train loss: 0.0116 test loss: 0.0113\n",
      "Epoch 4 batch 1300 train loss: 0.0080 test loss: 0.0117\n",
      "Epoch 4 batch 1400 train loss: 0.0121 test loss: 0.0115\n",
      "Epoch 4 batch 1500 train loss: 0.0096 test loss: 0.0112\n",
      "Epoch 4 batch 1600 train loss: 0.0088 test loss: 0.0119\n",
      "Epoch 4 batch 1700 train loss: 0.0141 test loss: 0.0118\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.60p/ckpt-35\n",
      "Epoch 4 batch 1800 train loss: 0.0091 test loss: 0.0111\n",
      "Epoch 4 batch 1900 train loss: 0.0157 test loss: 0.0113\n",
      "Epoch 4 batch 2000 train loss: 0.0150 test loss: 0.0115\n",
      "Epoch 4 batch 2100 train loss: 0.0090 test loss: 0.0115\n",
      "Epoch 4 batch 2200 train loss: 0.0130 test loss: 0.0117\n",
      "Epoch 4 batch 2300 train loss: 0.0125 test loss: 0.0114\n",
      "Epoch 4 batch 2400 train loss: 0.0112 test loss: 0.0119\n",
      "Epoch 4 batch 2500 train loss: 0.0125 test loss: 0.0115\n",
      "Epoch 4 batch 2600 train loss: 0.0097 test loss: 0.0117\n",
      "Epoch 4 batch 2700 train loss: 0.0106 test loss: 0.0120\n",
      "Epoch 4 batch 2800 train loss: 0.0068 test loss: 0.0115\n",
      "Epoch 4 batch 2900 train loss: 0.0129 test loss: 0.0113\n",
      "Epoch 4 batch 3000 train loss: 0.0114 test loss: 0.0114\n",
      "Epoch 5 batch 0 train loss: 0.0109 test loss: 0.0113\n",
      "Epoch 5 batch 100 train loss: 0.0084 test loss: 0.0117\n",
      "Epoch 5 batch 200 train loss: 0.0081 test loss: 0.0116\n",
      "Epoch 5 batch 300 train loss: 0.0086 test loss: 0.0115\n",
      "Epoch 5 batch 400 train loss: 0.0140 test loss: 0.0113\n",
      "Epoch 5 batch 500 train loss: 0.0129 test loss: 0.0117\n",
      "Epoch 5 batch 600 train loss: 0.0143 test loss: 0.0116\n",
      "Epoch 5 batch 700 train loss: 0.0094 test loss: 0.0114\n",
      "Epoch 5 batch 800 train loss: 0.0105 test loss: 0.0112\n",
      "Epoch 5 batch 900 train loss: 0.0079 test loss: 0.0115\n",
      "Epoch 5 batch 1000 train loss: 0.0092 test loss: 0.0116\n",
      "Epoch 5 batch 1100 train loss: 0.0116 test loss: 0.0115\n",
      "Epoch 5 batch 1200 train loss: 0.0151 test loss: 0.0115\n",
      "Epoch 5 batch 1300 train loss: 0.0172 test loss: 0.0116\n",
      "Epoch 5 batch 1400 train loss: 0.0116 test loss: 0.0117\n",
      "Epoch 5 batch 1500 train loss: 0.0089 test loss: 0.0115\n",
      "Epoch 5 batch 1600 train loss: 0.0116 test loss: 0.0116\n",
      "Epoch 5 batch 1700 train loss: 0.0117 test loss: 0.0116\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.60p/ckpt-36\n",
      "Epoch 5 batch 1800 train loss: 0.0093 test loss: 0.0110\n",
      "Epoch 5 batch 1900 train loss: 0.0095 test loss: 0.0116\n",
      "Epoch 5 batch 2000 train loss: 0.0084 test loss: 0.0113\n",
      "Epoch 5 batch 2100 train loss: 0.0097 test loss: 0.0114\n",
      "Epoch 5 batch 2200 train loss: 0.0138 test loss: 0.0119\n",
      "Epoch 5 batch 2300 train loss: 0.0106 test loss: 0.0116\n",
      "Epoch 5 batch 2400 train loss: 0.0089 test loss: 0.0114\n",
      "Epoch 5 batch 2500 train loss: 0.0082 test loss: 0.0117\n",
      "Epoch 5 batch 2600 train loss: 0.0058 test loss: 0.0114\n",
      "Epoch 5 batch 2700 train loss: 0.0078 test loss: 0.0123\n",
      "Epoch 5 batch 2800 train loss: 0.0109 test loss: 0.0112\n",
      "Epoch 5 batch 2900 train loss: 0.0077 test loss: 0.0117\n",
      "Epoch 5 batch 3000 train loss: 0.0160 test loss: 0.0114\n",
      "Epoch 6 batch 0 train loss: 0.0110 test loss: 0.0113\n",
      "Epoch 6 batch 100 train loss: 0.0119 test loss: 0.0117\n",
      "Epoch 6 batch 200 train loss: 0.0108 test loss: 0.0115\n",
      "Epoch 6 batch 300 train loss: 0.0104 test loss: 0.0113\n",
      "Epoch 6 batch 400 train loss: 0.0114 test loss: 0.0113\n",
      "Epoch 6 batch 500 train loss: 0.0130 test loss: 0.0117\n",
      "Epoch 6 batch 600 train loss: 0.0115 test loss: 0.0118\n",
      "Epoch 6 batch 700 train loss: 0.0068 test loss: 0.0112\n",
      "Epoch 6 batch 800 train loss: 0.0104 test loss: 0.0116\n",
      "Epoch 6 batch 900 train loss: 0.0132 test loss: 0.0116\n",
      "Epoch 6 batch 1000 train loss: 0.0121 test loss: 0.0116\n",
      "Epoch 6 batch 1100 train loss: 0.0125 test loss: 0.0116\n",
      "Epoch 6 batch 1200 train loss: 0.0125 test loss: 0.0115\n",
      "Epoch 6 batch 1300 train loss: 0.0206 test loss: 0.0117\n",
      "Epoch 6 batch 1400 train loss: 0.0169 test loss: 0.0110\n",
      "Epoch 6 batch 1500 train loss: 0.0093 test loss: 0.0113\n",
      "Epoch 6 batch 1600 train loss: 0.0077 test loss: 0.0117\n",
      "Epoch 6 batch 1700 train loss: 0.0138 test loss: 0.0116\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.60p/ckpt-37\n",
      "Epoch 6 batch 1800 train loss: 0.0141 test loss: 0.0109\n",
      "Epoch 6 batch 1900 train loss: 0.0085 test loss: 0.0115\n",
      "Epoch 6 batch 2000 train loss: 0.0127 test loss: 0.0115\n",
      "Epoch 6 batch 2100 train loss: 0.0131 test loss: 0.0114\n",
      "Epoch 6 batch 2200 train loss: 0.0134 test loss: 0.0115\n",
      "Epoch 6 batch 2300 train loss: 0.0142 test loss: 0.0115\n",
      "Epoch 6 batch 2400 train loss: 0.0096 test loss: 0.0116\n",
      "Epoch 6 batch 2500 train loss: 0.0138 test loss: 0.0117\n",
      "Epoch 6 batch 2600 train loss: 0.0139 test loss: 0.0118\n",
      "Epoch 6 batch 2700 train loss: 0.0100 test loss: 0.0118\n",
      "Epoch 6 batch 2800 train loss: 0.0063 test loss: 0.0119\n",
      "Epoch 6 batch 2900 train loss: 0.0070 test loss: 0.0114\n",
      "Epoch 6 batch 3000 train loss: 0.0097 test loss: 0.0117\n",
      "Epoch 7 batch 0 train loss: 0.0095 test loss: 0.0115\n",
      "Epoch 7 batch 100 train loss: 0.0090 test loss: 0.0118\n",
      "Epoch 7 batch 200 train loss: 0.0086 test loss: 0.0113\n",
      "Epoch 7 batch 300 train loss: 0.0116 test loss: 0.0116\n",
      "Epoch 7 batch 400 train loss: 0.0107 test loss: 0.0112\n",
      "Epoch 7 batch 500 train loss: 0.0110 test loss: 0.0115\n",
      "Epoch 7 batch 600 train loss: 0.0099 test loss: 0.0116\n",
      "Epoch 7 batch 700 train loss: 0.0093 test loss: 0.0113\n",
      "Epoch 7 batch 800 train loss: 0.0118 test loss: 0.0116\n",
      "Epoch 7 batch 900 train loss: 0.0159 test loss: 0.0122\n",
      "Epoch 7 batch 1000 train loss: 0.0097 test loss: 0.0115\n",
      "Epoch 7 batch 1100 train loss: 0.0088 test loss: 0.0118\n",
      "Epoch 7 batch 1200 train loss: 0.0126 test loss: 0.0116\n",
      "Epoch 7 batch 1300 train loss: 0.0102 test loss: 0.0113\n",
      "Epoch 7 batch 1400 train loss: 0.0109 test loss: 0.0114\n",
      "Epoch 7 batch 1500 train loss: 0.0175 test loss: 0.0116\n",
      "Epoch 7 batch 1600 train loss: 0.0172 test loss: 0.0116\n",
      "Epoch 7 batch 1700 train loss: 0.0084 test loss: 0.0119\n",
      "Epoch 7 batch 1800 train loss: 0.0086 test loss: 0.0111\n",
      "Epoch 7 batch 1900 train loss: 0.0108 test loss: 0.0114\n",
      "Epoch 7 batch 2000 train loss: 0.0139 test loss: 0.0114\n",
      "Epoch 7 batch 2100 train loss: 0.0132 test loss: 0.0121\n",
      "Epoch 7 batch 2200 train loss: 0.0076 test loss: 0.0117\n",
      "Epoch 7 batch 2300 train loss: 0.0085 test loss: 0.0115\n",
      "Epoch 7 batch 2400 train loss: 0.0104 test loss: 0.0115\n",
      "Epoch 7 batch 2500 train loss: 0.0115 test loss: 0.0116\n",
      "Epoch 7 batch 2600 train loss: 0.0075 test loss: 0.0116\n",
      "Epoch 7 batch 2700 train loss: 0.0112 test loss: 0.0119\n",
      "Epoch 7 batch 2800 train loss: 0.0095 test loss: 0.0114\n",
      "Epoch 7 batch 2900 train loss: 0.0143 test loss: 0.0117\n",
      "Epoch 7 batch 3000 train loss: 0.0100 test loss: 0.0114\n",
      "Epoch 8 batch 0 train loss: 0.0098 test loss: 0.0114\n",
      "Epoch 8 batch 100 train loss: 0.0124 test loss: 0.0116\n",
      "Epoch 8 batch 200 train loss: 0.0176 test loss: 0.0116\n",
      "Epoch 8 batch 300 train loss: 0.0121 test loss: 0.0117\n",
      "Epoch 8 batch 400 train loss: 0.0088 test loss: 0.0114\n",
      "Epoch 8 batch 500 train loss: 0.0122 test loss: 0.0115\n",
      "Epoch 8 batch 600 train loss: 0.0105 test loss: 0.0116\n",
      "Epoch 8 batch 700 train loss: 0.0105 test loss: 0.0114\n",
      "Epoch 8 batch 800 train loss: 0.0097 test loss: 0.0110\n",
      "Epoch 8 batch 900 train loss: 0.0132 test loss: 0.0118\n",
      "Epoch 8 batch 1000 train loss: 0.0082 test loss: 0.0114\n",
      "Epoch 8 batch 1100 train loss: 0.0085 test loss: 0.0116\n",
      "Epoch 8 batch 1200 train loss: 0.0112 test loss: 0.0113\n",
      "Epoch 8 batch 1300 train loss: 0.0145 test loss: 0.0113\n",
      "Epoch 8 batch 1400 train loss: 0.0129 test loss: 0.0116\n",
      "Epoch 8 batch 1500 train loss: 0.0076 test loss: 0.0116\n",
      "Epoch 8 batch 1600 train loss: 0.0117 test loss: 0.0116\n",
      "Epoch 8 batch 1700 train loss: 0.0085 test loss: 0.0117\n",
      "Epoch 8 batch 1800 train loss: 0.0127 test loss: 0.0110\n",
      "Epoch 8 batch 1900 train loss: 0.0078 test loss: 0.0115\n",
      "Epoch 8 batch 2000 train loss: 0.0073 test loss: 0.0115\n",
      "Epoch 8 batch 2100 train loss: 0.0135 test loss: 0.0114\n",
      "Epoch 8 batch 2200 train loss: 0.0091 test loss: 0.0116\n",
      "Epoch 8 batch 2300 train loss: 0.0102 test loss: 0.0116\n",
      "Epoch 8 batch 2400 train loss: 0.0098 test loss: 0.0114\n",
      "Epoch 8 batch 2500 train loss: 0.0110 test loss: 0.0115\n",
      "Epoch 8 batch 2600 train loss: 0.0113 test loss: 0.0115\n",
      "Epoch 8 batch 2700 train loss: 0.0133 test loss: 0.0120\n",
      "Epoch 8 batch 2800 train loss: 0.0118 test loss: 0.0116\n",
      "Epoch 8 batch 2900 train loss: 0.0127 test loss: 0.0117\n",
      "Epoch 8 batch 3000 train loss: 0.0088 test loss: 0.0112\n",
      "Epoch 9 batch 0 train loss: 0.0116 test loss: 0.0111\n",
      "Epoch 9 batch 100 train loss: 0.0104 test loss: 0.0115\n",
      "Epoch 9 batch 200 train loss: 0.0119 test loss: 0.0113\n",
      "Epoch 9 batch 300 train loss: 0.0093 test loss: 0.0115\n",
      "Epoch 9 batch 400 train loss: 0.0090 test loss: 0.0115\n",
      "Epoch 9 batch 500 train loss: 0.0120 test loss: 0.0116\n",
      "Epoch 9 batch 600 train loss: 0.0115 test loss: 0.0115\n",
      "Epoch 9 batch 700 train loss: 0.0107 test loss: 0.0114\n",
      "Epoch 9 batch 800 train loss: 0.0124 test loss: 0.0116\n",
      "Epoch 9 batch 900 train loss: 0.0129 test loss: 0.0118\n",
      "Epoch 9 batch 1000 train loss: 0.0113 test loss: 0.0115\n",
      "Epoch 9 batch 1100 train loss: 0.0113 test loss: 0.0117\n",
      "Epoch 9 batch 1200 train loss: 0.0127 test loss: 0.0115\n",
      "Epoch 9 batch 1300 train loss: 0.0095 test loss: 0.0113\n",
      "Epoch 9 batch 1400 train loss: 0.0158 test loss: 0.0113\n",
      "Epoch 9 batch 1500 train loss: 0.0090 test loss: 0.0114\n",
      "Epoch 9 batch 1600 train loss: 0.0112 test loss: 0.0119\n",
      "Epoch 9 batch 1700 train loss: 0.0074 test loss: 0.0118\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.60p/ckpt-38\n",
      "Epoch 9 batch 1800 train loss: 0.0076 test loss: 0.0108\n",
      "Epoch 9 batch 1900 train loss: 0.0087 test loss: 0.0117\n",
      "Epoch 9 batch 2000 train loss: 0.0085 test loss: 0.0113\n",
      "Epoch 9 batch 2100 train loss: 0.0133 test loss: 0.0118\n",
      "Epoch 9 batch 2200 train loss: 0.0100 test loss: 0.0117\n",
      "Epoch 9 batch 2300 train loss: 0.0087 test loss: 0.0114\n",
      "Epoch 9 batch 2400 train loss: 0.0094 test loss: 0.0117\n",
      "Epoch 9 batch 2500 train loss: 0.0092 test loss: 0.0116\n",
      "Epoch 9 batch 2600 train loss: 0.0104 test loss: 0.0115\n",
      "Epoch 9 batch 2700 train loss: 0.0092 test loss: 0.0118\n",
      "Epoch 9 batch 2800 train loss: 0.0134 test loss: 0.0117\n",
      "Epoch 9 batch 2900 train loss: 0.0096 test loss: 0.0117\n",
      "Epoch 9 batch 3000 train loss: 0.0133 test loss: 0.0116\n",
      "Epoch 10 batch 0 train loss: 0.0133 test loss: 0.0117\n",
      "Epoch 10 batch 100 train loss: 0.0114 test loss: 0.0116\n",
      "Epoch 10 batch 200 train loss: 0.0125 test loss: 0.0117\n",
      "Epoch 10 batch 300 train loss: 0.0144 test loss: 0.0117\n",
      "Epoch 10 batch 400 train loss: 0.0155 test loss: 0.0115\n",
      "Epoch 10 batch 500 train loss: 0.0098 test loss: 0.0116\n",
      "Epoch 10 batch 600 train loss: 0.0117 test loss: 0.0114\n",
      "Epoch 10 batch 700 train loss: 0.0151 test loss: 0.0112\n",
      "Epoch 10 batch 800 train loss: 0.0104 test loss: 0.0115\n",
      "Epoch 10 batch 900 train loss: 0.0120 test loss: 0.0115\n",
      "Epoch 10 batch 1000 train loss: 0.0114 test loss: 0.0117\n",
      "Epoch 10 batch 1100 train loss: 0.0118 test loss: 0.0112\n",
      "Epoch 10 batch 1200 train loss: 0.0083 test loss: 0.0115\n",
      "Epoch 10 batch 1300 train loss: 0.0083 test loss: 0.0116\n",
      "Epoch 10 batch 1400 train loss: 0.0119 test loss: 0.0117\n",
      "Epoch 10 batch 1500 train loss: 0.0076 test loss: 0.0114\n",
      "Epoch 10 batch 1600 train loss: 0.0106 test loss: 0.0118\n",
      "Epoch 10 batch 1700 train loss: 0.0109 test loss: 0.0116\n",
      "Epoch 10 batch 1800 train loss: 0.0061 test loss: 0.0112\n",
      "Epoch 10 batch 1900 train loss: 0.0089 test loss: 0.0115\n",
      "Epoch 10 batch 2000 train loss: 0.0089 test loss: 0.0112\n",
      "Epoch 10 batch 2100 train loss: 0.0109 test loss: 0.0118\n",
      "Epoch 10 batch 2200 train loss: 0.0095 test loss: 0.0116\n",
      "Epoch 10 batch 2300 train loss: 0.0108 test loss: 0.0114\n",
      "Epoch 10 batch 2400 train loss: 0.0080 test loss: 0.0117\n",
      "Epoch 10 batch 2500 train loss: 0.0119 test loss: 0.0116\n",
      "Epoch 10 batch 2600 train loss: 0.0158 test loss: 0.0114\n",
      "Epoch 10 batch 2700 train loss: 0.0092 test loss: 0.0117\n",
      "Epoch 10 batch 2800 train loss: 0.0131 test loss: 0.0117\n",
      "Epoch 10 batch 2900 train loss: 0.0105 test loss: 0.0116\n",
      "Epoch 10 batch 3000 train loss: 0.0116 test loss: 0.0112\n",
      "Epoch 11 batch 0 train loss: 0.0084 test loss: 0.0113\n",
      "Epoch 11 batch 100 train loss: 0.0109 test loss: 0.0116\n",
      "Epoch 11 batch 200 train loss: 0.0099 test loss: 0.0114\n",
      "Epoch 11 batch 300 train loss: 0.0113 test loss: 0.0120\n",
      "Epoch 11 batch 400 train loss: 0.0098 test loss: 0.0114\n",
      "Epoch 11 batch 500 train loss: 0.0114 test loss: 0.0115\n",
      "Epoch 11 batch 600 train loss: 0.0103 test loss: 0.0115\n",
      "Epoch 11 batch 700 train loss: 0.0119 test loss: 0.0116\n",
      "Epoch 11 batch 800 train loss: 0.0097 test loss: 0.0119\n",
      "Epoch 11 batch 900 train loss: 0.0083 test loss: 0.0116\n",
      "Epoch 11 batch 1000 train loss: 0.0080 test loss: 0.0115\n",
      "Epoch 11 batch 1100 train loss: 0.0109 test loss: 0.0113\n",
      "Epoch 11 batch 1200 train loss: 0.0126 test loss: 0.0114\n",
      "Epoch 11 batch 1300 train loss: 0.0113 test loss: 0.0115\n",
      "Epoch 11 batch 1400 train loss: 0.0063 test loss: 0.0112\n",
      "Epoch 11 batch 1500 train loss: 0.0154 test loss: 0.0110\n",
      "Epoch 11 batch 1600 train loss: 0.0084 test loss: 0.0120\n",
      "Epoch 11 batch 1700 train loss: 0.0108 test loss: 0.0115\n",
      "Epoch 11 batch 1800 train loss: 0.0129 test loss: 0.0114\n",
      "Epoch 11 batch 1900 train loss: 0.0115 test loss: 0.0115\n",
      "Epoch 11 batch 2000 train loss: 0.0068 test loss: 0.0116\n",
      "Epoch 11 batch 2100 train loss: 0.0085 test loss: 0.0119\n",
      "Epoch 11 batch 2200 train loss: 0.0110 test loss: 0.0115\n",
      "Epoch 11 batch 2300 train loss: 0.0116 test loss: 0.0115\n",
      "Epoch 11 batch 2400 train loss: 0.0151 test loss: 0.0118\n",
      "Epoch 11 batch 2500 train loss: 0.0112 test loss: 0.0113\n",
      "Epoch 11 batch 2600 train loss: 0.0107 test loss: 0.0114\n",
      "Epoch 11 batch 2700 train loss: 0.0126 test loss: 0.0119\n",
      "Epoch 11 batch 2800 train loss: 0.0061 test loss: 0.0118\n",
      "Epoch 11 batch 2900 train loss: 0.0114 test loss: 0.0118\n",
      "Epoch 11 batch 3000 train loss: 0.0102 test loss: 0.0112\n",
      "Epoch 12 batch 0 train loss: 0.0070 test loss: 0.0112\n",
      "Epoch 12 batch 100 train loss: 0.0123 test loss: 0.0118\n",
      "Epoch 12 batch 200 train loss: 0.0086 test loss: 0.0117\n",
      "Epoch 12 batch 300 train loss: 0.0122 test loss: 0.0116\n",
      "Epoch 12 batch 400 train loss: 0.0120 test loss: 0.0117\n",
      "Epoch 12 batch 500 train loss: 0.0147 test loss: 0.0114\n",
      "Epoch 12 batch 600 train loss: 0.0179 test loss: 0.0117\n",
      "Epoch 12 batch 700 train loss: 0.0231 test loss: 0.0112\n",
      "Epoch 12 batch 800 train loss: 0.0153 test loss: 0.0116\n",
      "Epoch 12 batch 900 train loss: 0.0126 test loss: 0.0118\n",
      "Epoch 12 batch 1000 train loss: 0.0139 test loss: 0.0114\n",
      "Epoch 12 batch 1100 train loss: 0.0121 test loss: 0.0117\n",
      "Epoch 12 batch 1200 train loss: 0.0115 test loss: 0.0114\n",
      "Epoch 12 batch 1300 train loss: 0.0088 test loss: 0.0116\n",
      "Epoch 12 batch 1400 train loss: 0.0108 test loss: 0.0113\n",
      "Epoch 12 batch 1500 train loss: 0.0093 test loss: 0.0112\n",
      "Epoch 12 batch 1600 train loss: 0.0117 test loss: 0.0118\n",
      "Epoch 12 batch 1700 train loss: 0.0143 test loss: 0.0118\n",
      "Epoch 12 batch 1800 train loss: 0.0125 test loss: 0.0113\n",
      "Epoch 12 batch 1900 train loss: 0.0185 test loss: 0.0116\n",
      "Epoch 12 batch 2000 train loss: 0.0133 test loss: 0.0115\n",
      "Epoch 12 batch 2100 train loss: 0.0088 test loss: 0.0114\n",
      "Epoch 12 batch 2200 train loss: 0.0124 test loss: 0.0120\n",
      "Epoch 12 batch 2300 train loss: 0.0108 test loss: 0.0118\n",
      "Epoch 12 batch 2400 train loss: 0.0106 test loss: 0.0114\n",
      "Epoch 12 batch 2500 train loss: 0.0107 test loss: 0.0118\n",
      "Epoch 12 batch 2600 train loss: 0.0131 test loss: 0.0115\n",
      "Epoch 12 batch 2700 train loss: 0.0136 test loss: 0.0116\n",
      "Epoch 12 batch 2800 train loss: 0.0137 test loss: 0.0117\n",
      "Epoch 12 batch 2900 train loss: 0.0115 test loss: 0.0118\n",
      "Epoch 12 batch 3000 train loss: 0.0120 test loss: 0.0113\n",
      "Epoch 13 batch 0 train loss: 0.0112 test loss: 0.0114\n",
      "Epoch 13 batch 100 train loss: 0.0128 test loss: 0.0116\n",
      "Epoch 13 batch 200 train loss: 0.0093 test loss: 0.0117\n",
      "Epoch 13 batch 300 train loss: 0.0066 test loss: 0.0116\n",
      "Epoch 13 batch 400 train loss: 0.0110 test loss: 0.0116\n",
      "Epoch 13 batch 500 train loss: 0.0165 test loss: 0.0116\n",
      "Epoch 13 batch 600 train loss: 0.0110 test loss: 0.0118\n",
      "Epoch 13 batch 700 train loss: 0.0097 test loss: 0.0113\n",
      "Epoch 13 batch 800 train loss: 0.0233 test loss: 0.0116\n",
      "Epoch 13 batch 900 train loss: 0.0107 test loss: 0.0118\n",
      "Epoch 13 batch 1000 train loss: 0.0108 test loss: 0.0114\n",
      "Epoch 13 batch 1100 train loss: 0.0130 test loss: 0.0114\n",
      "Epoch 13 batch 1200 train loss: 0.0103 test loss: 0.0113\n",
      "Epoch 13 batch 1300 train loss: 0.0115 test loss: 0.0116\n",
      "Epoch 13 batch 1400 train loss: 0.0083 test loss: 0.0115\n",
      "Epoch 13 batch 1500 train loss: 0.0113 test loss: 0.0115\n",
      "Epoch 13 batch 1600 train loss: 0.0090 test loss: 0.0116\n",
      "Epoch 13 batch 1700 train loss: 0.0051 test loss: 0.0119\n",
      "Epoch 13 batch 1800 train loss: 0.0069 test loss: 0.0109\n",
      "Epoch 13 batch 1900 train loss: 0.0110 test loss: 0.0119\n",
      "Epoch 13 batch 2000 train loss: 0.0080 test loss: 0.0118\n",
      "Epoch 13 batch 2100 train loss: 0.0089 test loss: 0.0117\n",
      "Epoch 13 batch 2200 train loss: 0.0114 test loss: 0.0121\n",
      "Epoch 13 batch 2300 train loss: 0.0102 test loss: 0.0112\n",
      "Epoch 13 batch 2400 train loss: 0.0157 test loss: 0.0117\n",
      "Epoch 13 batch 2500 train loss: 0.0105 test loss: 0.0117\n",
      "Epoch 13 batch 2600 train loss: 0.0122 test loss: 0.0117\n",
      "Epoch 13 batch 2700 train loss: 0.0123 test loss: 0.0115\n",
      "Epoch 13 batch 2800 train loss: 0.0136 test loss: 0.0118\n",
      "Epoch 13 batch 2900 train loss: 0.0112 test loss: 0.0118\n",
      "Epoch 13 batch 3000 train loss: 0.0103 test loss: 0.0114\n",
      "Epoch 14 batch 0 train loss: 0.0111 test loss: 0.0115\n",
      "Epoch 14 batch 100 train loss: 0.0112 test loss: 0.0115\n",
      "Epoch 14 batch 200 train loss: 0.0111 test loss: 0.0120\n",
      "Epoch 14 batch 300 train loss: 0.0196 test loss: 0.0116\n",
      "Epoch 14 batch 400 train loss: 0.0130 test loss: 0.0112\n",
      "Epoch 14 batch 500 train loss: 0.0122 test loss: 0.0113\n",
      "Epoch 14 batch 600 train loss: 0.0085 test loss: 0.0115\n",
      "Epoch 14 batch 700 train loss: 0.0096 test loss: 0.0113\n",
      "Epoch 14 batch 800 train loss: 0.0098 test loss: 0.0114\n",
      "Epoch 14 batch 900 train loss: 0.0197 test loss: 0.0115\n",
      "Epoch 14 batch 1000 train loss: 0.0119 test loss: 0.0113\n",
      "Epoch 14 batch 1100 train loss: 0.0106 test loss: 0.0117\n",
      "Epoch 14 batch 1200 train loss: 0.0080 test loss: 0.0114\n",
      "Epoch 14 batch 1300 train loss: 0.0136 test loss: 0.0115\n",
      "Epoch 14 batch 1400 train loss: 0.0112 test loss: 0.0112\n",
      "Epoch 14 batch 1500 train loss: 0.0145 test loss: 0.0114\n",
      "Epoch 14 batch 1600 train loss: 0.0097 test loss: 0.0119\n",
      "Epoch 14 batch 1700 train loss: 0.0119 test loss: 0.0116\n",
      "Epoch 14 batch 1800 train loss: 0.0066 test loss: 0.0110\n",
      "Epoch 14 batch 1900 train loss: 0.0241 test loss: 0.0117\n",
      "Epoch 14 batch 2000 train loss: 0.0146 test loss: 0.0114\n",
      "Epoch 14 batch 2100 train loss: 0.0069 test loss: 0.0117\n",
      "Epoch 14 batch 2200 train loss: 0.0080 test loss: 0.0114\n",
      "Epoch 14 batch 2300 train loss: 0.0103 test loss: 0.0117\n",
      "Epoch 14 batch 2400 train loss: 0.0125 test loss: 0.0119\n",
      "Epoch 14 batch 2500 train loss: 0.0119 test loss: 0.0114\n",
      "Epoch 14 batch 2600 train loss: 0.0185 test loss: 0.0112\n",
      "Epoch 14 batch 2700 train loss: 0.0100 test loss: 0.0120\n",
      "Epoch 14 batch 2800 train loss: 0.0164 test loss: 0.0117\n",
      "Epoch 14 batch 2900 train loss: 0.0083 test loss: 0.0115\n",
      "Epoch 14 batch 3000 train loss: 0.0125 test loss: 0.0113\n",
      "Epoch 15 batch 0 train loss: 0.0090 test loss: 0.0115\n",
      "Epoch 15 batch 100 train loss: 0.0105 test loss: 0.0119\n",
      "Epoch 15 batch 200 train loss: 0.0085 test loss: 0.0114\n",
      "Epoch 15 batch 300 train loss: 0.0087 test loss: 0.0119\n",
      "Epoch 15 batch 400 train loss: 0.0119 test loss: 0.0112\n",
      "Epoch 15 batch 500 train loss: 0.0118 test loss: 0.0116\n",
      "Epoch 15 batch 600 train loss: 0.0094 test loss: 0.0116\n",
      "Epoch 15 batch 700 train loss: 0.0093 test loss: 0.0116\n",
      "Epoch 15 batch 800 train loss: 0.0143 test loss: 0.0115\n",
      "Epoch 15 batch 900 train loss: 0.0150 test loss: 0.0115\n",
      "Epoch 15 batch 1000 train loss: 0.0087 test loss: 0.0118\n",
      "Epoch 15 batch 1100 train loss: 0.0083 test loss: 0.0113\n",
      "Epoch 15 batch 1200 train loss: 0.0121 test loss: 0.0114\n",
      "Epoch 15 batch 1300 train loss: 0.0104 test loss: 0.0116\n",
      "Epoch 15 batch 1400 train loss: 0.0092 test loss: 0.0115\n",
      "Epoch 15 batch 1500 train loss: 0.0094 test loss: 0.0115\n",
      "Epoch 15 batch 1600 train loss: 0.0087 test loss: 0.0117\n",
      "Epoch 15 batch 1700 train loss: 0.0143 test loss: 0.0118\n",
      "Epoch 15 batch 1800 train loss: 0.0123 test loss: 0.0109\n",
      "Epoch 15 batch 1900 train loss: 0.0090 test loss: 0.0114\n",
      "Epoch 15 batch 2000 train loss: 0.0106 test loss: 0.0115\n",
      "Epoch 15 batch 2100 train loss: 0.0114 test loss: 0.0116\n",
      "Epoch 15 batch 2200 train loss: 0.0075 test loss: 0.0118\n",
      "Epoch 15 batch 2300 train loss: 0.0055 test loss: 0.0118\n",
      "Epoch 15 batch 2400 train loss: 0.0130 test loss: 0.0116\n",
      "Epoch 15 batch 2500 train loss: 0.0116 test loss: 0.0115\n",
      "Epoch 15 batch 2600 train loss: 0.0130 test loss: 0.0116\n",
      "Epoch 15 batch 2700 train loss: 0.0070 test loss: 0.0116\n",
      "Epoch 15 batch 2800 train loss: 0.0073 test loss: 0.0118\n",
      "Epoch 15 batch 2900 train loss: 0.0106 test loss: 0.0115\n",
      "Epoch 15 batch 3000 train loss: 0.0106 test loss: 0.0114\n",
      "Epoch 16 batch 0 train loss: 0.0106 test loss: 0.0114\n",
      "Epoch 16 batch 100 train loss: 0.0103 test loss: 0.0117\n",
      "Epoch 16 batch 200 train loss: 0.0090 test loss: 0.0115\n",
      "Epoch 16 batch 300 train loss: 0.0108 test loss: 0.0117\n",
      "Epoch 16 batch 400 train loss: 0.0089 test loss: 0.0117\n",
      "Epoch 16 batch 500 train loss: 0.0096 test loss: 0.0114\n",
      "Epoch 16 batch 600 train loss: 0.0062 test loss: 0.0117\n",
      "Epoch 16 batch 700 train loss: 0.0146 test loss: 0.0113\n",
      "Epoch 16 batch 800 train loss: 0.0111 test loss: 0.0118\n",
      "Epoch 16 batch 900 train loss: 0.0083 test loss: 0.0110\n",
      "Epoch 16 batch 1000 train loss: 0.0154 test loss: 0.0118\n",
      "Epoch 16 batch 1100 train loss: 0.0095 test loss: 0.0118\n",
      "Epoch 16 batch 1200 train loss: 0.0147 test loss: 0.0115\n",
      "Epoch 16 batch 1300 train loss: 0.0154 test loss: 0.0116\n",
      "Epoch 16 batch 1400 train loss: 0.0153 test loss: 0.0116\n",
      "Epoch 16 batch 1500 train loss: 0.0101 test loss: 0.0115\n",
      "Epoch 16 batch 1600 train loss: 0.0091 test loss: 0.0117\n",
      "Epoch 16 batch 1700 train loss: 0.0123 test loss: 0.0116\n",
      "Epoch 16 batch 1800 train loss: 0.0142 test loss: 0.0111\n",
      "Epoch 16 batch 1900 train loss: 0.0161 test loss: 0.0117\n",
      "Epoch 16 batch 2000 train loss: 0.0094 test loss: 0.0116\n",
      "Epoch 16 batch 2100 train loss: 0.0128 test loss: 0.0114\n",
      "Epoch 16 batch 2200 train loss: 0.0082 test loss: 0.0117\n",
      "Epoch 16 batch 2300 train loss: 0.0157 test loss: 0.0113\n",
      "Epoch 16 batch 2400 train loss: 0.0091 test loss: 0.0114\n",
      "Epoch 16 batch 2500 train loss: 0.0078 test loss: 0.0117\n",
      "Epoch 16 batch 2600 train loss: 0.0128 test loss: 0.0113\n",
      "Epoch 16 batch 2700 train loss: 0.0173 test loss: 0.0119\n",
      "Epoch 16 batch 2800 train loss: 0.0176 test loss: 0.0119\n",
      "Epoch 16 batch 2900 train loss: 0.0101 test loss: 0.0119\n",
      "Epoch 16 batch 3000 train loss: 0.0153 test loss: 0.0115\n",
      "Epoch 17 batch 0 train loss: 0.0119 test loss: 0.0114\n",
      "Epoch 17 batch 100 train loss: 0.0116 test loss: 0.0117\n",
      "Epoch 17 batch 200 train loss: 0.0102 test loss: 0.0115\n",
      "Epoch 17 batch 300 train loss: 0.0076 test loss: 0.0116\n",
      "Epoch 17 batch 400 train loss: 0.0129 test loss: 0.0114\n",
      "Epoch 17 batch 500 train loss: 0.0099 test loss: 0.0112\n",
      "Epoch 17 batch 600 train loss: 0.0096 test loss: 0.0113\n",
      "Epoch 17 batch 700 train loss: 0.0124 test loss: 0.0116\n",
      "Epoch 17 batch 800 train loss: 0.0120 test loss: 0.0116\n",
      "Epoch 17 batch 900 train loss: 0.0109 test loss: 0.0115\n",
      "Epoch 17 batch 1000 train loss: 0.0092 test loss: 0.0118\n",
      "early stop.\n",
      "Checkpoint 38 restored!!\n",
      "Training for loss rate 0.70 start.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ffnn_6/dense_18/kernel:0', 'ffnn_6/dense_18/bias:0', 'ffnn_6/batch_normalization_6/gamma:0', 'ffnn_6/batch_normalization_6/beta:0', 'ffnn_6/dense_19/kernel:0', 'ffnn_6/dense_19/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ffnn_6/dense_18/kernel:0', 'ffnn_6/dense_18/bias:0', 'ffnn_6/batch_normalization_6/gamma:0', 'ffnn_6/batch_normalization_6/beta:0', 'ffnn_6/dense_19/kernel:0', 'ffnn_6/dense_19/bias:0'] when minimizing the loss.\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.70p/ckpt-1\n",
      "Epoch 0 batch 0 train loss: 0.2519 test loss: 0.3252\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.70p/ckpt-2\n",
      "Epoch 0 batch 100 train loss: 0.1437 test loss: 0.2155\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.70p/ckpt-3\n",
      "Epoch 0 batch 200 train loss: 0.1284 test loss: 0.1583\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.70p/ckpt-4\n",
      "Epoch 0 batch 300 train loss: 0.0836 test loss: 0.1224\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.70p/ckpt-5\n",
      "Epoch 0 batch 400 train loss: 0.0541 test loss: 0.0972\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.70p/ckpt-6\n",
      "Epoch 0 batch 500 train loss: 0.0602 test loss: 0.0771\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.70p/ckpt-7\n",
      "Epoch 0 batch 600 train loss: 0.0481 test loss: 0.0624\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.70p/ckpt-8\n",
      "Epoch 0 batch 700 train loss: 0.0327 test loss: 0.0509\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.70p/ckpt-9\n",
      "Epoch 0 batch 800 train loss: 0.0295 test loss: 0.0414\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.70p/ckpt-10\n",
      "Epoch 0 batch 900 train loss: 0.0311 test loss: 0.0347\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.70p/ckpt-11\n",
      "Epoch 0 batch 1000 train loss: 0.0250 test loss: 0.0293\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.70p/ckpt-12\n",
      "Epoch 0 batch 1100 train loss: 0.0177 test loss: 0.0251\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.70p/ckpt-13\n",
      "Epoch 0 batch 1200 train loss: 0.0138 test loss: 0.0220\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.70p/ckpt-14\n",
      "Epoch 0 batch 1300 train loss: 0.0118 test loss: 0.0199\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.70p/ckpt-15\n",
      "Epoch 0 batch 1400 train loss: 0.0152 test loss: 0.0178\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.70p/ckpt-16\n",
      "Epoch 0 batch 1500 train loss: 0.0118 test loss: 0.0167\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.70p/ckpt-17\n",
      "Epoch 0 batch 1600 train loss: 0.0117 test loss: 0.0161\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.70p/ckpt-18\n",
      "Epoch 0 batch 1700 train loss: 0.0125 test loss: 0.0153\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.70p/ckpt-19\n",
      "Epoch 0 batch 1800 train loss: 0.0187 test loss: 0.0147\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.70p/ckpt-20\n",
      "Epoch 0 batch 1900 train loss: 0.0138 test loss: 0.0143\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.70p/ckpt-21\n",
      "Epoch 0 batch 2000 train loss: 0.0113 test loss: 0.0138\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.70p/ckpt-22\n",
      "Epoch 0 batch 2100 train loss: 0.0157 test loss: 0.0137\n",
      "Epoch 0 batch 2200 train loss: 0.0110 test loss: 0.0140\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.70p/ckpt-23\n",
      "Epoch 0 batch 2300 train loss: 0.0123 test loss: 0.0135\n",
      "Epoch 0 batch 2400 train loss: 0.0100 test loss: 0.0136\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.70p/ckpt-24\n",
      "Epoch 0 batch 2500 train loss: 0.0142 test loss: 0.0135\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.70p/ckpt-25\n",
      "Epoch 0 batch 2600 train loss: 0.0124 test loss: 0.0134\n",
      "Epoch 0 batch 2700 train loss: 0.0163 test loss: 0.0134\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.70p/ckpt-26\n",
      "Epoch 0 batch 2800 train loss: 0.0165 test loss: 0.0133\n",
      "Epoch 0 batch 2900 train loss: 0.0105 test loss: 0.0133\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.70p/ckpt-27\n",
      "Epoch 0 batch 3000 train loss: 0.0115 test loss: 0.0130\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ffnn_6/dense_18/kernel:0', 'ffnn_6/dense_18/bias:0', 'ffnn_6/batch_normalization_6/gamma:0', 'ffnn_6/batch_normalization_6/beta:0', 'ffnn_6/dense_19/kernel:0', 'ffnn_6/dense_19/bias:0'] when minimizing the loss.\n",
      "Epoch 1 batch 0 train loss: 0.0129 test loss: 0.0131\n",
      "Epoch 1 batch 100 train loss: 0.0167 test loss: 0.0134\n",
      "Epoch 1 batch 200 train loss: 0.0157 test loss: 0.0133\n",
      "Epoch 1 batch 300 train loss: 0.0076 test loss: 0.0134\n",
      "Epoch 1 batch 400 train loss: 0.0124 test loss: 0.0131\n",
      "Epoch 1 batch 500 train loss: 0.0087 test loss: 0.0132\n",
      "Epoch 1 batch 600 train loss: 0.0154 test loss: 0.0130\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.70p/ckpt-28\n",
      "Epoch 1 batch 700 train loss: 0.0163 test loss: 0.0128\n",
      "Epoch 1 batch 800 train loss: 0.0143 test loss: 0.0132\n",
      "Epoch 1 batch 900 train loss: 0.0139 test loss: 0.0131\n",
      "Epoch 1 batch 1000 train loss: 0.0166 test loss: 0.0132\n",
      "Epoch 1 batch 1100 train loss: 0.0133 test loss: 0.0130\n",
      "Epoch 1 batch 1200 train loss: 0.0144 test loss: 0.0130\n",
      "Epoch 1 batch 1300 train loss: 0.0139 test loss: 0.0130\n",
      "Epoch 1 batch 1400 train loss: 0.0137 test loss: 0.0129\n",
      "Epoch 1 batch 1500 train loss: 0.0151 test loss: 0.0132\n",
      "Epoch 1 batch 1600 train loss: 0.0137 test loss: 0.0132\n",
      "Epoch 1 batch 1700 train loss: 0.0109 test loss: 0.0134\n",
      "Epoch 1 batch 1800 train loss: 0.0132 test loss: 0.0130\n",
      "Epoch 1 batch 1900 train loss: 0.0167 test loss: 0.0128\n",
      "Epoch 1 batch 2000 train loss: 0.0122 test loss: 0.0129\n",
      "Epoch 1 batch 2100 train loss: 0.0111 test loss: 0.0133\n",
      "Epoch 1 batch 2200 train loss: 0.0116 test loss: 0.0129\n",
      "Epoch 1 batch 2300 train loss: 0.0111 test loss: 0.0129\n",
      "Epoch 1 batch 2400 train loss: 0.0114 test loss: 0.0132\n",
      "Epoch 1 batch 2500 train loss: 0.0129 test loss: 0.0132\n",
      "Epoch 1 batch 2600 train loss: 0.0106 test loss: 0.0129\n",
      "Epoch 1 batch 2700 train loss: 0.0131 test loss: 0.0129\n",
      "Epoch 1 batch 2800 train loss: 0.0143 test loss: 0.0132\n",
      "Epoch 1 batch 2900 train loss: 0.0170 test loss: 0.0130\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.70p/ckpt-29\n",
      "Epoch 1 batch 3000 train loss: 0.0137 test loss: 0.0128\n",
      "Epoch 2 batch 0 train loss: 0.0147 test loss: 0.0129\n",
      "Epoch 2 batch 100 train loss: 0.0084 test loss: 0.0132\n",
      "Epoch 2 batch 200 train loss: 0.0092 test loss: 0.0134\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.70p/ckpt-30\n",
      "Epoch 2 batch 300 train loss: 0.0090 test loss: 0.0127\n",
      "Epoch 2 batch 400 train loss: 0.0136 test loss: 0.0129\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.70p/ckpt-31\n",
      "Epoch 2 batch 500 train loss: 0.0101 test loss: 0.0126\n",
      "Epoch 2 batch 600 train loss: 0.0106 test loss: 0.0132\n",
      "Epoch 2 batch 700 train loss: 0.0097 test loss: 0.0130\n",
      "Epoch 2 batch 800 train loss: 0.0136 test loss: 0.0128\n",
      "Epoch 2 batch 900 train loss: 0.0131 test loss: 0.0131\n",
      "Epoch 2 batch 1000 train loss: 0.0132 test loss: 0.0128\n",
      "Epoch 2 batch 1100 train loss: 0.0137 test loss: 0.0130\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.70p/ckpt-32\n",
      "Epoch 2 batch 1200 train loss: 0.0109 test loss: 0.0125\n",
      "Epoch 2 batch 1300 train loss: 0.0121 test loss: 0.0133\n",
      "Epoch 2 batch 1400 train loss: 0.0108 test loss: 0.0129\n",
      "Epoch 2 batch 1500 train loss: 0.0102 test loss: 0.0130\n",
      "Epoch 2 batch 1600 train loss: 0.0086 test loss: 0.0132\n",
      "Epoch 2 batch 1700 train loss: 0.0143 test loss: 0.0129\n",
      "Epoch 2 batch 1800 train loss: 0.0122 test loss: 0.0125\n",
      "Epoch 2 batch 1900 train loss: 0.0158 test loss: 0.0131\n",
      "Epoch 2 batch 2000 train loss: 0.0112 test loss: 0.0128\n",
      "Epoch 2 batch 2100 train loss: 0.0198 test loss: 0.0130\n",
      "Epoch 2 batch 2200 train loss: 0.0121 test loss: 0.0134\n",
      "Epoch 2 batch 2300 train loss: 0.0127 test loss: 0.0126\n",
      "Epoch 2 batch 2400 train loss: 0.0081 test loss: 0.0132\n",
      "Epoch 2 batch 2500 train loss: 0.0108 test loss: 0.0126\n",
      "Epoch 2 batch 2600 train loss: 0.0080 test loss: 0.0130\n",
      "Epoch 2 batch 2700 train loss: 0.0112 test loss: 0.0130\n",
      "Epoch 2 batch 2800 train loss: 0.0118 test loss: 0.0129\n",
      "Epoch 2 batch 2900 train loss: 0.0097 test loss: 0.0128\n",
      "Epoch 2 batch 3000 train loss: 0.0128 test loss: 0.0129\n",
      "Epoch 3 batch 0 train loss: 0.0135 test loss: 0.0128\n",
      "Epoch 3 batch 100 train loss: 0.0146 test loss: 0.0131\n",
      "Epoch 3 batch 200 train loss: 0.0140 test loss: 0.0131\n",
      "Epoch 3 batch 300 train loss: 0.0127 test loss: 0.0132\n",
      "Epoch 3 batch 400 train loss: 0.0094 test loss: 0.0130\n",
      "Epoch 3 batch 500 train loss: 0.0126 test loss: 0.0127\n",
      "Epoch 3 batch 600 train loss: 0.0136 test loss: 0.0131\n",
      "Epoch 3 batch 700 train loss: 0.0132 test loss: 0.0125\n",
      "Epoch 3 batch 800 train loss: 0.0137 test loss: 0.0132\n",
      "Epoch 3 batch 900 train loss: 0.0105 test loss: 0.0133\n",
      "Epoch 3 batch 1000 train loss: 0.0110 test loss: 0.0127\n",
      "Epoch 3 batch 1100 train loss: 0.0144 test loss: 0.0129\n",
      "Epoch 3 batch 1200 train loss: 0.0105 test loss: 0.0129\n",
      "Epoch 3 batch 1300 train loss: 0.0115 test loss: 0.0129\n",
      "Epoch 3 batch 1400 train loss: 0.0146 test loss: 0.0127\n",
      "Epoch 3 batch 1500 train loss: 0.0121 test loss: 0.0134\n",
      "Epoch 3 batch 1600 train loss: 0.0087 test loss: 0.0132\n",
      "Epoch 3 batch 1700 train loss: 0.0203 test loss: 0.0129\n",
      "Epoch 3 batch 1800 train loss: 0.0111 test loss: 0.0126\n",
      "Epoch 3 batch 1900 train loss: 0.0166 test loss: 0.0129\n",
      "Epoch 3 batch 2000 train loss: 0.0109 test loss: 0.0128\n",
      "Epoch 3 batch 2100 train loss: 0.0120 test loss: 0.0130\n",
      "Epoch 3 batch 2200 train loss: 0.0163 test loss: 0.0131\n",
      "Epoch 3 batch 2300 train loss: 0.0137 test loss: 0.0128\n",
      "Epoch 3 batch 2400 train loss: 0.0117 test loss: 0.0127\n",
      "Epoch 3 batch 2500 train loss: 0.0095 test loss: 0.0132\n",
      "Epoch 3 batch 2600 train loss: 0.0100 test loss: 0.0128\n",
      "Epoch 3 batch 2700 train loss: 0.0106 test loss: 0.0129\n",
      "Epoch 3 batch 2800 train loss: 0.0166 test loss: 0.0131\n",
      "Epoch 3 batch 2900 train loss: 0.0158 test loss: 0.0128\n",
      "Epoch 3 batch 3000 train loss: 0.0132 test loss: 0.0126\n",
      "Epoch 4 batch 0 train loss: 0.0107 test loss: 0.0127\n",
      "Epoch 4 batch 100 train loss: 0.0126 test loss: 0.0132\n",
      "Epoch 4 batch 200 train loss: 0.0115 test loss: 0.0128\n",
      "Epoch 4 batch 300 train loss: 0.0138 test loss: 0.0132\n",
      "Epoch 4 batch 400 train loss: 0.0093 test loss: 0.0129\n",
      "Epoch 4 batch 500 train loss: 0.0134 test loss: 0.0126\n",
      "Epoch 4 batch 600 train loss: 0.0129 test loss: 0.0134\n",
      "Epoch 4 batch 700 train loss: 0.0140 test loss: 0.0127\n",
      "Epoch 4 batch 800 train loss: 0.0190 test loss: 0.0130\n",
      "Epoch 4 batch 900 train loss: 0.0164 test loss: 0.0132\n",
      "Epoch 4 batch 1000 train loss: 0.0105 test loss: 0.0130\n",
      "Epoch 4 batch 1100 train loss: 0.0079 test loss: 0.0132\n",
      "Epoch 4 batch 1200 train loss: 0.0149 test loss: 0.0127\n",
      "Epoch 4 batch 1300 train loss: 0.0152 test loss: 0.0126\n",
      "Epoch 4 batch 1400 train loss: 0.0159 test loss: 0.0125\n",
      "Epoch 4 batch 1500 train loss: 0.0148 test loss: 0.0127\n",
      "Epoch 4 batch 1600 train loss: 0.0111 test loss: 0.0133\n",
      "Epoch 4 batch 1700 train loss: 0.0142 test loss: 0.0134\n",
      "Epoch 4 batch 1800 train loss: 0.0115 test loss: 0.0127\n",
      "Epoch 4 batch 1900 train loss: 0.0102 test loss: 0.0130\n",
      "Epoch 4 batch 2000 train loss: 0.0091 test loss: 0.0125\n",
      "Epoch 4 batch 2100 train loss: 0.0140 test loss: 0.0130\n",
      "Epoch 4 batch 2200 train loss: 0.0122 test loss: 0.0130\n",
      "Epoch 4 batch 2300 train loss: 0.0108 test loss: 0.0129\n",
      "Epoch 4 batch 2400 train loss: 0.0132 test loss: 0.0130\n",
      "Epoch 4 batch 2500 train loss: 0.0102 test loss: 0.0128\n",
      "Epoch 4 batch 2600 train loss: 0.0133 test loss: 0.0130\n",
      "Epoch 4 batch 2700 train loss: 0.0122 test loss: 0.0131\n",
      "Epoch 4 batch 2800 train loss: 0.0081 test loss: 0.0132\n",
      "Epoch 4 batch 2900 train loss: 0.0138 test loss: 0.0127\n",
      "Epoch 4 batch 3000 train loss: 0.0108 test loss: 0.0128\n",
      "Epoch 5 batch 0 train loss: 0.0171 test loss: 0.0126\n",
      "Epoch 5 batch 100 train loss: 0.0109 test loss: 0.0133\n",
      "Epoch 5 batch 200 train loss: 0.0120 test loss: 0.0130\n",
      "Epoch 5 batch 300 train loss: 0.0153 test loss: 0.0134\n",
      "Epoch 5 batch 400 train loss: 0.0112 test loss: 0.0129\n",
      "Epoch 5 batch 500 train loss: 0.0087 test loss: 0.0128\n",
      "Epoch 5 batch 600 train loss: 0.0105 test loss: 0.0129\n",
      "Epoch 5 batch 700 train loss: 0.0087 test loss: 0.0129\n",
      "Epoch 5 batch 800 train loss: 0.0153 test loss: 0.0130\n",
      "Epoch 5 batch 900 train loss: 0.0169 test loss: 0.0128\n",
      "Epoch 5 batch 1000 train loss: 0.0101 test loss: 0.0127\n",
      "Epoch 5 batch 1100 train loss: 0.0174 test loss: 0.0130\n",
      "Epoch 5 batch 1200 train loss: 0.0096 test loss: 0.0128\n",
      "Epoch 5 batch 1300 train loss: 0.0135 test loss: 0.0130\n",
      "Epoch 5 batch 1400 train loss: 0.0177 test loss: 0.0126\n",
      "Epoch 5 batch 1500 train loss: 0.0194 test loss: 0.0128\n",
      "Epoch 5 batch 1600 train loss: 0.0103 test loss: 0.0131\n",
      "Epoch 5 batch 1700 train loss: 0.0064 test loss: 0.0132\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.70p/ckpt-33\n",
      "Epoch 5 batch 1800 train loss: 0.0101 test loss: 0.0124\n",
      "Epoch 5 batch 1900 train loss: 0.0102 test loss: 0.0129\n",
      "Epoch 5 batch 2000 train loss: 0.0135 test loss: 0.0127\n",
      "Epoch 5 batch 2100 train loss: 0.0117 test loss: 0.0127\n",
      "Epoch 5 batch 2200 train loss: 0.0068 test loss: 0.0132\n",
      "Epoch 5 batch 2300 train loss: 0.0148 test loss: 0.0129\n",
      "Epoch 5 batch 2400 train loss: 0.0100 test loss: 0.0129\n",
      "Epoch 5 batch 2500 train loss: 0.0102 test loss: 0.0133\n",
      "Epoch 5 batch 2600 train loss: 0.0141 test loss: 0.0130\n",
      "Epoch 5 batch 2700 train loss: 0.0155 test loss: 0.0127\n",
      "Epoch 5 batch 2800 train loss: 0.0156 test loss: 0.0130\n",
      "Epoch 5 batch 2900 train loss: 0.0079 test loss: 0.0127\n",
      "Epoch 5 batch 3000 train loss: 0.0093 test loss: 0.0126\n",
      "Epoch 6 batch 0 train loss: 0.0112 test loss: 0.0127\n",
      "Epoch 6 batch 100 train loss: 0.0137 test loss: 0.0131\n",
      "Epoch 6 batch 200 train loss: 0.0115 test loss: 0.0128\n",
      "Epoch 6 batch 300 train loss: 0.0103 test loss: 0.0133\n",
      "Epoch 6 batch 400 train loss: 0.0142 test loss: 0.0128\n",
      "Epoch 6 batch 500 train loss: 0.0103 test loss: 0.0131\n",
      "Epoch 6 batch 600 train loss: 0.0111 test loss: 0.0129\n",
      "Epoch 6 batch 700 train loss: 0.0107 test loss: 0.0129\n",
      "Epoch 6 batch 800 train loss: 0.0116 test loss: 0.0126\n",
      "Epoch 6 batch 900 train loss: 0.0100 test loss: 0.0132\n",
      "Epoch 6 batch 1000 train loss: 0.0113 test loss: 0.0128\n",
      "Epoch 6 batch 1100 train loss: 0.0088 test loss: 0.0133\n",
      "Epoch 6 batch 1200 train loss: 0.0117 test loss: 0.0126\n",
      "Epoch 6 batch 1300 train loss: 0.0100 test loss: 0.0130\n",
      "Epoch 6 batch 1400 train loss: 0.0079 test loss: 0.0128\n",
      "Epoch 6 batch 1500 train loss: 0.0092 test loss: 0.0129\n",
      "Epoch 6 batch 1600 train loss: 0.0119 test loss: 0.0129\n",
      "Epoch 6 batch 1700 train loss: 0.0120 test loss: 0.0133\n",
      "Epoch 6 batch 1800 train loss: 0.0167 test loss: 0.0127\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.70p/ckpt-34\n",
      "Epoch 6 batch 1900 train loss: 0.0109 test loss: 0.0124\n",
      "Epoch 6 batch 2000 train loss: 0.0149 test loss: 0.0129\n",
      "Epoch 6 batch 2100 train loss: 0.0107 test loss: 0.0130\n",
      "Epoch 6 batch 2200 train loss: 0.0112 test loss: 0.0129\n",
      "Epoch 6 batch 2300 train loss: 0.0140 test loss: 0.0131\n",
      "Epoch 6 batch 2400 train loss: 0.0157 test loss: 0.0126\n",
      "Epoch 6 batch 2500 train loss: 0.0103 test loss: 0.0129\n",
      "Epoch 6 batch 2600 train loss: 0.0117 test loss: 0.0130\n",
      "Epoch 6 batch 2700 train loss: 0.0132 test loss: 0.0132\n",
      "Epoch 6 batch 2800 train loss: 0.0147 test loss: 0.0131\n",
      "Epoch 6 batch 2900 train loss: 0.0104 test loss: 0.0128\n",
      "Epoch 6 batch 3000 train loss: 0.0120 test loss: 0.0124\n",
      "Epoch 7 batch 0 train loss: 0.0132 test loss: 0.0127\n",
      "Epoch 7 batch 100 train loss: 0.0091 test loss: 0.0129\n",
      "Epoch 7 batch 200 train loss: 0.0160 test loss: 0.0128\n",
      "Epoch 7 batch 300 train loss: 0.0099 test loss: 0.0128\n",
      "Epoch 7 batch 400 train loss: 0.0108 test loss: 0.0126\n",
      "Epoch 7 batch 500 train loss: 0.0134 test loss: 0.0127\n",
      "Epoch 7 batch 600 train loss: 0.0140 test loss: 0.0133\n",
      "Epoch 7 batch 700 train loss: 0.0105 test loss: 0.0127\n",
      "Epoch 7 batch 800 train loss: 0.0142 test loss: 0.0130\n",
      "Epoch 7 batch 900 train loss: 0.0104 test loss: 0.0129\n",
      "Epoch 7 batch 1000 train loss: 0.0132 test loss: 0.0127\n",
      "Epoch 7 batch 1100 train loss: 0.0161 test loss: 0.0128\n",
      "Epoch 7 batch 1200 train loss: 0.0097 test loss: 0.0127\n",
      "Epoch 7 batch 1300 train loss: 0.0135 test loss: 0.0127\n",
      "Epoch 7 batch 1400 train loss: 0.0105 test loss: 0.0127\n",
      "Epoch 7 batch 1500 train loss: 0.0102 test loss: 0.0131\n",
      "Epoch 7 batch 1600 train loss: 0.0112 test loss: 0.0129\n",
      "Epoch 7 batch 1700 train loss: 0.0108 test loss: 0.0131\n",
      "Epoch 7 batch 1800 train loss: 0.0161 test loss: 0.0126\n",
      "Epoch 7 batch 1900 train loss: 0.0138 test loss: 0.0127\n",
      "Epoch 7 batch 2000 train loss: 0.0125 test loss: 0.0127\n",
      "Epoch 7 batch 2100 train loss: 0.0090 test loss: 0.0131\n",
      "Epoch 7 batch 2200 train loss: 0.0147 test loss: 0.0128\n",
      "Epoch 7 batch 2300 train loss: 0.0165 test loss: 0.0129\n",
      "Epoch 7 batch 2400 train loss: 0.0131 test loss: 0.0129\n",
      "Epoch 7 batch 2500 train loss: 0.0096 test loss: 0.0132\n",
      "Epoch 7 batch 2600 train loss: 0.0150 test loss: 0.0131\n",
      "Epoch 7 batch 2700 train loss: 0.0108 test loss: 0.0130\n",
      "Epoch 7 batch 2800 train loss: 0.0139 test loss: 0.0127\n",
      "Epoch 7 batch 2900 train loss: 0.0095 test loss: 0.0130\n",
      "Epoch 7 batch 3000 train loss: 0.0131 test loss: 0.0124\n",
      "Epoch 8 batch 0 train loss: 0.0155 test loss: 0.0124\n",
      "Epoch 8 batch 100 train loss: 0.0104 test loss: 0.0129\n",
      "Epoch 8 batch 200 train loss: 0.0155 test loss: 0.0129\n",
      "Epoch 8 batch 300 train loss: 0.0091 test loss: 0.0128\n",
      "Epoch 8 batch 400 train loss: 0.0111 test loss: 0.0128\n",
      "Epoch 8 batch 500 train loss: 0.0141 test loss: 0.0127\n",
      "Epoch 8 batch 600 train loss: 0.0099 test loss: 0.0131\n",
      "Epoch 8 batch 700 train loss: 0.0113 test loss: 0.0127\n",
      "Epoch 8 batch 800 train loss: 0.0111 test loss: 0.0129\n",
      "Epoch 8 batch 900 train loss: 0.0135 test loss: 0.0128\n",
      "Epoch 8 batch 1000 train loss: 0.0184 test loss: 0.0125\n",
      "Epoch 8 batch 1100 train loss: 0.0131 test loss: 0.0131\n",
      "Epoch 8 batch 1200 train loss: 0.0116 test loss: 0.0128\n",
      "Epoch 8 batch 1300 train loss: 0.0142 test loss: 0.0128\n",
      "Epoch 8 batch 1400 train loss: 0.0127 test loss: 0.0125\n",
      "Epoch 8 batch 1500 train loss: 0.0091 test loss: 0.0126\n",
      "Epoch 8 batch 1600 train loss: 0.0154 test loss: 0.0133\n",
      "Epoch 8 batch 1700 train loss: 0.0156 test loss: 0.0131\n",
      "Epoch 8 batch 1800 train loss: 0.0123 test loss: 0.0127\n",
      "Epoch 8 batch 1900 train loss: 0.0129 test loss: 0.0125\n",
      "Epoch 8 batch 2000 train loss: 0.0066 test loss: 0.0127\n",
      "Epoch 8 batch 2100 train loss: 0.0117 test loss: 0.0130\n",
      "Epoch 8 batch 2200 train loss: 0.0128 test loss: 0.0129\n",
      "Epoch 8 batch 2300 train loss: 0.0108 test loss: 0.0132\n",
      "Epoch 8 batch 2400 train loss: 0.0141 test loss: 0.0126\n",
      "Epoch 8 batch 2500 train loss: 0.0126 test loss: 0.0130\n",
      "Epoch 8 batch 2600 train loss: 0.0101 test loss: 0.0127\n",
      "Epoch 8 batch 2700 train loss: 0.0082 test loss: 0.0130\n",
      "Epoch 8 batch 2800 train loss: 0.0108 test loss: 0.0131\n",
      "Epoch 8 batch 2900 train loss: 0.0165 test loss: 0.0126\n",
      "Epoch 8 batch 3000 train loss: 0.0166 test loss: 0.0128\n",
      "Epoch 9 batch 0 train loss: 0.0169 test loss: 0.0129\n",
      "Epoch 9 batch 100 train loss: 0.0118 test loss: 0.0125\n",
      "Epoch 9 batch 200 train loss: 0.0127 test loss: 0.0134\n",
      "Epoch 9 batch 300 train loss: 0.0089 test loss: 0.0131\n",
      "Epoch 9 batch 400 train loss: 0.0112 test loss: 0.0129\n",
      "Epoch 9 batch 500 train loss: 0.0137 test loss: 0.0129\n",
      "Epoch 9 batch 600 train loss: 0.0089 test loss: 0.0126\n",
      "Epoch 9 batch 700 train loss: 0.0128 test loss: 0.0127\n",
      "Epoch 9 batch 800 train loss: 0.0153 test loss: 0.0132\n",
      "Epoch 9 batch 900 train loss: 0.0131 test loss: 0.0129\n",
      "Epoch 9 batch 1000 train loss: 0.0105 test loss: 0.0132\n",
      "Epoch 9 batch 1100 train loss: 0.0092 test loss: 0.0128\n",
      "Epoch 9 batch 1200 train loss: 0.0122 test loss: 0.0126\n",
      "Epoch 9 batch 1300 train loss: 0.0102 test loss: 0.0128\n",
      "Epoch 9 batch 1400 train loss: 0.0147 test loss: 0.0130\n",
      "Epoch 9 batch 1500 train loss: 0.0124 test loss: 0.0126\n",
      "Epoch 9 batch 1600 train loss: 0.0099 test loss: 0.0132\n",
      "Epoch 9 batch 1700 train loss: 0.0164 test loss: 0.0134\n",
      "Epoch 9 batch 1800 train loss: 0.0119 test loss: 0.0125\n",
      "Epoch 9 batch 1900 train loss: 0.0065 test loss: 0.0128\n",
      "Epoch 9 batch 2000 train loss: 0.0076 test loss: 0.0127\n",
      "Epoch 9 batch 2100 train loss: 0.0122 test loss: 0.0126\n",
      "Epoch 9 batch 2200 train loss: 0.0121 test loss: 0.0126\n",
      "Epoch 9 batch 2300 train loss: 0.0165 test loss: 0.0127\n",
      "Epoch 9 batch 2400 train loss: 0.0094 test loss: 0.0129\n",
      "Epoch 9 batch 2500 train loss: 0.0099 test loss: 0.0128\n",
      "Epoch 9 batch 2600 train loss: 0.0132 test loss: 0.0133\n",
      "Epoch 9 batch 2700 train loss: 0.0149 test loss: 0.0130\n",
      "Epoch 9 batch 2800 train loss: 0.0142 test loss: 0.0131\n",
      "Epoch 9 batch 2900 train loss: 0.0097 test loss: 0.0129\n",
      "Epoch 9 batch 3000 train loss: 0.0096 test loss: 0.0127\n",
      "Epoch 10 batch 0 train loss: 0.0147 test loss: 0.0125\n",
      "Epoch 10 batch 100 train loss: 0.0145 test loss: 0.0128\n",
      "Epoch 10 batch 200 train loss: 0.0177 test loss: 0.0133\n",
      "Epoch 10 batch 300 train loss: 0.0105 test loss: 0.0135\n",
      "Epoch 10 batch 400 train loss: 0.0194 test loss: 0.0128\n",
      "Epoch 10 batch 500 train loss: 0.0116 test loss: 0.0127\n",
      "Epoch 10 batch 600 train loss: 0.0201 test loss: 0.0127\n",
      "Epoch 10 batch 700 train loss: 0.0094 test loss: 0.0127\n",
      "Epoch 10 batch 800 train loss: 0.0116 test loss: 0.0130\n",
      "Epoch 10 batch 900 train loss: 0.0106 test loss: 0.0129\n",
      "Epoch 10 batch 1000 train loss: 0.0181 test loss: 0.0132\n",
      "Epoch 10 batch 1100 train loss: 0.0144 test loss: 0.0128\n",
      "Epoch 10 batch 1200 train loss: 0.0104 test loss: 0.0127\n",
      "Epoch 10 batch 1300 train loss: 0.0138 test loss: 0.0129\n",
      "Epoch 10 batch 1400 train loss: 0.0120 test loss: 0.0127\n",
      "Epoch 10 batch 1500 train loss: 0.0151 test loss: 0.0128\n",
      "Epoch 10 batch 1600 train loss: 0.0105 test loss: 0.0129\n",
      "Epoch 10 batch 1700 train loss: 0.0113 test loss: 0.0129\n",
      "Epoch 10 batch 1800 train loss: 0.0126 test loss: 0.0129\n",
      "Epoch 10 batch 1900 train loss: 0.0166 test loss: 0.0131\n",
      "Epoch 10 batch 2000 train loss: 0.0106 test loss: 0.0125\n",
      "Epoch 10 batch 2100 train loss: 0.0159 test loss: 0.0126\n",
      "Epoch 10 batch 2200 train loss: 0.0110 test loss: 0.0132\n",
      "Epoch 10 batch 2300 train loss: 0.0123 test loss: 0.0125\n",
      "Epoch 10 batch 2400 train loss: 0.0096 test loss: 0.0128\n",
      "Epoch 10 batch 2500 train loss: 0.0065 test loss: 0.0128\n",
      "Epoch 10 batch 2600 train loss: 0.0154 test loss: 0.0132\n",
      "Epoch 10 batch 2700 train loss: 0.0150 test loss: 0.0131\n",
      "Epoch 10 batch 2800 train loss: 0.0159 test loss: 0.0128\n",
      "Epoch 10 batch 2900 train loss: 0.0142 test loss: 0.0131\n",
      "Epoch 10 batch 3000 train loss: 0.0100 test loss: 0.0124\n",
      "Epoch 11 batch 0 train loss: 0.0120 test loss: 0.0125\n",
      "Epoch 11 batch 100 train loss: 0.0095 test loss: 0.0127\n",
      "Epoch 11 batch 200 train loss: 0.0129 test loss: 0.0132\n",
      "Epoch 11 batch 300 train loss: 0.0085 test loss: 0.0129\n",
      "Epoch 11 batch 400 train loss: 0.0077 test loss: 0.0124\n",
      "Epoch 11 batch 500 train loss: 0.0099 test loss: 0.0129\n",
      "Epoch 11 batch 600 train loss: 0.0147 test loss: 0.0129\n",
      "Epoch 11 batch 700 train loss: 0.0129 test loss: 0.0129\n",
      "Epoch 11 batch 800 train loss: 0.0126 test loss: 0.0130\n",
      "Epoch 11 batch 900 train loss: 0.0143 test loss: 0.0131\n",
      "Epoch 11 batch 1000 train loss: 0.0115 test loss: 0.0130\n",
      "Epoch 11 batch 1100 train loss: 0.0155 test loss: 0.0129\n",
      "Epoch 11 batch 1200 train loss: 0.0118 test loss: 0.0128\n",
      "Epoch 11 batch 1300 train loss: 0.0097 test loss: 0.0130\n",
      "Epoch 11 batch 1400 train loss: 0.0170 test loss: 0.0124\n",
      "Epoch 11 batch 1500 train loss: 0.0151 test loss: 0.0125\n",
      "Epoch 11 batch 1600 train loss: 0.0109 test loss: 0.0133\n",
      "Epoch 11 batch 1700 train loss: 0.0163 test loss: 0.0129\n",
      "Epoch 11 batch 1800 train loss: 0.0123 test loss: 0.0126\n",
      "Epoch 11 batch 1900 train loss: 0.0106 test loss: 0.0128\n",
      "Epoch 11 batch 2000 train loss: 0.0125 test loss: 0.0127\n",
      "Epoch 11 batch 2100 train loss: 0.0106 test loss: 0.0130\n",
      "Epoch 11 batch 2200 train loss: 0.0131 test loss: 0.0132\n",
      "Epoch 11 batch 2300 train loss: 0.0157 test loss: 0.0130\n",
      "Epoch 11 batch 2400 train loss: 0.0121 test loss: 0.0127\n",
      "Epoch 11 batch 2500 train loss: 0.0138 test loss: 0.0130\n",
      "Epoch 11 batch 2600 train loss: 0.0116 test loss: 0.0128\n",
      "Epoch 11 batch 2700 train loss: 0.0138 test loss: 0.0129\n",
      "Epoch 11 batch 2800 train loss: 0.0098 test loss: 0.0129\n",
      "Epoch 11 batch 2900 train loss: 0.0079 test loss: 0.0128\n",
      "Epoch 11 batch 3000 train loss: 0.0139 test loss: 0.0126\n",
      "Epoch 12 batch 0 train loss: 0.0130 test loss: 0.0127\n",
      "Epoch 12 batch 100 train loss: 0.0148 test loss: 0.0127\n",
      "Epoch 12 batch 200 train loss: 0.0126 test loss: 0.0132\n",
      "Epoch 12 batch 300 train loss: 0.0157 test loss: 0.0130\n",
      "Epoch 12 batch 400 train loss: 0.0143 test loss: 0.0129\n",
      "Epoch 12 batch 500 train loss: 0.0099 test loss: 0.0128\n",
      "Epoch 12 batch 600 train loss: 0.0159 test loss: 0.0131\n",
      "Epoch 12 batch 700 train loss: 0.0132 test loss: 0.0128\n",
      "Epoch 12 batch 800 train loss: 0.0106 test loss: 0.0129\n",
      "Epoch 12 batch 900 train loss: 0.0146 test loss: 0.0129\n",
      "Epoch 12 batch 1000 train loss: 0.0127 test loss: 0.0131\n",
      "Epoch 12 batch 1100 train loss: 0.0155 test loss: 0.0131\n",
      "Epoch 12 batch 1200 train loss: 0.0111 test loss: 0.0127\n",
      "Epoch 12 batch 1300 train loss: 0.0106 test loss: 0.0126\n",
      "Epoch 12 batch 1400 train loss: 0.0132 test loss: 0.0125\n",
      "Epoch 12 batch 1500 train loss: 0.0122 test loss: 0.0129\n",
      "Epoch 12 batch 1600 train loss: 0.0118 test loss: 0.0132\n",
      "Epoch 12 batch 1700 train loss: 0.0140 test loss: 0.0129\n",
      "Epoch 12 batch 1800 train loss: 0.0144 test loss: 0.0126\n",
      "Epoch 12 batch 1900 train loss: 0.0147 test loss: 0.0127\n",
      "Epoch 12 batch 2000 train loss: 0.0156 test loss: 0.0129\n",
      "Epoch 12 batch 2100 train loss: 0.0090 test loss: 0.0128\n",
      "Epoch 12 batch 2200 train loss: 0.0149 test loss: 0.0129\n",
      "Epoch 12 batch 2300 train loss: 0.0117 test loss: 0.0128\n",
      "Epoch 12 batch 2400 train loss: 0.0069 test loss: 0.0126\n",
      "Epoch 12 batch 2500 train loss: 0.0147 test loss: 0.0128\n",
      "Epoch 12 batch 2600 train loss: 0.0159 test loss: 0.0130\n",
      "Epoch 12 batch 2700 train loss: 0.0133 test loss: 0.0132\n",
      "Epoch 12 batch 2800 train loss: 0.0137 test loss: 0.0129\n",
      "Epoch 12 batch 2900 train loss: 0.0106 test loss: 0.0126\n",
      "Epoch 12 batch 3000 train loss: 0.0142 test loss: 0.0124\n",
      "Epoch 13 batch 0 train loss: 0.0164 test loss: 0.0125\n",
      "Epoch 13 batch 100 train loss: 0.0112 test loss: 0.0128\n",
      "Epoch 13 batch 200 train loss: 0.0093 test loss: 0.0129\n",
      "Epoch 13 batch 300 train loss: 0.0146 test loss: 0.0132\n",
      "Epoch 13 batch 400 train loss: 0.0161 test loss: 0.0129\n",
      "Epoch 13 batch 500 train loss: 0.0151 test loss: 0.0124\n",
      "Epoch 13 batch 600 train loss: 0.0134 test loss: 0.0129\n",
      "Epoch 13 batch 700 train loss: 0.0083 test loss: 0.0125\n",
      "Epoch 13 batch 800 train loss: 0.0203 test loss: 0.0127\n",
      "Epoch 13 batch 900 train loss: 0.0113 test loss: 0.0134\n",
      "Epoch 13 batch 1000 train loss: 0.0124 test loss: 0.0130\n",
      "Epoch 13 batch 1100 train loss: 0.0136 test loss: 0.0128\n",
      "Epoch 13 batch 1200 train loss: 0.0092 test loss: 0.0129\n",
      "Epoch 13 batch 1300 train loss: 0.0108 test loss: 0.0129\n",
      "Epoch 13 batch 1400 train loss: 0.0103 test loss: 0.0126\n",
      "Epoch 13 batch 1500 train loss: 0.0090 test loss: 0.0130\n",
      "Epoch 13 batch 1600 train loss: 0.0080 test loss: 0.0131\n",
      "Epoch 13 batch 1700 train loss: 0.0154 test loss: 0.0131\n",
      "Epoch 13 batch 1800 train loss: 0.0134 test loss: 0.0126\n",
      "Epoch 13 batch 1900 train loss: 0.0115 test loss: 0.0128\n",
      "Epoch 13 batch 2000 train loss: 0.0104 test loss: 0.0125\n",
      "Epoch 13 batch 2100 train loss: 0.0113 test loss: 0.0127\n",
      "Epoch 13 batch 2200 train loss: 0.0145 test loss: 0.0131\n",
      "Epoch 13 batch 2300 train loss: 0.0148 test loss: 0.0126\n",
      "Epoch 13 batch 2400 train loss: 0.0131 test loss: 0.0130\n",
      "Epoch 13 batch 2500 train loss: 0.0093 test loss: 0.0129\n",
      "Epoch 13 batch 2600 train loss: 0.0114 test loss: 0.0129\n",
      "Epoch 13 batch 2700 train loss: 0.0132 test loss: 0.0130\n",
      "Epoch 13 batch 2800 train loss: 0.0169 test loss: 0.0130\n",
      "Epoch 13 batch 2900 train loss: 0.0142 test loss: 0.0125\n",
      "Epoch 13 batch 3000 train loss: 0.0118 test loss: 0.0126\n",
      "Epoch 14 batch 0 train loss: 0.0089 test loss: 0.0128\n",
      "Epoch 14 batch 100 train loss: 0.0106 test loss: 0.0132\n",
      "Epoch 14 batch 200 train loss: 0.0133 test loss: 0.0131\n",
      "Epoch 14 batch 300 train loss: 0.0123 test loss: 0.0130\n",
      "Epoch 14 batch 400 train loss: 0.0111 test loss: 0.0127\n",
      "Epoch 14 batch 500 train loss: 0.0163 test loss: 0.0127\n",
      "Epoch 14 batch 600 train loss: 0.0082 test loss: 0.0129\n",
      "Epoch 14 batch 700 train loss: 0.0105 test loss: 0.0128\n",
      "Epoch 14 batch 800 train loss: 0.0107 test loss: 0.0130\n",
      "Epoch 14 batch 900 train loss: 0.0086 test loss: 0.0128\n",
      "Epoch 14 batch 1000 train loss: 0.0122 test loss: 0.0128\n",
      "Epoch 14 batch 1100 train loss: 0.0171 test loss: 0.0129\n",
      "Epoch 14 batch 1200 train loss: 0.0137 test loss: 0.0133\n",
      "Epoch 14 batch 1300 train loss: 0.0172 test loss: 0.0129\n",
      "Epoch 14 batch 1400 train loss: 0.0111 test loss: 0.0127\n",
      "Epoch 14 batch 1500 train loss: 0.0157 test loss: 0.0129\n",
      "Epoch 14 batch 1600 train loss: 0.0119 test loss: 0.0129\n",
      "Epoch 14 batch 1700 train loss: 0.0072 test loss: 0.0133\n",
      "Epoch 14 batch 1800 train loss: 0.0099 test loss: 0.0126\n",
      "Epoch 14 batch 1900 train loss: 0.0115 test loss: 0.0129\n",
      "Epoch 14 batch 2000 train loss: 0.0109 test loss: 0.0125\n",
      "Epoch 14 batch 2100 train loss: 0.0121 test loss: 0.0129\n",
      "Epoch 14 batch 2200 train loss: 0.0100 test loss: 0.0132\n",
      "Epoch 14 batch 2300 train loss: 0.0090 test loss: 0.0126\n",
      "Epoch 14 batch 2400 train loss: 0.0173 test loss: 0.0128\n",
      "Epoch 14 batch 2500 train loss: 0.0142 test loss: 0.0128\n",
      "Epoch 14 batch 2600 train loss: 0.0152 test loss: 0.0129\n",
      "Epoch 14 batch 2700 train loss: 0.0146 test loss: 0.0129\n",
      "Epoch 14 batch 2800 train loss: 0.0155 test loss: 0.0129\n",
      "Epoch 14 batch 2900 train loss: 0.0097 test loss: 0.0129\n",
      "Epoch 14 batch 3000 train loss: 0.0121 test loss: 0.0128\n",
      "Epoch 15 batch 0 train loss: 0.0190 test loss: 0.0129\n",
      "Epoch 15 batch 100 train loss: 0.0112 test loss: 0.0129\n",
      "Epoch 15 batch 200 train loss: 0.0114 test loss: 0.0130\n",
      "Epoch 15 batch 300 train loss: 0.0056 test loss: 0.0130\n",
      "Epoch 15 batch 400 train loss: 0.0093 test loss: 0.0130\n",
      "Epoch 15 batch 500 train loss: 0.0162 test loss: 0.0127\n",
      "Epoch 15 batch 600 train loss: 0.0156 test loss: 0.0130\n",
      "Epoch 15 batch 700 train loss: 0.0112 test loss: 0.0129\n",
      "Epoch 15 batch 800 train loss: 0.0113 test loss: 0.0127\n",
      "Epoch 15 batch 900 train loss: 0.0108 test loss: 0.0131\n",
      "Epoch 15 batch 1000 train loss: 0.0111 test loss: 0.0131\n",
      "Epoch 15 batch 1100 train loss: 0.0127 test loss: 0.0129\n",
      "Epoch 15 batch 1200 train loss: 0.0135 test loss: 0.0133\n",
      "Epoch 15 batch 1300 train loss: 0.0106 test loss: 0.0127\n",
      "Epoch 15 batch 1400 train loss: 0.0088 test loss: 0.0128\n",
      "Epoch 15 batch 1500 train loss: 0.0163 test loss: 0.0127\n",
      "Epoch 15 batch 1600 train loss: 0.0094 test loss: 0.0129\n",
      "Epoch 15 batch 1700 train loss: 0.0110 test loss: 0.0132\n",
      "Epoch 15 batch 1800 train loss: 0.0120 test loss: 0.0126\n",
      "Epoch 15 batch 1900 train loss: 0.0096 test loss: 0.0130\n",
      "Epoch 15 batch 2000 train loss: 0.0109 test loss: 0.0126\n",
      "Epoch 15 batch 2100 train loss: 0.0118 test loss: 0.0128\n",
      "Epoch 15 batch 2200 train loss: 0.0163 test loss: 0.0130\n",
      "Epoch 15 batch 2300 train loss: 0.0126 test loss: 0.0130\n",
      "Epoch 15 batch 2400 train loss: 0.0147 test loss: 0.0126\n",
      "Epoch 15 batch 2500 train loss: 0.0105 test loss: 0.0130\n",
      "Epoch 15 batch 2600 train loss: 0.0111 test loss: 0.0127\n",
      "Epoch 15 batch 2700 train loss: 0.0126 test loss: 0.0130\n",
      "Epoch 15 batch 2800 train loss: 0.0139 test loss: 0.0129\n",
      "Epoch 15 batch 2900 train loss: 0.0129 test loss: 0.0129\n",
      "Epoch 15 batch 3000 train loss: 0.0131 test loss: 0.0125\n",
      "Epoch 16 batch 0 train loss: 0.0096 test loss: 0.0127\n",
      "Epoch 16 batch 100 train loss: 0.0128 test loss: 0.0129\n",
      "Epoch 16 batch 200 train loss: 0.0152 test loss: 0.0128\n",
      "Epoch 16 batch 300 train loss: 0.0140 test loss: 0.0129\n",
      "Epoch 16 batch 400 train loss: 0.0130 test loss: 0.0129\n",
      "Epoch 16 batch 500 train loss: 0.0125 test loss: 0.0129\n",
      "Epoch 16 batch 600 train loss: 0.0147 test loss: 0.0131\n",
      "Epoch 16 batch 700 train loss: 0.0130 test loss: 0.0126\n",
      "Epoch 16 batch 800 train loss: 0.0127 test loss: 0.0126\n",
      "Epoch 16 batch 900 train loss: 0.0107 test loss: 0.0135\n",
      "Epoch 16 batch 1000 train loss: 0.0113 test loss: 0.0129\n",
      "Epoch 16 batch 1100 train loss: 0.0176 test loss: 0.0129\n",
      "Epoch 16 batch 1200 train loss: 0.0154 test loss: 0.0125\n",
      "Epoch 16 batch 1300 train loss: 0.0118 test loss: 0.0130\n",
      "Epoch 16 batch 1400 train loss: 0.0157 test loss: 0.0127\n",
      "Epoch 16 batch 1500 train loss: 0.0139 test loss: 0.0129\n",
      "Epoch 16 batch 1600 train loss: 0.0138 test loss: 0.0131\n",
      "Epoch 16 batch 1700 train loss: 0.0144 test loss: 0.0126\n",
      "Epoch 16 batch 1800 train loss: 0.0131 test loss: 0.0126\n",
      "Epoch 16 batch 1900 train loss: 0.0110 test loss: 0.0128\n",
      "Epoch 16 batch 2000 train loss: 0.0106 test loss: 0.0124\n",
      "Epoch 16 batch 2100 train loss: 0.0081 test loss: 0.0130\n",
      "Epoch 16 batch 2200 train loss: 0.0151 test loss: 0.0131\n",
      "Epoch 16 batch 2300 train loss: 0.0155 test loss: 0.0127\n",
      "Epoch 16 batch 2400 train loss: 0.0104 test loss: 0.0132\n",
      "Epoch 16 batch 2500 train loss: 0.0110 test loss: 0.0132\n",
      "Epoch 16 batch 2600 train loss: 0.0125 test loss: 0.0130\n",
      "Epoch 16 batch 2700 train loss: 0.0138 test loss: 0.0134\n",
      "Epoch 16 batch 2800 train loss: 0.0140 test loss: 0.0129\n",
      "Epoch 16 batch 2900 train loss: 0.0136 test loss: 0.0130\n",
      "Epoch 16 batch 3000 train loss: 0.0172 test loss: 0.0125\n",
      "Epoch 17 batch 0 train loss: 0.0157 test loss: 0.0124\n",
      "Epoch 17 batch 100 train loss: 0.0134 test loss: 0.0127\n",
      "Epoch 17 batch 200 train loss: 0.0134 test loss: 0.0130\n",
      "Epoch 17 batch 300 train loss: 0.0116 test loss: 0.0133\n",
      "Epoch 17 batch 400 train loss: 0.0144 test loss: 0.0125\n",
      "Epoch 17 batch 500 train loss: 0.0118 test loss: 0.0129\n",
      "Epoch 17 batch 600 train loss: 0.0146 test loss: 0.0129\n",
      "early stop.\n",
      "Checkpoint 34 restored!!\n",
      "Training for loss rate 0.80 start.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ffnn_7/dense_21/kernel:0', 'ffnn_7/dense_21/bias:0', 'ffnn_7/batch_normalization_7/gamma:0', 'ffnn_7/batch_normalization_7/beta:0', 'ffnn_7/dense_22/kernel:0', 'ffnn_7/dense_22/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ffnn_7/dense_21/kernel:0', 'ffnn_7/dense_21/bias:0', 'ffnn_7/batch_normalization_7/gamma:0', 'ffnn_7/batch_normalization_7/beta:0', 'ffnn_7/dense_22/kernel:0', 'ffnn_7/dense_22/bias:0'] when minimizing the loss.\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.80p/ckpt-1\n",
      "Epoch 0 batch 0 train loss: 0.2439 test loss: 0.3197\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.80p/ckpt-2\n",
      "Epoch 0 batch 100 train loss: 0.1817 test loss: 0.2302\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.80p/ckpt-3\n",
      "Epoch 0 batch 200 train loss: 0.1475 test loss: 0.1760\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.80p/ckpt-4\n",
      "Epoch 0 batch 300 train loss: 0.0960 test loss: 0.1370\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.80p/ckpt-5\n",
      "Epoch 0 batch 400 train loss: 0.0752 test loss: 0.1075\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.80p/ckpt-6\n",
      "Epoch 0 batch 500 train loss: 0.0515 test loss: 0.0841\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.80p/ckpt-7\n",
      "Epoch 0 batch 600 train loss: 0.0450 test loss: 0.0666\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.80p/ckpt-8\n",
      "Epoch 0 batch 700 train loss: 0.0346 test loss: 0.0535\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.80p/ckpt-9\n",
      "Epoch 0 batch 800 train loss: 0.0305 test loss: 0.0433\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.80p/ckpt-10\n",
      "Epoch 0 batch 900 train loss: 0.0268 test loss: 0.0357\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.80p/ckpt-11\n",
      "Epoch 0 batch 1000 train loss: 0.0170 test loss: 0.0302\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.80p/ckpt-12\n",
      "Epoch 0 batch 1100 train loss: 0.0185 test loss: 0.0260\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.80p/ckpt-13\n",
      "Epoch 0 batch 1200 train loss: 0.0172 test loss: 0.0230\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.80p/ckpt-14\n",
      "Epoch 0 batch 1300 train loss: 0.0210 test loss: 0.0207\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.80p/ckpt-15\n",
      "Epoch 0 batch 1400 train loss: 0.0189 test loss: 0.0189\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.80p/ckpt-16\n",
      "Epoch 0 batch 1500 train loss: 0.0139 test loss: 0.0179\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.80p/ckpt-17\n",
      "Epoch 0 batch 1600 train loss: 0.0110 test loss: 0.0171\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.80p/ckpt-18\n",
      "Epoch 0 batch 1700 train loss: 0.0156 test loss: 0.0165\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.80p/ckpt-19\n",
      "Epoch 0 batch 1800 train loss: 0.0214 test loss: 0.0160\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.80p/ckpt-20\n",
      "Epoch 0 batch 1900 train loss: 0.0137 test loss: 0.0156\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.80p/ckpt-21\n",
      "Epoch 0 batch 2000 train loss: 0.0158 test loss: 0.0153\n",
      "Epoch 0 batch 2100 train loss: 0.0173 test loss: 0.0154\n",
      "Epoch 0 batch 2200 train loss: 0.0175 test loss: 0.0153\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.80p/ckpt-22\n",
      "Epoch 0 batch 2300 train loss: 0.0127 test loss: 0.0152\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.80p/ckpt-23\n",
      "Epoch 0 batch 2400 train loss: 0.0176 test loss: 0.0151\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.80p/ckpt-24\n",
      "Epoch 0 batch 2500 train loss: 0.0124 test loss: 0.0151\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.80p/ckpt-25\n",
      "Epoch 0 batch 2600 train loss: 0.0120 test loss: 0.0150\n",
      "Epoch 0 batch 2700 train loss: 0.0147 test loss: 0.0151\n",
      "Epoch 0 batch 2800 train loss: 0.0102 test loss: 0.0152\n",
      "Epoch 0 batch 2900 train loss: 0.0129 test loss: 0.0151\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.80p/ckpt-26\n",
      "Epoch 0 batch 3000 train loss: 0.0149 test loss: 0.0147\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ffnn_7/dense_21/kernel:0', 'ffnn_7/dense_21/bias:0', 'ffnn_7/batch_normalization_7/gamma:0', 'ffnn_7/batch_normalization_7/beta:0', 'ffnn_7/dense_22/kernel:0', 'ffnn_7/dense_22/bias:0'] when minimizing the loss.\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.80p/ckpt-27\n",
      "Epoch 1 batch 0 train loss: 0.0123 test loss: 0.0145\n",
      "Epoch 1 batch 100 train loss: 0.0116 test loss: 0.0148\n",
      "Epoch 1 batch 200 train loss: 0.0139 test loss: 0.0151\n",
      "Epoch 1 batch 300 train loss: 0.0113 test loss: 0.0149\n",
      "Epoch 1 batch 400 train loss: 0.0197 test loss: 0.0149\n",
      "Epoch 1 batch 500 train loss: 0.0115 test loss: 0.0149\n",
      "Epoch 1 batch 600 train loss: 0.0157 test loss: 0.0149\n",
      "Epoch 1 batch 700 train loss: 0.0154 test loss: 0.0150\n",
      "Epoch 1 batch 800 train loss: 0.0171 test loss: 0.0150\n",
      "Epoch 1 batch 900 train loss: 0.0127 test loss: 0.0151\n",
      "Epoch 1 batch 1000 train loss: 0.0122 test loss: 0.0151\n",
      "Epoch 1 batch 1100 train loss: 0.0140 test loss: 0.0149\n",
      "Epoch 1 batch 1200 train loss: 0.0117 test loss: 0.0148\n",
      "Epoch 1 batch 1300 train loss: 0.0167 test loss: 0.0149\n",
      "Epoch 1 batch 1400 train loss: 0.0147 test loss: 0.0147\n",
      "Epoch 1 batch 1500 train loss: 0.0132 test loss: 0.0149\n",
      "Epoch 1 batch 1600 train loss: 0.0150 test loss: 0.0153\n",
      "Epoch 1 batch 1700 train loss: 0.0158 test loss: 0.0149\n",
      "Epoch 1 batch 1800 train loss: 0.0125 test loss: 0.0147\n",
      "Epoch 1 batch 1900 train loss: 0.0183 test loss: 0.0149\n",
      "Epoch 1 batch 2000 train loss: 0.0140 test loss: 0.0151\n",
      "Epoch 1 batch 2100 train loss: 0.0143 test loss: 0.0148\n",
      "Epoch 1 batch 2200 train loss: 0.0080 test loss: 0.0150\n",
      "Epoch 1 batch 2300 train loss: 0.0088 test loss: 0.0148\n",
      "Epoch 1 batch 2400 train loss: 0.0190 test loss: 0.0150\n",
      "Epoch 1 batch 2500 train loss: 0.0141 test loss: 0.0150\n",
      "Epoch 1 batch 2600 train loss: 0.0173 test loss: 0.0149\n",
      "Epoch 1 batch 2700 train loss: 0.0128 test loss: 0.0150\n",
      "Epoch 1 batch 2800 train loss: 0.0166 test loss: 0.0151\n",
      "Epoch 1 batch 2900 train loss: 0.0136 test loss: 0.0148\n",
      "Epoch 1 batch 3000 train loss: 0.0103 test loss: 0.0146\n",
      "Epoch 2 batch 0 train loss: 0.0127 test loss: 0.0145\n",
      "Epoch 2 batch 100 train loss: 0.0144 test loss: 0.0151\n",
      "Epoch 2 batch 200 train loss: 0.0163 test loss: 0.0151\n",
      "Epoch 2 batch 300 train loss: 0.0123 test loss: 0.0148\n",
      "Epoch 2 batch 400 train loss: 0.0156 test loss: 0.0148\n",
      "Epoch 2 batch 500 train loss: 0.0138 test loss: 0.0147\n",
      "Epoch 2 batch 600 train loss: 0.0150 test loss: 0.0150\n",
      "Epoch 2 batch 700 train loss: 0.0184 test loss: 0.0149\n",
      "Epoch 2 batch 800 train loss: 0.0131 test loss: 0.0148\n",
      "Epoch 2 batch 900 train loss: 0.0186 test loss: 0.0147\n",
      "Epoch 2 batch 1000 train loss: 0.0145 test loss: 0.0152\n",
      "Epoch 2 batch 1100 train loss: 0.0134 test loss: 0.0151\n",
      "Epoch 2 batch 1200 train loss: 0.0085 test loss: 0.0149\n",
      "Epoch 2 batch 1300 train loss: 0.0133 test loss: 0.0149\n",
      "Epoch 2 batch 1400 train loss: 0.0155 test loss: 0.0146\n",
      "Epoch 2 batch 1500 train loss: 0.0101 test loss: 0.0151\n",
      "Epoch 2 batch 1600 train loss: 0.0164 test loss: 0.0150\n",
      "Epoch 2 batch 1700 train loss: 0.0133 test loss: 0.0152\n",
      "Epoch 2 batch 1800 train loss: 0.0119 test loss: 0.0147\n",
      "Epoch 2 batch 1900 train loss: 0.0207 test loss: 0.0147\n",
      "Epoch 2 batch 2000 train loss: 0.0175 test loss: 0.0148\n",
      "Epoch 2 batch 2100 train loss: 0.0073 test loss: 0.0152\n",
      "Epoch 2 batch 2200 train loss: 0.0138 test loss: 0.0148\n",
      "Epoch 2 batch 2300 train loss: 0.0190 test loss: 0.0148\n",
      "Epoch 2 batch 2400 train loss: 0.0146 test loss: 0.0153\n",
      "Epoch 2 batch 2500 train loss: 0.0129 test loss: 0.0147\n",
      "Epoch 2 batch 2600 train loss: 0.0201 test loss: 0.0146\n",
      "Epoch 2 batch 2700 train loss: 0.0127 test loss: 0.0151\n",
      "Epoch 2 batch 2800 train loss: 0.0138 test loss: 0.0148\n",
      "Epoch 2 batch 2900 train loss: 0.0169 test loss: 0.0151\n",
      "Epoch 2 batch 3000 train loss: 0.0100 test loss: 0.0147\n",
      "Epoch 3 batch 0 train loss: 0.0165 test loss: 0.0148\n",
      "Epoch 3 batch 100 train loss: 0.0144 test loss: 0.0148\n",
      "Epoch 3 batch 200 train loss: 0.0132 test loss: 0.0150\n",
      "Epoch 3 batch 300 train loss: 0.0145 test loss: 0.0150\n",
      "Epoch 3 batch 400 train loss: 0.0147 test loss: 0.0145\n",
      "Epoch 3 batch 500 train loss: 0.0123 test loss: 0.0148\n",
      "Epoch 3 batch 600 train loss: 0.0113 test loss: 0.0147\n",
      "Epoch 3 batch 700 train loss: 0.0143 test loss: 0.0148\n",
      "Epoch 3 batch 800 train loss: 0.0131 test loss: 0.0151\n",
      "Epoch 3 batch 900 train loss: 0.0123 test loss: 0.0147\n",
      "Epoch 3 batch 1000 train loss: 0.0119 test loss: 0.0148\n",
      "Epoch 3 batch 1100 train loss: 0.0144 test loss: 0.0151\n",
      "Epoch 3 batch 1200 train loss: 0.0117 test loss: 0.0149\n",
      "Epoch 3 batch 1300 train loss: 0.0137 test loss: 0.0146\n",
      "Epoch 3 batch 1400 train loss: 0.0161 test loss: 0.0146\n",
      "Epoch 3 batch 1500 train loss: 0.0095 test loss: 0.0153\n",
      "Epoch 3 batch 1600 train loss: 0.0132 test loss: 0.0152\n",
      "Epoch 3 batch 1700 train loss: 0.0123 test loss: 0.0149\n",
      "Epoch 3 batch 1800 train loss: 0.0125 test loss: 0.0148\n",
      "Epoch 3 batch 1900 train loss: 0.0129 test loss: 0.0148\n",
      "Epoch 3 batch 2000 train loss: 0.0094 test loss: 0.0146\n",
      "Epoch 3 batch 2100 train loss: 0.0202 test loss: 0.0148\n",
      "Epoch 3 batch 2200 train loss: 0.0157 test loss: 0.0150\n",
      "Epoch 3 batch 2300 train loss: 0.0141 test loss: 0.0146\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.80p/ckpt-28\n",
      "Epoch 3 batch 2400 train loss: 0.0093 test loss: 0.0145\n",
      "Epoch 3 batch 2500 train loss: 0.0155 test loss: 0.0146\n",
      "Epoch 3 batch 2600 train loss: 0.0161 test loss: 0.0148\n",
      "Epoch 3 batch 2700 train loss: 0.0129 test loss: 0.0153\n",
      "Epoch 3 batch 2800 train loss: 0.0138 test loss: 0.0146\n",
      "Epoch 3 batch 2900 train loss: 0.0119 test loss: 0.0147\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.80p/ckpt-29\n",
      "Epoch 3 batch 3000 train loss: 0.0122 test loss: 0.0144\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.80p/ckpt-30\n",
      "Epoch 4 batch 0 train loss: 0.0162 test loss: 0.0144\n",
      "Epoch 4 batch 100 train loss: 0.0144 test loss: 0.0148\n",
      "Epoch 4 batch 200 train loss: 0.0116 test loss: 0.0152\n",
      "Epoch 4 batch 300 train loss: 0.0200 test loss: 0.0149\n",
      "Epoch 4 batch 400 train loss: 0.0121 test loss: 0.0147\n",
      "Epoch 4 batch 500 train loss: 0.0165 test loss: 0.0150\n",
      "Epoch 4 batch 600 train loss: 0.0154 test loss: 0.0149\n",
      "Epoch 4 batch 700 train loss: 0.0160 test loss: 0.0145\n",
      "Epoch 4 batch 800 train loss: 0.0110 test loss: 0.0148\n",
      "Epoch 4 batch 900 train loss: 0.0144 test loss: 0.0153\n",
      "Epoch 4 batch 1000 train loss: 0.0102 test loss: 0.0150\n",
      "Epoch 4 batch 1100 train loss: 0.0119 test loss: 0.0149\n",
      "Epoch 4 batch 1200 train loss: 0.0141 test loss: 0.0147\n",
      "Epoch 4 batch 1300 train loss: 0.0130 test loss: 0.0145\n",
      "Epoch 4 batch 1400 train loss: 0.0113 test loss: 0.0145\n",
      "Epoch 4 batch 1500 train loss: 0.0077 test loss: 0.0150\n",
      "Epoch 4 batch 1600 train loss: 0.0137 test loss: 0.0148\n",
      "Epoch 4 batch 1700 train loss: 0.0165 test loss: 0.0151\n",
      "Epoch 4 batch 1800 train loss: 0.0162 test loss: 0.0144\n",
      "Epoch 4 batch 1900 train loss: 0.0178 test loss: 0.0150\n",
      "Epoch 4 batch 2000 train loss: 0.0127 test loss: 0.0145\n",
      "Epoch 4 batch 2100 train loss: 0.0163 test loss: 0.0149\n",
      "Epoch 4 batch 2200 train loss: 0.0101 test loss: 0.0148\n",
      "Epoch 4 batch 2300 train loss: 0.0205 test loss: 0.0144\n",
      "Epoch 4 batch 2400 train loss: 0.0132 test loss: 0.0148\n",
      "Epoch 4 batch 2500 train loss: 0.0097 test loss: 0.0147\n",
      "Epoch 4 batch 2600 train loss: 0.0144 test loss: 0.0148\n",
      "Epoch 4 batch 2700 train loss: 0.0120 test loss: 0.0150\n",
      "Epoch 4 batch 2800 train loss: 0.0177 test loss: 0.0152\n",
      "Epoch 4 batch 2900 train loss: 0.0130 test loss: 0.0149\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.80p/ckpt-31\n",
      "Epoch 4 batch 3000 train loss: 0.0124 test loss: 0.0143\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.80p/ckpt-32\n",
      "Epoch 5 batch 0 train loss: 0.0095 test loss: 0.0142\n",
      "Epoch 5 batch 100 train loss: 0.0110 test loss: 0.0150\n",
      "Epoch 5 batch 200 train loss: 0.0120 test loss: 0.0150\n",
      "Epoch 5 batch 300 train loss: 0.0166 test loss: 0.0148\n",
      "Epoch 5 batch 400 train loss: 0.0157 test loss: 0.0148\n",
      "Epoch 5 batch 500 train loss: 0.0159 test loss: 0.0146\n",
      "Epoch 5 batch 600 train loss: 0.0181 test loss: 0.0149\n",
      "Epoch 5 batch 700 train loss: 0.0099 test loss: 0.0147\n",
      "Epoch 5 batch 800 train loss: 0.0146 test loss: 0.0150\n",
      "Epoch 5 batch 900 train loss: 0.0161 test loss: 0.0149\n",
      "Epoch 5 batch 1000 train loss: 0.0127 test loss: 0.0148\n",
      "Epoch 5 batch 1100 train loss: 0.0136 test loss: 0.0148\n",
      "Epoch 5 batch 1200 train loss: 0.0132 test loss: 0.0146\n",
      "Epoch 5 batch 1300 train loss: 0.0127 test loss: 0.0149\n",
      "Epoch 5 batch 1400 train loss: 0.0166 test loss: 0.0144\n",
      "Epoch 5 batch 1500 train loss: 0.0100 test loss: 0.0151\n",
      "Epoch 5 batch 1600 train loss: 0.0099 test loss: 0.0151\n",
      "Epoch 5 batch 1700 train loss: 0.0169 test loss: 0.0156\n",
      "Epoch 5 batch 1800 train loss: 0.0212 test loss: 0.0145\n",
      "Epoch 5 batch 1900 train loss: 0.0145 test loss: 0.0149\n",
      "Epoch 5 batch 2000 train loss: 0.0199 test loss: 0.0148\n",
      "Epoch 5 batch 2100 train loss: 0.0143 test loss: 0.0151\n",
      "Epoch 5 batch 2200 train loss: 0.0109 test loss: 0.0148\n",
      "Epoch 5 batch 2300 train loss: 0.0142 test loss: 0.0145\n",
      "Epoch 5 batch 2400 train loss: 0.0201 test loss: 0.0146\n",
      "Epoch 5 batch 2500 train loss: 0.0167 test loss: 0.0149\n",
      "Epoch 5 batch 2600 train loss: 0.0114 test loss: 0.0145\n",
      "Epoch 5 batch 2700 train loss: 0.0101 test loss: 0.0150\n",
      "Epoch 5 batch 2800 train loss: 0.0155 test loss: 0.0150\n",
      "Epoch 5 batch 2900 train loss: 0.0170 test loss: 0.0151\n",
      "Epoch 5 batch 3000 train loss: 0.0162 test loss: 0.0147\n",
      "Epoch 6 batch 0 train loss: 0.0136 test loss: 0.0145\n",
      "Epoch 6 batch 100 train loss: 0.0110 test loss: 0.0151\n",
      "Epoch 6 batch 200 train loss: 0.0139 test loss: 0.0149\n",
      "Epoch 6 batch 300 train loss: 0.0151 test loss: 0.0150\n",
      "Epoch 6 batch 400 train loss: 0.0120 test loss: 0.0148\n",
      "Epoch 6 batch 500 train loss: 0.0162 test loss: 0.0148\n",
      "Epoch 6 batch 600 train loss: 0.0159 test loss: 0.0147\n",
      "Epoch 6 batch 700 train loss: 0.0096 test loss: 0.0145\n",
      "Epoch 6 batch 800 train loss: 0.0153 test loss: 0.0146\n",
      "Epoch 6 batch 900 train loss: 0.0138 test loss: 0.0150\n",
      "Epoch 6 batch 1000 train loss: 0.0166 test loss: 0.0151\n",
      "Epoch 6 batch 1100 train loss: 0.0133 test loss: 0.0149\n",
      "Epoch 6 batch 1200 train loss: 0.0105 test loss: 0.0148\n",
      "Epoch 6 batch 1300 train loss: 0.0118 test loss: 0.0144\n",
      "Epoch 6 batch 1400 train loss: 0.0116 test loss: 0.0146\n",
      "Epoch 6 batch 1500 train loss: 0.0166 test loss: 0.0148\n",
      "Epoch 6 batch 1600 train loss: 0.0162 test loss: 0.0151\n",
      "Epoch 6 batch 1700 train loss: 0.0111 test loss: 0.0152\n",
      "Epoch 6 batch 1800 train loss: 0.0122 test loss: 0.0149\n",
      "Epoch 6 batch 1900 train loss: 0.0159 test loss: 0.0151\n",
      "Epoch 6 batch 2000 train loss: 0.0102 test loss: 0.0144\n",
      "Epoch 6 batch 2100 train loss: 0.0140 test loss: 0.0149\n",
      "Epoch 6 batch 2200 train loss: 0.0176 test loss: 0.0151\n",
      "Epoch 6 batch 2300 train loss: 0.0131 test loss: 0.0150\n",
      "Epoch 6 batch 2400 train loss: 0.0114 test loss: 0.0149\n",
      "Epoch 6 batch 2500 train loss: 0.0163 test loss: 0.0149\n",
      "Epoch 6 batch 2600 train loss: 0.0114 test loss: 0.0148\n",
      "Epoch 6 batch 2700 train loss: 0.0190 test loss: 0.0148\n",
      "Epoch 6 batch 2800 train loss: 0.0102 test loss: 0.0150\n",
      "Epoch 6 batch 2900 train loss: 0.0186 test loss: 0.0148\n",
      "Epoch 6 batch 3000 train loss: 0.0132 test loss: 0.0145\n",
      "Epoch 7 batch 0 train loss: 0.0127 test loss: 0.0146\n",
      "Epoch 7 batch 100 train loss: 0.0144 test loss: 0.0148\n",
      "Epoch 7 batch 200 train loss: 0.0144 test loss: 0.0154\n",
      "Epoch 7 batch 300 train loss: 0.0114 test loss: 0.0150\n",
      "Epoch 7 batch 400 train loss: 0.0129 test loss: 0.0146\n",
      "Epoch 7 batch 500 train loss: 0.0142 test loss: 0.0150\n",
      "Epoch 7 batch 600 train loss: 0.0143 test loss: 0.0150\n",
      "Epoch 7 batch 700 train loss: 0.0165 test loss: 0.0147\n",
      "Epoch 7 batch 800 train loss: 0.0121 test loss: 0.0150\n",
      "Epoch 7 batch 900 train loss: 0.0122 test loss: 0.0153\n",
      "Epoch 7 batch 1000 train loss: 0.0173 test loss: 0.0150\n",
      "Epoch 7 batch 1100 train loss: 0.0107 test loss: 0.0148\n",
      "Epoch 7 batch 1200 train loss: 0.0128 test loss: 0.0149\n",
      "Epoch 7 batch 1300 train loss: 0.0120 test loss: 0.0143\n",
      "Epoch 7 batch 1400 train loss: 0.0121 test loss: 0.0144\n",
      "Epoch 7 batch 1500 train loss: 0.0121 test loss: 0.0150\n",
      "Epoch 7 batch 1600 train loss: 0.0109 test loss: 0.0149\n",
      "Epoch 7 batch 1700 train loss: 0.0167 test loss: 0.0149\n",
      "Epoch 7 batch 1800 train loss: 0.0125 test loss: 0.0144\n",
      "Epoch 7 batch 1900 train loss: 0.0156 test loss: 0.0150\n",
      "Epoch 7 batch 2000 train loss: 0.0122 test loss: 0.0144\n",
      "Epoch 7 batch 2100 train loss: 0.0107 test loss: 0.0150\n",
      "Epoch 7 batch 2200 train loss: 0.0119 test loss: 0.0149\n",
      "Epoch 7 batch 2300 train loss: 0.0159 test loss: 0.0146\n",
      "Epoch 7 batch 2400 train loss: 0.0151 test loss: 0.0148\n",
      "Epoch 7 batch 2500 train loss: 0.0178 test loss: 0.0146\n",
      "Epoch 7 batch 2600 train loss: 0.0122 test loss: 0.0147\n",
      "Epoch 7 batch 2700 train loss: 0.0163 test loss: 0.0148\n",
      "Epoch 7 batch 2800 train loss: 0.0180 test loss: 0.0147\n",
      "Epoch 7 batch 2900 train loss: 0.0110 test loss: 0.0146\n",
      "Epoch 7 batch 3000 train loss: 0.0143 test loss: 0.0146\n",
      "Epoch 8 batch 0 train loss: 0.0118 test loss: 0.0147\n",
      "Epoch 8 batch 100 train loss: 0.0103 test loss: 0.0147\n",
      "Epoch 8 batch 200 train loss: 0.0149 test loss: 0.0150\n",
      "Epoch 8 batch 300 train loss: 0.0118 test loss: 0.0146\n",
      "Epoch 8 batch 400 train loss: 0.0140 test loss: 0.0149\n",
      "Epoch 8 batch 500 train loss: 0.0164 test loss: 0.0146\n",
      "Epoch 8 batch 600 train loss: 0.0159 test loss: 0.0152\n",
      "Epoch 8 batch 700 train loss: 0.0165 test loss: 0.0143\n",
      "Epoch 8 batch 800 train loss: 0.0164 test loss: 0.0153\n",
      "Epoch 8 batch 900 train loss: 0.0204 test loss: 0.0145\n",
      "Epoch 8 batch 1000 train loss: 0.0148 test loss: 0.0153\n",
      "Epoch 8 batch 1100 train loss: 0.0177 test loss: 0.0148\n",
      "Epoch 8 batch 1200 train loss: 0.0112 test loss: 0.0149\n",
      "Epoch 8 batch 1300 train loss: 0.0122 test loss: 0.0144\n",
      "Epoch 8 batch 1400 train loss: 0.0160 test loss: 0.0148\n",
      "Epoch 8 batch 1500 train loss: 0.0127 test loss: 0.0148\n",
      "Epoch 8 batch 1600 train loss: 0.0132 test loss: 0.0152\n",
      "Epoch 8 batch 1700 train loss: 0.0117 test loss: 0.0152\n",
      "Epoch 8 batch 1800 train loss: 0.0167 test loss: 0.0146\n",
      "Epoch 8 batch 1900 train loss: 0.0115 test loss: 0.0147\n",
      "Epoch 8 batch 2000 train loss: 0.0180 test loss: 0.0146\n",
      "Epoch 8 batch 2100 train loss: 0.0116 test loss: 0.0149\n",
      "Epoch 8 batch 2200 train loss: 0.0181 test loss: 0.0149\n",
      "Epoch 8 batch 2300 train loss: 0.0176 test loss: 0.0148\n",
      "Epoch 8 batch 2400 train loss: 0.0141 test loss: 0.0146\n",
      "Epoch 8 batch 2500 train loss: 0.0126 test loss: 0.0151\n",
      "Epoch 8 batch 2600 train loss: 0.0120 test loss: 0.0147\n",
      "Epoch 8 batch 2700 train loss: 0.0137 test loss: 0.0149\n",
      "Epoch 8 batch 2800 train loss: 0.0107 test loss: 0.0150\n",
      "Epoch 8 batch 2900 train loss: 0.0154 test loss: 0.0149\n",
      "Epoch 8 batch 3000 train loss: 0.0118 test loss: 0.0143\n",
      "Epoch 9 batch 0 train loss: 0.0140 test loss: 0.0144\n",
      "Epoch 9 batch 100 train loss: 0.0153 test loss: 0.0149\n",
      "Epoch 9 batch 200 train loss: 0.0124 test loss: 0.0150\n",
      "Epoch 9 batch 300 train loss: 0.0154 test loss: 0.0150\n",
      "Epoch 9 batch 400 train loss: 0.0139 test loss: 0.0149\n",
      "Epoch 9 batch 500 train loss: 0.0109 test loss: 0.0147\n",
      "Epoch 9 batch 600 train loss: 0.0132 test loss: 0.0145\n",
      "Epoch 9 batch 700 train loss: 0.0171 test loss: 0.0145\n",
      "Epoch 9 batch 800 train loss: 0.0151 test loss: 0.0151\n",
      "Epoch 9 batch 900 train loss: 0.0172 test loss: 0.0152\n",
      "Epoch 9 batch 1000 train loss: 0.0190 test loss: 0.0149\n",
      "Epoch 9 batch 1100 train loss: 0.0188 test loss: 0.0148\n",
      "Epoch 9 batch 1200 train loss: 0.0119 test loss: 0.0145\n",
      "Epoch 9 batch 1300 train loss: 0.0123 test loss: 0.0149\n",
      "Epoch 9 batch 1400 train loss: 0.0154 test loss: 0.0149\n",
      "Epoch 9 batch 1500 train loss: 0.0194 test loss: 0.0151\n",
      "Epoch 9 batch 1600 train loss: 0.0162 test loss: 0.0150\n",
      "Epoch 9 batch 1700 train loss: 0.0192 test loss: 0.0152\n",
      "Epoch 9 batch 1800 train loss: 0.0105 test loss: 0.0147\n",
      "Epoch 9 batch 1900 train loss: 0.0138 test loss: 0.0149\n",
      "Epoch 9 batch 2000 train loss: 0.0143 test loss: 0.0150\n",
      "Epoch 9 batch 2100 train loss: 0.0148 test loss: 0.0149\n",
      "Epoch 9 batch 2200 train loss: 0.0112 test loss: 0.0149\n",
      "Epoch 9 batch 2300 train loss: 0.0167 test loss: 0.0146\n",
      "Epoch 9 batch 2400 train loss: 0.0109 test loss: 0.0144\n",
      "Epoch 9 batch 2500 train loss: 0.0110 test loss: 0.0148\n",
      "Epoch 9 batch 2600 train loss: 0.0113 test loss: 0.0149\n",
      "Epoch 9 batch 2700 train loss: 0.0166 test loss: 0.0149\n",
      "Epoch 9 batch 2800 train loss: 0.0119 test loss: 0.0149\n",
      "Epoch 9 batch 2900 train loss: 0.0118 test loss: 0.0148\n",
      "Epoch 9 batch 3000 train loss: 0.0165 test loss: 0.0147\n",
      "Epoch 10 batch 0 train loss: 0.0119 test loss: 0.0148\n",
      "Epoch 10 batch 100 train loss: 0.0118 test loss: 0.0148\n",
      "Epoch 10 batch 200 train loss: 0.0144 test loss: 0.0148\n",
      "Epoch 10 batch 300 train loss: 0.0121 test loss: 0.0147\n",
      "Epoch 10 batch 400 train loss: 0.0156 test loss: 0.0145\n",
      "Epoch 10 batch 500 train loss: 0.0175 test loss: 0.0148\n",
      "Epoch 10 batch 600 train loss: 0.0105 test loss: 0.0147\n",
      "Epoch 10 batch 700 train loss: 0.0133 test loss: 0.0145\n",
      "Epoch 10 batch 800 train loss: 0.0139 test loss: 0.0146\n",
      "Epoch 10 batch 900 train loss: 0.0176 test loss: 0.0156\n",
      "Epoch 10 batch 1000 train loss: 0.0146 test loss: 0.0151\n",
      "Epoch 10 batch 1100 train loss: 0.0140 test loss: 0.0149\n",
      "Epoch 10 batch 1200 train loss: 0.0174 test loss: 0.0149\n",
      "Epoch 10 batch 1300 train loss: 0.0092 test loss: 0.0148\n",
      "Epoch 10 batch 1400 train loss: 0.0156 test loss: 0.0146\n",
      "Epoch 10 batch 1500 train loss: 0.0123 test loss: 0.0151\n",
      "Epoch 10 batch 1600 train loss: 0.0147 test loss: 0.0153\n",
      "Epoch 10 batch 1700 train loss: 0.0138 test loss: 0.0150\n",
      "Epoch 10 batch 1800 train loss: 0.0122 test loss: 0.0145\n",
      "Epoch 10 batch 1900 train loss: 0.0110 test loss: 0.0149\n",
      "Epoch 10 batch 2000 train loss: 0.0118 test loss: 0.0144\n",
      "Epoch 10 batch 2100 train loss: 0.0134 test loss: 0.0151\n",
      "Epoch 10 batch 2200 train loss: 0.0148 test loss: 0.0146\n",
      "Epoch 10 batch 2300 train loss: 0.0125 test loss: 0.0146\n",
      "Epoch 10 batch 2400 train loss: 0.0172 test loss: 0.0146\n",
      "Epoch 10 batch 2500 train loss: 0.0133 test loss: 0.0148\n",
      "Epoch 10 batch 2600 train loss: 0.0131 test loss: 0.0148\n",
      "Epoch 10 batch 2700 train loss: 0.0155 test loss: 0.0150\n",
      "Epoch 10 batch 2800 train loss: 0.0133 test loss: 0.0149\n",
      "Epoch 10 batch 2900 train loss: 0.0130 test loss: 0.0149\n",
      "Epoch 10 batch 3000 train loss: 0.0137 test loss: 0.0143\n",
      "Epoch 11 batch 0 train loss: 0.0194 test loss: 0.0142\n",
      "Epoch 11 batch 100 train loss: 0.0180 test loss: 0.0150\n",
      "Epoch 11 batch 200 train loss: 0.0150 test loss: 0.0150\n",
      "Epoch 11 batch 300 train loss: 0.0110 test loss: 0.0151\n",
      "Epoch 11 batch 400 train loss: 0.0121 test loss: 0.0146\n",
      "Epoch 11 batch 500 train loss: 0.0157 test loss: 0.0148\n",
      "Epoch 11 batch 600 train loss: 0.0110 test loss: 0.0150\n",
      "Epoch 11 batch 700 train loss: 0.0097 test loss: 0.0143\n",
      "Epoch 11 batch 800 train loss: 0.0121 test loss: 0.0148\n",
      "Epoch 11 batch 900 train loss: 0.0153 test loss: 0.0150\n",
      "Epoch 11 batch 1000 train loss: 0.0151 test loss: 0.0150\n",
      "Epoch 11 batch 1100 train loss: 0.0117 test loss: 0.0147\n",
      "Epoch 11 batch 1200 train loss: 0.0136 test loss: 0.0147\n",
      "Epoch 11 batch 1300 train loss: 0.0163 test loss: 0.0145\n",
      "Epoch 11 batch 1400 train loss: 0.0119 test loss: 0.0146\n",
      "Epoch 11 batch 1500 train loss: 0.0156 test loss: 0.0147\n",
      "Epoch 11 batch 1600 train loss: 0.0165 test loss: 0.0152\n",
      "Epoch 11 batch 1700 train loss: 0.0135 test loss: 0.0149\n",
      "Epoch 11 batch 1800 train loss: 0.0113 test loss: 0.0147\n",
      "Epoch 11 batch 1900 train loss: 0.0128 test loss: 0.0147\n",
      "Epoch 11 batch 2000 train loss: 0.0104 test loss: 0.0146\n",
      "Epoch 11 batch 2100 train loss: 0.0126 test loss: 0.0150\n",
      "Epoch 11 batch 2200 train loss: 0.0180 test loss: 0.0149\n",
      "Epoch 11 batch 2300 train loss: 0.0103 test loss: 0.0147\n",
      "Epoch 11 batch 2400 train loss: 0.0148 test loss: 0.0149\n",
      "Epoch 11 batch 2500 train loss: 0.0113 test loss: 0.0151\n",
      "Epoch 11 batch 2600 train loss: 0.0149 test loss: 0.0149\n",
      "Epoch 11 batch 2700 train loss: 0.0108 test loss: 0.0145\n",
      "Epoch 11 batch 2800 train loss: 0.0161 test loss: 0.0149\n",
      "Epoch 11 batch 2900 train loss: 0.0148 test loss: 0.0145\n",
      "Epoch 11 batch 3000 train loss: 0.0148 test loss: 0.0146\n",
      "Epoch 12 batch 0 train loss: 0.0125 test loss: 0.0148\n",
      "Epoch 12 batch 100 train loss: 0.0150 test loss: 0.0150\n",
      "Epoch 12 batch 200 train loss: 0.0122 test loss: 0.0151\n",
      "Epoch 12 batch 300 train loss: 0.0145 test loss: 0.0148\n",
      "Epoch 12 batch 400 train loss: 0.0102 test loss: 0.0145\n",
      "Epoch 12 batch 500 train loss: 0.0179 test loss: 0.0152\n",
      "Epoch 12 batch 600 train loss: 0.0168 test loss: 0.0147\n",
      "Epoch 12 batch 700 train loss: 0.0144 test loss: 0.0148\n",
      "Epoch 12 batch 800 train loss: 0.0118 test loss: 0.0149\n",
      "Epoch 12 batch 900 train loss: 0.0155 test loss: 0.0148\n",
      "Epoch 12 batch 1000 train loss: 0.0133 test loss: 0.0147\n",
      "Epoch 12 batch 1100 train loss: 0.0151 test loss: 0.0148\n",
      "Epoch 12 batch 1200 train loss: 0.0092 test loss: 0.0147\n",
      "Epoch 12 batch 1300 train loss: 0.0157 test loss: 0.0148\n",
      "Epoch 12 batch 1400 train loss: 0.0177 test loss: 0.0148\n",
      "Epoch 12 batch 1500 train loss: 0.0098 test loss: 0.0148\n",
      "Epoch 12 batch 1600 train loss: 0.0107 test loss: 0.0148\n",
      "Epoch 12 batch 1700 train loss: 0.0170 test loss: 0.0149\n",
      "Epoch 12 batch 1800 train loss: 0.0179 test loss: 0.0146\n",
      "Epoch 12 batch 1900 train loss: 0.0264 test loss: 0.0148\n",
      "Epoch 12 batch 2000 train loss: 0.0152 test loss: 0.0148\n",
      "Epoch 12 batch 2100 train loss: 0.0143 test loss: 0.0147\n",
      "Epoch 12 batch 2200 train loss: 0.0168 test loss: 0.0150\n",
      "Epoch 12 batch 2300 train loss: 0.0100 test loss: 0.0144\n",
      "Epoch 12 batch 2400 train loss: 0.0180 test loss: 0.0145\n",
      "Epoch 12 batch 2500 train loss: 0.0152 test loss: 0.0150\n",
      "Epoch 12 batch 2600 train loss: 0.0132 test loss: 0.0148\n",
      "Epoch 12 batch 2700 train loss: 0.0146 test loss: 0.0147\n",
      "Epoch 12 batch 2800 train loss: 0.0145 test loss: 0.0152\n",
      "Epoch 12 batch 2900 train loss: 0.0106 test loss: 0.0144\n",
      "Epoch 12 batch 3000 train loss: 0.0113 test loss: 0.0148\n",
      "Epoch 13 batch 0 train loss: 0.0117 test loss: 0.0147\n",
      "Epoch 13 batch 100 train loss: 0.0142 test loss: 0.0151\n",
      "Epoch 13 batch 200 train loss: 0.0180 test loss: 0.0146\n",
      "Epoch 13 batch 300 train loss: 0.0137 test loss: 0.0152\n",
      "Epoch 13 batch 400 train loss: 0.0150 test loss: 0.0148\n",
      "Epoch 13 batch 500 train loss: 0.0162 test loss: 0.0151\n",
      "Epoch 13 batch 600 train loss: 0.0136 test loss: 0.0147\n",
      "Epoch 13 batch 700 train loss: 0.0107 test loss: 0.0149\n",
      "Epoch 13 batch 800 train loss: 0.0110 test loss: 0.0151\n",
      "Epoch 13 batch 900 train loss: 0.0144 test loss: 0.0153\n",
      "Epoch 13 batch 1000 train loss: 0.0119 test loss: 0.0150\n",
      "Epoch 13 batch 1100 train loss: 0.0110 test loss: 0.0147\n",
      "Epoch 13 batch 1200 train loss: 0.0132 test loss: 0.0148\n",
      "Epoch 13 batch 1300 train loss: 0.0117 test loss: 0.0148\n",
      "Epoch 13 batch 1400 train loss: 0.0206 test loss: 0.0144\n",
      "Epoch 13 batch 1500 train loss: 0.0129 test loss: 0.0149\n",
      "Epoch 13 batch 1600 train loss: 0.0135 test loss: 0.0151\n",
      "Epoch 13 batch 1700 train loss: 0.0164 test loss: 0.0152\n",
      "Epoch 13 batch 1800 train loss: 0.0111 test loss: 0.0146\n",
      "Epoch 13 batch 1900 train loss: 0.0130 test loss: 0.0149\n",
      "Epoch 13 batch 2000 train loss: 0.0158 test loss: 0.0144\n",
      "Epoch 13 batch 2100 train loss: 0.0072 test loss: 0.0151\n",
      "Epoch 13 batch 2200 train loss: 0.0146 test loss: 0.0149\n",
      "Epoch 13 batch 2300 train loss: 0.0171 test loss: 0.0145\n",
      "Epoch 13 batch 2400 train loss: 0.0108 test loss: 0.0149\n",
      "Epoch 13 batch 2500 train loss: 0.0149 test loss: 0.0148\n",
      "Epoch 13 batch 2600 train loss: 0.0175 test loss: 0.0148\n",
      "Epoch 13 batch 2700 train loss: 0.0140 test loss: 0.0149\n",
      "Epoch 13 batch 2800 train loss: 0.0139 test loss: 0.0149\n",
      "Epoch 13 batch 2900 train loss: 0.0149 test loss: 0.0147\n",
      "Epoch 13 batch 3000 train loss: 0.0133 test loss: 0.0146\n",
      "Epoch 14 batch 0 train loss: 0.0149 test loss: 0.0144\n",
      "Epoch 14 batch 100 train loss: 0.0142 test loss: 0.0148\n",
      "Epoch 14 batch 200 train loss: 0.0113 test loss: 0.0150\n",
      "Epoch 14 batch 300 train loss: 0.0100 test loss: 0.0150\n",
      "Epoch 14 batch 400 train loss: 0.0162 test loss: 0.0148\n",
      "Epoch 14 batch 500 train loss: 0.0194 test loss: 0.0148\n",
      "Epoch 14 batch 600 train loss: 0.0147 test loss: 0.0150\n",
      "Epoch 14 batch 700 train loss: 0.0116 test loss: 0.0145\n",
      "Epoch 14 batch 800 train loss: 0.0132 test loss: 0.0147\n",
      "Epoch 14 batch 900 train loss: 0.0119 test loss: 0.0151\n",
      "Epoch 14 batch 1000 train loss: 0.0097 test loss: 0.0152\n",
      "Epoch 14 batch 1100 train loss: 0.0098 test loss: 0.0149\n",
      "Epoch 14 batch 1200 train loss: 0.0138 test loss: 0.0145\n",
      "Epoch 14 batch 1300 train loss: 0.0189 test loss: 0.0147\n",
      "Epoch 14 batch 1400 train loss: 0.0111 test loss: 0.0147\n",
      "Epoch 14 batch 1500 train loss: 0.0181 test loss: 0.0149\n",
      "Epoch 14 batch 1600 train loss: 0.0120 test loss: 0.0152\n",
      "Epoch 14 batch 1700 train loss: 0.0152 test loss: 0.0149\n",
      "Epoch 14 batch 1800 train loss: 0.0168 test loss: 0.0146\n",
      "Epoch 14 batch 1900 train loss: 0.0211 test loss: 0.0151\n",
      "Epoch 14 batch 2000 train loss: 0.0151 test loss: 0.0146\n",
      "Epoch 14 batch 2100 train loss: 0.0149 test loss: 0.0148\n",
      "Epoch 14 batch 2200 train loss: 0.0164 test loss: 0.0150\n",
      "Epoch 14 batch 2300 train loss: 0.0129 test loss: 0.0149\n",
      "Epoch 14 batch 2400 train loss: 0.0129 test loss: 0.0147\n",
      "Epoch 14 batch 2500 train loss: 0.0192 test loss: 0.0149\n",
      "Epoch 14 batch 2600 train loss: 0.0164 test loss: 0.0148\n",
      "Epoch 14 batch 2700 train loss: 0.0139 test loss: 0.0153\n",
      "Epoch 14 batch 2800 train loss: 0.0163 test loss: 0.0147\n",
      "Epoch 14 batch 2900 train loss: 0.0145 test loss: 0.0147\n",
      "Epoch 14 batch 3000 train loss: 0.0123 test loss: 0.0145\n",
      "Epoch 15 batch 0 train loss: 0.0152 test loss: 0.0148\n",
      "Epoch 15 batch 100 train loss: 0.0096 test loss: 0.0147\n",
      "Epoch 15 batch 200 train loss: 0.0150 test loss: 0.0147\n",
      "Epoch 15 batch 300 train loss: 0.0138 test loss: 0.0152\n",
      "Epoch 15 batch 400 train loss: 0.0135 test loss: 0.0147\n",
      "Epoch 15 batch 500 train loss: 0.0121 test loss: 0.0149\n",
      "Epoch 15 batch 600 train loss: 0.0123 test loss: 0.0151\n",
      "Epoch 15 batch 700 train loss: 0.0150 test loss: 0.0146\n",
      "Epoch 15 batch 800 train loss: 0.0109 test loss: 0.0152\n",
      "Epoch 15 batch 900 train loss: 0.0091 test loss: 0.0149\n",
      "Epoch 15 batch 1000 train loss: 0.0158 test loss: 0.0154\n",
      "Epoch 15 batch 1100 train loss: 0.0145 test loss: 0.0147\n",
      "Epoch 15 batch 1200 train loss: 0.0170 test loss: 0.0147\n",
      "Epoch 15 batch 1300 train loss: 0.0143 test loss: 0.0149\n",
      "Epoch 15 batch 1400 train loss: 0.0135 test loss: 0.0147\n",
      "Epoch 15 batch 1500 train loss: 0.0184 test loss: 0.0149\n",
      "Epoch 15 batch 1600 train loss: 0.0124 test loss: 0.0150\n",
      "Epoch 15 batch 1700 train loss: 0.0143 test loss: 0.0152\n",
      "Epoch 15 batch 1800 train loss: 0.0110 test loss: 0.0144\n",
      "Epoch 15 batch 1900 train loss: 0.0130 test loss: 0.0150\n",
      "Epoch 15 batch 2000 train loss: 0.0070 test loss: 0.0144\n",
      "Epoch 15 batch 2100 train loss: 0.0120 test loss: 0.0148\n",
      "Epoch 15 batch 2200 train loss: 0.0144 test loss: 0.0146\n",
      "Epoch 15 batch 2300 train loss: 0.0133 test loss: 0.0146\n",
      "Epoch 15 batch 2400 train loss: 0.0174 test loss: 0.0148\n",
      "Epoch 15 batch 2500 train loss: 0.0143 test loss: 0.0153\n",
      "Epoch 15 batch 2600 train loss: 0.0207 test loss: 0.0145\n",
      "Epoch 15 batch 2700 train loss: 0.0131 test loss: 0.0150\n",
      "Epoch 15 batch 2800 train loss: 0.0172 test loss: 0.0150\n",
      "Epoch 15 batch 2900 train loss: 0.0097 test loss: 0.0148\n",
      "Epoch 15 batch 3000 train loss: 0.0083 test loss: 0.0144\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.80p/ckpt-33\n",
      "Epoch 16 batch 0 train loss: 0.0140 test loss: 0.0139\n",
      "Epoch 16 batch 100 train loss: 0.0127 test loss: 0.0149\n",
      "Epoch 16 batch 200 train loss: 0.0131 test loss: 0.0148\n",
      "Epoch 16 batch 300 train loss: 0.0128 test loss: 0.0149\n",
      "Epoch 16 batch 400 train loss: 0.0148 test loss: 0.0151\n",
      "Epoch 16 batch 500 train loss: 0.0103 test loss: 0.0145\n",
      "Epoch 16 batch 600 train loss: 0.0135 test loss: 0.0153\n",
      "Epoch 16 batch 700 train loss: 0.0124 test loss: 0.0146\n",
      "Epoch 16 batch 800 train loss: 0.0124 test loss: 0.0149\n",
      "Epoch 16 batch 900 train loss: 0.0175 test loss: 0.0154\n",
      "Epoch 16 batch 1000 train loss: 0.0218 test loss: 0.0149\n",
      "Epoch 16 batch 1100 train loss: 0.0103 test loss: 0.0148\n",
      "Epoch 16 batch 1200 train loss: 0.0167 test loss: 0.0148\n",
      "Epoch 16 batch 1300 train loss: 0.0181 test loss: 0.0147\n",
      "Epoch 16 batch 1400 train loss: 0.0116 test loss: 0.0146\n",
      "Epoch 16 batch 1500 train loss: 0.0113 test loss: 0.0149\n",
      "Epoch 16 batch 1600 train loss: 0.0102 test loss: 0.0150\n",
      "Epoch 16 batch 1700 train loss: 0.0178 test loss: 0.0148\n",
      "Epoch 16 batch 1800 train loss: 0.0168 test loss: 0.0146\n",
      "Epoch 16 batch 1900 train loss: 0.0150 test loss: 0.0146\n",
      "Epoch 16 batch 2000 train loss: 0.0095 test loss: 0.0145\n",
      "Epoch 16 batch 2100 train loss: 0.0146 test loss: 0.0146\n",
      "Epoch 16 batch 2200 train loss: 0.0123 test loss: 0.0149\n",
      "Epoch 16 batch 2300 train loss: 0.0168 test loss: 0.0149\n",
      "Epoch 16 batch 2400 train loss: 0.0144 test loss: 0.0145\n",
      "Epoch 16 batch 2500 train loss: 0.0114 test loss: 0.0151\n",
      "Epoch 16 batch 2600 train loss: 0.0152 test loss: 0.0149\n",
      "Epoch 16 batch 2700 train loss: 0.0114 test loss: 0.0147\n",
      "Epoch 16 batch 2800 train loss: 0.0130 test loss: 0.0150\n",
      "Epoch 16 batch 2900 train loss: 0.0173 test loss: 0.0150\n",
      "Epoch 16 batch 3000 train loss: 0.0120 test loss: 0.0142\n",
      "Epoch 17 batch 0 train loss: 0.0116 test loss: 0.0142\n",
      "Epoch 17 batch 100 train loss: 0.0164 test loss: 0.0152\n",
      "Epoch 17 batch 200 train loss: 0.0141 test loss: 0.0148\n",
      "Epoch 17 batch 300 train loss: 0.0140 test loss: 0.0148\n",
      "Epoch 17 batch 400 train loss: 0.0110 test loss: 0.0146\n",
      "Epoch 17 batch 500 train loss: 0.0195 test loss: 0.0145\n",
      "early stop.\n",
      "Checkpoint 33 restored!!\n",
      "Training for loss rate 0.90 start.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ffnn_8/dense_24/kernel:0', 'ffnn_8/dense_24/bias:0', 'ffnn_8/batch_normalization_8/gamma:0', 'ffnn_8/batch_normalization_8/beta:0', 'ffnn_8/dense_25/kernel:0', 'ffnn_8/dense_25/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ffnn_8/dense_24/kernel:0', 'ffnn_8/dense_24/bias:0', 'ffnn_8/batch_normalization_8/gamma:0', 'ffnn_8/batch_normalization_8/beta:0', 'ffnn_8/dense_25/kernel:0', 'ffnn_8/dense_25/bias:0'] when minimizing the loss.\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.90p/ckpt-1\n",
      "Epoch 0 batch 0 train loss: 0.2467 test loss: 0.3418\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.90p/ckpt-2\n",
      "Epoch 0 batch 100 train loss: 0.2105 test loss: 0.2482\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.90p/ckpt-3\n",
      "Epoch 0 batch 200 train loss: 0.1346 test loss: 0.1868\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.90p/ckpt-4\n",
      "Epoch 0 batch 300 train loss: 0.1037 test loss: 0.1435\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.90p/ckpt-5\n",
      "Epoch 0 batch 400 train loss: 0.0760 test loss: 0.1117\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.90p/ckpt-6\n",
      "Epoch 0 batch 500 train loss: 0.0514 test loss: 0.0880\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.90p/ckpt-7\n",
      "Epoch 0 batch 600 train loss: 0.0467 test loss: 0.0698\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.90p/ckpt-8\n",
      "Epoch 0 batch 700 train loss: 0.0361 test loss: 0.0560\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.90p/ckpt-9\n",
      "Epoch 0 batch 800 train loss: 0.0277 test loss: 0.0457\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.90p/ckpt-10\n",
      "Epoch 0 batch 900 train loss: 0.0260 test loss: 0.0380\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.90p/ckpt-11\n",
      "Epoch 0 batch 1000 train loss: 0.0225 test loss: 0.0321\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.90p/ckpt-12\n",
      "Epoch 0 batch 1100 train loss: 0.0197 test loss: 0.0277\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.90p/ckpt-13\n",
      "Epoch 0 batch 1200 train loss: 0.0167 test loss: 0.0244\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.90p/ckpt-14\n",
      "Epoch 0 batch 1300 train loss: 0.0208 test loss: 0.0221\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.90p/ckpt-15\n",
      "Epoch 0 batch 1400 train loss: 0.0162 test loss: 0.0202\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.90p/ckpt-16\n",
      "Epoch 0 batch 1500 train loss: 0.0134 test loss: 0.0190\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.90p/ckpt-17\n",
      "Epoch 0 batch 1600 train loss: 0.0149 test loss: 0.0182\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.90p/ckpt-18\n",
      "Epoch 0 batch 1700 train loss: 0.0152 test loss: 0.0177\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.90p/ckpt-19\n",
      "Epoch 0 batch 1800 train loss: 0.0123 test loss: 0.0171\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.90p/ckpt-20\n",
      "Epoch 0 batch 1900 train loss: 0.0166 test loss: 0.0166\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.90p/ckpt-21\n",
      "Epoch 0 batch 2000 train loss: 0.0098 test loss: 0.0163\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.90p/ckpt-22\n",
      "Epoch 0 batch 2100 train loss: 0.0187 test loss: 0.0159\n",
      "Epoch 0 batch 2200 train loss: 0.0191 test loss: 0.0161\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.90p/ckpt-23\n",
      "Epoch 0 batch 2300 train loss: 0.0145 test loss: 0.0159\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.90p/ckpt-24\n",
      "Epoch 0 batch 2400 train loss: 0.0135 test loss: 0.0158\n",
      "Epoch 0 batch 2500 train loss: 0.0158 test loss: 0.0158\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.90p/ckpt-25\n",
      "Epoch 0 batch 2600 train loss: 0.0179 test loss: 0.0157\n",
      "Epoch 0 batch 2700 train loss: 0.0147 test loss: 0.0158\n",
      "Epoch 0 batch 2800 train loss: 0.0169 test loss: 0.0158\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.90p/ckpt-26\n",
      "Epoch 0 batch 2900 train loss: 0.0125 test loss: 0.0156\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.90p/ckpt-27\n",
      "Epoch 0 batch 3000 train loss: 0.0165 test loss: 0.0155\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ffnn_8/dense_24/kernel:0', 'ffnn_8/dense_24/bias:0', 'ffnn_8/batch_normalization_8/gamma:0', 'ffnn_8/batch_normalization_8/beta:0', 'ffnn_8/dense_25/kernel:0', 'ffnn_8/dense_25/bias:0'] when minimizing the loss.\n",
      "Epoch 1 batch 0 train loss: 0.0159 test loss: 0.0156\n",
      "Epoch 1 batch 100 train loss: 0.0117 test loss: 0.0157\n",
      "Epoch 1 batch 200 train loss: 0.0145 test loss: 0.0158\n",
      "Epoch 1 batch 300 train loss: 0.0153 test loss: 0.0157\n",
      "Epoch 1 batch 400 train loss: 0.0138 test loss: 0.0156\n",
      "Epoch 1 batch 500 train loss: 0.0138 test loss: 0.0155\n",
      "Epoch 1 batch 600 train loss: 0.0152 test loss: 0.0156\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.90p/ckpt-28\n",
      "Epoch 1 batch 700 train loss: 0.0174 test loss: 0.0154\n",
      "Epoch 1 batch 800 train loss: 0.0151 test loss: 0.0158\n",
      "Epoch 1 batch 900 train loss: 0.0135 test loss: 0.0158\n",
      "Epoch 1 batch 1000 train loss: 0.0137 test loss: 0.0159\n",
      "Epoch 1 batch 1100 train loss: 0.0165 test loss: 0.0155\n",
      "Epoch 1 batch 1200 train loss: 0.0212 test loss: 0.0156\n",
      "Epoch 1 batch 1300 train loss: 0.0133 test loss: 0.0156\n",
      "Epoch 1 batch 1400 train loss: 0.0164 test loss: 0.0156\n",
      "Epoch 1 batch 1500 train loss: 0.0133 test loss: 0.0157\n",
      "Epoch 1 batch 1600 train loss: 0.0167 test loss: 0.0159\n",
      "Epoch 1 batch 1700 train loss: 0.0120 test loss: 0.0154\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.90p/ckpt-29\n",
      "Epoch 1 batch 1800 train loss: 0.0201 test loss: 0.0153\n",
      "Epoch 1 batch 1900 train loss: 0.0120 test loss: 0.0156\n",
      "Epoch 1 batch 2000 train loss: 0.0112 test loss: 0.0154\n",
      "Epoch 1 batch 2100 train loss: 0.0146 test loss: 0.0155\n",
      "Epoch 1 batch 2200 train loss: 0.0174 test loss: 0.0155\n",
      "Epoch 1 batch 2300 train loss: 0.0159 test loss: 0.0154\n",
      "Epoch 1 batch 2400 train loss: 0.0143 test loss: 0.0155\n",
      "Epoch 1 batch 2500 train loss: 0.0155 test loss: 0.0157\n",
      "Epoch 1 batch 2600 train loss: 0.0169 test loss: 0.0155\n",
      "Epoch 1 batch 2700 train loss: 0.0093 test loss: 0.0157\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.90p/ckpt-30\n",
      "Epoch 1 batch 2800 train loss: 0.0167 test loss: 0.0152\n",
      "Epoch 1 batch 2900 train loss: 0.0115 test loss: 0.0156\n",
      "Epoch 1 batch 3000 train loss: 0.0160 test loss: 0.0153\n",
      "Epoch 2 batch 0 train loss: 0.0167 test loss: 0.0152\n",
      "Epoch 2 batch 100 train loss: 0.0213 test loss: 0.0154\n",
      "Epoch 2 batch 200 train loss: 0.0184 test loss: 0.0157\n",
      "Epoch 2 batch 300 train loss: 0.0165 test loss: 0.0159\n",
      "Epoch 2 batch 400 train loss: 0.0166 test loss: 0.0153\n",
      "Epoch 2 batch 500 train loss: 0.0129 test loss: 0.0156\n",
      "Epoch 2 batch 600 train loss: 0.0132 test loss: 0.0155\n",
      "Epoch 2 batch 700 train loss: 0.0214 test loss: 0.0153\n",
      "Epoch 2 batch 800 train loss: 0.0132 test loss: 0.0161\n",
      "Epoch 2 batch 900 train loss: 0.0154 test loss: 0.0156\n",
      "Epoch 2 batch 1000 train loss: 0.0103 test loss: 0.0155\n",
      "Epoch 2 batch 1100 train loss: 0.0119 test loss: 0.0156\n",
      "Epoch 2 batch 1200 train loss: 0.0166 test loss: 0.0157\n",
      "Epoch 2 batch 1300 train loss: 0.0160 test loss: 0.0156\n",
      "Epoch 2 batch 1400 train loss: 0.0203 test loss: 0.0153\n",
      "Epoch 2 batch 1500 train loss: 0.0118 test loss: 0.0157\n",
      "Epoch 2 batch 1600 train loss: 0.0190 test loss: 0.0156\n",
      "Epoch 2 batch 1700 train loss: 0.0155 test loss: 0.0158\n",
      "Epoch 2 batch 1800 train loss: 0.0148 test loss: 0.0154\n",
      "Epoch 2 batch 1900 train loss: 0.0169 test loss: 0.0153\n",
      "Epoch 2 batch 2000 train loss: 0.0130 test loss: 0.0154\n",
      "Epoch 2 batch 2100 train loss: 0.0187 test loss: 0.0154\n",
      "Epoch 2 batch 2200 train loss: 0.0186 test loss: 0.0156\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.90p/ckpt-31\n",
      "Epoch 2 batch 2300 train loss: 0.0158 test loss: 0.0150\n",
      "Epoch 2 batch 2400 train loss: 0.0172 test loss: 0.0153\n",
      "Epoch 2 batch 2500 train loss: 0.0105 test loss: 0.0155\n",
      "Epoch 2 batch 2600 train loss: 0.0167 test loss: 0.0157\n",
      "Epoch 2 batch 2700 train loss: 0.0115 test loss: 0.0155\n",
      "Epoch 2 batch 2800 train loss: 0.0165 test loss: 0.0156\n",
      "Epoch 2 batch 2900 train loss: 0.0135 test loss: 0.0159\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.90p/ckpt-32\n",
      "Epoch 2 batch 3000 train loss: 0.0169 test loss: 0.0150\n",
      "Epoch 3 batch 0 train loss: 0.0107 test loss: 0.0151\n",
      "Epoch 3 batch 100 train loss: 0.0104 test loss: 0.0156\n",
      "Epoch 3 batch 200 train loss: 0.0183 test loss: 0.0156\n",
      "Epoch 3 batch 300 train loss: 0.0165 test loss: 0.0156\n",
      "Epoch 3 batch 400 train loss: 0.0114 test loss: 0.0154\n",
      "Epoch 3 batch 500 train loss: 0.0113 test loss: 0.0153\n",
      "Epoch 3 batch 600 train loss: 0.0139 test loss: 0.0157\n",
      "Epoch 3 batch 700 train loss: 0.0136 test loss: 0.0154\n",
      "Epoch 3 batch 800 train loss: 0.0187 test loss: 0.0155\n",
      "Epoch 3 batch 900 train loss: 0.0145 test loss: 0.0154\n",
      "Epoch 3 batch 1000 train loss: 0.0178 test loss: 0.0155\n",
      "Epoch 3 batch 1100 train loss: 0.0148 test loss: 0.0156\n",
      "Epoch 3 batch 1200 train loss: 0.0136 test loss: 0.0151\n",
      "Epoch 3 batch 1300 train loss: 0.0137 test loss: 0.0155\n",
      "Epoch 3 batch 1400 train loss: 0.0159 test loss: 0.0154\n",
      "Epoch 3 batch 1500 train loss: 0.0143 test loss: 0.0157\n",
      "Epoch 3 batch 1600 train loss: 0.0157 test loss: 0.0156\n",
      "Epoch 3 batch 1700 train loss: 0.0201 test loss: 0.0158\n",
      "Epoch 3 batch 1800 train loss: 0.0151 test loss: 0.0153\n",
      "Epoch 3 batch 1900 train loss: 0.0122 test loss: 0.0156\n",
      "Epoch 3 batch 2000 train loss: 0.0105 test loss: 0.0155\n",
      "Epoch 3 batch 2100 train loss: 0.0120 test loss: 0.0155\n",
      "Epoch 3 batch 2200 train loss: 0.0210 test loss: 0.0157\n",
      "Epoch 3 batch 2300 train loss: 0.0159 test loss: 0.0153\n",
      "Epoch 3 batch 2400 train loss: 0.0122 test loss: 0.0155\n",
      "Epoch 3 batch 2500 train loss: 0.0141 test loss: 0.0152\n",
      "Epoch 3 batch 2600 train loss: 0.0132 test loss: 0.0153\n",
      "Epoch 3 batch 2700 train loss: 0.0180 test loss: 0.0157\n",
      "Epoch 3 batch 2800 train loss: 0.0189 test loss: 0.0154\n",
      "Epoch 3 batch 2900 train loss: 0.0195 test loss: 0.0155\n",
      "Epoch 3 batch 3000 train loss: 0.0155 test loss: 0.0152\n",
      "Epoch 4 batch 0 train loss: 0.0161 test loss: 0.0152\n",
      "Epoch 4 batch 100 train loss: 0.0173 test loss: 0.0154\n",
      "Epoch 4 batch 200 train loss: 0.0144 test loss: 0.0154\n",
      "Epoch 4 batch 300 train loss: 0.0122 test loss: 0.0155\n",
      "Epoch 4 batch 400 train loss: 0.0145 test loss: 0.0156\n",
      "Epoch 4 batch 500 train loss: 0.0191 test loss: 0.0154\n",
      "Epoch 4 batch 600 train loss: 0.0134 test loss: 0.0153\n",
      "Epoch 4 batch 700 train loss: 0.0126 test loss: 0.0156\n",
      "Epoch 4 batch 800 train loss: 0.0137 test loss: 0.0157\n",
      "Epoch 4 batch 900 train loss: 0.0183 test loss: 0.0156\n",
      "Epoch 4 batch 1000 train loss: 0.0168 test loss: 0.0159\n",
      "Epoch 4 batch 1100 train loss: 0.0194 test loss: 0.0158\n",
      "Epoch 4 batch 1200 train loss: 0.0122 test loss: 0.0150\n",
      "Epoch 4 batch 1300 train loss: 0.0156 test loss: 0.0158\n",
      "Epoch 4 batch 1400 train loss: 0.0159 test loss: 0.0154\n",
      "Epoch 4 batch 1500 train loss: 0.0222 test loss: 0.0153\n",
      "Epoch 4 batch 1600 train loss: 0.0156 test loss: 0.0158\n",
      "Epoch 4 batch 1700 train loss: 0.0158 test loss: 0.0159\n",
      "Epoch 4 batch 1800 train loss: 0.0174 test loss: 0.0152\n",
      "Epoch 4 batch 1900 train loss: 0.0148 test loss: 0.0153\n",
      "Epoch 4 batch 2000 train loss: 0.0144 test loss: 0.0153\n",
      "Epoch 4 batch 2100 train loss: 0.0166 test loss: 0.0154\n",
      "Epoch 4 batch 2200 train loss: 0.0173 test loss: 0.0157\n",
      "Epoch 4 batch 2300 train loss: 0.0183 test loss: 0.0155\n",
      "Epoch 4 batch 2400 train loss: 0.0144 test loss: 0.0152\n",
      "Epoch 4 batch 2500 train loss: 0.0144 test loss: 0.0153\n",
      "Epoch 4 batch 2600 train loss: 0.0191 test loss: 0.0156\n",
      "Epoch 4 batch 2700 train loss: 0.0134 test loss: 0.0153\n",
      "Epoch 4 batch 2800 train loss: 0.0134 test loss: 0.0156\n",
      "Epoch 4 batch 2900 train loss: 0.0167 test loss: 0.0153\n",
      "Epoch 4 batch 3000 train loss: 0.0138 test loss: 0.0154\n",
      "Epoch 5 batch 0 train loss: 0.0126 test loss: 0.0150\n",
      "Epoch 5 batch 100 train loss: 0.0181 test loss: 0.0153\n",
      "Epoch 5 batch 200 train loss: 0.0134 test loss: 0.0155\n",
      "Epoch 5 batch 300 train loss: 0.0174 test loss: 0.0156\n",
      "Epoch 5 batch 400 train loss: 0.0120 test loss: 0.0151\n",
      "Epoch 5 batch 500 train loss: 0.0160 test loss: 0.0152\n",
      "Epoch 5 batch 600 train loss: 0.0168 test loss: 0.0156\n",
      "Epoch 5 batch 700 train loss: 0.0192 test loss: 0.0154\n",
      "Epoch 5 batch 800 train loss: 0.0160 test loss: 0.0156\n",
      "Epoch 5 batch 900 train loss: 0.0157 test loss: 0.0159\n",
      "Epoch 5 batch 1000 train loss: 0.0128 test loss: 0.0155\n",
      "Epoch 5 batch 1100 train loss: 0.0165 test loss: 0.0160\n",
      "Epoch 5 batch 1200 train loss: 0.0135 test loss: 0.0155\n",
      "Epoch 5 batch 1300 train loss: 0.0141 test loss: 0.0155\n",
      "Epoch 5 batch 1400 train loss: 0.0134 test loss: 0.0155\n",
      "Epoch 5 batch 1500 train loss: 0.0128 test loss: 0.0157\n",
      "Epoch 5 batch 1600 train loss: 0.0136 test loss: 0.0155\n",
      "Epoch 5 batch 1700 train loss: 0.0201 test loss: 0.0155\n",
      "Epoch 5 batch 1800 train loss: 0.0174 test loss: 0.0152\n",
      "Epoch 5 batch 1900 train loss: 0.0152 test loss: 0.0152\n",
      "Epoch 5 batch 2000 train loss: 0.0132 test loss: 0.0154\n",
      "Epoch 5 batch 2100 train loss: 0.0114 test loss: 0.0155\n",
      "Epoch 5 batch 2200 train loss: 0.0117 test loss: 0.0159\n",
      "Epoch 5 batch 2300 train loss: 0.0215 test loss: 0.0154\n",
      "Epoch 5 batch 2400 train loss: 0.0141 test loss: 0.0153\n",
      "Epoch 5 batch 2500 train loss: 0.0146 test loss: 0.0155\n",
      "Epoch 5 batch 2600 train loss: 0.0134 test loss: 0.0153\n",
      "Epoch 5 batch 2700 train loss: 0.0131 test loss: 0.0156\n",
      "Epoch 5 batch 2800 train loss: 0.0194 test loss: 0.0156\n",
      "Epoch 5 batch 2900 train loss: 0.0166 test loss: 0.0155\n",
      "Epoch 5 batch 3000 train loss: 0.0159 test loss: 0.0153\n",
      "Epoch 6 batch 0 train loss: 0.0153 test loss: 0.0153\n",
      "Epoch 6 batch 100 train loss: 0.0169 test loss: 0.0156\n",
      "Epoch 6 batch 200 train loss: 0.0135 test loss: 0.0153\n",
      "Epoch 6 batch 300 train loss: 0.0138 test loss: 0.0155\n",
      "Epoch 6 batch 400 train loss: 0.0141 test loss: 0.0152\n",
      "Epoch 6 batch 500 train loss: 0.0220 test loss: 0.0155\n",
      "Epoch 6 batch 600 train loss: 0.0124 test loss: 0.0153\n",
      "Epoch 6 batch 700 train loss: 0.0170 test loss: 0.0153\n",
      "Epoch 6 batch 800 train loss: 0.0135 test loss: 0.0157\n",
      "Epoch 6 batch 900 train loss: 0.0149 test loss: 0.0157\n",
      "Epoch 6 batch 1000 train loss: 0.0146 test loss: 0.0154\n",
      "Epoch 6 batch 1100 train loss: 0.0143 test loss: 0.0160\n",
      "Epoch 6 batch 1200 train loss: 0.0134 test loss: 0.0156\n",
      "Epoch 6 batch 1300 train loss: 0.0154 test loss: 0.0156\n",
      "Epoch 6 batch 1400 train loss: 0.0169 test loss: 0.0154\n",
      "Epoch 6 batch 1500 train loss: 0.0142 test loss: 0.0155\n",
      "Epoch 6 batch 1600 train loss: 0.0221 test loss: 0.0156\n",
      "Epoch 6 batch 1700 train loss: 0.0189 test loss: 0.0152\n",
      "Epoch 6 batch 1800 train loss: 0.0173 test loss: 0.0152\n",
      "Epoch 6 batch 1900 train loss: 0.0146 test loss: 0.0154\n",
      "Epoch 6 batch 2000 train loss: 0.0122 test loss: 0.0151\n",
      "Epoch 6 batch 2100 train loss: 0.0192 test loss: 0.0156\n",
      "Epoch 6 batch 2200 train loss: 0.0150 test loss: 0.0157\n",
      "Epoch 6 batch 2300 train loss: 0.0156 test loss: 0.0154\n",
      "Epoch 6 batch 2400 train loss: 0.0164 test loss: 0.0156\n",
      "Epoch 6 batch 2500 train loss: 0.0132 test loss: 0.0154\n",
      "Epoch 6 batch 2600 train loss: 0.0180 test loss: 0.0156\n",
      "Epoch 6 batch 2700 train loss: 0.0129 test loss: 0.0155\n",
      "Epoch 6 batch 2800 train loss: 0.0203 test loss: 0.0152\n",
      "Epoch 6 batch 2900 train loss: 0.0133 test loss: 0.0153\n",
      "Epoch 6 batch 3000 train loss: 0.0127 test loss: 0.0152\n",
      "Epoch 7 batch 0 train loss: 0.0180 test loss: 0.0153\n",
      "Epoch 7 batch 100 train loss: 0.0143 test loss: 0.0154\n",
      "Epoch 7 batch 200 train loss: 0.0160 test loss: 0.0154\n",
      "Epoch 7 batch 300 train loss: 0.0123 test loss: 0.0155\n",
      "Epoch 7 batch 400 train loss: 0.0134 test loss: 0.0153\n",
      "Epoch 7 batch 500 train loss: 0.0210 test loss: 0.0154\n",
      "Epoch 7 batch 600 train loss: 0.0107 test loss: 0.0155\n",
      "Epoch 7 batch 700 train loss: 0.0199 test loss: 0.0154\n",
      "Epoch 7 batch 800 train loss: 0.0151 test loss: 0.0157\n",
      "Epoch 7 batch 900 train loss: 0.0127 test loss: 0.0156\n",
      "Epoch 7 batch 1000 train loss: 0.0206 test loss: 0.0157\n",
      "Epoch 7 batch 1100 train loss: 0.0117 test loss: 0.0155\n",
      "Epoch 7 batch 1200 train loss: 0.0122 test loss: 0.0151\n",
      "Epoch 7 batch 1300 train loss: 0.0143 test loss: 0.0152\n",
      "Epoch 7 batch 1400 train loss: 0.0190 test loss: 0.0154\n",
      "Epoch 7 batch 1500 train loss: 0.0132 test loss: 0.0155\n",
      "Epoch 7 batch 1600 train loss: 0.0139 test loss: 0.0155\n",
      "Epoch 7 batch 1700 train loss: 0.0135 test loss: 0.0158\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.90p/ckpt-33\n",
      "Epoch 7 batch 1800 train loss: 0.0158 test loss: 0.0148\n",
      "Epoch 7 batch 1900 train loss: 0.0092 test loss: 0.0154\n",
      "Epoch 7 batch 2000 train loss: 0.0145 test loss: 0.0150\n",
      "Epoch 7 batch 2100 train loss: 0.0178 test loss: 0.0154\n",
      "Epoch 7 batch 2200 train loss: 0.0139 test loss: 0.0154\n",
      "Epoch 7 batch 2300 train loss: 0.0156 test loss: 0.0155\n",
      "Epoch 7 batch 2400 train loss: 0.0146 test loss: 0.0150\n",
      "Epoch 7 batch 2500 train loss: 0.0128 test loss: 0.0154\n",
      "Epoch 7 batch 2600 train loss: 0.0246 test loss: 0.0157\n",
      "Epoch 7 batch 2700 train loss: 0.0199 test loss: 0.0155\n",
      "Epoch 7 batch 2800 train loss: 0.0148 test loss: 0.0155\n",
      "Epoch 7 batch 2900 train loss: 0.0109 test loss: 0.0153\n",
      "Epoch 7 batch 3000 train loss: 0.0197 test loss: 0.0152\n",
      "Epoch 8 batch 0 train loss: 0.0163 test loss: 0.0150\n",
      "Epoch 8 batch 100 train loss: 0.0176 test loss: 0.0154\n",
      "Epoch 8 batch 200 train loss: 0.0171 test loss: 0.0158\n",
      "Epoch 8 batch 300 train loss: 0.0143 test loss: 0.0155\n",
      "Epoch 8 batch 400 train loss: 0.0148 test loss: 0.0151\n",
      "Epoch 8 batch 500 train loss: 0.0174 test loss: 0.0154\n",
      "Epoch 8 batch 600 train loss: 0.0189 test loss: 0.0157\n",
      "Epoch 8 batch 700 train loss: 0.0166 test loss: 0.0152\n",
      "Epoch 8 batch 800 train loss: 0.0131 test loss: 0.0160\n",
      "Epoch 8 batch 900 train loss: 0.0218 test loss: 0.0156\n",
      "Epoch 8 batch 1000 train loss: 0.0183 test loss: 0.0155\n",
      "Epoch 8 batch 1100 train loss: 0.0167 test loss: 0.0156\n",
      "Epoch 8 batch 1200 train loss: 0.0168 test loss: 0.0156\n",
      "Epoch 8 batch 1300 train loss: 0.0161 test loss: 0.0154\n",
      "Epoch 8 batch 1400 train loss: 0.0153 test loss: 0.0153\n",
      "Epoch 8 batch 1500 train loss: 0.0161 test loss: 0.0153\n",
      "Epoch 8 batch 1600 train loss: 0.0120 test loss: 0.0155\n",
      "Epoch 8 batch 1700 train loss: 0.0156 test loss: 0.0154\n",
      "Epoch 8 batch 1800 train loss: 0.0171 test loss: 0.0150\n",
      "Epoch 8 batch 1900 train loss: 0.0149 test loss: 0.0155\n",
      "Epoch 8 batch 2000 train loss: 0.0159 test loss: 0.0153\n",
      "Epoch 8 batch 2100 train loss: 0.0131 test loss: 0.0154\n",
      "Epoch 8 batch 2200 train loss: 0.0182 test loss: 0.0154\n",
      "Epoch 8 batch 2300 train loss: 0.0151 test loss: 0.0155\n",
      "Epoch 8 batch 2400 train loss: 0.0182 test loss: 0.0155\n",
      "Epoch 8 batch 2500 train loss: 0.0135 test loss: 0.0155\n",
      "Epoch 8 batch 2600 train loss: 0.0190 test loss: 0.0152\n",
      "Epoch 8 batch 2700 train loss: 0.0122 test loss: 0.0152\n",
      "Epoch 8 batch 2800 train loss: 0.0131 test loss: 0.0156\n",
      "Epoch 8 batch 2900 train loss: 0.0123 test loss: 0.0157\n",
      "Epoch 8 batch 3000 train loss: 0.0159 test loss: 0.0150\n",
      "Epoch 9 batch 0 train loss: 0.0220 test loss: 0.0151\n",
      "Epoch 9 batch 100 train loss: 0.0129 test loss: 0.0153\n",
      "Epoch 9 batch 200 train loss: 0.0146 test loss: 0.0156\n",
      "Epoch 9 batch 300 train loss: 0.0147 test loss: 0.0156\n",
      "Epoch 9 batch 400 train loss: 0.0223 test loss: 0.0154\n",
      "Epoch 9 batch 500 train loss: 0.0150 test loss: 0.0152\n",
      "Epoch 9 batch 600 train loss: 0.0131 test loss: 0.0155\n",
      "Epoch 9 batch 700 train loss: 0.0184 test loss: 0.0154\n",
      "Epoch 9 batch 800 train loss: 0.0116 test loss: 0.0154\n",
      "Epoch 9 batch 900 train loss: 0.0153 test loss: 0.0157\n",
      "Epoch 9 batch 1000 train loss: 0.0158 test loss: 0.0153\n",
      "Epoch 9 batch 1100 train loss: 0.0178 test loss: 0.0158\n",
      "Epoch 9 batch 1200 train loss: 0.0174 test loss: 0.0155\n",
      "Epoch 9 batch 1300 train loss: 0.0158 test loss: 0.0152\n",
      "Epoch 9 batch 1400 train loss: 0.0171 test loss: 0.0152\n",
      "Epoch 9 batch 1500 train loss: 0.0187 test loss: 0.0154\n",
      "Epoch 9 batch 1600 train loss: 0.0190 test loss: 0.0159\n",
      "Epoch 9 batch 1700 train loss: 0.0164 test loss: 0.0158\n",
      "Epoch 9 batch 1800 train loss: 0.0169 test loss: 0.0153\n",
      "Epoch 9 batch 1900 train loss: 0.0154 test loss: 0.0150\n",
      "Epoch 9 batch 2000 train loss: 0.0200 test loss: 0.0152\n",
      "Epoch 9 batch 2100 train loss: 0.0125 test loss: 0.0152\n",
      "Epoch 9 batch 2200 train loss: 0.0155 test loss: 0.0156\n",
      "Epoch 9 batch 2300 train loss: 0.0130 test loss: 0.0153\n",
      "Epoch 9 batch 2400 train loss: 0.0132 test loss: 0.0153\n",
      "Epoch 9 batch 2500 train loss: 0.0122 test loss: 0.0153\n",
      "Epoch 9 batch 2600 train loss: 0.0135 test loss: 0.0154\n",
      "Epoch 9 batch 2700 train loss: 0.0154 test loss: 0.0156\n",
      "Epoch 9 batch 2800 train loss: 0.0168 test loss: 0.0156\n",
      "Epoch 9 batch 2900 train loss: 0.0198 test loss: 0.0156\n",
      "Epoch 9 batch 3000 train loss: 0.0169 test loss: 0.0152\n",
      "Epoch 10 batch 0 train loss: 0.0199 test loss: 0.0150\n",
      "Epoch 10 batch 100 train loss: 0.0137 test loss: 0.0153\n",
      "Epoch 10 batch 200 train loss: 0.0166 test loss: 0.0155\n",
      "Epoch 10 batch 300 train loss: 0.0129 test loss: 0.0156\n",
      "Epoch 10 batch 400 train loss: 0.0176 test loss: 0.0150\n",
      "Epoch 10 batch 500 train loss: 0.0155 test loss: 0.0151\n",
      "Epoch 10 batch 600 train loss: 0.0184 test loss: 0.0156\n",
      "Epoch 10 batch 700 train loss: 0.0128 test loss: 0.0154\n",
      "Epoch 10 batch 800 train loss: 0.0219 test loss: 0.0157\n",
      "Epoch 10 batch 900 train loss: 0.0170 test loss: 0.0155\n",
      "Epoch 10 batch 1000 train loss: 0.0119 test loss: 0.0153\n",
      "Epoch 10 batch 1100 train loss: 0.0125 test loss: 0.0151\n",
      "Epoch 10 batch 1200 train loss: 0.0135 test loss: 0.0153\n",
      "Epoch 10 batch 1300 train loss: 0.0119 test loss: 0.0151\n",
      "Epoch 10 batch 1400 train loss: 0.0176 test loss: 0.0153\n",
      "Epoch 10 batch 1500 train loss: 0.0130 test loss: 0.0155\n",
      "Epoch 10 batch 1600 train loss: 0.0163 test loss: 0.0155\n",
      "Epoch 10 batch 1700 train loss: 0.0193 test loss: 0.0156\n",
      "Epoch 10 batch 1800 train loss: 0.0207 test loss: 0.0152\n",
      "Epoch 10 batch 1900 train loss: 0.0140 test loss: 0.0151\n",
      "Epoch 10 batch 2000 train loss: 0.0145 test loss: 0.0152\n",
      "Epoch 10 batch 2100 train loss: 0.0103 test loss: 0.0154\n",
      "Epoch 10 batch 2200 train loss: 0.0115 test loss: 0.0153\n",
      "Epoch 10 batch 2300 train loss: 0.0113 test loss: 0.0153\n",
      "Epoch 10 batch 2400 train loss: 0.0162 test loss: 0.0153\n",
      "Epoch 10 batch 2500 train loss: 0.0128 test loss: 0.0157\n",
      "Epoch 10 batch 2600 train loss: 0.0172 test loss: 0.0157\n",
      "Epoch 10 batch 2700 train loss: 0.0178 test loss: 0.0156\n",
      "Epoch 10 batch 2800 train loss: 0.0183 test loss: 0.0157\n",
      "Epoch 10 batch 2900 train loss: 0.0165 test loss: 0.0153\n",
      "Epoch 10 batch 3000 train loss: 0.0175 test loss: 0.0153\n",
      "Epoch 11 batch 0 train loss: 0.0178 test loss: 0.0152\n",
      "Epoch 11 batch 100 train loss: 0.0132 test loss: 0.0155\n",
      "Epoch 11 batch 200 train loss: 0.0142 test loss: 0.0153\n",
      "Epoch 11 batch 300 train loss: 0.0145 test loss: 0.0158\n",
      "Epoch 11 batch 400 train loss: 0.0137 test loss: 0.0152\n",
      "Epoch 11 batch 500 train loss: 0.0134 test loss: 0.0155\n",
      "Epoch 11 batch 600 train loss: 0.0137 test loss: 0.0158\n",
      "Epoch 11 batch 700 train loss: 0.0167 test loss: 0.0151\n",
      "Epoch 11 batch 800 train loss: 0.0148 test loss: 0.0155\n",
      "Epoch 11 batch 900 train loss: 0.0107 test loss: 0.0155\n",
      "Epoch 11 batch 1000 train loss: 0.0129 test loss: 0.0157\n",
      "Epoch 11 batch 1100 train loss: 0.0218 test loss: 0.0153\n",
      "Epoch 11 batch 1200 train loss: 0.0169 test loss: 0.0154\n",
      "Epoch 11 batch 1300 train loss: 0.0125 test loss: 0.0153\n",
      "Epoch 11 batch 1400 train loss: 0.0116 test loss: 0.0153\n",
      "Epoch 11 batch 1500 train loss: 0.0121 test loss: 0.0152\n",
      "Epoch 11 batch 1600 train loss: 0.0203 test loss: 0.0156\n",
      "Epoch 11 batch 1700 train loss: 0.0242 test loss: 0.0156\n",
      "Epoch 11 batch 1800 train loss: 0.0200 test loss: 0.0154\n",
      "Epoch 11 batch 1900 train loss: 0.0125 test loss: 0.0152\n",
      "Epoch 11 batch 2000 train loss: 0.0163 test loss: 0.0155\n",
      "Epoch 11 batch 2100 train loss: 0.0179 test loss: 0.0158\n",
      "Epoch 11 batch 2200 train loss: 0.0138 test loss: 0.0152\n",
      "Epoch 11 batch 2300 train loss: 0.0179 test loss: 0.0151\n",
      "Epoch 11 batch 2400 train loss: 0.0129 test loss: 0.0152\n",
      "Epoch 11 batch 2500 train loss: 0.0166 test loss: 0.0153\n",
      "Epoch 11 batch 2600 train loss: 0.0183 test loss: 0.0157\n",
      "Epoch 11 batch 2700 train loss: 0.0160 test loss: 0.0152\n",
      "Epoch 11 batch 2800 train loss: 0.0230 test loss: 0.0158\n",
      "Epoch 11 batch 2900 train loss: 0.0109 test loss: 0.0154\n",
      "Epoch 11 batch 3000 train loss: 0.0141 test loss: 0.0151\n",
      "Epoch 12 batch 0 train loss: 0.0146 test loss: 0.0151\n",
      "Epoch 12 batch 100 train loss: 0.0205 test loss: 0.0153\n",
      "Epoch 12 batch 200 train loss: 0.0130 test loss: 0.0158\n",
      "Epoch 12 batch 300 train loss: 0.0166 test loss: 0.0151\n",
      "Epoch 12 batch 400 train loss: 0.0166 test loss: 0.0154\n",
      "Epoch 12 batch 500 train loss: 0.0132 test loss: 0.0154\n",
      "Epoch 12 batch 600 train loss: 0.0183 test loss: 0.0153\n",
      "Epoch 12 batch 700 train loss: 0.0164 test loss: 0.0152\n",
      "Epoch 12 batch 800 train loss: 0.0147 test loss: 0.0157\n",
      "Epoch 12 batch 900 train loss: 0.0099 test loss: 0.0154\n",
      "Epoch 12 batch 1000 train loss: 0.0157 test loss: 0.0156\n",
      "Epoch 12 batch 1100 train loss: 0.0210 test loss: 0.0155\n",
      "Epoch 12 batch 1200 train loss: 0.0131 test loss: 0.0151\n",
      "Epoch 12 batch 1300 train loss: 0.0148 test loss: 0.0154\n",
      "Epoch 12 batch 1400 train loss: 0.0157 test loss: 0.0155\n",
      "Epoch 12 batch 1500 train loss: 0.0111 test loss: 0.0152\n",
      "Epoch 12 batch 1600 train loss: 0.0132 test loss: 0.0158\n",
      "Epoch 12 batch 1700 train loss: 0.0149 test loss: 0.0154\n",
      "Epoch 12 batch 1800 train loss: 0.0137 test loss: 0.0153\n",
      "Epoch 12 batch 1900 train loss: 0.0165 test loss: 0.0155\n",
      "Epoch 12 batch 2000 train loss: 0.0195 test loss: 0.0152\n",
      "Epoch 12 batch 2100 train loss: 0.0156 test loss: 0.0155\n",
      "Epoch 12 batch 2200 train loss: 0.0111 test loss: 0.0155\n",
      "Epoch 12 batch 2300 train loss: 0.0128 test loss: 0.0152\n",
      "Epoch 12 batch 2400 train loss: 0.0127 test loss: 0.0150\n",
      "Epoch 12 batch 2500 train loss: 0.0194 test loss: 0.0156\n",
      "Epoch 12 batch 2600 train loss: 0.0177 test loss: 0.0154\n",
      "Epoch 12 batch 2700 train loss: 0.0160 test loss: 0.0153\n",
      "Epoch 12 batch 2800 train loss: 0.0168 test loss: 0.0157\n",
      "Epoch 12 batch 2900 train loss: 0.0194 test loss: 0.0153\n",
      "Epoch 12 batch 3000 train loss: 0.0169 test loss: 0.0155\n",
      "Epoch 13 batch 0 train loss: 0.0129 test loss: 0.0153\n",
      "Epoch 13 batch 100 train loss: 0.0159 test loss: 0.0158\n",
      "Epoch 13 batch 200 train loss: 0.0140 test loss: 0.0152\n",
      "Epoch 13 batch 300 train loss: 0.0127 test loss: 0.0157\n",
      "Epoch 13 batch 400 train loss: 0.0123 test loss: 0.0153\n",
      "Epoch 13 batch 500 train loss: 0.0153 test loss: 0.0155\n",
      "Epoch 13 batch 600 train loss: 0.0227 test loss: 0.0151\n",
      "Epoch 13 batch 700 train loss: 0.0184 test loss: 0.0153\n",
      "Epoch 13 batch 800 train loss: 0.0121 test loss: 0.0158\n",
      "Epoch 13 batch 900 train loss: 0.0126 test loss: 0.0158\n",
      "Epoch 13 batch 1000 train loss: 0.0175 test loss: 0.0159\n",
      "Epoch 13 batch 1100 train loss: 0.0127 test loss: 0.0155\n",
      "Epoch 13 batch 1200 train loss: 0.0169 test loss: 0.0153\n",
      "Epoch 13 batch 1300 train loss: 0.0120 test loss: 0.0155\n",
      "Epoch 13 batch 1400 train loss: 0.0160 test loss: 0.0149\n",
      "Epoch 13 batch 1500 train loss: 0.0148 test loss: 0.0154\n",
      "Epoch 13 batch 1600 train loss: 0.0114 test loss: 0.0158\n",
      "Epoch 13 batch 1700 train loss: 0.0198 test loss: 0.0154\n",
      "Epoch 13 batch 1800 train loss: 0.0156 test loss: 0.0150\n",
      "Epoch 13 batch 1900 train loss: 0.0101 test loss: 0.0153\n",
      "Epoch 13 batch 2000 train loss: 0.0109 test loss: 0.0153\n",
      "Epoch 13 batch 2100 train loss: 0.0140 test loss: 0.0153\n",
      "Epoch 13 batch 2200 train loss: 0.0184 test loss: 0.0156\n",
      "Epoch 13 batch 2300 train loss: 0.0139 test loss: 0.0151\n",
      "Epoch 13 batch 2400 train loss: 0.0173 test loss: 0.0155\n",
      "Epoch 13 batch 2500 train loss: 0.0193 test loss: 0.0157\n",
      "Epoch 13 batch 2600 train loss: 0.0162 test loss: 0.0155\n",
      "Epoch 13 batch 2700 train loss: 0.0111 test loss: 0.0155\n",
      "Epoch 13 batch 2800 train loss: 0.0163 test loss: 0.0155\n",
      "Epoch 13 batch 2900 train loss: 0.0198 test loss: 0.0156\n",
      "Epoch 13 batch 3000 train loss: 0.0119 test loss: 0.0154\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.90p/ckpt-34\n",
      "Epoch 14 batch 0 train loss: 0.0123 test loss: 0.0148\n",
      "Epoch 14 batch 100 train loss: 0.0161 test loss: 0.0150\n",
      "Epoch 14 batch 200 train loss: 0.0126 test loss: 0.0154\n",
      "Epoch 14 batch 300 train loss: 0.0170 test loss: 0.0157\n",
      "Epoch 14 batch 400 train loss: 0.0117 test loss: 0.0151\n",
      "Epoch 14 batch 500 train loss: 0.0159 test loss: 0.0154\n",
      "Epoch 14 batch 600 train loss: 0.0143 test loss: 0.0155\n",
      "Epoch 14 batch 700 train loss: 0.0108 test loss: 0.0149\n",
      "Epoch 14 batch 800 train loss: 0.0199 test loss: 0.0157\n",
      "Epoch 14 batch 900 train loss: 0.0168 test loss: 0.0156\n",
      "Epoch 14 batch 1000 train loss: 0.0193 test loss: 0.0160\n",
      "Epoch 14 batch 1100 train loss: 0.0099 test loss: 0.0153\n",
      "Epoch 14 batch 1200 train loss: 0.0128 test loss: 0.0152\n",
      "Epoch 14 batch 1300 train loss: 0.0171 test loss: 0.0153\n",
      "Epoch 14 batch 1400 train loss: 0.0191 test loss: 0.0156\n",
      "Epoch 14 batch 1500 train loss: 0.0169 test loss: 0.0152\n",
      "Epoch 14 batch 1600 train loss: 0.0127 test loss: 0.0159\n",
      "Epoch 14 batch 1700 train loss: 0.0187 test loss: 0.0154\n",
      "Epoch 14 batch 1800 train loss: 0.0125 test loss: 0.0152\n",
      "Epoch 14 batch 1900 train loss: 0.0137 test loss: 0.0154\n",
      "Epoch 14 batch 2000 train loss: 0.0105 test loss: 0.0150\n",
      "Epoch 14 batch 2100 train loss: 0.0150 test loss: 0.0152\n",
      "Epoch 14 batch 2200 train loss: 0.0161 test loss: 0.0156\n",
      "Epoch 14 batch 2300 train loss: 0.0109 test loss: 0.0150\n",
      "Epoch 14 batch 2400 train loss: 0.0148 test loss: 0.0155\n",
      "Epoch 14 batch 2500 train loss: 0.0199 test loss: 0.0150\n",
      "Epoch 14 batch 2600 train loss: 0.0115 test loss: 0.0154\n",
      "Epoch 14 batch 2700 train loss: 0.0156 test loss: 0.0156\n",
      "Epoch 14 batch 2800 train loss: 0.0163 test loss: 0.0156\n",
      "Epoch 14 batch 2900 train loss: 0.0224 test loss: 0.0151\n",
      "Epoch 14 batch 3000 train loss: 0.0132 test loss: 0.0151\n",
      "Epoch 15 batch 0 train loss: 0.0165 test loss: 0.0150\n",
      "Epoch 15 batch 100 train loss: 0.0151 test loss: 0.0153\n",
      "Epoch 15 batch 200 train loss: 0.0215 test loss: 0.0157\n",
      "Epoch 15 batch 300 train loss: 0.0133 test loss: 0.0152\n",
      "Epoch 15 batch 400 train loss: 0.0149 test loss: 0.0150\n",
      "Epoch 15 batch 500 train loss: 0.0124 test loss: 0.0154\n",
      "Epoch 15 batch 600 train loss: 0.0119 test loss: 0.0159\n",
      "Epoch 15 batch 700 train loss: 0.0214 test loss: 0.0153\n",
      "Epoch 15 batch 800 train loss: 0.0132 test loss: 0.0156\n",
      "Epoch 15 batch 900 train loss: 0.0170 test loss: 0.0151\n",
      "Epoch 15 batch 1000 train loss: 0.0160 test loss: 0.0157\n",
      "Epoch 15 batch 1100 train loss: 0.0164 test loss: 0.0156\n",
      "Epoch 15 batch 1200 train loss: 0.0179 test loss: 0.0155\n",
      "Epoch 15 batch 1300 train loss: 0.0149 test loss: 0.0156\n",
      "Epoch 15 batch 1400 train loss: 0.0165 test loss: 0.0153\n",
      "Epoch 15 batch 1500 train loss: 0.0103 test loss: 0.0155\n",
      "Epoch 15 batch 1600 train loss: 0.0131 test loss: 0.0157\n",
      "Epoch 15 batch 1700 train loss: 0.0171 test loss: 0.0158\n",
      "Epoch 15 batch 1800 train loss: 0.0132 test loss: 0.0151\n",
      "Epoch 15 batch 1900 train loss: 0.0141 test loss: 0.0153\n",
      "Epoch 15 batch 2000 train loss: 0.0165 test loss: 0.0153\n",
      "Epoch 15 batch 2100 train loss: 0.0198 test loss: 0.0153\n",
      "Epoch 15 batch 2200 train loss: 0.0107 test loss: 0.0159\n",
      "Epoch 15 batch 2300 train loss: 0.0118 test loss: 0.0152\n",
      "Epoch 15 batch 2400 train loss: 0.0134 test loss: 0.0153\n",
      "Epoch 15 batch 2500 train loss: 0.0157 test loss: 0.0155\n",
      "Epoch 15 batch 2600 train loss: 0.0157 test loss: 0.0158\n",
      "Epoch 15 batch 2700 train loss: 0.0205 test loss: 0.0152\n",
      "Epoch 15 batch 2800 train loss: 0.0172 test loss: 0.0156\n",
      "Epoch 15 batch 2900 train loss: 0.0092 test loss: 0.0154\n",
      "Epoch 15 batch 3000 train loss: 0.0251 test loss: 0.0151\n",
      "Epoch 16 batch 0 train loss: 0.0139 test loss: 0.0150\n",
      "Epoch 16 batch 100 train loss: 0.0153 test loss: 0.0156\n",
      "Epoch 16 batch 200 train loss: 0.0153 test loss: 0.0154\n",
      "Epoch 16 batch 300 train loss: 0.0108 test loss: 0.0155\n",
      "Epoch 16 batch 400 train loss: 0.0220 test loss: 0.0155\n",
      "Epoch 16 batch 500 train loss: 0.0206 test loss: 0.0151\n",
      "Epoch 16 batch 600 train loss: 0.0132 test loss: 0.0156\n",
      "Epoch 16 batch 700 train loss: 0.0156 test loss: 0.0154\n",
      "Epoch 16 batch 800 train loss: 0.0161 test loss: 0.0156\n",
      "Epoch 16 batch 900 train loss: 0.0141 test loss: 0.0156\n",
      "Epoch 16 batch 1000 train loss: 0.0144 test loss: 0.0156\n",
      "Epoch 16 batch 1100 train loss: 0.0178 test loss: 0.0155\n",
      "Epoch 16 batch 1200 train loss: 0.0147 test loss: 0.0155\n",
      "Epoch 16 batch 1300 train loss: 0.0157 test loss: 0.0152\n",
      "Epoch 16 batch 1400 train loss: 0.0156 test loss: 0.0154\n",
      "Epoch 16 batch 1500 train loss: 0.0128 test loss: 0.0156\n",
      "Epoch 16 batch 1600 train loss: 0.0110 test loss: 0.0156\n",
      "Epoch 16 batch 1700 train loss: 0.0173 test loss: 0.0161\n",
      "Epoch 16 batch 1800 train loss: 0.0209 test loss: 0.0153\n",
      "Epoch 16 batch 1900 train loss: 0.0170 test loss: 0.0153\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.90p/ckpt-35\n",
      "Epoch 16 batch 2000 train loss: 0.0200 test loss: 0.0148\n",
      "Epoch 16 batch 2100 train loss: 0.0135 test loss: 0.0155\n",
      "Epoch 16 batch 2200 train loss: 0.0143 test loss: 0.0158\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.90p/ckpt-36\n",
      "Epoch 16 batch 2300 train loss: 0.0136 test loss: 0.0147\n",
      "Epoch 16 batch 2400 train loss: 0.0218 test loss: 0.0153\n",
      "Epoch 16 batch 2500 train loss: 0.0155 test loss: 0.0154\n",
      "Epoch 16 batch 2600 train loss: 0.0151 test loss: 0.0153\n",
      "Epoch 16 batch 2700 train loss: 0.0190 test loss: 0.0158\n",
      "Epoch 16 batch 2800 train loss: 0.0207 test loss: 0.0153\n",
      "Epoch 16 batch 2900 train loss: 0.0097 test loss: 0.0152\n",
      "Epoch 16 batch 3000 train loss: 0.0115 test loss: 0.0151\n",
      "Epoch 17 batch 0 train loss: 0.0150 test loss: 0.0151\n",
      "Epoch 17 batch 100 train loss: 0.0151 test loss: 0.0152\n",
      "Epoch 17 batch 200 train loss: 0.0152 test loss: 0.0157\n",
      "Epoch 17 batch 300 train loss: 0.0155 test loss: 0.0156\n",
      "Epoch 17 batch 400 train loss: 0.0123 test loss: 0.0152\n",
      "Epoch 17 batch 500 train loss: 0.0188 test loss: 0.0150\n",
      "Epoch 17 batch 600 train loss: 0.0186 test loss: 0.0156\n",
      "Epoch 17 batch 700 train loss: 0.0162 test loss: 0.0155\n",
      "Epoch 17 batch 800 train loss: 0.0186 test loss: 0.0152\n",
      "early stop.\n",
      "Checkpoint 36 restored!!\n",
      "Training for loss rate 0.95 start.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ffnn_9/dense_27/kernel:0', 'ffnn_9/dense_27/bias:0', 'ffnn_9/batch_normalization_9/gamma:0', 'ffnn_9/batch_normalization_9/beta:0', 'ffnn_9/dense_28/kernel:0', 'ffnn_9/dense_28/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ffnn_9/dense_27/kernel:0', 'ffnn_9/dense_27/bias:0', 'ffnn_9/batch_normalization_9/gamma:0', 'ffnn_9/batch_normalization_9/beta:0', 'ffnn_9/dense_28/kernel:0', 'ffnn_9/dense_28/bias:0'] when minimizing the loss.\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.95p/ckpt-1\n",
      "Epoch 0 batch 0 train loss: 0.2524 test loss: 0.3418\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.95p/ckpt-2\n",
      "Epoch 0 batch 100 train loss: 0.1878 test loss: 0.2575\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.95p/ckpt-3\n",
      "Epoch 0 batch 200 train loss: 0.1478 test loss: 0.1973\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.95p/ckpt-4\n",
      "Epoch 0 batch 300 train loss: 0.1087 test loss: 0.1518\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.95p/ckpt-5\n",
      "Epoch 0 batch 400 train loss: 0.0927 test loss: 0.1178\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.95p/ckpt-6\n",
      "Epoch 0 batch 500 train loss: 0.0603 test loss: 0.0917\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.95p/ckpt-7\n",
      "Epoch 0 batch 600 train loss: 0.0493 test loss: 0.0725\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.95p/ckpt-8\n",
      "Epoch 0 batch 700 train loss: 0.0400 test loss: 0.0579\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.95p/ckpt-9\n",
      "Epoch 0 batch 800 train loss: 0.0351 test loss: 0.0469\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.95p/ckpt-10\n",
      "Epoch 0 batch 900 train loss: 0.0209 test loss: 0.0390\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.95p/ckpt-11\n",
      "Epoch 0 batch 1000 train loss: 0.0215 test loss: 0.0329\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.95p/ckpt-12\n",
      "Epoch 0 batch 1100 train loss: 0.0197 test loss: 0.0283\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.95p/ckpt-13\n",
      "Epoch 0 batch 1200 train loss: 0.0193 test loss: 0.0251\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.95p/ckpt-14\n",
      "Epoch 0 batch 1300 train loss: 0.0180 test loss: 0.0226\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.95p/ckpt-15\n",
      "Epoch 0 batch 1400 train loss: 0.0133 test loss: 0.0209\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.95p/ckpt-16\n",
      "Epoch 0 batch 1500 train loss: 0.0250 test loss: 0.0195\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.95p/ckpt-17\n",
      "Epoch 0 batch 1600 train loss: 0.0139 test loss: 0.0187\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.95p/ckpt-18\n",
      "Epoch 0 batch 1700 train loss: 0.0226 test loss: 0.0183\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.95p/ckpt-19\n",
      "Epoch 0 batch 1800 train loss: 0.0181 test loss: 0.0175\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.95p/ckpt-20\n",
      "Epoch 0 batch 1900 train loss: 0.0178 test loss: 0.0172\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.95p/ckpt-21\n",
      "Epoch 0 batch 2000 train loss: 0.0154 test loss: 0.0168\n",
      "Epoch 0 batch 2100 train loss: 0.0189 test loss: 0.0169\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.95p/ckpt-22\n",
      "Epoch 0 batch 2200 train loss: 0.0248 test loss: 0.0167\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.95p/ckpt-23\n",
      "Epoch 0 batch 2300 train loss: 0.0135 test loss: 0.0166\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.95p/ckpt-24\n",
      "Epoch 0 batch 2400 train loss: 0.0132 test loss: 0.0165\n",
      "Epoch 0 batch 2500 train loss: 0.0186 test loss: 0.0166\n",
      "Epoch 0 batch 2600 train loss: 0.0222 test loss: 0.0165\n",
      "Epoch 0 batch 2700 train loss: 0.0208 test loss: 0.0166\n",
      "Epoch 0 batch 2800 train loss: 0.0139 test loss: 0.0166\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.95p/ckpt-25\n",
      "Epoch 0 batch 2900 train loss: 0.0202 test loss: 0.0164\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.95p/ckpt-26\n",
      "Epoch 0 batch 3000 train loss: 0.0163 test loss: 0.0163\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ffnn_9/dense_27/kernel:0', 'ffnn_9/dense_27/bias:0', 'ffnn_9/batch_normalization_9/gamma:0', 'ffnn_9/batch_normalization_9/beta:0', 'ffnn_9/dense_28/kernel:0', 'ffnn_9/dense_28/bias:0'] when minimizing the loss.\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.95p/ckpt-27\n",
      "Epoch 1 batch 0 train loss: 0.0208 test loss: 0.0162\n",
      "Epoch 1 batch 100 train loss: 0.0140 test loss: 0.0166\n",
      "Epoch 1 batch 200 train loss: 0.0148 test loss: 0.0165\n",
      "Epoch 1 batch 300 train loss: 0.0147 test loss: 0.0163\n",
      "Epoch 1 batch 400 train loss: 0.0128 test loss: 0.0163\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.95p/ckpt-28\n",
      "Epoch 1 batch 500 train loss: 0.0157 test loss: 0.0161\n",
      "Epoch 1 batch 600 train loss: 0.0184 test loss: 0.0165\n",
      "Epoch 1 batch 700 train loss: 0.0178 test loss: 0.0165\n",
      "Epoch 1 batch 800 train loss: 0.0147 test loss: 0.0166\n",
      "Epoch 1 batch 900 train loss: 0.0176 test loss: 0.0166\n",
      "Epoch 1 batch 1000 train loss: 0.0228 test loss: 0.0165\n",
      "Epoch 1 batch 1100 train loss: 0.0137 test loss: 0.0165\n",
      "Epoch 1 batch 1200 train loss: 0.0145 test loss: 0.0164\n",
      "Epoch 1 batch 1300 train loss: 0.0157 test loss: 0.0166\n",
      "Epoch 1 batch 1400 train loss: 0.0139 test loss: 0.0164\n",
      "Epoch 1 batch 1500 train loss: 0.0157 test loss: 0.0167\n",
      "Epoch 1 batch 1600 train loss: 0.0172 test loss: 0.0167\n",
      "Epoch 1 batch 1700 train loss: 0.0183 test loss: 0.0165\n",
      "Epoch 1 batch 1800 train loss: 0.0144 test loss: 0.0163\n",
      "Epoch 1 batch 1900 train loss: 0.0170 test loss: 0.0162\n",
      "Epoch 1 batch 2000 train loss: 0.0142 test loss: 0.0164\n",
      "Epoch 1 batch 2100 train loss: 0.0171 test loss: 0.0165\n",
      "Epoch 1 batch 2200 train loss: 0.0235 test loss: 0.0167\n",
      "Epoch 1 batch 2300 train loss: 0.0187 test loss: 0.0162\n",
      "Epoch 1 batch 2400 train loss: 0.0155 test loss: 0.0164\n",
      "Epoch 1 batch 2500 train loss: 0.0223 test loss: 0.0164\n",
      "Epoch 1 batch 2600 train loss: 0.0137 test loss: 0.0169\n",
      "Epoch 1 batch 2700 train loss: 0.0180 test loss: 0.0164\n",
      "Epoch 1 batch 2800 train loss: 0.0207 test loss: 0.0164\n",
      "Epoch 1 batch 2900 train loss: 0.0171 test loss: 0.0163\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.95p/ckpt-29\n",
      "Epoch 1 batch 3000 train loss: 0.0160 test loss: 0.0160\n",
      "Epoch 2 batch 0 train loss: 0.0168 test loss: 0.0161\n",
      "Epoch 2 batch 100 train loss: 0.0204 test loss: 0.0164\n",
      "Epoch 2 batch 200 train loss: 0.0195 test loss: 0.0164\n",
      "Epoch 2 batch 300 train loss: 0.0130 test loss: 0.0165\n",
      "Epoch 2 batch 400 train loss: 0.0152 test loss: 0.0161\n",
      "Epoch 2 batch 500 train loss: 0.0161 test loss: 0.0162\n",
      "Epoch 2 batch 600 train loss: 0.0209 test loss: 0.0167\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.95p/ckpt-30\n",
      "Epoch 2 batch 700 train loss: 0.0199 test loss: 0.0160\n",
      "Epoch 2 batch 800 train loss: 0.0175 test loss: 0.0169\n",
      "Epoch 2 batch 900 train loss: 0.0141 test loss: 0.0165\n",
      "Epoch 2 batch 1000 train loss: 0.0187 test loss: 0.0168\n",
      "Epoch 2 batch 1100 train loss: 0.0159 test loss: 0.0168\n",
      "Epoch 2 batch 1200 train loss: 0.0170 test loss: 0.0165\n",
      "Epoch 2 batch 1300 train loss: 0.0151 test loss: 0.0161\n",
      "Epoch 2 batch 1400 train loss: 0.0131 test loss: 0.0165\n",
      "Epoch 2 batch 1500 train loss: 0.0154 test loss: 0.0164\n",
      "Epoch 2 batch 1600 train loss: 0.0146 test loss: 0.0167\n",
      "Epoch 2 batch 1700 train loss: 0.0169 test loss: 0.0166\n",
      "Epoch 2 batch 1800 train loss: 0.0178 test loss: 0.0160\n",
      "Epoch 2 batch 1900 train loss: 0.0152 test loss: 0.0166\n",
      "Epoch 2 batch 2000 train loss: 0.0214 test loss: 0.0165\n",
      "Epoch 2 batch 2100 train loss: 0.0169 test loss: 0.0166\n",
      "Epoch 2 batch 2200 train loss: 0.0188 test loss: 0.0167\n",
      "Epoch 2 batch 2300 train loss: 0.0131 test loss: 0.0165\n",
      "Epoch 2 batch 2400 train loss: 0.0117 test loss: 0.0162\n",
      "Epoch 2 batch 2500 train loss: 0.0176 test loss: 0.0169\n",
      "Epoch 2 batch 2600 train loss: 0.0174 test loss: 0.0165\n",
      "Epoch 2 batch 2700 train loss: 0.0102 test loss: 0.0166\n",
      "Epoch 2 batch 2800 train loss: 0.0152 test loss: 0.0163\n",
      "Epoch 2 batch 2900 train loss: 0.0117 test loss: 0.0165\n",
      "Epoch 2 batch 3000 train loss: 0.0179 test loss: 0.0161\n",
      "Saving checkpoint at ./checkpoints/FFNN_best_loss_0.95p/ckpt-31\n",
      "Epoch 3 batch 0 train loss: 0.0210 test loss: 0.0157\n",
      "Epoch 3 batch 100 train loss: 0.0146 test loss: 0.0164\n",
      "Epoch 3 batch 200 train loss: 0.0149 test loss: 0.0165\n",
      "Epoch 3 batch 300 train loss: 0.0138 test loss: 0.0167\n",
      "Epoch 3 batch 400 train loss: 0.0182 test loss: 0.0158\n",
      "Epoch 3 batch 500 train loss: 0.0152 test loss: 0.0164\n",
      "Epoch 3 batch 600 train loss: 0.0132 test loss: 0.0166\n",
      "Epoch 3 batch 700 train loss: 0.0107 test loss: 0.0164\n",
      "Epoch 3 batch 800 train loss: 0.0139 test loss: 0.0164\n",
      "Epoch 3 batch 900 train loss: 0.0215 test loss: 0.0163\n",
      "Epoch 3 batch 1000 train loss: 0.0134 test loss: 0.0165\n",
      "Epoch 3 batch 1100 train loss: 0.0131 test loss: 0.0167\n",
      "Epoch 3 batch 1200 train loss: 0.0210 test loss: 0.0163\n",
      "Epoch 3 batch 1300 train loss: 0.0153 test loss: 0.0160\n",
      "Epoch 3 batch 1400 train loss: 0.0114 test loss: 0.0167\n",
      "Epoch 3 batch 1500 train loss: 0.0192 test loss: 0.0167\n",
      "Epoch 3 batch 1600 train loss: 0.0152 test loss: 0.0164\n",
      "Epoch 3 batch 1700 train loss: 0.0164 test loss: 0.0165\n",
      "Epoch 3 batch 1800 train loss: 0.0204 test loss: 0.0160\n",
      "Epoch 3 batch 1900 train loss: 0.0193 test loss: 0.0168\n",
      "Epoch 3 batch 2000 train loss: 0.0174 test loss: 0.0158\n",
      "Epoch 3 batch 2100 train loss: 0.0122 test loss: 0.0163\n",
      "Epoch 3 batch 2200 train loss: 0.0164 test loss: 0.0164\n",
      "Epoch 3 batch 2300 train loss: 0.0173 test loss: 0.0161\n",
      "Epoch 3 batch 2400 train loss: 0.0265 test loss: 0.0162\n",
      "Epoch 3 batch 2500 train loss: 0.0127 test loss: 0.0161\n",
      "Epoch 3 batch 2600 train loss: 0.0160 test loss: 0.0165\n",
      "Epoch 3 batch 2700 train loss: 0.0174 test loss: 0.0166\n",
      "Epoch 3 batch 2800 train loss: 0.0140 test loss: 0.0165\n",
      "Epoch 3 batch 2900 train loss: 0.0210 test loss: 0.0162\n",
      "Epoch 3 batch 3000 train loss: 0.0156 test loss: 0.0157\n",
      "Epoch 4 batch 0 train loss: 0.0198 test loss: 0.0158\n",
      "Epoch 4 batch 100 train loss: 0.0173 test loss: 0.0167\n",
      "Epoch 4 batch 200 train loss: 0.0200 test loss: 0.0164\n",
      "Epoch 4 batch 300 train loss: 0.0187 test loss: 0.0167\n",
      "Epoch 4 batch 400 train loss: 0.0125 test loss: 0.0164\n",
      "Epoch 4 batch 500 train loss: 0.0159 test loss: 0.0162\n",
      "Epoch 4 batch 600 train loss: 0.0201 test loss: 0.0166\n",
      "Epoch 4 batch 700 train loss: 0.0167 test loss: 0.0162\n",
      "Epoch 4 batch 800 train loss: 0.0160 test loss: 0.0169\n",
      "Epoch 4 batch 900 train loss: 0.0194 test loss: 0.0162\n",
      "Epoch 4 batch 1000 train loss: 0.0142 test loss: 0.0163\n",
      "Epoch 4 batch 1100 train loss: 0.0192 test loss: 0.0170\n",
      "Epoch 4 batch 1200 train loss: 0.0235 test loss: 0.0164\n",
      "Epoch 4 batch 1300 train loss: 0.0131 test loss: 0.0165\n",
      "Epoch 4 batch 1400 train loss: 0.0147 test loss: 0.0161\n",
      "Epoch 4 batch 1500 train loss: 0.0227 test loss: 0.0167\n",
      "Epoch 4 batch 1600 train loss: 0.0187 test loss: 0.0162\n",
      "Epoch 4 batch 1700 train loss: 0.0196 test loss: 0.0167\n",
      "Epoch 4 batch 1800 train loss: 0.0153 test loss: 0.0162\n",
      "Epoch 4 batch 1900 train loss: 0.0176 test loss: 0.0164\n",
      "Epoch 4 batch 2000 train loss: 0.0130 test loss: 0.0161\n",
      "Epoch 4 batch 2100 train loss: 0.0174 test loss: 0.0161\n",
      "Epoch 4 batch 2200 train loss: 0.0220 test loss: 0.0167\n",
      "Epoch 4 batch 2300 train loss: 0.0215 test loss: 0.0162\n",
      "Epoch 4 batch 2400 train loss: 0.0213 test loss: 0.0163\n",
      "Epoch 4 batch 2500 train loss: 0.0144 test loss: 0.0164\n",
      "Epoch 4 batch 2600 train loss: 0.0132 test loss: 0.0163\n",
      "Epoch 4 batch 2700 train loss: 0.0109 test loss: 0.0167\n",
      "Epoch 4 batch 2800 train loss: 0.0182 test loss: 0.0164\n",
      "Epoch 4 batch 2900 train loss: 0.0186 test loss: 0.0163\n",
      "Epoch 4 batch 3000 train loss: 0.0162 test loss: 0.0160\n",
      "Epoch 5 batch 0 train loss: 0.0160 test loss: 0.0159\n",
      "Epoch 5 batch 100 train loss: 0.0140 test loss: 0.0166\n",
      "Epoch 5 batch 200 train loss: 0.0140 test loss: 0.0163\n",
      "Epoch 5 batch 300 train loss: 0.0183 test loss: 0.0166\n",
      "Epoch 5 batch 400 train loss: 0.0210 test loss: 0.0163\n",
      "Epoch 5 batch 500 train loss: 0.0169 test loss: 0.0166\n",
      "Epoch 5 batch 600 train loss: 0.0165 test loss: 0.0166\n",
      "Epoch 5 batch 700 train loss: 0.0120 test loss: 0.0162\n",
      "Epoch 5 batch 800 train loss: 0.0165 test loss: 0.0164\n",
      "Epoch 5 batch 900 train loss: 0.0144 test loss: 0.0165\n",
      "Epoch 5 batch 1000 train loss: 0.0183 test loss: 0.0168\n",
      "Epoch 5 batch 1100 train loss: 0.0139 test loss: 0.0166\n",
      "Epoch 5 batch 1200 train loss: 0.0131 test loss: 0.0161\n",
      "Epoch 5 batch 1300 train loss: 0.0166 test loss: 0.0164\n",
      "Epoch 5 batch 1400 train loss: 0.0136 test loss: 0.0167\n",
      "Epoch 5 batch 1500 train loss: 0.0202 test loss: 0.0167\n",
      "Epoch 5 batch 1600 train loss: 0.0115 test loss: 0.0166\n",
      "Epoch 5 batch 1700 train loss: 0.0200 test loss: 0.0165\n",
      "Epoch 5 batch 1800 train loss: 0.0154 test loss: 0.0159\n",
      "Epoch 5 batch 1900 train loss: 0.0170 test loss: 0.0164\n",
      "Epoch 5 batch 2000 train loss: 0.0157 test loss: 0.0161\n",
      "Epoch 5 batch 2100 train loss: 0.0143 test loss: 0.0164\n",
      "Epoch 5 batch 2200 train loss: 0.0256 test loss: 0.0169\n",
      "Epoch 5 batch 2300 train loss: 0.0168 test loss: 0.0162\n",
      "Epoch 5 batch 2400 train loss: 0.0147 test loss: 0.0165\n",
      "Epoch 5 batch 2500 train loss: 0.0137 test loss: 0.0163\n",
      "Epoch 5 batch 2600 train loss: 0.0147 test loss: 0.0166\n",
      "Epoch 5 batch 2700 train loss: 0.0137 test loss: 0.0167\n",
      "Epoch 5 batch 2800 train loss: 0.0130 test loss: 0.0163\n",
      "Epoch 5 batch 2900 train loss: 0.0147 test loss: 0.0169\n",
      "Epoch 5 batch 3000 train loss: 0.0149 test loss: 0.0162\n",
      "Epoch 6 batch 0 train loss: 0.0173 test loss: 0.0159\n",
      "Epoch 6 batch 100 train loss: 0.0149 test loss: 0.0163\n",
      "Epoch 6 batch 200 train loss: 0.0230 test loss: 0.0165\n",
      "Epoch 6 batch 300 train loss: 0.0107 test loss: 0.0163\n",
      "Epoch 6 batch 400 train loss: 0.0245 test loss: 0.0164\n",
      "Epoch 6 batch 500 train loss: 0.0143 test loss: 0.0163\n",
      "Epoch 6 batch 600 train loss: 0.0141 test loss: 0.0167\n",
      "Epoch 6 batch 700 train loss: 0.0119 test loss: 0.0165\n",
      "Epoch 6 batch 800 train loss: 0.0145 test loss: 0.0164\n",
      "Epoch 6 batch 900 train loss: 0.0173 test loss: 0.0166\n",
      "Epoch 6 batch 1000 train loss: 0.0227 test loss: 0.0167\n",
      "Epoch 6 batch 1100 train loss: 0.0134 test loss: 0.0164\n",
      "Epoch 6 batch 1200 train loss: 0.0165 test loss: 0.0162\n",
      "Epoch 6 batch 1300 train loss: 0.0166 test loss: 0.0160\n",
      "Epoch 6 batch 1400 train loss: 0.0174 test loss: 0.0164\n",
      "Epoch 6 batch 1500 train loss: 0.0161 test loss: 0.0164\n",
      "Epoch 6 batch 1600 train loss: 0.0174 test loss: 0.0167\n",
      "Epoch 6 batch 1700 train loss: 0.0142 test loss: 0.0169\n",
      "Epoch 6 batch 1800 train loss: 0.0164 test loss: 0.0164\n",
      "Epoch 6 batch 1900 train loss: 0.0159 test loss: 0.0163\n",
      "Epoch 6 batch 2000 train loss: 0.0189 test loss: 0.0163\n",
      "Epoch 6 batch 2100 train loss: 0.0141 test loss: 0.0168\n",
      "Epoch 6 batch 2200 train loss: 0.0144 test loss: 0.0163\n",
      "Epoch 6 batch 2300 train loss: 0.0152 test loss: 0.0161\n",
      "Epoch 6 batch 2400 train loss: 0.0163 test loss: 0.0162\n",
      "Epoch 6 batch 2500 train loss: 0.0118 test loss: 0.0165\n",
      "Epoch 6 batch 2600 train loss: 0.0160 test loss: 0.0162\n",
      "Epoch 6 batch 2700 train loss: 0.0148 test loss: 0.0165\n",
      "Epoch 6 batch 2800 train loss: 0.0126 test loss: 0.0164\n",
      "Epoch 6 batch 2900 train loss: 0.0205 test loss: 0.0163\n",
      "Epoch 6 batch 3000 train loss: 0.0152 test loss: 0.0162\n",
      "Epoch 7 batch 0 train loss: 0.0169 test loss: 0.0157\n",
      "Epoch 7 batch 100 train loss: 0.0206 test loss: 0.0168\n",
      "Epoch 7 batch 200 train loss: 0.0171 test loss: 0.0164\n",
      "Epoch 7 batch 300 train loss: 0.0139 test loss: 0.0163\n",
      "Epoch 7 batch 400 train loss: 0.0133 test loss: 0.0161\n",
      "Epoch 7 batch 500 train loss: 0.0157 test loss: 0.0159\n",
      "Epoch 7 batch 600 train loss: 0.0104 test loss: 0.0164\n",
      "Epoch 7 batch 700 train loss: 0.0178 test loss: 0.0165\n",
      "Epoch 7 batch 800 train loss: 0.0201 test loss: 0.0169\n",
      "Epoch 7 batch 900 train loss: 0.0147 test loss: 0.0163\n",
      "Epoch 7 batch 1000 train loss: 0.0143 test loss: 0.0167\n",
      "Epoch 7 batch 1100 train loss: 0.0174 test loss: 0.0165\n",
      "Epoch 7 batch 1200 train loss: 0.0187 test loss: 0.0161\n",
      "Epoch 7 batch 1300 train loss: 0.0128 test loss: 0.0163\n",
      "Epoch 7 batch 1400 train loss: 0.0144 test loss: 0.0167\n",
      "Epoch 7 batch 1500 train loss: 0.0137 test loss: 0.0168\n",
      "Epoch 7 batch 1600 train loss: 0.0218 test loss: 0.0167\n",
      "Epoch 7 batch 1700 train loss: 0.0170 test loss: 0.0166\n",
      "Epoch 7 batch 1800 train loss: 0.0115 test loss: 0.0159\n",
      "Epoch 7 batch 1900 train loss: 0.0168 test loss: 0.0163\n",
      "Epoch 7 batch 2000 train loss: 0.0108 test loss: 0.0163\n",
      "Epoch 7 batch 2100 train loss: 0.0163 test loss: 0.0164\n",
      "Epoch 7 batch 2200 train loss: 0.0158 test loss: 0.0169\n",
      "Epoch 7 batch 2300 train loss: 0.0222 test loss: 0.0158\n",
      "Epoch 7 batch 2400 train loss: 0.0150 test loss: 0.0164\n",
      "Epoch 7 batch 2500 train loss: 0.0157 test loss: 0.0164\n",
      "Epoch 7 batch 2600 train loss: 0.0205 test loss: 0.0165\n",
      "Epoch 7 batch 2700 train loss: 0.0166 test loss: 0.0170\n",
      "Epoch 7 batch 2800 train loss: 0.0141 test loss: 0.0166\n",
      "Epoch 7 batch 2900 train loss: 0.0199 test loss: 0.0163\n",
      "Epoch 7 batch 3000 train loss: 0.0193 test loss: 0.0160\n",
      "Epoch 8 batch 0 train loss: 0.0193 test loss: 0.0162\n",
      "Epoch 8 batch 100 train loss: 0.0201 test loss: 0.0165\n",
      "Epoch 8 batch 200 train loss: 0.0161 test loss: 0.0166\n",
      "Epoch 8 batch 300 train loss: 0.0160 test loss: 0.0166\n",
      "Epoch 8 batch 400 train loss: 0.0181 test loss: 0.0160\n",
      "Epoch 8 batch 500 train loss: 0.0185 test loss: 0.0163\n",
      "Epoch 8 batch 600 train loss: 0.0145 test loss: 0.0164\n",
      "Epoch 8 batch 700 train loss: 0.0151 test loss: 0.0161\n",
      "Epoch 8 batch 800 train loss: 0.0137 test loss: 0.0167\n",
      "Epoch 8 batch 900 train loss: 0.0127 test loss: 0.0164\n",
      "Epoch 8 batch 1000 train loss: 0.0159 test loss: 0.0170\n",
      "Epoch 8 batch 1100 train loss: 0.0166 test loss: 0.0167\n",
      "Epoch 8 batch 1200 train loss: 0.0157 test loss: 0.0162\n",
      "Epoch 8 batch 1300 train loss: 0.0203 test loss: 0.0163\n",
      "Epoch 8 batch 1400 train loss: 0.0163 test loss: 0.0168\n",
      "Epoch 8 batch 1500 train loss: 0.0226 test loss: 0.0164\n",
      "Epoch 8 batch 1600 train loss: 0.0144 test loss: 0.0166\n",
      "Epoch 8 batch 1700 train loss: 0.0211 test loss: 0.0164\n",
      "Epoch 8 batch 1800 train loss: 0.0168 test loss: 0.0159\n",
      "Epoch 8 batch 1900 train loss: 0.0189 test loss: 0.0165\n",
      "Epoch 8 batch 2000 train loss: 0.0200 test loss: 0.0160\n",
      "Epoch 8 batch 2100 train loss: 0.0148 test loss: 0.0165\n",
      "Epoch 8 batch 2200 train loss: 0.0103 test loss: 0.0166\n",
      "Epoch 8 batch 2300 train loss: 0.0104 test loss: 0.0159\n",
      "Epoch 8 batch 2400 train loss: 0.0174 test loss: 0.0161\n",
      "Epoch 8 batch 2500 train loss: 0.0165 test loss: 0.0164\n",
      "Epoch 8 batch 2600 train loss: 0.0167 test loss: 0.0167\n",
      "Epoch 8 batch 2700 train loss: 0.0126 test loss: 0.0164\n",
      "Epoch 8 batch 2800 train loss: 0.0177 test loss: 0.0167\n",
      "Epoch 8 batch 2900 train loss: 0.0192 test loss: 0.0165\n",
      "Epoch 8 batch 3000 train loss: 0.0151 test loss: 0.0159\n",
      "Epoch 9 batch 0 train loss: 0.0176 test loss: 0.0161\n",
      "Epoch 9 batch 100 train loss: 0.0196 test loss: 0.0165\n",
      "Epoch 9 batch 200 train loss: 0.0124 test loss: 0.0165\n",
      "Epoch 9 batch 300 train loss: 0.0122 test loss: 0.0165\n",
      "Epoch 9 batch 400 train loss: 0.0134 test loss: 0.0160\n",
      "Epoch 9 batch 500 train loss: 0.0121 test loss: 0.0162\n",
      "Epoch 9 batch 600 train loss: 0.0182 test loss: 0.0166\n",
      "Epoch 9 batch 700 train loss: 0.0126 test loss: 0.0159\n",
      "Epoch 9 batch 800 train loss: 0.0158 test loss: 0.0167\n",
      "Epoch 9 batch 900 train loss: 0.0187 test loss: 0.0164\n",
      "Epoch 9 batch 1000 train loss: 0.0154 test loss: 0.0168\n",
      "Epoch 9 batch 1100 train loss: 0.0149 test loss: 0.0166\n",
      "Epoch 9 batch 1200 train loss: 0.0168 test loss: 0.0164\n",
      "Epoch 9 batch 1300 train loss: 0.0231 test loss: 0.0169\n",
      "Epoch 9 batch 1400 train loss: 0.0200 test loss: 0.0164\n",
      "Epoch 9 batch 1500 train loss: 0.0224 test loss: 0.0166\n",
      "Epoch 9 batch 1600 train loss: 0.0167 test loss: 0.0168\n",
      "Epoch 9 batch 1700 train loss: 0.0153 test loss: 0.0169\n",
      "Epoch 9 batch 1800 train loss: 0.0123 test loss: 0.0160\n",
      "Epoch 9 batch 1900 train loss: 0.0155 test loss: 0.0166\n",
      "Epoch 9 batch 2000 train loss: 0.0158 test loss: 0.0163\n",
      "Epoch 9 batch 2100 train loss: 0.0150 test loss: 0.0164\n",
      "Epoch 9 batch 2200 train loss: 0.0147 test loss: 0.0162\n",
      "Epoch 9 batch 2300 train loss: 0.0147 test loss: 0.0161\n",
      "Epoch 9 batch 2400 train loss: 0.0157 test loss: 0.0163\n",
      "Epoch 9 batch 2500 train loss: 0.0130 test loss: 0.0168\n",
      "Epoch 9 batch 2600 train loss: 0.0128 test loss: 0.0167\n",
      "Epoch 9 batch 2700 train loss: 0.0151 test loss: 0.0166\n",
      "Epoch 9 batch 2800 train loss: 0.0148 test loss: 0.0166\n",
      "Epoch 9 batch 2900 train loss: 0.0125 test loss: 0.0166\n",
      "Epoch 9 batch 3000 train loss: 0.0171 test loss: 0.0164\n",
      "Epoch 10 batch 0 train loss: 0.0144 test loss: 0.0161\n",
      "Epoch 10 batch 100 train loss: 0.0158 test loss: 0.0166\n",
      "Epoch 10 batch 200 train loss: 0.0188 test loss: 0.0166\n",
      "Epoch 10 batch 300 train loss: 0.0152 test loss: 0.0164\n",
      "Epoch 10 batch 400 train loss: 0.0189 test loss: 0.0163\n",
      "Epoch 10 batch 500 train loss: 0.0114 test loss: 0.0159\n",
      "Epoch 10 batch 600 train loss: 0.0137 test loss: 0.0165\n",
      "Epoch 10 batch 700 train loss: 0.0175 test loss: 0.0162\n",
      "Epoch 10 batch 800 train loss: 0.0142 test loss: 0.0166\n",
      "Epoch 10 batch 900 train loss: 0.0101 test loss: 0.0168\n",
      "Epoch 10 batch 1000 train loss: 0.0159 test loss: 0.0167\n",
      "Epoch 10 batch 1100 train loss: 0.0199 test loss: 0.0165\n",
      "Epoch 10 batch 1200 train loss: 0.0178 test loss: 0.0165\n",
      "Epoch 10 batch 1300 train loss: 0.0248 test loss: 0.0163\n",
      "Epoch 10 batch 1400 train loss: 0.0136 test loss: 0.0164\n",
      "Epoch 10 batch 1500 train loss: 0.0188 test loss: 0.0166\n",
      "Epoch 10 batch 1600 train loss: 0.0141 test loss: 0.0169\n",
      "Epoch 10 batch 1700 train loss: 0.0150 test loss: 0.0167\n",
      "Epoch 10 batch 1800 train loss: 0.0264 test loss: 0.0161\n",
      "Epoch 10 batch 1900 train loss: 0.0189 test loss: 0.0164\n",
      "Epoch 10 batch 2000 train loss: 0.0131 test loss: 0.0163\n",
      "Epoch 10 batch 2100 train loss: 0.0131 test loss: 0.0167\n",
      "Epoch 10 batch 2200 train loss: 0.0229 test loss: 0.0166\n",
      "Epoch 10 batch 2300 train loss: 0.0160 test loss: 0.0163\n",
      "Epoch 10 batch 2400 train loss: 0.0186 test loss: 0.0165\n",
      "Epoch 10 batch 2500 train loss: 0.0124 test loss: 0.0164\n",
      "Epoch 10 batch 2600 train loss: 0.0184 test loss: 0.0165\n",
      "Epoch 10 batch 2700 train loss: 0.0126 test loss: 0.0167\n",
      "Epoch 10 batch 2800 train loss: 0.0155 test loss: 0.0164\n",
      "Epoch 10 batch 2900 train loss: 0.0187 test loss: 0.0164\n",
      "Epoch 10 batch 3000 train loss: 0.0132 test loss: 0.0162\n",
      "Epoch 11 batch 0 train loss: 0.0149 test loss: 0.0160\n",
      "Epoch 11 batch 100 train loss: 0.0183 test loss: 0.0163\n",
      "Epoch 11 batch 200 train loss: 0.0150 test loss: 0.0166\n",
      "Epoch 11 batch 300 train loss: 0.0196 test loss: 0.0165\n",
      "Epoch 11 batch 400 train loss: 0.0152 test loss: 0.0158\n",
      "Epoch 11 batch 500 train loss: 0.0161 test loss: 0.0162\n",
      "Epoch 11 batch 600 train loss: 0.0179 test loss: 0.0167\n",
      "Epoch 11 batch 700 train loss: 0.0200 test loss: 0.0166\n",
      "Epoch 11 batch 800 train loss: 0.0162 test loss: 0.0164\n",
      "Epoch 11 batch 900 train loss: 0.0189 test loss: 0.0163\n",
      "Epoch 11 batch 1000 train loss: 0.0150 test loss: 0.0165\n",
      "Epoch 11 batch 1100 train loss: 0.0187 test loss: 0.0164\n",
      "Epoch 11 batch 1200 train loss: 0.0152 test loss: 0.0168\n",
      "Epoch 11 batch 1300 train loss: 0.0206 test loss: 0.0159\n",
      "Epoch 11 batch 1400 train loss: 0.0166 test loss: 0.0164\n",
      "Epoch 11 batch 1500 train loss: 0.0128 test loss: 0.0164\n",
      "Epoch 11 batch 1600 train loss: 0.0135 test loss: 0.0170\n",
      "Epoch 11 batch 1700 train loss: 0.0136 test loss: 0.0168\n",
      "Epoch 11 batch 1800 train loss: 0.0124 test loss: 0.0159\n",
      "Epoch 11 batch 1900 train loss: 0.0179 test loss: 0.0162\n",
      "Epoch 11 batch 2000 train loss: 0.0118 test loss: 0.0165\n",
      "Epoch 11 batch 2100 train loss: 0.0173 test loss: 0.0166\n",
      "Epoch 11 batch 2200 train loss: 0.0105 test loss: 0.0165\n",
      "Epoch 11 batch 2300 train loss: 0.0175 test loss: 0.0162\n",
      "Epoch 11 batch 2400 train loss: 0.0174 test loss: 0.0164\n",
      "Epoch 11 batch 2500 train loss: 0.0163 test loss: 0.0166\n",
      "Epoch 11 batch 2600 train loss: 0.0219 test loss: 0.0168\n",
      "Epoch 11 batch 2700 train loss: 0.0186 test loss: 0.0167\n",
      "Epoch 11 batch 2800 train loss: 0.0159 test loss: 0.0165\n",
      "Epoch 11 batch 2900 train loss: 0.0153 test loss: 0.0164\n",
      "Epoch 11 batch 3000 train loss: 0.0140 test loss: 0.0159\n",
      "Epoch 12 batch 0 train loss: 0.0161 test loss: 0.0162\n",
      "Epoch 12 batch 100 train loss: 0.0247 test loss: 0.0166\n",
      "Epoch 12 batch 200 train loss: 0.0204 test loss: 0.0165\n",
      "Epoch 12 batch 300 train loss: 0.0133 test loss: 0.0166\n",
      "Epoch 12 batch 400 train loss: 0.0202 test loss: 0.0160\n",
      "Epoch 12 batch 500 train loss: 0.0226 test loss: 0.0166\n",
      "Epoch 12 batch 600 train loss: 0.0151 test loss: 0.0164\n",
      "Epoch 12 batch 700 train loss: 0.0144 test loss: 0.0163\n",
      "Epoch 12 batch 800 train loss: 0.0175 test loss: 0.0164\n",
      "Epoch 12 batch 900 train loss: 0.0213 test loss: 0.0165\n",
      "Epoch 12 batch 1000 train loss: 0.0157 test loss: 0.0163\n",
      "Epoch 12 batch 1100 train loss: 0.0172 test loss: 0.0166\n",
      "Epoch 12 batch 1200 train loss: 0.0169 test loss: 0.0163\n",
      "Epoch 12 batch 1300 train loss: 0.0158 test loss: 0.0161\n",
      "Epoch 12 batch 1400 train loss: 0.0175 test loss: 0.0161\n",
      "Epoch 12 batch 1500 train loss: 0.0165 test loss: 0.0167\n",
      "Epoch 12 batch 1600 train loss: 0.0215 test loss: 0.0166\n",
      "Epoch 12 batch 1700 train loss: 0.0144 test loss: 0.0167\n",
      "Epoch 12 batch 1800 train loss: 0.0168 test loss: 0.0163\n",
      "Epoch 12 batch 1900 train loss: 0.0135 test loss: 0.0165\n",
      "Epoch 12 batch 2000 train loss: 0.0138 test loss: 0.0162\n",
      "Epoch 12 batch 2100 train loss: 0.0116 test loss: 0.0162\n",
      "Epoch 12 batch 2200 train loss: 0.0203 test loss: 0.0166\n",
      "Epoch 12 batch 2300 train loss: 0.0151 test loss: 0.0164\n",
      "Epoch 12 batch 2400 train loss: 0.0194 test loss: 0.0163\n",
      "Epoch 12 batch 2500 train loss: 0.0131 test loss: 0.0164\n",
      "Epoch 12 batch 2600 train loss: 0.0141 test loss: 0.0164\n",
      "Epoch 12 batch 2700 train loss: 0.0238 test loss: 0.0165\n",
      "Epoch 12 batch 2800 train loss: 0.0154 test loss: 0.0169\n",
      "Epoch 12 batch 2900 train loss: 0.0181 test loss: 0.0162\n",
      "Epoch 12 batch 3000 train loss: 0.0153 test loss: 0.0160\n",
      "Epoch 13 batch 0 train loss: 0.0151 test loss: 0.0159\n",
      "Epoch 13 batch 100 train loss: 0.0168 test loss: 0.0165\n",
      "Epoch 13 batch 200 train loss: 0.0111 test loss: 0.0162\n",
      "Epoch 13 batch 300 train loss: 0.0190 test loss: 0.0165\n",
      "Epoch 13 batch 400 train loss: 0.0225 test loss: 0.0163\n",
      "Epoch 13 batch 500 train loss: 0.0181 test loss: 0.0164\n",
      "Epoch 13 batch 600 train loss: 0.0175 test loss: 0.0166\n",
      "Epoch 13 batch 700 train loss: 0.0146 test loss: 0.0163\n",
      "Epoch 13 batch 800 train loss: 0.0161 test loss: 0.0167\n",
      "Epoch 13 batch 900 train loss: 0.0216 test loss: 0.0167\n",
      "Epoch 13 batch 1000 train loss: 0.0169 test loss: 0.0167\n",
      "Epoch 13 batch 1100 train loss: 0.0215 test loss: 0.0164\n",
      "Epoch 13 batch 1200 train loss: 0.0144 test loss: 0.0163\n",
      "Epoch 13 batch 1300 train loss: 0.0136 test loss: 0.0166\n",
      "Epoch 13 batch 1400 train loss: 0.0153 test loss: 0.0163\n",
      "Epoch 13 batch 1500 train loss: 0.0207 test loss: 0.0164\n",
      "Epoch 13 batch 1600 train loss: 0.0104 test loss: 0.0168\n",
      "Epoch 13 batch 1700 train loss: 0.0141 test loss: 0.0167\n",
      "Epoch 13 batch 1800 train loss: 0.0187 test loss: 0.0163\n",
      "Epoch 13 batch 1900 train loss: 0.0143 test loss: 0.0163\n",
      "Epoch 13 batch 2000 train loss: 0.0155 test loss: 0.0162\n",
      "Epoch 13 batch 2100 train loss: 0.0185 test loss: 0.0164\n",
      "Epoch 13 batch 2200 train loss: 0.0141 test loss: 0.0166\n",
      "Epoch 13 batch 2300 train loss: 0.0186 test loss: 0.0164\n",
      "Epoch 13 batch 2400 train loss: 0.0195 test loss: 0.0163\n",
      "Epoch 13 batch 2500 train loss: 0.0157 test loss: 0.0166\n",
      "Epoch 13 batch 2600 train loss: 0.0106 test loss: 0.0167\n",
      "Epoch 13 batch 2700 train loss: 0.0127 test loss: 0.0165\n",
      "Epoch 13 batch 2800 train loss: 0.0144 test loss: 0.0165\n",
      "Epoch 13 batch 2900 train loss: 0.0185 test loss: 0.0163\n",
      "Epoch 13 batch 3000 train loss: 0.0133 test loss: 0.0163\n",
      "Epoch 14 batch 0 train loss: 0.0155 test loss: 0.0163\n",
      "Epoch 14 batch 100 train loss: 0.0175 test loss: 0.0166\n",
      "Epoch 14 batch 200 train loss: 0.0201 test loss: 0.0168\n",
      "Epoch 14 batch 300 train loss: 0.0138 test loss: 0.0168\n",
      "Epoch 14 batch 400 train loss: 0.0183 test loss: 0.0162\n",
      "Epoch 14 batch 500 train loss: 0.0167 test loss: 0.0164\n",
      "Epoch 14 batch 600 train loss: 0.0109 test loss: 0.0165\n",
      "Epoch 14 batch 700 train loss: 0.0197 test loss: 0.0163\n",
      "Epoch 14 batch 800 train loss: 0.0110 test loss: 0.0165\n",
      "Epoch 14 batch 900 train loss: 0.0119 test loss: 0.0167\n",
      "Epoch 14 batch 1000 train loss: 0.0180 test loss: 0.0168\n",
      "Epoch 14 batch 1100 train loss: 0.0148 test loss: 0.0165\n",
      "Epoch 14 batch 1200 train loss: 0.0143 test loss: 0.0160\n",
      "Epoch 14 batch 1300 train loss: 0.0126 test loss: 0.0163\n",
      "Epoch 14 batch 1400 train loss: 0.0131 test loss: 0.0165\n",
      "Epoch 14 batch 1500 train loss: 0.0237 test loss: 0.0166\n",
      "Epoch 14 batch 1600 train loss: 0.0185 test loss: 0.0164\n",
      "Epoch 14 batch 1700 train loss: 0.0132 test loss: 0.0169\n",
      "Epoch 14 batch 1800 train loss: 0.0169 test loss: 0.0159\n",
      "Epoch 14 batch 1900 train loss: 0.0174 test loss: 0.0161\n",
      "Epoch 14 batch 2000 train loss: 0.0148 test loss: 0.0163\n",
      "Epoch 14 batch 2100 train loss: 0.0114 test loss: 0.0168\n",
      "Epoch 14 batch 2200 train loss: 0.0211 test loss: 0.0163\n",
      "Epoch 14 batch 2300 train loss: 0.0215 test loss: 0.0165\n",
      "Epoch 14 batch 2400 train loss: 0.0123 test loss: 0.0163\n",
      "Epoch 14 batch 2500 train loss: 0.0139 test loss: 0.0160\n",
      "Epoch 14 batch 2600 train loss: 0.0152 test loss: 0.0167\n",
      "Epoch 14 batch 2700 train loss: 0.0202 test loss: 0.0168\n",
      "Epoch 14 batch 2800 train loss: 0.0183 test loss: 0.0169\n",
      "Epoch 14 batch 2900 train loss: 0.0197 test loss: 0.0164\n",
      "Epoch 14 batch 3000 train loss: 0.0143 test loss: 0.0163\n",
      "Epoch 15 batch 0 train loss: 0.0145 test loss: 0.0159\n",
      "Epoch 15 batch 100 train loss: 0.0177 test loss: 0.0164\n",
      "Epoch 15 batch 200 train loss: 0.0192 test loss: 0.0168\n",
      "Epoch 15 batch 300 train loss: 0.0133 test loss: 0.0165\n",
      "Epoch 15 batch 400 train loss: 0.0228 test loss: 0.0166\n",
      "Epoch 15 batch 500 train loss: 0.0148 test loss: 0.0164\n",
      "Epoch 15 batch 600 train loss: 0.0127 test loss: 0.0164\n",
      "Epoch 15 batch 700 train loss: 0.0180 test loss: 0.0162\n",
      "Epoch 15 batch 800 train loss: 0.0146 test loss: 0.0164\n",
      "Epoch 15 batch 900 train loss: 0.0145 test loss: 0.0165\n",
      "Epoch 15 batch 1000 train loss: 0.0134 test loss: 0.0166\n",
      "Epoch 15 batch 1100 train loss: 0.0167 test loss: 0.0165\n",
      "Epoch 15 batch 1200 train loss: 0.0108 test loss: 0.0167\n",
      "Epoch 15 batch 1300 train loss: 0.0140 test loss: 0.0163\n",
      "Epoch 15 batch 1400 train loss: 0.0199 test loss: 0.0163\n",
      "Epoch 15 batch 1500 train loss: 0.0109 test loss: 0.0167\n",
      "Epoch 15 batch 1600 train loss: 0.0177 test loss: 0.0168\n",
      "Epoch 15 batch 1700 train loss: 0.0119 test loss: 0.0167\n",
      "Epoch 15 batch 1800 train loss: 0.0167 test loss: 0.0161\n",
      "Epoch 15 batch 1900 train loss: 0.0163 test loss: 0.0163\n",
      "Epoch 15 batch 2000 train loss: 0.0166 test loss: 0.0162\n",
      "Epoch 15 batch 2100 train loss: 0.0158 test loss: 0.0163\n",
      "Epoch 15 batch 2200 train loss: 0.0123 test loss: 0.0169\n",
      "Epoch 15 batch 2300 train loss: 0.0189 test loss: 0.0162\n",
      "Epoch 15 batch 2400 train loss: 0.0122 test loss: 0.0163\n",
      "Epoch 15 batch 2500 train loss: 0.0128 test loss: 0.0164\n",
      "Epoch 15 batch 2600 train loss: 0.0188 test loss: 0.0162\n",
      "Epoch 15 batch 2700 train loss: 0.0136 test loss: 0.0165\n",
      "Epoch 15 batch 2800 train loss: 0.0129 test loss: 0.0170\n",
      "Epoch 15 batch 2900 train loss: 0.0154 test loss: 0.0164\n",
      "Epoch 15 batch 3000 train loss: 0.0151 test loss: 0.0162\n",
      "Epoch 16 batch 0 train loss: 0.0178 test loss: 0.0161\n",
      "Epoch 16 batch 100 train loss: 0.0144 test loss: 0.0164\n",
      "Epoch 16 batch 200 train loss: 0.0126 test loss: 0.0165\n",
      "Epoch 16 batch 300 train loss: 0.0162 test loss: 0.0165\n",
      "Epoch 16 batch 400 train loss: 0.0252 test loss: 0.0163\n",
      "Epoch 16 batch 500 train loss: 0.0136 test loss: 0.0160\n",
      "Epoch 16 batch 600 train loss: 0.0187 test loss: 0.0166\n",
      "Epoch 16 batch 700 train loss: 0.0169 test loss: 0.0164\n",
      "Epoch 16 batch 800 train loss: 0.0168 test loss: 0.0170\n",
      "Epoch 16 batch 900 train loss: 0.0176 test loss: 0.0163\n",
      "Epoch 16 batch 1000 train loss: 0.0187 test loss: 0.0168\n",
      "Epoch 16 batch 1100 train loss: 0.0141 test loss: 0.0166\n",
      "Epoch 16 batch 1200 train loss: 0.0137 test loss: 0.0164\n",
      "Epoch 16 batch 1300 train loss: 0.0144 test loss: 0.0165\n",
      "Epoch 16 batch 1400 train loss: 0.0197 test loss: 0.0162\n",
      "Epoch 16 batch 1500 train loss: 0.0175 test loss: 0.0167\n",
      "Epoch 16 batch 1600 train loss: 0.0184 test loss: 0.0167\n",
      "Epoch 16 batch 1700 train loss: 0.0171 test loss: 0.0168\n",
      "Epoch 16 batch 1800 train loss: 0.0148 test loss: 0.0161\n",
      "Epoch 16 batch 1900 train loss: 0.0221 test loss: 0.0166\n",
      "Epoch 16 batch 2000 train loss: 0.0164 test loss: 0.0162\n",
      "Epoch 16 batch 2100 train loss: 0.0166 test loss: 0.0168\n",
      "Epoch 16 batch 2200 train loss: 0.0138 test loss: 0.0166\n",
      "Epoch 16 batch 2300 train loss: 0.0159 test loss: 0.0164\n",
      "Epoch 16 batch 2400 train loss: 0.0168 test loss: 0.0165\n",
      "Epoch 16 batch 2500 train loss: 0.0149 test loss: 0.0164\n",
      "Epoch 16 batch 2600 train loss: 0.0164 test loss: 0.0167\n",
      "Epoch 16 batch 2700 train loss: 0.0122 test loss: 0.0168\n",
      "Epoch 16 batch 2800 train loss: 0.0164 test loss: 0.0163\n",
      "Epoch 16 batch 2900 train loss: 0.0148 test loss: 0.0164\n",
      "Epoch 16 batch 3000 train loss: 0.0191 test loss: 0.0164\n",
      "Epoch 17 batch 0 train loss: 0.0158 test loss: 0.0161\n",
      "Epoch 17 batch 100 train loss: 0.0155 test loss: 0.0168\n",
      "Epoch 17 batch 200 train loss: 0.0174 test loss: 0.0166\n",
      "Epoch 17 batch 300 train loss: 0.0173 test loss: 0.0165\n",
      "early stop.\n",
      "Checkpoint 31 restored!!\n"
     ]
    }
   ],
   "source": [
    "for LOSS_RATE in LOSS_RATES:\n",
    "    \n",
    "    l = np.load('./data/tot_dataset_FFNN_loss_%.2f.npz' % LOSS_RATE)\n",
    "    raw_input = l['raw_input']\n",
    "    raw_label = l['raw_label']\n",
    "    test_input = l['test_input']\n",
    "    test_label = l['test_label']\n",
    "    MAXS = l['MAXS']\n",
    "    MINS = l['MINS']\n",
    "    MAXS = MAXS[:5]\n",
    "    MINS = MINS[:5]\n",
    "    \n",
    "    raw_input = raw_input.astype(np.float32)\n",
    "    raw_label = raw_label.astype(np.float32)\n",
    "    test_input = test_input.astype(np.float32)\n",
    "    test_label = test_label.astype(np.float32)\n",
    "\n",
    "    num_train = int(raw_input.shape[0]*.7)\n",
    "    raw_input, raw_label = shuffle(raw_input, raw_label, random_state=4574)\n",
    "    train_input, train_label = raw_input[:num_train, ...], raw_label[:num_train, ...]\n",
    "    val_input, val_label = raw_input[num_train:, ...], raw_label[num_train:, ...]\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_input, train_label))\n",
    "    train_dataset = train_dataset.cache().shuffle(BATCH_SIZE*50).batch(BATCH_SIZE)\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((val_input, val_label))\n",
    "    val_dataset = val_dataset.cache().shuffle(BATCH_SIZE*50).batch(BATCH_SIZE)\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((test_input, test_label))\n",
    "    test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "    ffnn_model = FFNN(test_label.shape[-1])\n",
    "    opt = tf.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "    print('Training for loss rate %.2f start.' % LOSS_RATE)\n",
    "    BEST_PATH = './checkpoints/FFNN_best_loss_%.2fp' % LOSS_RATE\n",
    "\n",
    "    @tf.function\n",
    "    def train(loss_function, model, opt, inp, tar):\n",
    "        with tf.GradientTape() as tape:\n",
    "            gradients = tape.gradient(loss_function(model, inp, tar), model.trainable_variables)\n",
    "            gradient_variables = zip(gradients, model.trainable_variables)\n",
    "            opt.apply_gradients(gradient_variables)\n",
    "\n",
    "    checkpoint_path = BEST_PATH\n",
    "\n",
    "    ckpt = tf.train.Checkpoint(ffnn_model=ffnn_model,\n",
    "                               opt=opt)\n",
    "\n",
    "    ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=10)\n",
    "\n",
    "    writer = tf.summary.create_file_writer('tmp')\n",
    "\n",
    "\n",
    "    prev_test_loss = 100.0\n",
    "    early_stop_buffer = 500\n",
    "    with writer.as_default():\n",
    "        with tf.summary.record_if(True):\n",
    "            for epoch in range(TRAINING_EPOCHS):\n",
    "                for step, (inp, tar) in enumerate(train_dataset):\n",
    "                    train(loss_function, ffnn_model, opt, inp, tar)\n",
    "                    loss_values = loss_function(ffnn_model, inp, tar)\n",
    "                    tf.summary.scalar('loss', loss_values, step=step)\n",
    "\n",
    "                    if step % DISP_STEPS == 0:\n",
    "                        test_loss = 0\n",
    "                        for step_, (inp_, tar_) in enumerate(test_dataset):\n",
    "                            test_loss += loss_function(ffnn_model, inp_, tar_)\n",
    "\n",
    "                            if step_ > DISP_STEPS:\n",
    "                                test_loss /= DISP_STEPS\n",
    "                                break\n",
    "                        if test_loss.numpy() < prev_test_loss:\n",
    "                            ckpt_save_path = ckpt_manager.save()\n",
    "                            prev_test_loss = test_loss.numpy()\n",
    "                            print('Saving checkpoint at {}'.format(ckpt_save_path))\n",
    "                        else:\n",
    "                            early_stop_buffer -= 1\n",
    "\n",
    "                        print('Epoch {} batch {} train loss: {:.4f} test loss: {:.4f}'\n",
    "                              .format(epoch, step, loss_values.numpy(), test_loss.numpy()))\n",
    "                    if early_stop_buffer <= 0:\n",
    "                        print('early stop.')\n",
    "                        break\n",
    "                if early_stop_buffer <= 0:\n",
    "                    break\n",
    "\n",
    "    i = -1\n",
    "    if ckpt_manager.checkpoints:\n",
    "        ckpt.restore(ckpt_manager.checkpoints[i])\n",
    "        print ('Checkpoint ' + ckpt_manager.checkpoints[i][-2:] +' restored!!')\n",
    "\n",
    "    ffnn_model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "                       loss = tf.keras.losses.MeanSquaredError())\n",
    "\n",
    "    pred_result = ffnn_model.predict(test_dataset)\n",
    "    masking = test_input[..., 5:10]\n",
    "    masked_pred = np.ma.array(pred_result, mask=masking)\n",
    "    masked_label = np.ma.array(test_label, mask=masking)\n",
    "\n",
    "    plot_label = ((MAXS[:5]-MINS[:5])*masked_label[..., :5] + MINS[:5])\n",
    "    plot_label.fill_value = np.nan\n",
    "    plot_pred = ((MAXS[:5]-MINS[:5])*masked_pred[..., :5] + MINS[:5])\n",
    "    plot_pred.fill_value = np.nan\n",
    "\n",
    "    f = open('./results/FFNN_%.2fp.npz' % LOSS_RATE, 'wb')\n",
    "    np.savez(f,\n",
    "             test_label = plot_label.filled(),\n",
    "             test_pred = plot_pred.filled()\n",
    "            )\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p36)",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
